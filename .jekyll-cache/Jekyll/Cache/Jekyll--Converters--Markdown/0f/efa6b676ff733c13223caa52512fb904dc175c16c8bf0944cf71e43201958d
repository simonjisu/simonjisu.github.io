I"ª1<h1 id="end-to-end-memory-network-ë…¼ë¬¸-ìš”ì•½-ë°-ì •ë¦¬">End-to-End Memory Network ë…¼ë¬¸ ìš”ì•½ ë° ì •ë¦¬</h1>

<ul>
  <li>Pytorch êµ¬í˜„ ì½”ë“œ: <a href="https://github.com/simonjisu/E2EMN">https://github.com/simonjisu/E2EMN</a></li>
</ul>

<p>memN2N ê²€ìƒ‰í•˜ë©´ ë‹¤ë¥¸ íŒ¨í‚¤ì§€ë¡œ êµ¬í˜„í•œ repo ë“¤ì´ ë§ìŒìœ¼ë¡œ í•œë²ˆ ì°¾ì•„ ë³¼ê²ƒ
facebook: <a href="https://github.com/facebook/MemNN">https://github.com/facebook/MemNN</a></p>

<hr />
<h2 id="a-single-layer">A. Single layer</h2>

<p><img src="/assets/E2EMN.png" alt="" /></p>

<h3 id="input">Input</h3>
<ol>
  <li>$T_c$ ê°œì˜ ë‹¨ì–´ê°€ í¬í•¨ëœ í•œ <strong>ë¬¸ì¥ sentence i</strong> ëŠ” $x_i = [x_{i1}, x_{i2}, \cdots, x_{iT_c}]$ ë¡œ í‘œí˜„ í•  ìˆ˜ ìˆìœ¼ë©°, í•˜ë‚˜ì˜ ë‹¨ì–´ ëŠ” BoW(Bag-of-Words)ë°©ì‹ìœ¼ë¡œ ì¸ì½”ë”© í•˜ì—¬ vectorë¡œ ë°”ê¿”ì¤€ë‹¤. (<strong>ë‹¨ì–´ ì‚¬ì „ì˜ index</strong>), ì´ë ‡ê²Œ êµ¬ì„±ëœ ì—¬ëŸ¬ ë¬¸ì¥ë“¤ì˜ ì§‘í•©ì„ Context(Sentences) ${x_i}$ë¼ê³  í•œë‹¤. ì‹¤ì œ êµ¬í˜„ ì‹œ, ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ì„œ batch ì—ì„œ ìµœëŒ€ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ $T_c$ ë¡œ ê¸°ì–µí•œ ë‹¤ìŒì— $T_c$ ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ì˜ ë’· ë¶€ë¶„ì€ $0$ ìœ¼ë¡œ íŒ¨ë”©(padding)í•´ì¤€ë‹¤.</li>
</ol>

<script type="math/tex; mode=display">Context = [x_1, x_2, \cdots ,x_n]\quad (n \times T_c)</script>

<ol>
  <li>$T_q$ ê°œì˜ ë‹¨ì–´ê°€ í¬í•¨ëœ ì§ˆë¬¸ question që„ ë§ˆì°¬ê°€ì§€ë¡œ BoWë°©ì‹ìœ¼ë¡œ ì¸ì½”ë”©í•´ì¤€ë‹¤.</li>
</ol>

<h3 id="input-memory">Input Memory</h3>
<ol>
  <li>í•˜ë‚˜ì˜ ë¬¸ì¥ $x_i$ ê°ê°ì˜ ë‹¨ì–´ì— Embedding matrix $A$ë¥¼ ê³±í•˜ì—¬ ê°ê°ì˜ ë‹¨ì–´ë¥¼ Embedding Vectorsë¡œ ë³€í™˜í•˜ê³  ì´ë¥¼ ëª¨ë‘ ë”í•˜ì—¬ ë©”ëª¨ë¦¬ ë²¡í„°(Memory Vector) $m_i$ë¥¼ êµ¬í•œë‹¤. ì´ë ‡ê²Œ ì´ $n$ê°œì˜ ë©”ëª¨ë¦¬ê°€ ë§Œë“¤ì–´ ì§„ë‹¤. ì¡°ê¸ˆ ë” ì„¤ëª… í•˜ìë©´, ì„ë² ë”© í›„ $Ax_{ij}$ ì˜ ì‚¬ì´ì¦ˆëŠ” $(T_c \times d)$ ê°€ ë˜ê³  ì´ë¥¼ $T_c$ ì°¨ì›ìœ¼ë¡œ summation í•˜ê²Œ ë˜ë©´ ë¬¸ì¥ í•˜ë‚˜ì— ëŒ€í•œ ë©”ëª¨ë¦¬ $m_i$ ì˜ ì‚¬ì´ì¦ˆëŠ” $(1\times d)$ ê°€ ëœë‹¤.</li>
</ol>

<script type="math/tex; mode=display">m_i = \sum_{j}^{n} Ax_{ij} = Ax_{i1} + Ax_{i2} + \cdots + Ax_{in} \quad (1 \times d)</script>

<ol>
  <li>Questionë„ ë§ˆì°¬ê°€ì§€ë¡œ Embedding matrix $B$ë¥¼ ê³±í•˜ì—¬ ê°ê°ì˜ ë‹¨ì–´ë¥¼ Embedding Vectorsë¡œ ë³€í™˜í•˜ê³  ì´ë¥¼ ëª¨ë‘ ë”í•˜ì—¬ Internal state $u$ë¥¼ êµ¬í•œë‹¤.</li>
</ol>

<script type="math/tex; mode=display">u = \sum_{j} Bx_{ij} = Bx_{i1} + Bx_{i2} + \cdots + Bx_{in} \quad (1 \times d)</script>

<ol>
  <li>ì´í›„ Contextì™€ Questionì˜ ìœ ì‚¬ì„±(match)ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ inner productë¥¼ ì‹œí–‰í•œ í›„, Softmax Functionìœ¼ë¡œ ì¶œë ¥í•´ì¤€ë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ë¡œ inputì— ëŒ€í•œ í™•ë¥ ì„ ë„ì¶œ í•´ë‚¼ ìˆ˜ ìˆë‹¤.</li>
</ol>

<script type="math/tex; mode=display">p_i = Softmax(u^Tm_i)</script>

<p>ì¦‰ $p_i$ê°€ ë†’ì„ ìˆ˜ë¡ ë†’ì€ ìœ ì‚¬ì„±ì„ ëˆë‹¤.</p>

<p>ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ì„œ <strong>Input Memory</strong> ì—ëŠ” Context ë¬¸ì¥ë“¤(${x_i}$)ê³¼ ì§ˆë¬¸($q$)ì˜ ì¶•ì•½ëœ ì •ë³´ê°€ ë“¤ì–´ê°€ê²Œ ëœë‹¤.</p>

<h3 id="output-memory">Output Memory</h3>
<ol>
  <li>ëª¨ë“  Context ë¬¸ì¥ë“¤ ${x_i}$ ì˜ ê°ê°ì˜ ë‹¨ì–´ì—  ë‹¤ì‹œ Embedding matrix Cë¥¼ ê³±í•˜ê³  ë”í•˜ì—¬ $c_i$ë¡œ ë³€í™˜í•œë‹¤.</li>
</ol>

<script type="math/tex; mode=display">c_i = \sum_{j} Cx_{ij} = Cx_{i1} + Cx_{i2} + \cdots + Cx_{in} \quad (1 \times d)</script>

<ol>
  <li>ì´ëŠ” ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” Response vector ì¸$o$ ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ ì¸ë°, $o$ëŠ” ì•„ë˜ì™€ ê°™ì´ Input Memoryì—ì„œ ë‚˜ì˜¤ëŠ” ìœ ì‚¬ì„±(match, $p_i$)ì™€ ê°€ì¤‘í‰ê· í•©ì„ ì§„í–‰í•œë‹¤.</li>
</ol>

<script type="math/tex; mode=display">o = \sum_{i} p_i \otimes c_i</script>

<h3 id="final-prediction">Final Prediction</h3>
<p>output $o$ì™€ ì§ˆë¬¸ìœ¼ë¡œë¶€í„° ì¶”ì¶œí•œ Internal state $u$ì— ê°€ì¤‘ì¹˜ê°’ $W$ë¥¼ ê³±í•˜ì—¬ ë”í•œë’¤ì— Softmax Functionì„ ì ìš©í•˜ì—¬ ë‹µ $\hat{a}$ì„ ì¶”ë¡ í•œë‹¤.</p>

<script type="math/tex; mode=display">\hat{a} = Softmax(W(o+u))</script>

<h3 id="weight-updating">Weight Updating</h3>
<p>Loss Functionì€ standard cross-entropy lossë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì¹˜ $\hat{a}$ ì™€ ì •ë‹µì¸ true ê°’ $a$ ê°„ì˜ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•´ì„œ í•™ìŠµ ì‹œí‚¨ë‹¤.</p>

<p>Inputì—ì„œ Outputê¹Œì§€ í•¨ìˆ˜ë“¤ì€ ë¬´í•œì •ë¯¸ë¶„ê°€ëŠ¥(function is smooth)í•˜ê¸° ë•Œë¬¸ì—, ì†ì‰½ê²Œ Gradientì™€ back-propagateì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.</p>

<p>ì—…ë°ì´íŠ¸ ë˜ëŠ” weight MatrixëŠ” $A$, $B$, $C$ ê·¸ë¦¬ê³  $W$ë‹¤.</p>

<p><br /></p>

<hr />
<h2 id="b-multiple-layers">B. Multiple layers</h2>

<p>ìœ„ì™€ ê°™ì€ Final Prediction ì „ ë‹¨ê³„ê¹Œì§€ë¥¼ 1 hopë¼ê³  ê·œì •í•˜ë©°, Multiple layers $K$ hopsê¹Œì§€ í™•ì¥ ì‹œí‚¨ë‹¤.</p>

<ul>
  <li>ì²«ë²ˆì§¸, kë²ˆì§¸ layerì—ì„œ ë‚˜ì˜¨ outputìœ¼ë¡œ ë‚˜ì˜¨ $o^k$ê³¼ input $u^k$ ëŠ” í•©ì³ì ¸ì„œ ìƒˆë¡œìš´ input $u^{k+1}$ ê°€ ë˜ì–´ì„œ k + 1 layerë¡œ ë“¤ì–´ê°€ê²Œ ëœë‹¤.</li>
</ul>

<script type="math/tex; mode=display">u^{k+1} = u^k + o^k</script>

<ul>
  <li>
    <p>ê° layerë§ˆë‹¤ inputì— embedë¡œ ì‚¬ìš©ëœ embedding matrices $A^k$ ì™€ $C^k$ ê°€ ì¡´ì¬í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë“¤ì€ ì‰½ê²Œ íŠ¸ë ˆì´ë‹í•˜ê³ , parameter ê°¯ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ ì œì•½ì´ ì¡´ì¬í•œë‹¤.</p>
  </li>
  <li>
    <p>Networkì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œë§Œ Wë¥¼ ê³±í•´ì„œ Softmax ë¡œ ì¶œë ¥í•œë‹¤.</p>
  </li>
</ul>

<script type="math/tex; mode=display">\hat{a} = Softmax(Wu^{K+1})</script>

<h3 id="ë‘-ê°€ì§€-ê°€ì¤‘ì¹˜-ë²„ì ¼">ë‘ ê°€ì§€ ê°€ì¤‘ì¹˜ ë²„ì ¼</h3>
<ol>
  <li>
    <p>Adjacent:
  $k_{th}$ output layer embedding matrixê°€ ë‹¤ìŒ input layerì˜ embedding matrixê°€ ëœë‹¤. ì˜ˆë¥¼ ë“¤ë©´, $A^{k+1} = C^k$. ë˜í•œ, ë‘ ê°€ì§€ ì œì•½ ì¡°ê±´ì„ ì¶”ê°€í–ˆëŠ”ë°, (a) answer prediction matrixê°€ ìµœì¢… output embeddingê³¼ ê°™ê³  ($W^T = C^K)$, (b) question embedding ê³¼ ì²«ë²ˆì§¸ layerì˜ input embeddingê³¼ ê°™ê²Œ í–ˆë‹¤($B = A^1$).</p>
  </li>
  <li>
    <p>Layer-wise (RNN - like):
  Input ê³¼ Output embeddingë“¤ì´ layerë§ˆë‹¤ ë‹¤ ê°™ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, $A^1 = A^2 = \cdots = A^K$ ê³¼ $C^1 = C^2 = \cdots = C^K$ ê°™ì€ ê²ƒë“¤. ë˜í•œ, hopsê°„ uë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸°ìœ„í•œ linear mapping $H$ ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ëœë‹¤ëŠ” ê²ƒì„ ì•Œì•„ëƒˆë‹¤. $u^{k+1} = Hu^k + o^k$.</p>
  </li>
</ol>

<p>ì¸µë³„ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¬¶ëŠ” ë‘ ë²ˆì§¸ ë°©ë²•ì€, tranditional í•œ RNN ë°©ì‹ìœ¼ë¡œ ìƒê°í•  ìˆ˜ê°€ ìˆë‹¤. Internal output($u$)ì„ ë‚´ë³´ë‚´ëŠ” ê²ƒì€ memoryì— í•´ë‹¹í•˜ê³ , external outputs($\hat{a}$)ëŠ” ë¼ë²¨ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. RNN ê´€ì ì—ì„œ ë³´ë©´, $u$, $u^{k+1}$ ì€ hidden stateê³ , ëª¨ë¸ì€ $A$ ë¥¼ ì‚¬ìš©í•˜ì—¬internal output $p$ ë¥¼ ìƒì„±í•œë‹¤. ëª¨ë¸ì€ $C$ ë¥¼ ì‚¬ìš©í•´ì„œ $p$ ì˜ ì •ë³´ë¥¼ í¡ìˆ˜í•˜ê³ , hidden stateë¥¼ ì—…ë°ì´íŠ¸ë©´ì„œ ì´ëŸ° ì‹ìœ¼ë¡œ ê³„ì† ì§„í–‰í•œë‹¤. ì—¬ê¸°ì„œ í‘œì¤€ RNNê³¼ ë‹¤ë¥´ê²Œ outputë“¤ì„ $K$ hops ë™ì•ˆ ê³„ì† ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ê³ , samplingí•˜ëŠ” ëŒ€ì‹ ì— softí•˜ê²Œ ë‘”ë‹¤. ê·¸ë ‡ê²Œ í•˜ì—¬ ë‹µë³€ì´ â€œì§„ì§œ ì„¸ìƒâ€ì— ë‚˜ì˜¤ê¸° ì „ì— ì—¬ëŸ¬ë²ˆ ê³„ì‚°ì„ ê±°ì¹˜ê²Œ ëœë‹¤.</p>

<p><br /></p>

<hr />
<h2 id="c-synthetic-question-and-answering-experiments">C. Synthetic Question and Answering Experiments</h2>

<ul>
  <li>ì˜ˆì‹œ1:
    <blockquote>
      <p>Sam walks into the kitchen.</p>

      <p>Sam picks up an apple.</p>

      <p>Sam walks into the bedroom.</p>

      <p>Sam drops the apple.</p>

      <p><span style="color: #7d7ee8">Q: Where is the apple?</span></p>

      <p><span style="color: #e87d7d">A. Bedroom</span></p>
    </blockquote>
  </li>
  <li>ì˜ˆì‹œ2:
    <blockquote>
      <p>Brian is a lion.</p>

      <p>Julius is white.</p>

      <p>Julius is a lion.</p>

      <p>Bernhard is green.</p>

      <p><span style="color: #7d7ee8">Q: What color is Brian?</span></p>

      <p><span style="color: #e87d7d">A. White</span></p>
    </blockquote>
  </li>
  <li>ì˜ˆì‹œ3:
    <blockquote>
      <p>Mary journeyed to the den.</p>

      <p>Mary went back to the kitchen.</p>

      <p>John journeyed to the bedroom.</p>

      <p>Mary discarded the milk.</p>

      <p><span style="color: #7d7ee8">Q: Where was the milk before the den?</span></p>

      <p><span style="color: #e87d7d">A. Hallway</span></p>
    </blockquote>
  </li>
</ul>

<p>ì˜ˆì‹œì—ë„ ë³´ë“¯ì´ ë¬¸ì¥ì˜ ì¼ë¶€ë§Œ ë‹µë³€ì˜ ì •ë‹µì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ë¥¼ support subsetì´ë¼ê³  í•˜ë©°, training í• ë•Œ support subsetì„ ëª…ì‹œí•œë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œ í…ŒìŠ¤íŠ¸í•  ë•ŒëŠ” ì´ support subsetì´ í‘œì‹œë˜ì§€ ì•ŠëŠ”ë‹¤.</p>

<h3 id="model-details">Model details</h3>
<p>$K=3$ hopsì´ê³  weight sharing(Layer-wise) ëª¨ë¸ì„ ì“¸ ê²ƒì´ë‹¤.  ëª¨ë“  output lists(ë‹µë³€ì— ì—¬ëŸ¬ ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš°)ì— ëŒ€í•˜ì—¬ ë‹¨ì–´ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤.</p>

<h4 id="sentence-representaion">Sentence Representaion:</h4>
<p>ë¬¸ì¥ë“¤ì„ í‘œí˜„í•  ë•Œ ë‘ ê°€ì§€ ë°©ë²•ì„ ì“°ê¸°ë¡œ í•œë‹¤. ì²«ë²ˆì§¸ë¡œëŠ” BoWê°€ í•˜ë‚˜ì˜ ë¬¸ì¥ì„ í‘œí˜„í•˜ëŠ” ê²ƒì¸ë°, ì´ ë°©ë²•ì€ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ì˜ ìˆœì„œ(the order of the words in sentence)ë¼ëŠ” íŠ¹ì§•ì„ ì¡ì„ ìˆ˜ê°€ ì—†ë‹¤. ë”°ë¼ì„œ ë‘ ë²ˆì§¸ ë°©ë²•ìœ¼ë¡œ, ë¬¸ì¥ì—ì„œ ë‹¨ì–´ì˜ ìˆœì„œ(the position of words)ë¥¼ ì¸ì½”ë”© í•œë‹¤. $m_i = \sum_{j} l_j \otimes Ax_{ij}$ ì—¬ê¸°ì„œ $l_j$ ì—°ì‚°ì€ element-wise multiplicationì´ë‹¤.</p>

<p>ë˜í•œ, $l_j$ ëŠ” $l_{kj} = (1-j/J) - (k/d)(1-2j/J)$, JëŠ” ë¬¸ì¥ì— ìˆëŠ” ë‹¨ì–´ ê°¯ìˆ˜ì¸ column vector êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ê²ƒì„ $PE$ (position encoding)ì´ë¼ê³  í•˜ë©°, ì´ëŠ” ë‹¨ì–´ì˜ ìˆœì„œê°€ ì–¼ë§Œí¼ ë¬¸ì¥$m_i$ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ ì•Œë ¤ì¤€ë‹¤. ë‚˜ë¨¸ì§€ question, memory inputs ê·¸ë¦¬ê³  memory outputsì—ì„œë„ ë‘ ë²ˆì§¸ ë°©ë²•ìœ¼ë¡œ ë¬¸ì¥ì„ í‘œí˜„í•  ê²ƒì´ë‹¤.</p>

<h4 id="temporal-encoding">Temporal Encoding:</h4>
<p>ë§ì€ QA tasksì—ì„œëŠ” temporal contextë¼ëŠ” ê°œë…ì´ í•„ìš”í•œë°, ì˜ˆë¥¼ ë“¤ì–´ ì²« ë²ˆì§¸ ì˜ˆì‹œì—ì„œ Samì´ kitchenì— ê°„ ë‹¤ìŒì— bedroomì— ë“¤ì–´ê°„ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ê²ƒì„ ëª¨ë¸ì— ì ìš©í•˜ë ¤ë©´, memory vectorë¥¼ ì•½ê°„ ë³€í˜•ì‹œí‚¨ë‹¤. $m_i = \sum_{j} Ax_{ij} + T_A(i)$, ì—¬ê¸°ì„œ $T_A(i)$ ëŠ” ì¼ì‹œì ì¸ ì •ë³´ë¥¼ ì €ì¥í•  íŠ¹ë³„í•œ í–‰ë ¬ $T_A$ ì˜ i ë²ˆì§¸ í–‰ì´ë‹¤. Ouput embeddingí•  ë•Œë„ ë§ˆì°¬ê°€ì§€ë¡œ í•´ì¤€ë‹¤. $c_i = \sum_{j} Cx_{ij} + T_C(i)$. $T_A$ ì™€ $T_C$ ë‘˜ë‹¤ training í•  ë•Œ ê°±ì‹ í•œë‹¤. ê·¸ë¦¬ê³  A ì™€ C ë‘ ë§ˆì°¬ê°€ì§€ë¡œ ì œì•½ ë˜í•œ ê°™ì´ ê³µìœ í•œë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ ë¬¸ì¥ë“¤ì´ ì—­ìˆœìœ¼ë¡œ ì¸ë±ì‹±ë˜ì–´ìˆë‹¤.  ë¬¸ì¥ì´ ì§ˆë¬¸ìœ¼ë¡œë¶€í„° ìƒëŒ€ì ì¸ ê±°ë¦¬ë¥¼ ë°˜ì˜í•œë‹¤, ì¦‰ $x_1$ ì€ ì´ì•¼ê¸°ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ëœë‹¤.</p>

<h4 id="learning-time-invariance-by-injecting-random-noise">Learning time invariance by injecting random noise:</h4>
<p>$T_A$ ë¥¼ ì •ê·œí™” ì‹œí‚¬ë•Œ ë”ë¯¸ ë³€ìˆ˜ë¥¼ ë„£ëŠ” ê²ƒì´ ë„ì›€ì´ ëœë‹¤. ì¦‰, íŠ¸ë ˆì´ë‹í•  ë•ŒëŠ” ëœë¤ìœ¼ë¡œ 10%ì˜ ë¹ˆ ë©”ëª¨ë¦¬ë¥¼ ìŠ¤í† ë¦¬ì— ë„£ëŠ” ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ì´ë¥¼ Random Noise (RN)ë¼ê³  í•œë‹¤</p>

<h3 id="training-details">Training Details</h3>
<p>bAbI training setì¤‘ 10%ëŠ” Validationìš©ìœ¼ë¡œ ì“´ë‹¤. ì´ëŠ” optimal model architecture ê³¼ hyperparametersë¥¼ ì„ íƒí•˜ê¸° ìœ„í•´ì„œë‹¤. Learning rate $\eta$ëŠ” 0.01ë¡œ ì„¤ì •í•˜ê³ , 100ë²ˆì§¸ epochê°€ ë ë•Œ ê¹Œì§€, ë§¤ 25ë²ˆì§¸ epochs ë§ˆë‹¤, $\eta$ ë¥¼ 2ë¡œ ë‚˜ëˆ ì¤€ë‹¤. Momentum ì´ë‚˜ weight decayëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì•˜ë‹¤. ê°€ì¤‘ì¹˜ë“¤ì€ $\mu = 0$, $\sigma = 0.1$ ì¸ ê°€ìš°ì‹œì•ˆ ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°ê°’ì„ ì„¤ì •í–ˆë‹¤. ëª¨ë“  trainingì— ì‚¬ìš©ëœ batch sizeëŠ” 32 ì´ë©°, gradientsëŠ” L2ë¡œ ì •ê·œí™”í•´ì„œ 40ì´ ë„˜ìœ¼ë©´ ì–´ë–¤ ìŠ¤ì¹¼ë¼ë¥¼ ë‚˜ëˆ ì„œ normì„ 40ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.</p>

<p>ì–´ë–¤ ëª¨ë¸ì—ì„œëŠ” ì²˜ìŒì‹œì‘ì— softmaxë¥¼ ì•ˆì“°ë‹¤ê°€ (linearí•˜ê²Œ ë§Œë“œëŠ” ê²ƒ) ë‚˜ì¤‘ì— ìµœì¢… ì˜ˆì¸¡ì‹œì— softmaxë¥¼ ì¼ë‹¤. ê·¸ëŸ¬ë‹¤ validation lossê°€ ë” ì´ìƒ ë–¨ì–´ì§€ì§€ ì•Šì„ ë•Œ, ë‹¤ì‹œ softmax ì¸µì´ ë‹¤ì‹œ ì…ë ¥ì´ ë˜ì„œ íŠ¸ë ˆì´ë‹ì„ í•œë‹¤. ì´ë¥¼ Linear Start (LS) trainingì´ë¼ê³  í•˜ë©°, ì´ë•Œ ì´ˆê¸° learning rate ë¥¼ $\eta = 0.005$ ë¡œ ì„¤ì •í•œë‹¤.</p>

<h3 id="baselines">Baselines</h3>
<ul>
  <li>MemNN: strongly supervised, softmaxëŒ€ì‹  max operationì‚¬ìš©</li>
  <li>MemNN-WSH: weakly supervised, íŠ¸ë ˆì´ë‹ì‹œ supporting sentence labelsë¥¼ ì•ˆì”€</li>
  <li>LSTM: weakly supervised</li>
</ul>

<h3 id="result">Result</h3>
<p>ëª¨ë¸ ì„ íƒì„ ë‹¤ì–‘í•˜ê²Œ í–ˆë‹¤.</p>

<p>1) BoW vs Position Encoding</p>

<p>2) 20 tasksë¥¼ ë…ë¦½ì ìœ¼ë¡œ íŠ¸ë ˆì´ë‹ê³µìœ ($d = 20$) vs joint íŠ¸ë ˆì´ë‹ ($d = 50$)</p>

<p>3) Linear Start Training(Softmaxì²˜ìŒì— ì—†ì—” ê²ƒ) vs Softmaxê°€ ì²˜ìŒë¶€í„° ìˆëŠ” ê²ƒ</p>

<p>4) hopsë¥¼ 1 ~ 3ê¹Œì§€ ì„¤ì •</p>

<p>ê²°ê³¼ëŠ” ë…¼ë¬¸ ì°¸ì¡°. í¼í¬ë¨¼ìŠ¤ëŠ” supervised modelsì´ ì œì¼ ì¢‹ê²Œ ë‚˜ì™”ìœ¼ë‚˜, MemN2N with position encoding + linear start + random noise, jointly trained ë„ ê·¼ì ‘í•˜ê²Œ ë‚˜ì˜´</p>

<h2><br /></h2>
<h2 id="dì°¸ê³ ë¬¸í—Œ">D.ì°¸ê³ ë¬¸í—Œ</h2>

<p><a href="https://arxiv.org/abs/1503.08895">End-To-End Memory Networks: Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus</a></p>
:ET