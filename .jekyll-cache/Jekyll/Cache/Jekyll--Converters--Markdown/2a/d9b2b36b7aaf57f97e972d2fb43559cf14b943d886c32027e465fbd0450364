I"ïO<h1 id="numpyë¡œ-ì§œë³´ëŠ”-neural-network-basic---5">Numpyë¡œ ì§œë³´ëŠ” Neural Network Basic - 5</h1>
<hr />
<h2 id="í•™ìŠµê´€ë ¨-ê¸°ìˆ -part-1">í•™ìŠµê´€ë ¨ ê¸°ìˆ  Part 1</h2>

<h3 id="optimizer">Optimizer</h3>

<p>ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ê°€ëŠ¥í•œ ë‚®ê²Œ ë§Œë“¤ì–´ ë§¤ê°œë³€ìˆ˜ ìµœì ê°’ì„ ì°¾ëŠ” ê³¼ì •ì„ <strong>ìµœì í™”</strong> ë¼ê³  í•œë‹¤. ì—¬ê¸°ì„œ ëª‡ê°€ì§€ ë°©ë²•ì„ í•œë²ˆ ì‚´í´ë³¸ë‹¤.</p>

<h4 id="sgdí™•ë¥ ì -ê²½ì‚¬-í•˜ê°•ë²•">SGD(í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)</h4>
<p>$W \leftarrow W - \eta \dfrac{\partial L}{\partial W}$</p>

<p>$\eta$ ëŠ” í•™ìŠµë¥ ë¡œ ì–¼ë§Œí¼ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í• ì§€ ì •í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë‹¤. ì¦‰ ìš°ë¦¬ê°€ ë¯¸ë¦¬ ì •í•´ì¤˜ì•¼í•˜ëŠ” ë³€ìˆ˜ë‹¤. ê·¸ëŸ¬ë‚˜ SGD ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” ì´ ë³€ìˆ˜ì— ë”°ë¼ì„œ í•™ìŠµë˜ëŠ” ëª¨ì–‘ì´ ë‹¤ë¥´ë‹¤.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class SGD(object):
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
</code></pre></div></div>
<p><strong>ì¥ì </strong>:</p>
<ul>
  <li>ì¼ë¶€ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ë¥¼ í•´ì„œ ì§„ë™ì´ ì‹¬í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì „ì²´ ë°ì´í„°ì˜ Gradientë¥¼ êµ¬í•˜ëŠ” ê²ƒë³´ë‹¤ ë¹ ë¥´ë‹¤</li>
</ul>

<p><strong>ë‹¨ì </strong>:</p>
<ul>
  <li>learning rateì— ë”°ë¼ì„œ global minì„ ì°¾ì§€ ëª»í•˜ê³  local minì— ê°‡í ê°€ëŠ¥ì„œ ì¡´ì¬</li>
  <li>Oscilation(ë°œì§„ í˜„ìƒ): í•´ì— ì ‘ê·¼ í•  ìˆ˜ë¡ ìˆ˜ë ´ ì†ë„($\dfrac{\partial L}{\partial W}$)ê°€ ëŠë ¤ì§, ë”°ë¼ì„œ í˜‘ê³¡ ê°™ì€ ëª¨ì–‘ì—ì„œ í—¤ë§¤ëŠ” ê²½ìš° ì¡´ì¬í•œë‹¤. ê·¸ë ‡ë‹¤ê³  lrì„ ë„ˆë¬´ ë†’íˆë©´ ë°œì‚° í•  ìˆ˜ë„ ìˆìŒ(lossê°’ì´ ì»¤ì§€ëŠ” í˜„ìƒ)</li>
</ul>

<p>ì•„ë˜ì™€ ê°™ì€ í•¨ìˆ˜ì˜ ìµœì ê°’ì„ ì°¾ì•„ë³´ì.</p>

<p>$f(x, y) = \dfrac{1}{20} x^2 + y^2$</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def f(x, y):
    return np.array((1/20)*(x**2) + (y**2))
</code></pre></div></div>
<p>$f$ ë¥¼ ë¯¸ë¶„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

<p>$\dfrac{\partial f}{\partial x}, \dfrac{\partial f}{\partial y} = \dfrac{x}{10}, 2y$</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def f_prime(x, y, grads=None):
    if grads is None:
        grads = {}

    grads['x'] = (1/10)*x
    grads['y'] = 2*y
    return grads
</code></pre></div></div>
<p>ì‹œì‘ì€ <strong>(-7, 2)</strong> ì ë¶€í„° ì‹œì‘í•œë‹¤ê³  í•˜ë©´ ì•„ë˜ì²˜ëŸ¼ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/ML/nn/fgraph.png" alt="Drawing" style="width: 400px;" /></p>

<p>ì´ í•¨ìˆ˜ì˜ ìµœì €ì ì€ (0, 0) ì ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>ì´ì œ learning rate ë¥¼ 0.1 ê³¼ 0.9ë¡œ ê°ê° ì •í•´ì„œ SGDë¥¼ ì ìš”í•´ë³´ì. ì´ 30 epochë™ì•ˆ Gradientë¥¼ êµ¬í•˜ê³  ì´ë¥¼ ì¡°ê¸ˆì”© ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì„ ì·¨í–ˆë‹¤.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Video</th>
      <th style="text-align: center">Graph</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><video controls="controls" style="width: 400px;" autoplay="" loop="" muted=""> <source type="video/mp4" src="/assets/ML/nn/SGD_0.1.mp4" /> &lt;/source&gt;</video></td>
      <td style="text-align: center"><img src="/assets/ML/nn/SGD_0.1.png" alt="Drawing" style="width: 400px;" /></td>
    </tr>
  </tbody>
</table>

<p>learning rate ê°€ 0.1 ì¼ë•Œ í•™ìŠµì´ ì¡°ê¸ˆì”© ì§„í–‰ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ epoch íšŸìˆ˜ê°€ ë„ˆë¬´ ì ì–´ ìµœì ì˜ ê°’ê¹Œì§€ ë„ë‹¬ì„ ëª»í–ˆë‹¤.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Video</th>
      <th style="text-align: center">Graph</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><video controls="controls" style="width: 400px;" autoplay="" loop="" muted=""> <source type="video/mp4" src="/assets/ML/nn/SGD_0.9.mp4" /> &lt;/source&gt;</video></td>
      <td style="text-align: center"><img src="/assets/ML/nn/SGD_0.9.png" alt="Drawing" style="width: 400px;" /></td>
    </tr>
  </tbody>
</table>

<p>learning rate ê°€ 0.9 ì¼ë•Œ í•™ìŠµì´ í¬ê²Œ ì§„í–‰ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ë³€ë™ì´ ì‹¬í•´ì„œ í¬ê²Œ í”ë“¤ë¦¬ë©´ì„œ ìµœì €ì ìœ¼ë¡œ ê°€ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>í•™ìŠµë¥ ì´ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì€ í•œ ë²ˆ ë‚˜ì•„ê°ˆë•Œ í­ì˜ ê¸¸ì´ë¥¼ ë³´ë©´ ê·¸ ì°¨ì´ë¥¼ ì•Œ ìˆ˜ ê°€ ìˆë‹¤.</p>

<h4 id="momentum">Momentum</h4>
<p>$v \leftarrow \gamma v - \eta \dfrac{\partial L}{\partial W}$</p>

<p>$W \leftarrow W + v$</p>

<p>ëª¨ë©˜í…€ ë°©ì‹ì€ gradient ë°©í–¥ì— ì¼ì¢…ì˜ ê´€ì„±ì„ ë”í•´ì¤˜ì„œ ê¸°ì¡´ì˜ ì´ë™ ë°©í–¥ì— í˜ë“¤ ì‹¤ì–´ì¤˜ ë” ì´ë™í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ì¤€ë‹¤. $v$ ì˜ ì´ˆê¸°ê°’ì€ 0ìœ¼ë¡œ ì„¤ì •í•˜ê³  ì§„í–‰í•œë‹¤. ë”°ë¼ì„œ ì²« stepì´ í›„ ê¸°ì¡´ì— ì´ë™í–ˆë˜ ë°©í–¥ì„ ì €ì¥í•´ë‘” $v$ ê°€ ì¶”ê°€ë¡œ ì €ì¥ ë˜ì–´ ë‹¤ìŒ stepì— ë”í•´ì ¸ ì¡°ê¸ˆ ë” ì›€ì§ì´ê²Œ ëœë‹¤.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Video</th>
      <th style="text-align: center">Graph</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><video controls="controls" style="width: 400px;" autoplay="" loop="" muted=""> <source type="video/mp4" src="/assets/ML/nn/Momentum.mp4" /> &lt;/source&gt;</video></td>
      <td style="text-align: center"><img src="/assets/ML/nn/Momentum.png" alt="Drawing" style="width: 400px;" /></td>
    </tr>
  </tbody>
</table>

<p>learning rateê°€ 0.1 ì¼ë•Œ SGDë³´ë‹¤ ë” ë§ì´ ê°€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Video</th>
      <th style="text-align: center">Graph</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><video controls="controls" style="width: 400px;" autoplay="" loop="" muted=""> <source type="video/mp4" src="/assets/ML/nn/Momentum_0.9.mp4" /> &lt;/source&gt;</video></td>
      <td style="text-align: center"><img src="/assets/ML/nn/Momentum_0.9.png" alt="Drawing" style="width: 400px;" /></td>
    </tr>
  </tbody>
</table>

<p>learning rateê°€ 0.9 ì¼ë•Œ ì£¼ë³€ì„ í—¤ë§¤ë©´ì„œ ê°€ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì˜ ì¡°ì •í•´ì•¼ í•™ìŠµì´ ë¹ ë¥´ê²Œ ì§„í–‰ ëœ ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<h4 id="adagrad">Adagrad</h4>
<p>$h \leftarrow h + \dfrac{\partial L}{\partial W} \odot \dfrac{\partial L}{\partial W}$</p>

<p>$W \leftarrow W - \eta \dfrac{1}{\sqrt{h +\epsilon}} \dfrac{\partial L}{\partial W}$</p>

<p>í•™ìŠµë¥ ($\eta$)ì— ëŒ€í•œ ê³ ë¯¼ì´ ë§ì´ì§€ì ì´ë¥¼ í•´ê²°í•´ë³´ê¸° ìœ„í•´ ë‚˜ì˜¨ ì•Œê³ ë¦¬ì¦˜ì´ AdaGrad ë‹¤.</p>

<p>í•™ìŠµë¥ ì„ ì²˜ìŒì— í¬ê²Œ í–ˆë‹¤ ë‚˜ì¤‘ì— ì°¨ì°¨ ì¤„ì—¬ê°€ëŠ” <strong>í•™ìŠµë¥  ê°ì†Œ(learning rate decay)</strong> ê¸°ìˆ ì´ ì´ ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹ì§•ì´ë‹¤. ê°ê°ì˜ ë§¤ê°œë³€ìˆ˜ì— ë§ì¶¤í˜• í•™ìŠµë¥  ê°’ì„ ë§ì¶° ì¤„ ìˆ˜ê°€ ìˆë‹¤.</p>

<p>$\odot$ ëŠ” ì—¬ê¸°ì„œ dot productê°€ ì•„ë‹Œ element-wise multiplicationë¥¼ ë§í•œë‹¤. ìˆ˜ì‹ì„ ë³´ë©´ gradientë¥¼ ì œê³±í•˜ì—¬ hì— ì €ì¥í•œë‹¤. ì—…ë°ì´íŠ¸ì‹œ ì—¬íƒœê¹Œì§€ ì €ì¥í•´ì˜¨ gradient ì œê³± ê°’ì„ ë¶„ëª¨ë¡œ ë‘ê²Œ ëœë‹¤. ë”°ë¼ì„œ ì‹œê°„ì´ ì§€ë‚  ìˆ˜ë¡ gradient ëˆ„ì  ê°’ì´ í° ê²ƒì€ learning rate ê°€ ë°˜ëŒ€ë¡œ ì‘ì•„ì§€ê²Œ ëœì„œ í•™ìŠµë¥ ì´ ì¡°ì • ëœë‹¤. ì´ë¥¼ ì ì‘ì ìœ¼ë¡œ(adaptive) í•™ìŠµë¥ ì„ ì¡°ì •í•œë‹¤ê³  í•œë‹¤.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Adagrad(object):
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        self.epsilon = 1e-6  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëˆˆ ê²ƒì„ ë°©ì§€

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + self.epsilon)
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Video</th>
      <th style="text-align: center">Graph</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><video controls="controls" style="width: 400px;" autoplay="" loop="" muted=""> <source type="video/mp4" src="/assets/ML/nn/Adagrad.mp4" /> &lt;/source&gt;</video></td>
      <td style="text-align: center"><img src="/assets/ML/nn/Adagrad.png" alt="Drawing" style="width: 400px;" /></td>
    </tr>
  </tbody>
</table>

<p>í•™ìŠµë¥ ì„ 1.5ë¡œ í¬ê²Œ ì£¼ì—ˆëŠ”ë°ë„ ì°¨ì°¨ ê°ì†Œí•˜ë©´ì„œ í•™ìŠµë˜ëŠ” ê³¼ì •ì„ ë³¼ ìˆ˜ ê°€ ìˆë‹¤.</p>

<p>ê·¸ëŸ¬ë‚˜ ì´ë ‡ê²Œ ì¢‹ì•„ë³´ì´ëŠ” ë°©ë²•ë„ <strong>ë‹¨ì </strong> ì´ ìˆë‹¤.</p>

<p>ê³¼ê±°ì˜ ê¸°ìš¸ê¸° ê°’ë“¤ì„ ì „ë¶€ ëˆ„ì í•´ì„œ ë”í•˜ê¸° ë•Œë¬¸ì— h ê°’ì´ ë§ì´ ì»¤ì§€ê²Œ ë˜ë©´ í•™ìŠµë¥  ë¶€ë¶„($\dfrac{1}{\sqrt{h +\epsilon}}$)ì´ 1ì— ê°€ê¹Œì›Œì ¸ ì—…ë°ì´íŠ¸ í•  ë•Œ ë°œì‚°í•˜ëŠ” í˜•íƒœë¡œ ê°€ê¸° ë•Œë¬¸ì— ë” ì´ìƒ í•™ìŠµì´ ì§„í–‰ì´ ì•ˆë˜ëŠ” ìƒí™©ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.</p>

<p>ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ì„œ RMSPropê³¼ Adadeltaë¼ëŠ” ë°©ë²•ì´ ìˆë‹¤. (ì½”ë“œëŠ” ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ ì›ë¦¬ë§Œ êµ¬í˜„í•´ë†¨ë‹¤. êµ¬ì²´ì ìœ¼ë¡œ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œ ì¡°ê¸ˆì”© ë³€í˜•ì´ ê°€í•´ì§„ë‹¤. ë…¼ë¬¸ ì°¸ì¡° í•  ê²ƒ, <del>ì•„ì§ ì´í•´ì¤‘</del>)</p>

<p>RMSProp:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class RMSProp(object):
    def __init__(self, lr=0.01, gamma=0.9):
    """GëŠ” ì´ë™í‰ê· ì˜ ê°œë…ìœ¼ë¡œ ê³¼ê±° 1ë³´ë‹¤ ì‘ì€ gammaê°’ì„ ê³±í•´ì„œ ì„œì„œíˆ ìŠê²Œ í•˜ê³  ìƒˆë¡œìš´ ê°’ì„ ì¡°ê¸ˆì”© ë” í•´ì¤€ë‹¤."""
        self.lr = lr
        self.gamma = gamma  # decay term
        self.G = None
        self.epsilon = 1e-6  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëˆˆ ê²ƒì„ ë°©ì§€

    def update(self, params, grads):
        if self.G is None:
            self.G = {}
            for key, val in params.items():
                self.G[key] = np.zeros_like(val)

        for key in params.keys():
            self.G[key] += self.gamma * self.G[key] + (1 - self.gamma) * (grads[key] * grads[key])
            params[key] -= self.lr * grads[key] / np.sqrt(self.G[key] + self.epsilon)
</code></pre></div></div>
<p>AdaDelta:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class AdaDelta(object):
    def __init__(self, gamma=0.9):
        """
        https://arxiv.org/pdf/1212.5701
        """
        self.gamma = gamma  # decay term
        self.G = None  # accumulated gradients
        self.s = None  # accumulated updates
        self.del_W = None
        self.epsilon = 1e-6  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëˆˆ ê²ƒì„ ë°©ì§€
        self.iter = 0

    def update(self, params, grads):
        if (self.G is None) | (self.s is None) | (self.del_W is None):
            # Initialize accumulation variables
            self.G = {}
            self.s = {}  
            self.del_W = {}
            for key, val in params.items():
                self.G[key] = np.zeros_like(val)
                self.s[key] = np.zeros_like(val)
                self.del_W[key] = np.zeros_like(val)

        for key in params.keys():
            self.G[key] += self.gamma * self.G[key] + (1 - self.gamma) * (grads[key] * grads[key])
            self.del_W[key] = -(np.sqrt(self.s[key] + self.epsilon) / np.sqrt(self.G[key] + self.epsilon)) * grads[key]
            self.s[key] += self.gamma * self.s[key] + (1 - self.gamma) * self.del_W[key]**2
            params[key] += self.del_W[key]
</code></pre></div></div>
<h4 id="adamadaptive-moment-estimation">Adam(Adaptive Moment Estimation)</h4>
<p><strong>Adam</strong> (Adaptive Moment Estimation)ì€ RMSPropê³¼ Momentum ë°©ì‹ì„ í•©ì¹œ ê²ƒ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.</p>

<p><img src="/assets/ML/nn/Algorithm_Adam.png" alt="Drawing" style="width: 800px;" /></p>

<p>ì¶œì²˜: <a href="https://arxiv.org/abs/1412.6980v8">https://arxiv.org/abs/1412.6980v8</a></p>

<ul>
  <li>$m_t$: the exponential moving averages of the gradient (Momentumìª½)</li>
  <li>$v_t$: the squared gradient (RMSPropìª½)</li>
  <li>$\beta_1$: the exponential decay rates for $m_t$, ë³´í†µ 0.9 ì·¨í•¨</li>
  <li>$\beta_2$: the exponential decay rates for $v_t$, ë³´í†µ 0.999 ì·¨í•¨</li>
</ul>

<p>ì•Œê³ ë¦¬ì¦˜ ê·¸ëŒ€ë¡œ ì§œëŠ”ê²Œ ì•„ë‹ˆë¼ ì¡°ê¸ˆë” íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì€ ë‚´ìš©ì„ ì´í•´í•˜ê³  ë³´ì •í•´ì¤˜ì•¼ í•œë‹¤â€¦(ìì„¸í•œ ê±´ ë…¼ë¬¸ì— ë” ìˆìŒ)</p>

<hr />

<h4 id="ì¶”ê°€-ì„¤ëª…180116-initialization-bias-correction">ì¶”ê°€ ì„¤ëª…:(18.01.16) Initialization Bias Correction</h4>
<p>ìš°ë¦¬ê°€ êµ¬í•œ $m_t$, $v_t$ ê°’ì´ ì´ˆê¸° ê°’ì´ 0ìœ¼ë¡œ ì„¤ì •í•˜ê³ , $\beta$ ë„ 1ì— ê°€ê¹ê¸° ë•Œë¬¸ì— ì²˜ìŒì— ì ìš©í•˜ëŠ” gradient($g_t$) ê°’ì´ ì ìš©ì´ ì˜ ì•ˆë˜ì„œ(ì¦‰, ì—…ë°ì´íŠ¸ê°€ ì•ˆëœë‹¤), ì´ˆê¸° epochì—ì„œëŠ” í•™ìŠµ ì§„í–‰ì´ ì•ˆë˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤.</p>

<p>ì´ëŠ” $m_t$, $v_t$ ê°’ì´ ì‹¤ì œë¡œ $g_t$, $g_t^2$ ê°€ ë§ëŠ”ì§€ í™•ì¸í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤. ë”°ë¼ì„œ ê°ê° ê¸°ëŒ€ê°’(Expectation)ì„ ì”Œì›Œì„œ</p>

<script type="math/tex; mode=display">\begin{cases}
E[m_t] = E[g_t] \\
E[v_t] = E[g_t^2]
\end{cases}</script>

<p>ê°€ ì„±ë¦½í•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ ëœë‹¤. $v_t$ë¥¼ ë³´ë©´,</p>

<p>$v_0 = 0$ (0 vector) ìœ¼ë¡œ ì´ˆê¸° ê°’ì„ ì£¼ì—ˆê¸° ë•Œë¬¸ì—, $t = 1 \cdots t$ ê¹Œì§€ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•´ì„œ ì“¸ ìˆ˜ê°€ ìˆë‹¤.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
v_0 &= 0 \\
v_1 &= \beta_2 v_0 + (1-\beta_2) g_1^2 = (1-\beta_2) g_1^2 \\
v_2 &= \beta_2 v_1 + (1-\beta_2) g_2^2 = \beta_2 (1-\beta_2) g_1^2 + (1-\beta_2) g_2^2 = (1-\beta_2)(\beta_2^1 g_1^2 + \beta_2^0 g_2^2)\\
\vdots \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 = (1-\beta_2)(\beta_2^{t-1} g_1^2 + \cdots + \beta_2^0 g_t^2) = (1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i}g_i^2 \cdots (1)
\end{aligned} %]]></script>

<p>(1) ì‹ì—ì„œ $g_i^2$ ë¥¼ $g_i^2 - g_t^2 + g_t^2$ ë¡œ ë°”ê¿” ì¤„ ìˆ˜ê°€ ìˆë‹¤. ê·¸í›„ ì–‘ë³€ì— Expectationì„ ì·¨í•˜ê²Œ ëœë‹¤.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
E[v_t] &= E[(1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i}(g_i^2 - g_t^2 + g_t^2))] \\
&= E[(1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i}g_t^2] + E[(1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i}(g_i^2 - g_t^2))] \\
&= E[g_t^2](1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i} + \zeta \\
&= E[g_t^2](1-\beta_2)(\beta_2^{t-1} + \cdots + \beta_2^{0}) + \zeta \\
&= E[g_t^2]\{(\beta_2^{t-1} + \cdots + \beta_2^{0}) - (\beta_2^{t} + \cdots + \beta_2^{1})\} + \zeta \\
&= E[g_t^2](1-\beta_2^t) + \zeta \cdots (2)
\end{aligned} %]]></script>

<p><br /></p>

<p>$E[g_t^2]$ê°€ stationary í• ë•Œ $\zeta = 0$ ì´ ë˜ê³ , ì•„ë‹ˆë”ë¼ë„ $\zeta$ ê°’ì€ ì´ë™í‰ê· ì˜ íŠ¹ì„±ìƒ ë”°ë¼ ë©€ë¦¬ ìˆëŠ” $\beta_2^{t-i}$ ê°’ì´ ì•„ì£¼ ì‘ì•„ 0ì— ê°€ê¹Œì›Œ ì§„ë‹¤. ë”°ë¼ì„œ (2) ì‹ë§Œ ë‚¨ê²Œ ë˜ëŠ”ë°, ìš°ë¦¬ê°€ ì›í•˜ëŠ” $E[g_t^2]$ ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” $E[g_t^2] = \dfrac{E[v_t]}{1-\beta_2^t}$ ë¥¼ í•´ì£¼ë©´ ì´ˆê¸°ê°’ 0ìœ¼ë¡œ ì„¤ì •í•˜ê²Œ ë˜ì–´ ìƒê¸´ biasë¥¼ ì¡°ì • í•  ìˆ˜ ìˆê²Œ ëœë‹¤.</p>

<p>ìˆ˜ì‹ì˜ ì´í•´ëŠ” ì•„ë˜ ë¸”ë¡œê·¸ì—ì„œ ë„ì›€ì„ ì¡°ê¸ˆ ë°›ì•˜ìŠµë‹ˆë‹¤.</p>

<p><a href="http://dalpo0814.tistory.com/29#comment5316278">http://dalpo0814.tistory.com</a></p>

<p>ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ ì½”ë“œ:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Adam(object):
    """Adam (http://arxiv.org/abs/1412.6980v8)"""

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.unbias_m = None
        self.v = None
        self.unbias_v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1

        for key in params.keys():
            self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)

            self.unbias_m = self.m[key] / (1 - self.beta1**self.iter) # correct bias
            self.unbias_v = self.v[key] / (1 - self.beta2**self.iter) # correct bias
            params[key] -= self.lr * self.unbias_m / (np.sqrt(self.unbias_v) + 1e-7)
</code></pre></div></div>

<p>ì•„ë˜ëŠ” ë‹¤ë¥¸ ì‚¬ëŒì˜ ì½”ë“œë¥¼ ë”°ì™€ì„œ ê°œì¡°í–ˆë‹¤. ì¶œì²˜: <a href="https://github.com/WegraLee/deep-learning-from-scratch/blob/master/common/optimizer.py">https://github.com/WegraLee/deep-learning-from-scratch/</a></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Adam(object):
    """Adam (http://arxiv.org/abs/1412.6980v8)"""

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1
        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)

        for key in params.keys():
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])

            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
</code></pre></div></div>

<p>ì¡°ê¸ˆ ë” íš¨ìœ¨ ì ìœ¼ë¡œ ê°œì„ ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. <strong>lr_t</strong> ëŠ” ìœ„ì— unbias í•­ë“¤ì„ ë„£ì–´ì„œ ì •ë¦¬í•´ì£¼ë©´ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ í•  ìˆ˜ ìˆë‹¤.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
&= \beta_1 m_{t-1} + m_{t-1} - m_{t-1} + (1-\beta_1) g_t\\
&= m_{t-1} - (1-\beta_1) m_{t-1} + (1-\beta_1) g_t\\
&= m_{t-1} + (1-\beta_1)(g_t-m_{t-1})\\
v_t &= v_{t-1} + (1-\beta_1)(g_t^2-v_{t-1}) \\
\alpha_t &= \alpha \dfrac{\sqrt{1-\beta_2^t}}{1-\beta_1} \\
\theta_t & \leftarrow \theta_{t-1} - \alpha_t \dfrac{m_t}{\sqrt{v_t} + \epsilon}
\end{aligned} %]]></script>

<h4 id="signal-to-noisesnr">Signal-to-Noise(SNR)</h4>
<p>ë³´í†µì˜ ê²½ìš° $\hat{v}_t$ (gradient ì œê³±ì˜ ì§€ìˆ˜ í‰ê· ) ì´ $\hat{m}_t$ (gradientì˜ ì§€ìˆ˜ í‰ê· ) ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì— $\dfrac{\hat{m}_t}{\sqrt{\hat{v}_t}} \leq 1$ ($\epsilon = 0$ ì´ë¼ ê°€ì •) ê°€ ë˜ì„œ learning rate ë³´ë‹¤ ì‘ì€ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë  ê²ƒì´ë¼ëŠ” ì ì´ë‹¤.</p>

<p>ì´ë¥¼ ë…¼ë¬¸ì—ì„œëŠ” $\dfrac{\hat{m}_t}{\sqrt{\hat{v}_t}}$ ë¥¼ <strong>signal-to-noise ratio(SNR)</strong> ë¼ê³  í•˜ë©°, SNR ê°’ì´ ì‘ì•„ì§ˆ ìˆ˜ë¡ step sizeë„ 0ì— ê·¼ì ‘í•˜ê²Œ ëœë‹¤. ì¦‰, learning rate ê°€ ì ì  ì‘ì•„ì ¸ ìë™ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ ëœë‹¤ëŠ” ì´ì•¼ê¸°ë‹¤. ì§€ê¸ˆê¹Œì§€ ê³ ë¯¼í•˜ë˜ ê³ ì • í•™ìŠµë¥ ì˜ ê³ ë¯¼ì„ í•´ê²°í•´ ì¤€ë‹¤.</p>

<ul>
  <li>step size : $\Delta_t = \theta_t - \theta_{t-1}$</li>
</ul>

<p>ê·¸ëŸ¬ë‚˜ ì¡°ê¸ˆ ì£¼ì˜í•  ì ì€ ë°ì´í„°ê°€ êµ‰ì¥íˆ sparseí•œ ë°ì´í„° ê²½ìš°, ëŒ€ë¶€ë¶„ì˜ $m_{t-1}$, $v_{t-1}$ ì˜ ê°’ì€ 0ì´ ë  ê²ƒì´ê³ , epoch($t$) ê°€ ì»¤ì§ˆìˆ˜ë¡ $\hat{m}_t$, $\hat{v}_t$ ëŠ” ê·¸ ì‹œì ì—ì„œì˜ gradient ë¡œ êµ¬ì„±ë˜ì–´ ìˆê²Œ ëœë‹¤. ë”°ë¼ì„œ ì—…ë°ì´íŠ¸ ì‹ì€ ì•„ë˜ì™€ ê°™ê²Œ ëœë‹¤.</p>

<script type="math/tex; mode=display">\theta_t \leftarrow \theta_{t-1} - \alpha \dfrac{1-\beta_1}{\sqrt{1-\beta_2}}</script>

<p>ì´ëŸ° ìƒí™©ì—ì„œëŠ” $\dfrac{1-\beta_1}{\sqrt{1-\beta_2}}$ ê°’ì´ 1 ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì—($beta_1 = 0.9, \beta_2 = 0.999$, ê³„ì‚°í•˜ë©´ ì•½ 3.16) ë°œì‚°í•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§„ë‹¤. ì´ëŸ° ìƒí™©ì€ ê±°ì˜ ë“œë¬¼ë‹¤ê³  í•œë‹¤.</p>

<hr />

<table>
  <thead>
    <tr>
      <th style="text-align: center">Video</th>
      <th style="text-align: center">Graph</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><video controls="controls" style="width: 400px;" autoplay="" loop="" muted=""> <source type="video/mp4" src="/assets/ML/nn/Adam.mp4" /> &lt;/source&gt; </video></td>
      <td style="text-align: center"><img src="/assets/ML/nn/Adam.png" alt="Drawing" style="width: 400px;" /></td>
    </tr>
  </tbody>
</table>

<p>ë…¼ë¬¸ ê²°ë¡  ë¶€ì—ëŠ” Adam ì•Œê³ ë¦¬ì¦˜ì´ í° ë°ì´í„° ì…‹ì´ë‚˜ ê³ ì°¨ì› íŒŒë¼ë¯¸í„° ê³µê°„ì„ í•™ìŠµí•˜ëŠ”ë° íš¨ìœ¨ì ì´ë‹¤ë¼ê³  ì´ì•¼ê¸° í•˜ê³  ìˆë‹¤.</p>

<p>ë‹¤ìŒ ì‹œê°„ì—ëŠ” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì™€ ë°°ì¹˜ ë…¸ë§ë¼ì´ì œì´ì…˜ì— ëŒ€ì—ì„œ ì´ì•¼ê¸° í•´ë³´ë„ë¡ í•˜ê² ë‹¤.</p>
:ET