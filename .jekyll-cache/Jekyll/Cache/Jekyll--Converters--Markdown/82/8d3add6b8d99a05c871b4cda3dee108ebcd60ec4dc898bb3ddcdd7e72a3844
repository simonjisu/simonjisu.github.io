I"Î-<h1 id="numpyë¡œ-ì§œë³´ëŠ”-neural-network-basic---3">Numpyë¡œ ì§œë³´ëŠ” Neural Network Basic - 3</h1>
<hr />

<p>ì €ë²ˆ ì‹œê°„ì—ëŠ” Feedforward ê³¼ì •ì„ ë³´ì•˜ëŠ”ë°, ì •í™•ë„ê°€ 8.578% ë°–ì— ì•ˆëë‹¤. ì´ì œ Neural Networkê°€ ë°ì´í„°ë¡œë¶€í„° ì–´ë–»ê²Œ í•™ìŠµí•˜ì—¬ ì •í™•ë„ë¥¼ ì˜¬ë¦¬ëŠ”ì§€ ë³´ì.</p>

<h2 id="ì†ì‹¤-í•¨ìˆ˜loss-function">ì†ì‹¤ í•¨ìˆ˜(Loss Function)</h2>
<p>ì™œ ìš°ë¦¬ì˜ ëª©í‘œì¸ ì •í™•ë„ë¥¼ ì•ˆì“°ê³  ì†ì‹¤ í•¨ìˆ˜ë¼ëŠ” ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ” ê±¸ê¹Œ?</p>

<p>ê·¸ ì´ìœ ëŠ” ë¨¼ì € ë°íˆë©´ ì‹ ê²½ë§ í•™ìŠµì— ë¯¸ë¶„ì´ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ìµœì ì˜ ê°€ì¤‘ì¹˜(ê·¸ë¦¬ê³  í¸í–¥)ì„ íƒìƒ‰í•  ë•Œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ëŠ¥í•œ ì‘ê²Œí•˜ëŠ” ê°€ì¤‘ì¹˜ ê°’ì„ ì°¾ëŠ”ë°, ì´ë•Œ ê°€ì¤‘ì¹˜ì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ê³ , ê·¸ ë¯¸ë¶„ ê°’ì„ ë‹¨ì„œë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì„œì„œíˆ ê°±ì‹ í•˜ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤. ê·¸ëŸ¬ë‚˜ ì†ì‹¤í•¨ìˆ˜ì— ì •í™•ë„ë¥¼ ì“°ë©´ ê°€ì¤‘ì¹˜ì˜ ë¯¸ë¶„ì´ ëŒ€ë¶€ë¶„ì˜ ì¥ì†Œì—ì„œ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ê°€ì¤‘ì¹˜ ê°’ì„ ê°±ì‹ í•  ìˆ˜ê°€ ì—†ë‹¤.</p>

<p>mnist ë°ì´í„°ì˜ ê²½ìš° ìµœì¢… ì¶œë ¥ì¸µì— ë‚˜ì˜¨ $y$ ê°’ì€ Softmaxì— ì˜í•´ $(10 \times 1)$ í–‰ë ¬ì˜ í™•ë¥ ë¡œ ì¶œë ¥ë˜ê³ , ê·¸ì— ì‘ë‹µí•˜ëŠ” ì •ë‹µ $t$ ëŠ” one-hot encodedëœ í–‰ë ¬ì´ë‹¤.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y = np.array([0.05, 0.01, 0.7, 0.14, 0.05, 0.0, 0.05, 0.0, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
</code></pre></div></div>

<h3 id="í‰ê· -ì œê³±-ì˜¤ì°¨mse">í‰ê·  ì œê³± ì˜¤ì°¨(MSE)</h3>

<p>\(E=\frac{1}{2}\sum_{k}{(y_k - t_k)^2}\)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def mean_squared_error(y, t):
    return (1/2) * np.sum((y - t) ** 2)

mean_squared_error(y, t)
</code></pre></div></div>
<blockquote>
  <p>0.05860000000000002</p>
</blockquote>

<h3 id="êµì°¨-ì—”íŠ¸ë¡œí”¼-ì˜¤ì°¨cross-entropy">êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨(Cross Entropy)</h3>

<p>\(E=-\sum_{k}{t_k\log{y_k}}\)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

cross_entropy_error(y, t)
</code></pre></div></div>
<blockquote>
  <p>0.51082545709933802</p>
</blockquote>

<p>ì—¬ê¸°ì„œ deltaë¼ëŠ” ì‘ì€ ê°’ì„ ë”í•´ì¤€ ì´ìœ ëŠ” yê°’ì´ 0ì´ë©´ $\log 0= -\inf$ê°€ ë˜ì„œ ë¯¸ë¶„ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h3 id="ë¯¸ë‹ˆ-ë°°ì¹˜-í•™ìŠµ">ë¯¸ë‹ˆ ë°°ì¹˜ í•™ìŠµ</h3>

\[E=-\frac{1}{N}\sum_{n}{\sum_{k}{t_k\log{y_k}}}\]

<p>ì—„ì²­ë‚˜ê²Œ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ì˜¤ì°¨ë¥¼ í•œë²ˆì— ê³„ì‚°í•˜ë ¤ë©´ ì˜¤ëœ ì‹œê°„ì´ ë“ ë‹¤. ë”°ë¼ì„œ ì‘ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ì¡°ê¸ˆì”© ì˜¤ì°¨ì˜ í•©ì„ êµ¬í•œë‹¤ìŒì— ê·¸ê²ƒì˜ í‰ê· ì„ ë‚´ë©´ ì „ì²´ì˜ ê·¼ì‚¬ì¹˜ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def cross_entropy_error(y, t):
    delta = 1e-7
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(t * np.log(y[np.arange(batch_size), t] + delta)) / batch_size
</code></pre></div></div>
<h2 id="ë¯¸ë¶„">ë¯¸ë¶„</h2>

<p>ëª©ì ì„ ì •í–ˆìœ¼ë‹ˆ ì´ì œ í•™ìŠµì— ë“¤ì–´ê°€ë©´ëœë‹¤. ì†ì‹¤í•¨ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ë¯¸ë¶„ì„ êµ¬í•´ì•¼ í•œë‹¤.</p>

<h3 id="ìˆ˜ì¹˜-ë¯¸ë¶„ê³¼-ì¤‘ì‹¬-ì°¨ë¶„ë²•">ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ì¤‘ì‹¬ ì°¨ë¶„ë²•</h3>

<p>ìˆ˜ì¹˜ ë¯¸ë¶„ì´ë€ ë³€í™”ìœ¨ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. $x$ì—ì„œ $h$ë§Œí¼ ë³€í–ˆì„ ë•Œ $f(x)$ì˜ ë³€í™”ëŸ‰ì„ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.</p>

\[\frac{df(x)}{dx} = \lim_{h\rightarrow0}{\frac{f(x+h) - f(x)}{h}}\]

<p>ê·¸ëŸ¬ë‚˜ $f(x+h) - f(x)$ ëŠ” êµ‰ì¥íˆ ì‘ì€ ìˆ˜ë¼ ì»´í“¨í„°ë¡œ êµ¬í˜„ì‹œ Underflowë¬¸ì œì— ë´‰ì°©í•˜ê²Œ ëœë‹¤.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.float32(1e-50)
</code></pre></div></div>
<blockquote>
  <p>0.0</p>
</blockquote>

<p>ë”°ë¼ì„œ ìˆ˜ì¹˜ ë¯¸ë¶„ì—ì„œ $h$ëŠ” ë˜ë„ë¡ ë„ˆë¬´ ì‘ì€ ê°’ì€ ëª»ì“´ë‹¤.</p>

<p><strong>ì¤‘ì‹¬ ì°¨ë¶„ë²•</strong> ì„ ì´ìš©í•˜ë©´ ë¯¸ë¶„ì€ ì•„ë˜ì™€ ê°™ë‹¤.</p>

<p>\(\frac{df(x)}{dx} = \lim_{h\rightarrow0}{\frac{f(x+h) - f(x-h)}{2h}}\)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def numerical_diff(f, x):
    h = 1e-4
    return (f(x + h) - f(x - h)) / (2*h)
</code></pre></div></div>
<p>ì˜ˆì‹œ í•¨ìˆ˜ $y = 0.01 x^2 + 0.1 x$ ì˜ ìˆ˜ì¹˜ ë¯¸ë¶„ì„ ë³´ì</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def f1(x):
    return 0.01 * x**2 + 0.1 * x

print(numerical_diff(f1, 5))
print(numerical_diff(f1, 10))
</code></pre></div></div>
<blockquote>
  <p>0.1999999999990898</p>

  <p>0.2999999999986347</p>
</blockquote>

<p>ì •í™•í•˜ê²Œ 0.2ì™€ 0.3ì´ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ì´ìœ ëŠ” ì´ì§„ìˆ˜ ë¶€ë™ì†Œìˆ˜ì  ë°©ì‹[<a href="https://ko.wikipedia.org/wiki/%EB%B6%80%EB%8F%99%EC%86%8C%EC%88%98%EC%A0%90">ë§í¬</a>]ì˜ ì •í™•ë„ ë¬¸ì œë‹ˆê¹Œ round í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì‚¬ìš©í•´ì•¼í•œë‹¤.</p>

<p><img src="/assets/ML/nn/numerical_diff.png" alt="Drawing" style="width: 500px;" /></p>

<p>2ì°¨ì› ì´ìƒì˜ ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ì§œì•¼í• ê¹Œ? ì•„ë˜ì˜ ì½”ë“œë¥¼ ì°¸ì¡°í•˜ì</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)

    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)  # f(x+h)

        x[idx] = tmp_val - h
        fxh2 = f(x)  # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2 * h)

        x[idx] = tmp_val  # ê°’ ë³µì›
        it.iternext()

    return grad
</code></pre></div></div>
<ul>
  <li>np.nditer: iterator ê°ì²´ë¥¼ ë§Œë“¤ì–´ ì¤€ë‹¤. í–‰ë§ˆë‹¤ ì›ì†Œê°€ iterate ëœë‹¤.</li>
</ul>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>ê²½ì‚¬ í•˜ê°•ë²•ì´ë€ í˜„ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ì¼ì • ê±°ë¦¬ë¥¼ ì´ë™í•˜ê³ , ë˜ ê·¸ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•´ì„œ ê·¸ ë°©í–¥ìœ¼ë¡œ ê³„ì† ë‚˜ì•„ê°€ëŠ” ë°©ë²•ì´ë‹¤. ì´ë ‡ê²Œ í•´ì„œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì ì  ì‘ê²Œ ë§Œë“¤ì–´ ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì €ì ìœ¼ë¡œ ì´ëŒê³  ê°„ë‹¤(ê°€ëŠ¥í•˜ë‹¤ë©´).</p>

\[w_{new} = w_{old} - \eta \cdot \frac{\partial f}{\partial w_{old}}\]

<p>$\eta$ ëŠ” í•™ìŠµë¥ (learning rate)ë¼ê³  í•˜ë©° ê°±ì‹ í•˜ëŠ” ì–‘ì„ ë‚˜íƒ€ë‚¸ë‹¤.</p>

<p>ì•„ë˜ ê·¸ë¦¼ì€ ì£¼ë³€ì´ ë†’ê³  ì¤‘ì•™ì´ ë‚®ì€ ëª¨ì–‘(ê·¸ë¦‡ì„ ìƒê°í•˜ì)ì„ 3ì°¨ì›ì—ì„œ 2ì°¨ì›ìœ¼ë¡œ ê·¸ë ¸ì„ ë•Œ, $(4, 5)$ ì ì—ì„œ ì‹œì‘í•´ì„œ ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ìµœì €ì ì„ ì°¾ëŠ” ê³¼ì •ì´ë‹¤. í•¨ìˆ˜ëŠ” $f(x) = x^2\ , x\in \mathbb{R}^3$ ë‹¤.</p>

<p><img src="/assets/ML/nn/GDanimation.gif" alt="Drawing" style="width: 500px;" /></p>

<h2 id="í•™ìŠµ-ì•Œê³ ë¦¬ì¦˜">í•™ìŠµ ì•Œê³ ë¦¬ì¦˜</h2>

<h3 id="ê°„ë‹¨í•œ-nn-ìœ¼ë¡œ-ê°€ì¤‘ì¹˜ì˜-ë¯¸ë¶„-êµ¬í•´ë³´ê¸°">ê°„ë‹¨í•œ NN ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì˜ ë¯¸ë¶„ êµ¬í•´ë³´ê¸°</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class simpleNet(object):
    def __init__(self):
        # Input size = 2
        # Output size = 3
        self.W = np.random.normal(size=(2,3))

    def predict(self, x):
        a = np.dot(x, self.W)
        y = softmax(a)

        return y

    def loss(self, x, t):
        y = self.predict(x)
        loss = cross_entropy_error(y, t)

        return loss

x = np.array([0.6, 0.9])
t = np.array([0, 0, 1])
nn = simpleNet()

f = lambda w: nn.loss(x, t)
dW = numerical_gradient(f, nn.W)
print(dW)
</code></pre></div></div>
<blockquote>
  <p>[[ 0.05244267  0.24743359 -0.29987626]</p>

  <p>[ 0.07866401  0.37115039 -0.44981439]]</p>
</blockquote>

<h3 id="í™•ë¥ ì -ê²½ì‚¬-í•˜ê°•ë²•sgd">í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)</h3>
<p>ì•„ë˜ ë°©ë²•ì€ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì™€ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent)ì´ë¼ê³ ë„ í•œë‹¤.</p>

<ul>
  <li>
    <p>1ë‹¨ê³„: ë¯¸ë‹ˆë°°ì¹˜</p>

    <p>í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆ ë°°ì¹˜ë¼ê³  í•˜ë©°, ë¯¸ë‹ˆ ë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œë‹¤.</p>
  </li>
  <li>
    <p>2ë‹¨ê³„: ê¸°ìš¸ê¸° ì‚°ì¶œ</p>

    <p>ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ì¥ ì‘ê²Œ ë§Œë“ ë‹¤.</p>
  </li>
  <li>
    <p>3ë‹¨ê³„: ë§¤ê°œë³€ìˆ˜(ê°€ì¤‘ì¹˜) ê°±ì‹ </p>

    <p>ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì•„ì£¼ ì¡°ê¸ˆ ê°±ì‹ í•œë‹¤.</p>
  </li>
  <li>
    <p>4ë‹¨ê³„: ë°˜ë³µ</p>

    <p>1~3 ë‹¨ê³„ë¥¼ ë°˜ë³µí•œë‹¤.</p>
  </li>
</ul>

<h3 id="2ì¸µ-neural-network-ì‹¤ìŠµ">2ì¸µ Neural Network ì‹¤ìŠµ</h3>

<p><img src="/assets/ML/nn/NN_3.png" alt="Drawing" style="width: 350px;" /></p>

<ul>
  <li>Input Size: m = 3</li>
  <li>Hidden Size: h = 4</li>
  <li>Output Size: o = 3
\(X_{(batch,\ m)} \cdot W1_{(m,\ h)} + B1_{(batch,\ h)} \rightarrow A1_{(batch,\ h)}\)
\(sigmoid(A1_{(batch,\ h)}) \rightarrow Z1_{(batch,\ h)}\)
\(Z1_{(batch,\ h)} \cdot W1_{(h,\ o)} + B1_{(batch,\ o)} \rightarrow A2_{(batch,\ o)}\)
\(\sigma(A2_{(batch,\ o)}) \rightarrow Y_{(batch,\ o)}\)</li>
</ul>

<p>ì´ê²ƒì„ êµ¬í˜„í•´ë³´ì. ìˆ˜ì¹˜ë¡œ êµ¬í˜„í•œ 2ì¸µ Neural Network ì½”ë“œëŠ” [<a href="https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch04/two_layer_net.py">ì—¬ê¸°</a>]ì„œ ê°€ì ¸ì™”ë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []
train_acc_list = []
test_acc_list = []

#highper parameter
epoch_num = 1
train_size = x_train.shape[0]
batch_size = 100
alpha = 0.1  # learning rate
epsilon = 1e-6

# 1ì—í­ë‹¹ ë°˜ë³µ ìˆ˜
iter_per_epoch = max(train_size / batch_size, 1)
nn = TwoLayer(input_size=784, hidden_size=100, output_size=10)

start = time.time()
for epoch in range(epoch_num):
    # get mini batch:
    batch_mask = np.random.choice(train_size, batch_size) # shuffle íš¨ê³¼
    x_batch = x_train[batch_mask]
    y_batch = y_train[batch_mask]

    # gradient ê³„ì‚°
    grad = nn.num_gradient(x_batch, y_batch)

    # update
    for key in ['W1', 'b1', 'W2', 'b2']:
        nn.params[key] = nn.params[key] - alpha * grad[key]

    # record
    loss = nn.loss(x_batch, y_batch)
    train_loss_list.append(loss)

    # 1ì—í­ë‹¹ ì •í™•ë„ ê³„ì‚°
    if epoch % iter_per_epoch == 0:
        train_acc = nn.accuracy(x_train, y_train)
        test_acc = nn.accuracy(x_test, y_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print('trian acc: {0:.5f} | test acc: {1:.5f}'.format(train_acc, test_acc))

    # stop point
    if epoch &gt; 10:
        stop_point = np.sum(np.diff(np.array(train_loss_list[i-11:])) &lt; epsilon)
        if stop_point == 10:
            print(epoch)
            break

end = time.time()
print('total time:', (end - start))
</code></pre></div></div>
<blockquote>
  <table>
    <tbody>
      <tr>
        <td>trian acc: 0.10442</td>
        <td>test acc: 0.10280</td>
      </tr>
    </tbody>
  </table>

  <p>total time: 175.5657160282135</p>
</blockquote>

<p>2ë‹¨ê³„ì—ì„œ ìˆ˜ì¹˜ë¯¸ë¶„ì„ êµ¬í˜„í•˜ê¸°ëŠ” ì‰¬ìš°ë‚˜ ì—…ë°ì´íŠ¸ í•˜ëŠ”ë° ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦°ë‹¤. 1 Epochë§Œ ëŒë ¸ëŠ”ë° 175ì´ˆ ê±¸ë ¸ë‹¤.</p>

<p>ë”°ë¼ì„œ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” <strong>ì˜¤ì°¨ì—­ì „íŒŒ</strong> ë°©ë²•ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•´ì•¼í•œë‹¤. ì´ê±´ ë‹¤ìŒ ê¸€ì—ì„œ ê³„ì† ì§„í–‰í•˜ê² ë‹¤.</p>
:ET