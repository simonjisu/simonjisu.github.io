I"<h1 id="numpy로-짜보는-neural-network-basic---9">Numpy로 짜보는 Neural Network Basic - 9</h1>

<h1 id="rnn-과-lstm---1">RNN 과 LSTM - 1</h1>

<h2 id="rnnrecurrent-neural-network">RNN(Recurrent Neural Network)</h2>
<p>우리가 사는 세상에 연속된 일들, 혹은 시간과 연관된 일은 매우매우 많을 것이다. 예를 들자면, 지금 이 글을 읽은 당신도 앞에 있는 내용을 기억하면서 글을 읽고 있을 것이다. 일반적인 신경망 구조에서는 이 ‘기억’ 이라는 시스템이 존재 하지 않는다. 하지만 RNN은 다르다. 이놈은 ‘기억’을 할 수가 있다. 그렇다면 RNN과 기존 신경망과 어떻게 다른지를 한번 살펴보자.</p>

<h2 id="rnn-구조">RNN 구조</h2>
<p><img src="/assets/ML/rnn/rnn.png" alt="Drawing" style="width=500px" /></p>

<p>RNN은 중간의 Hidden 층이 순환한다고해서 순환 신경망이라고 한다. 왼쪽의 구조를 펼쳐서 보면, 중간의 Hidden 노드가 어떤 방향으로 계속 이어진 다는 것을 알 수 있다. 이러한 쇠사슬 같은 성격은 RNN으로 하여금 연속된 이벤트와 리스트에 적합한 구조로 만들어 준다.</p>

<p>이렇게 보면 엄청 어렵게 느껴질 수 있다. 그렇다면 예시를 들어서 RNN이 어떻게 돌아가는지 수학적으로 살펴보자.</p>

<h3 id="기본-신경망-구조">기본 신경망 구조</h3>

<p>기존의 신경 구조를 한번 다시 되새겨보자.</p>

<p><img src="/assets/ML/rnn/stick.png" alt="Drawing" height="200" width="200" /></p>

<p>여러개의 노드로 구성된 작은 블럭을 하나의 층이라고 가정하자. 기존의 신경망 구조는 아래와 같다.</p>

<p><img src="/assets/ML/rnn/basic_nn_mnist.png" alt="Drawing" /></p>

<p>Input $x$ 가 선형 결합 후, Hidden 에 Activation function을 거쳐 다시 선형결합을 통해 Output $y$를 구해 예측하는 알고리즘이다. 여기서 첫번째 데이터($x_1$)와 그 다음 데이터($x_2$ 등)간의 구조는 독립적이라고 할 수 있다.</p>

<h3 id="forward">Forward</h3>
<p>예시로 time step($T$)이 3인 RNN을 살펴보자. (좌우 클릭으로 프로세스 과정 볼 수 있다)</p>

<ul id="light-slider1">
    <li><img src="/assets/ML/rnn/rnn_0.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_1.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_2.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_3.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_4.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_5.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_6.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_7.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_8.png" /></li>
  </ul>

<p>Time step = 0 일때, 각각 Layer들의 Weight를 초기화하게 된다. $h_0$ 층은 0으로, 나머지는 Xavier 가중치 초기값으로 초기화한다. 또한 각 가중치는 각각 layer에서 공유하게 된다.
(가중치 초기화를 잊어 버렸다면 <a href="https://simonjisu.github.io/datascience/2018/01/24/numpywithnn_6.html">여기</a>로)</p>

\[\begin{aligned}
h_t &amp;= \tanh(W_{hh} h_{t-1}+W_{xh}x_t+b_h) \\
y_t &amp;= W_{hy} h_t + b_y
\end{aligned}
\quad for\ t\ in\ T\]

<p>그리고, 시간이 지날때마 위의 식 처럼 Forward가 진행된다.</p>

<p>최종 Cost는 모든 Cost Function의 평균으로 구해진다.</p>

<h3 id="backward">Backward</h3>
<p>RNN에서는 일반적인 신경망과 다른 Backward 알고리즘을 쓴다. 시간 경과에 따른 BackPropagation을 BPTT(BackPropagation Through Time)이라고 부른다.</p>

<ul id="light-slider1">
    <li><img src="/assets/ML/rnn/rnn_back0.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back1.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back2.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back3.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back4.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back5.png" /></li>
  </ul>

<p>최종적으로 학습 될 값은 Loss Function에서 각 미분한 ${\frac{\partial L}{\partial W}}^{(1)}$, ${\frac{\partial L}{\partial W}}^{(2)}$, ${\frac{\partial L}{\partial W}}^{(3)}$ 의 합으로 구해진다.</p>

<h3 id="장기-의존성long-term-dependency-문제">장기 의존성(Long-Term Dependency) 문제</h3>
<p>RNN이 이론상으로는 sequence의 첫번째 항부터 끝까지(즉, $x_1 \cdots x_T$ 까지) 학습 할 수 있을 것으로 보이나, 실제로는 장기기억, 즉 Time Step이 길어 질 수록 예전에 있던 정보를 기억 못한다. 이를 <strong>장기 의존성(Long-Term Dependency)</strong> 문제라고 한다.</p>

<p><img src="/assets/ML/rnn/rnn_bad.png" alt="Drawing" /></p>

<p>그 이유는 우리가 업데이트 하려는 미분 식을 살펴보면 알 수 있다. 예를 들어 $W_{hh}$ 를 업데이트 한다고 하자.</p>

\[\begin{aligned}
\dfrac{\partial L}{\partial W_{hh}}  
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial W_{hh}} + \cdots +
\dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial W_{hh}} \\
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial y_T} \dfrac{\partial y_T}{\partial h_T} \dfrac{\partial h_T}{\partial h_{T-1}}  \cdots \dfrac{\partial h_2}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}} +
\cdots + \dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}} \\
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial y_T} \dfrac{\partial y_T}{\partial h_T} \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}} \dfrac{\partial h_1}{\partial W_{hh}} + \cdots + \dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}}
\end{aligned}\]

<p>위의 식중에 $\prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}}$ 부분을 자세히 펼쳐보면 아래와 같다.</p>

\[\begin{aligned}
\prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}}
&amp;= \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial a_{T-i+1}} \dfrac{\partial a_{T-i+1}}{\partial h_{T-i}} \\
&amp;= \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial a_{T-i+1}} W_{hh}
\end{aligned}\]

<p>여기서 $a_t=W_{hh}h_{t-1} + W_{xh}x_t + b_h$ 이다.</p>

<p>앞부분 $\frac{\partial h_{T-i+1}}{\partial a_{T-i+1}}$은 <strong>tanh</strong> 의 미분 값이다. 아래 그림과 같이 tanh의 미분 값은 0과 1사이의 값이다.</p>

<p><img src="/assets/ML/rnn/tanh.png" style="width=500px" />
(그림출처: http://nn.readthedocs.io/en/latest/transfer/)</p>

<p>뒷부분인 $W_{hh}$의 값들은 세가지 경우가 있다. 1과 같게 되면 Gradient가 수렴될 가능성이 높다. 그러나 1보다 클 경우 gradient가 무한대로 발산하는 <strong>Exploding Gradient</strong> 문제가 발생한다. 그러나 보통의 경우 $W_{hh}$ 의 값들은 1보다 작다. (아래 논문 참고)</p>

<p>0과 1사이의 작은 값을 계속 곱하게 되면 0으로 수렴한다. 따라서, 두 가지를 종합 해보았을 때, 출력값과 멀리 떨어진 Time Step일 수록 역전파가 전달 되지 않는 <strong>Vanishing Gradient</strong> 문제가 생기게 된다.</p>

<ul>
  <li><a href="http://proceedings.mlr.press/v28/pascanu13.pdf">On the difficulty of training recurrent neural networks</a> 논문에서는 Vanishing &amp; Exploding Gradient 문제를 자세히 다루고 있다.</li>
</ul>

<p>장기기억을 하지 못한다는 문제가 생기면서, 이를 해결하기 위해서 몇 가지 방법이 나왔다. 첫째로, Activation Function을 <strong>tanh</strong> 을 쓰면 기울기가 0과 1사이의 값으로 고정되니 <strong>ReLU</strong> 를 쓰자는 방법이 있었다. 둘째로, <strong>LSTM</strong>, <strong>GRU</strong> 등 새로운 방법들이 등장했다. 이 방법은 다음 시간에 설명하겠다. 더불어 Backward 의 계산 그래프도 같이 첨부하겠다.</p>
:ET