I"½D<h1 id="ìì„¸í•˜ê²Œ-ì„¤ëª…í•œ-rnn-ê³¼-lstm-ì‹œë¦¬ì¦ˆ---2">ìì„¸í•˜ê²Œ ì„¤ëª…í•œ RNN ê³¼ LSTM ì‹œë¦¬ì¦ˆ - 2</h1>

<h2 id="numpy-ë¡œ-rnn-ë§Œë“¤ì–´ë³´ê¸°">Numpy ë¡œ RNN ë§Œë“¤ì–´ë³´ê¸°</h2>
<p>ëª¨ë“  ì½”ë“œëŠ” Github: <a href="https://github.com/simonjisu/NUMPYwithNN">NUMPYwithNN</a> ì— ì˜¬ë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.</p>

<p>Jupyter Notebook ìœ¼ë¡œ ì „ì²´ê³¼ì • ë³´ê¸°: <a href="https://nbviewer.jupyter.org/github/simonjisu/NUMPYwithNN/blob/master/Notebook/Character_Predicting_RNN.ipynb">ë§í¬ </a></p>

<hr />

<h2 id="rnn-forward-ì™€-backwardì˜-ê³„ì‚°-ê·¸ë˜í”„">RNN Forward ì™€ Backwardì˜ ê³„ì‚° ê·¸ë˜í”„</h2>

<ul id="light-slider1">
  <li><img src="/assets/ML/rnn/graph_forward0.png" /></li>
  <li><img src="/assets/ML/rnn/graph_forward1.png" /></li>
  <li><img src="/assets/ML/rnn/graph_forward2.png" /></li>
</ul>

<ul id="light-slider1">
  <li><img src="/assets/ML/rnn/graph_backward0.png" /></li>
  <li><img src="/assets/ML/rnn/graph_backward1.png" /></li>
  <li><img src="/assets/ML/rnn/graph_backward2.png" /></li>
  <li><img src="/assets/ML/rnn/graph_backward3.png" /></li>
  <li><img src="/assets/ML/rnn/graph_backward4.png" /></li>
  <li><img src="/assets/ML/rnn/graph_backward5.png" /></li>
</ul>

<p>backwardì—ì„œ ìŠì§€ ë§ì•„ì•¼ í•  ë¶€ë¶„ì€ $t=T$ì¼ ë•Œ(ë§ˆì§€ë§‰ Stepì¼ ë•Œ) $d h_T$ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™” ë˜ë©°, êµ¬í•´ì§„ $d h_{t-1}^{raw}$ ê°€ ì´ ë‹¤ìŒ ì—­ì „íŒŒë¡œ ë“¤ì–´ê°€ê¸° ì „ì— ì´ì „ ë‹¨ê³„ë¡œ ë¶€í„° ì–»ì€ $dh_{t-1}$ ì™€ ë”í•´ì ¸ ê³„ì‚°í•œë‹¤ëŠ” ì ì´ë‹¤. ê·¸ ì´ìœ ëŠ” forward ì‹œ ë‹¤ìŒ stepìœ¼ë¡œ hidden ê°’($h_t$)ì„ ì „íŒŒí•˜ê¸° ë•Œë¬¸ì´ë¼ëŠ” ê²ƒì„ ìŠì§€ ë§ì.</p>

<p>ìœ„ ê·¸ë¦¼ì€ <a href="https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/">ratsgoâ€™s blog</a> ë‹˜ì˜ í¬ìŠ¤íŠ¸ì—ì„œ ë§ì€ ì°¸ì¡°ë¥¼ í•˜ê³  ìƒˆë¡œ ë§Œë“¤ì—ˆìŒì„ ë°í™ë‹ˆë‹¤.</p>

<h3 id="ì°¸ê³ -bptt-ìˆ˜ì‹ì -ì´í•´">ì°¸ê³ ) BPTT ìˆ˜ì‹ì  ì´í•´</h3>
<p>$tanh$ì˜ ë¯¸ë¶„ì„ $f(x) = 1 - tanh^2(x)$ ë¼ê³  í•˜ë©´,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&= \dfrac{\partial L}{\partial y_t} \dfrac{\partial y_t}{\partial h_0} + \dfrac{\partial L}{\partial y_{t-1}} \dfrac{\partial y_{t-1}}{\partial h_0} \cdots + \dfrac{\partial L}{\partial y_1} \dfrac{\partial y_1}{\partial h_0}\\
&= \dfrac{\partial L}{\partial y_t} \dfrac{\partial y_t}{\partial h_t} \dfrac{\partial h_t}{\partial a_t} \dfrac{\partial a_t}{\partial h_{t-1}} \cdots \dfrac{\partial a_1}{\partial h_{0}} + \cdots +
\dfrac{\partial L}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial a_1} \dfrac{\partial a_1}{\partial h_0} \\
&= W_{hy} dy_t W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_1) + \cdots + W_{hy} dy_2 W_{hh} f(a_2) W_{hh} f(a_1) + W_{hy} dy_1 W_{hh} f(a_1) \\
&= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{i} \prod_{j=1}^{i} f(a_j) \Big)
\end{aligned} %]]></script>

<p>ìœ„ ì‹ì„ ìœ„ì— ìˆëŠ” ê·¸ë¦¼ëŒ€ë¡œ ê·¸ë ¤ë³´ì, ë’¤ì— $W_{hh} f(a_1)$ ë¶€ì²˜ ì°¨ê·¼ì°¨ê·¼ ë¬¶ì–´ì„œ ì•„ë˜ì˜ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\dfrac{\partial L}{\partial h_{0}}
&= W_{hh} f(a_1) \bigg( W_{hy} dy_t  W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_2) + \cdots + W_{hy} dy_2 W_{hh} f(a_2) + W_{hy} dy_1 \bigg) \\
&= W_{hh} f(a_1) \bigg( W_{hh} f(a_2) \Big( W_{hy} dy_t W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_3) + \cdots + W_{hy} dy_2 \Big) + W_{hy} dy_1 \bigg) \\
&= W_{hh} f(a_1) \bigg( W_{hh} f(a_2) \Big( \cdots W_{hh} f(a_{t-1}) \big( \underbrace{W_{hh} f(a_t) (\underbrace{ W_{hy} dy_t }_{dh_t^{raw}} + 0)}_{dh_{t-1}} + \underbrace{ W_{hy} dy_{t-1} }_{dh_{t-1}^{raw}} \big) \cdots + W_{hy} dy_2 \Big) + W_{hy} dy_1 \bigg) \\
\end{aligned} %]]></script>

<p>ìœ„ì— ê·¸ë¦¼ê³¼ ë¹„êµí•´ë³´ë©´ ì´ëŸ° ì‹ìœ¼ë¡œ ê³„ì† ë”í•´ì§„ë‹¤.</p>

<hr />

<h3 id="backpropagation-through-time-bptt-êµ¬í˜„">BackPropagation Through Time (BPTT) êµ¬í˜„</h3>

<ul>
  <li><strong>Single_Layer_RNN</strong> ì˜ ì½”ë“œëŠ” <a href="https://github.com/simonjisu/NUMPYwithNN/blob/master/common/SimpleRNN.py">ì—¬ê¸°</a>ì— ìˆìŠµë‹ˆë‹¤.</li>
  <li><strong>Layer</strong> ì˜ êµ¬í˜„ì„ ì°¸ê³ í•˜ë ¤ë©´ Githubì˜ <a href="https://github.com/simonjisu/NUMPYwithNN/blob/master/common/layers.py">common/layers</a> ì°¸ê³ í•˜ì„¸ìš”!</li>
  <li>ì²˜ìŒ Layerë¥¼ ì§œë³´ì‹œëŠ” ë¶„ì€ <a href="https://simonjisu.github.io/deeplearning/2017/12/07/numpywithnn_1.html">Numpyë¡œ ì§œë³´ëŠ” Neural Network Basic</a> ì‹œë¦¬ì¦ˆë¥¼ ì°¸ê³ í•˜ì„¸ìš”!</li>
</ul>

<p>ìš°ì„  ë¯¸ë¶„í•œ ê°’ì˜ í•©ì„ êµ¬í•˜ê¸° ìœ„í•´ ê°ê° Layerì˜ íŒŒë¼ë¯¸í„°ì™€ê°™ì€ í˜•íƒœ(shape)ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def _params_summation_init(self):
    self.params_summ = {}
    self.params_summ['W_xh'] = np.zeros_like(self.params['W_xh'])
    self.params_summ['W_hh'] = np.zeros_like(self.params['W_hh'])
    self.params_summ['W_hy'] = np.zeros_like(self.params['W_hy'])
    self.params_summ['b_h'] = np.zeros_like(self.params['b_h'])
    self.params_summ['b_y'] = np.zeros_like(self.params['b_y'])
</code></pre></div></div>
<p>ë˜í•œ, $dh_T$ ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” í•œë‹¤.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dht = np.zeros_like(self.h0)
</code></pre></div></div>

<p>ê·¸í›„ì— ì´ ê¸¸ì´ $T$ì˜ ì—­ìˆœìœ¼ë¡œ ê° Layer ì˜ Back Propagation ì„ ì§„í–‰í•œë‹¤.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for t in np.arange(self.T)[::-1]:
    dout = self.last_layers[t].backward()
    dht_raw = self.layers['Affine_hy'][t].backward(dout)
    dat = self.layers['Activation'][t].backward(dht_raw + dht)
    dht = self.layers['Affine_hh'][t].backward(dat)
    dx = self.layers['Affine_xh'][t].backward(dat)
</code></pre></div></div>

<p>ë˜í•œ, íŒŒë¼ë¯¸í„° $W$ ì™€ $b$ ì˜ í•©ë„ ê°™ì´ êµ¬í•´ì¤€ë‹¤. ê·¸ ì´ìœ ëŠ” ì „í¸ì—ì„œ ì„¤ëª…ë˜ì–´ ìˆì§€ë§Œ, ë‹¤ì‹œ í•œë²ˆ ì´ì•¼ê¸° í•˜ìë©´, ìµœì¢… Loss Functionì€ ê° Output Lossì˜ í‰ê· ì´ê¸° ë•Œë¬¸ì—, ê° Output ë§ˆë‹¤ íŒŒë¼ë¯¸í„°ë“¤ì„ summation í•˜ëŠ” ê³¼ì •ì´ ìˆë‹¤. (í‰ê· ì„ êµ¬í• ë•Œ ìš°ì„  summationì„ í•œë‹¤ëŠ” ê²ƒì„ ìŠì§€ ë§ì.)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>self.params_summ['W_xh'] += self.layers['Affine_xh'][t].dW
self.params_summ['W_hh'] += self.layers['Affine_hh'][t].dW
self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
self.params_summ['b_h'] += self.layers['Affine_hh'][t].db
self.params_summ['b_y'] += self.layers['Affine_hy'][t].db
</code></pre></div></div>

<p>ì „ì²´ Backward ê³¼ì •</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def backward(self):
    # BPTT
    self._params_summation_init()
    dht = np.zeros_like(self.h0)

    for t in np.arange(self.T)[::-1]:
        dout = self.last_layers[t].backward()
        dht_raw = self.layers['Affine_hy'][t].backward(dout)
        dat = self.layers['Activation'][t].backward(dht_raw + dht)
        dht = self.layers['Affine_hh'][t].backward(dat)
        dx = self.layers['Affine_xh'][t].backward(dat)

        self.params_summ['W_xh'] += self.layers['Affine_xh'][t].dW
        self.params_summ['W_hh'] += self.layers['Affine_hh'][t].dW
        self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
        self.params_summ['b_h'] += self.layers['Affine_hh'][t].db
        self.params_summ['b_y'] += self.layers['Affine_hy'][t].db
</code></pre></div></div>

<h2 id="truncate-backpropagation-through-time-t-bptt">Truncate BackPropagation Through Time (T-BPTT)</h2>

<p><strong>Truncate BackPropagation Through Time (T-BPTT)</strong> ì€ ê¸°ì¡´ BPTT ì—ì„œ ê³¼ê±° ëª¨ë“  ë¯¸ë¶„ê°’ì„ ì°¸ì¡°í•˜ëŠ” ëŒ€ì‹  ê³ ì •ëœ ê¸¸ì´ë¡œ ì°¸ì¡° í•  ìˆ˜ ìˆë„ë¡ ë§Œë“  ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.</p>

<p>ì™œ ì´ëŸ°ê²ƒì„ ë§Œë“¤ì—ˆì„ ê¹Œ? BPTT ì•Œê³ ë¦¬ì¦˜ì˜ ë¯¸ë¶„ì‹ì„ ë‹¤ì‹œ ìƒê°í•´ë³´ì.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{i} \prod_{j=1}^{i} f(a_j) \Big)
\end{aligned} %]]></script>

<p>ìœ„ì—ì„œ ì„¤ëª…í–ˆì§€ë§Œ, BPTT ê³¼ì •ì—ì„œ Time-stepì´ ê¸¸ì–´ì§ˆ ìˆ˜ë¡, ë§ì€ ì–‘ì˜ ê³±ì…ˆì´ ì´ë£¨ì–´ ì§„ë‹¤. ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ì„œ ì´ëŸ° ì•Œê³ ë¦¬ì¦˜ì´ ë‚˜ì™”ì„ ìˆ˜ ìˆë‹¤.</p>

<p>ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²•ìœ¼ë¡œ, í•™ìŠµí•˜ê³  ì‹¶ì€ Sequenceì˜ ì¼ì • ê¸¸ì´ë§Œí¼ë§Œ ê³¼ê±°ë¥¼ ì°¸ì¡°í•˜ê³  ì‹¶ê¸° ë•Œë¬¸ì¼ ìˆ˜ë„ ìˆë‹¤.</p>

<p>ì˜ˆë¥¼ ë“¤ì–´ â€œI live in Seoul. (ì¤‘ëµ) I am Korean.â€ ì´ë¼ëŠ” ë¬¸ì¥ì„ ìƒê°í•´ë³´ì. í•™ìŠµ ë°ì´í„°ëŠ” ì•„ë˜ì™€ ê°™ì„ ê²ƒì´ë‹¤.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>["I", "live", "in", "Seoul", ".", (ì¤‘ëµ), "I", "am", "Korean", "."]
</code></pre></div></div>

<p>Forward í• ë•ŒëŠ” ìˆœì°¨ì ìœ¼ë¡œ ë“¤ì–´ê°ˆí…ë°, Backward í• ë•ŒëŠ” ë°ì´í„°ì˜ ì—­ìˆœìœ¼ë¡œ(â€œ.â€, â€œKoreanâ€) ì§„í–‰ë  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë‚´ê°€ í•œêµ­ì¸ì´ë¼ëŠ” ê²ƒì€ ë‚´ê°€ ì„œìš¸ì— ì‚´ê³  ìˆê¸° ë•Œë¬¸ì¸ë°, êµ³ì´ ì•ë‹¨ì˜ â€œIâ€, â€œliveâ€, â€œinâ€ ê¹Œì§€ ì°¸ì¡°í•  í•„ìš”ëŠ” ì—†ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ìœ„ì— ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ë³€í•  ê²ƒì´ë‹¤.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{k} \prod_{j=k}^{t} f(a_j) \Big) \\
where \quad k &= \max(1, t - truncate)
\end{aligned} %]]></script>

<h3 id="t-bptt-êµ¬í˜„">T-BPTT êµ¬í˜„</h3>
<p><img src="/assets/ML/rnn/normal_truncate.png" /></p>

<p>ê·¸ë¦¼ ì¶œì²˜: <a href="https://r2rt.com/styles-of-truncated-backpropagation.html">r2rt.com</a></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def backward_truncate(self):
    # TBPTT
    self._params_summation_init()
    dht = np.zeros_like(self.h0)

    for t in np.arange(self.T)[::-1]:
        dout = self.last_layers[t].backward()
        dht_raw = self.layers['Affine_hy'][t].backward(dout)
        self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
        self.params_summ['b_y'] += self.layers['Affine_hy'][t].db

        for bptt_step in np.arange(max(0, t + 1 - self.bptt_truncate), t + 1)[::-1]:
            dat = self.layers['Activation'][bptt_step].backward(dht_raw + dht)
            dht = self.layers['Affine_hh'][bptt_step].backward(dat)  # dh_t-1
            dx = self.layers['Affine_xh'][bptt_step].backward(dat)  # dx
            self.params_summ['W_xh'] += self.layers['Affine_xh'][bptt_step].dW
            self.params_summ['W_hh'] += self.layers['Affine_hh'][bptt_step].dW
            self.params_summ['b_h'] += self.layers['Affine_hh'][bptt_step].db
</code></pre></div></div>

<p>ê·¸ëŸ¬ë‚˜ Tensorflow ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬í˜„í•œë‹¤ê³  í•œë‹¤.</p>

<p><img src="/assets/ML/rnn/tensorflow_truncate.png" /></p>

<p>ê·¸ë¦¼ ì¶œì²˜: <a href="https://r2rt.com/styles-of-truncated-backpropagation.html">r2rt.com</a></p>

<h2 id="ì‹¤ìŠµ">ì‹¤ìŠµ</h2>

<h3 id="ëª©ì ">ëª©ì </h3>
<p><strong>â€œhello world! nice to meet you! i love iron-manâ€</strong> ì„ RNN ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê¸°.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input</th>
      <th style="text-align: center">Â </th>
      <th style="text-align: center">Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">h</td>
      <td style="text-align: center">â†’</td>
      <td style="text-align: center">e</td>
    </tr>
    <tr>
      <td style="text-align: center">e</td>
      <td style="text-align: center">â†’</td>
      <td style="text-align: center">l</td>
    </tr>
    <tr>
      <td style="text-align: center">l</td>
      <td style="text-align: center">â†’</td>
      <td style="text-align: center">l</td>
    </tr>
    <tr>
      <td style="text-align: center">l</td>
      <td style="text-align: center">â†’</td>
      <td style="text-align: center">o</td>
    </tr>
    <tr>
      <td style="text-align: center">â‹®</td>
      <td style="text-align: center">â‹®</td>
      <td style="text-align: center">â‹®</td>
    </tr>
    <tr>
      <td style="text-align: center">m</td>
      <td style="text-align: center">â†’</td>
      <td style="text-align: center">a</td>
    </tr>
    <tr>
      <td style="text-align: center">a</td>
      <td style="text-align: center">â†’</td>
      <td style="text-align: center">n</td>
    </tr>
  </tbody>
</table>

<h3 id="ë°ì´í„°-ë°-ìš°ë¦¬ê°€-ë§Œë“ -íŒ¨í‚¤ì§€-ì¤€ë¹„">ë°ì´í„° ë° ìš°ë¦¬ê°€ ë§Œë“  íŒ¨í‚¤ì§€ ì¤€ë¹„</h3>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
from common.SimpleRNN import Single_layer_RNN
from common.optimizer import Adam
from common.train_graph import loss_graph
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = 'hello world! nice to meet you! i love iron-man'
</code></pre></div></div>
<p>ì¸ì½”ë”© í´ë˜ìŠ¤ í•˜ë‚˜ë¥¼ ë§Œë“¤ì–´ì„œ ë¬¸ìì—´ì„ one-hot ì¸ì½”ë”© í•´ì¤€ë‹¤.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class chr_coding(object):
    def __init__(self):
        self._dict = None
        self._one_hot_matrix = None
        self._dict_reversed = None

    def fit(self, x):
        if isinstance(x, str):
            x = list(x)

        self._one_hot_matrix = np.eye(len(set(x)))
        self._dict = {d: i for i, d in enumerate(list(set(x)))}
        self._dict_reversed = {v: k for k, v in self._dict.items()}

    def encode(self, x):
        encoded_data = np.array([self._one_hot_matrix[self._dict[d]] for d in x])
        return encoded_data

    def decode(self, x, probs=None):
        if probs is None:
            decoded_data = self._dict_reversed[x]
        else:
            decoded_data = self._dict_reversed[np.argmax(probs)]
        return decoded_data
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>encoder = chr_coding()
encoder.fit(x)
one_hot_data = encoder.encode(x)
</code></pre></div></div>

<p>í•™ìŠµ ë°ì´í„° x, yë¥¼ ì§€ì •í•´ì¤€ë‹¤.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_x = one_hot_data[:-1]
train_y = one_hot_data[1:]
</code></pre></div></div>

<h3 id="hyperparameters">hyperparameters</h3>

<p>INPUT_SIZE ì™€ OUTPUT_SIZE ëŠ” ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ë¬¸ìì—´ ì‚¬ì „ì˜ ê¸¸ì´ë¼ëŠ” ê²ƒì„ ìŠì§€ ë§ì.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NUM_EPOCHS = 600
PRINT_EPOCH = 30
INPUT_SIZE = one_hot_data.shape[1]
OUTPUT_SIZE = one_hot_data.shape[1]
HIDDEN_SIZE = 20
</code></pre></div></div>

<h3 id="í•„ìš”í•œ-í•¨ìˆ˜-ì„¤ì •-accuracy-ì™€-train-í•¨ìˆ˜">í•„ìš”í•œ í•¨ìˆ˜ ì„¤ì •: accuracy ì™€ train í•¨ìˆ˜</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_accuracy(x, test_string):
    bool_ = np.array(list(x))[1:] == np.array(list(test_string))[1:]
    return bool_.sum() / len(bool_)

def train(rnn, optim, print_epoch=20):
    total_loss_list = []
    total_acc_list = []
    for epoch in range(NUM_EPOCHS):
        test_string = 'h'
        # forward
        total_loss = rnn.loss(train_x, train_y)

        # backward
        rnn.backward()

        optim.update(rnn.params, rnn.params_summ)

        # test string
        predicted_idx = rnn.predict(train_x)
        for idx in predicted_idx:
            test_string += encoder.decode(idx)

        # get accuracy
        acc = get_accuracy(x, test_string)

        if epoch % print_epoch == 0:
            print('#{0}, Loss: {1:.6f}, Acc: {2:.6f}, Test_string: "{3}"'\
                  .format(epoch, total_loss, acc, test_string))
        elif epoch == (NUM_EPOCHS-1):
            print('#{0}, Loss: {1:.6f}, Acc: {2:.6f}, Test_string: "{3}"'\
                  .format(epoch, total_loss, acc, test_string))

        total_loss_list.append(total_loss)
        total_acc_list.append(acc)
    return total_loss_list, total_acc_list
</code></pre></div></div>

<h3 id="í•™ìŠµí•˜ê¸°">í•™ìŠµí•˜ê¸°</h3>

<p>rnn ëª¨ë¸ì„ ë§Œë“¤ê³ , ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•  ê²ƒì¸ì§€ ì •í•˜ì. ì—¬ê¸°ì„œëŠ” Adamì„ ì¼ë‹¤.</p>

<ul>
  <li><strong>Optimizer</strong> ì˜ ì„¤ëª…ì€ <a href="https://github.com/simonjisu/NUMPYwithNN/blob/master/common/SimpleRNN.py">Numpyë¡œ ì§œë³´ëŠ” Neural Network Basic - 5</a>ì— ìˆìŠµë‹ˆë‹¤.</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rnn = Single_layer_RNN(input_size=INPUT_SIZE,
                       hidden_size=HIDDEN_SIZE,
                       output_size=OUTPUT_SIZE)
optim = Adam()
</code></pre></div></div>

<p>í•™ìŠµì‹œí‚¤ê¸°!</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>total_loss_list, total_acc_list = train(rnn, optim, print_epoch=PRINT_EPOCH)
</code></pre></div></div>
<p><img src="/assets/ML/rnn/rnn_bptt.png" /></p>

<p>Loss Graph ë„ ì°ì–´ë³´ì</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss_graph(train_loss_list=total_loss_list, train_acc_list=total_acc_list)
</code></pre></div></div>

<p><img src="/assets/ML/rnn/rnn_bptt_loss.png" /></p>

<h2 id="ê³µë¶€ì—-ë„ì›€-ë˜ì—ˆë˜-ì‹¸ì´íŠ¸">ê³µë¶€ì— ë„ì›€ ë˜ì—ˆë˜ ì‹¸ì´íŠ¸:</h2>
<ul>
  <li><a href="https://gist.github.com/karpathy/d4dee566867f8291f086">karpathy github RNN part</a></li>
  <li><a href="https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/">ratsgoâ€™s blog</a></li>
</ul>
:ET