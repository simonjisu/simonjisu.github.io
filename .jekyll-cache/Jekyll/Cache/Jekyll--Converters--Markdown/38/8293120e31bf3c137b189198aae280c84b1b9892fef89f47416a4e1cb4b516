I"ë/<h1 id="numpyë¡œ-ì§œë³´ëŠ”-neural-network-basic---4">Numpyë¡œ ì§œë³´ëŠ” Neural Network Basic - 4</h1>
<hr />

<h2 id="ì˜¤ì°¨ì—­ì „íŒŒbackpropagation">ì˜¤ì°¨ì—­ì „íŒŒ(Backpropagation)</h2>

<h3 id="ì—°ì‡„ë²•ì¹™ì˜-ì›ë¦¬">ì—°ì‡„ë²•ì¹™ì˜ ì›ë¦¬</h3>
<p>í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ í•©ì„± í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</p>

<script type="math/tex; mode=display">\begin{cases} z = t^2 \\ t = x + y \end{cases}</script>

<p>ìœ„ ì‹ì˜ ë¯¸ë¶„ì„ ë‚˜íƒ€ë‚´ë©´
<script type="math/tex">\frac{\partial{z}}{\partial{x}} = \frac{\partial{z}}{\partial{t}} \cdot \frac{\partial{t}}{\partial{x}}</script></p>

<p>ë”°ë¼ì„œ $z$, $t$ ì‹ì„ ë¯¸ë¶„í•˜ê²Œ ë˜ë©´</p>

<p><script type="math/tex">\frac{\partial{z}}{\partial{t}} = 2t</script>
<script type="math/tex">\frac{\partial{t}}{\partial{x}}=1</script></p>

<script type="math/tex; mode=display">\therefore\  \frac{\partial{z}}{\partial{x}} = \frac{\partial{z}}{\partial{t}} \cdot \frac{\partial{t}}{\partial{x}} = 2t \cdot 1 = 2(x+y)</script>

<p>ìš°ë¦¬ì˜ ëª©ì ì€ $L$ ì— ëŒ€í•´ì„œ $W$ ë¥¼ ë¯¸ë¶„í•˜ì—¬ ì¡°ê¸ˆì”© ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì„ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.
<script type="math/tex">\frac{\partial{L}}{\partial{W}} = \frac{\partial{L}}{\partial{Y}} \cdot \frac{\partial{Y}}{\partial{W}}</script></p>

<h3 id="ë§ì…ˆë…¸ë“œì™€-ê³±ì…ˆë…¸ë“œì˜-ì—­ì „íŒŒ">ë§ì…ˆë…¸ë“œì™€ ê³±ì…ˆë…¸ë“œì˜ ì—­ì „íŒŒ</h3>

<p><img src="/assets/ML/nn/NN_add.png" alt="Drawing" style="width: 400px;" /></p>

<p>(ê·¸ë¦¼ì¶œì²˜: ratsgoë‹˜ì˜ ë¸”ë¡œê·¸[<a href="https://ratsgo.github.io/deep%20learning/2017/05/14/backprop/">ë§í¬</a>])</p>

<script type="math/tex; mode=display">\begin{cases} L(z) \\ z = x + y \end{cases}</script>

<p>ê°ê° ë¯¸ë¶„í•˜ê²Œ ë˜ë©´</p>

<script type="math/tex; mode=display">\begin{cases}
    \dfrac{\partial{L}}{\partial{z}} \\
    \dfrac{\partial{z}}{\partial{x}} =
    \dfrac{\partial{z}}{\partial{y}} = 1
  \end{cases}</script>

<p>ë”°ë¼ì„œ $L$ ì„ ê°ê° $x$ ì™€ $y$ ë¡œ ë¯¸ë¶„í•˜ë ¤ë©´</p>

<script type="math/tex; mode=display">\begin{cases}
    \dfrac{\partial{L}}{\partial{x}} = \dfrac{\partial{L}}{\partial{z}} \cdot \dfrac{\partial{z}}{\partial{x}} = \dfrac{\partial{L}}{\partial{z}} \cdot 1 \\
    \dfrac{\partial{L}}{\partial{y}} = \dfrac{\partial{L}}{\partial{z}} \cdot \dfrac{\partial{z}}{\partial{y}} = \dfrac{\partial{L}}{\partial{z}} \cdot 1
  \end{cases}</script>

<p>ë”°ë¼ì„œ <strong>ë§ì…ˆ</strong> ë…¸ë“œëŠ” ë“¤ì–´ì˜¨ ì‹ í˜¸($\frac{\partial{L}}{\partial{z}}$)ë¥¼ <strong>ê·¸ëŒ€ë¡œ</strong> ë³´ë‚¸ë‹¤.</p>

<p><img src="/assets/ML/nn/NN_multiply.png" alt="Drawing" style="width: 400px;" /></p>

<p>(ê·¸ë¦¼ì¶œì²˜: ratsgoë‹˜ì˜ ë¸”ë¡œê·¸[<a href="https://ratsgo.github.io/deep%20learning/2017/05/14/backprop/">ë§í¬</a>])</p>

<script type="math/tex; mode=display">\begin{cases} L(z) \\ z = x \times y \end{cases}</script>

<p>ê°ê° ë¯¸ë¶„í•˜ê²Œ ë˜ë©´</p>

<script type="math/tex; mode=display">\begin{cases}
    \dfrac{\partial{L}}{\partial{z}} \\
    \dfrac{\partial{z}}{\partial{x}} = y \\
    \dfrac{\partial{z}}{\partial{y}} = x
  \end{cases}</script>

<p>ë”°ë¼ì„œ $L$ ì„ ê°ê° $x$ ì™€ $y$ ë¡œ ë¯¸ë¶„í•˜ë ¤ë©´</p>

<script type="math/tex; mode=display">\begin{cases}
    \dfrac{\partial{L}}{\partial{x}} = \dfrac{\partial{L}}{\partial{z}} \cdot \dfrac{\partial{z}}{\partial{x}} = \dfrac{\partial{L}}{\partial{z}} \cdot y \\
    \dfrac{\partial{L}}{\partial{y}} = \dfrac{\partial{L}}{\partial{z}} \cdot \dfrac{\partial{z}}{\partial{y}} = \dfrac{\partial{L}}{\partial{z}} \cdot x
  \end{cases}</script>

<p>ë”°ë¼ì„œ <strong>ê³±ì…ˆ</strong> ë…¸ë“œëŠ” ë“¤ì–´ì˜¨ ì‹ í˜¸ì— ì„œë¡œ ë°”ë€ ì…ë ¥ì‹ í˜¸ ê°’ì„ <strong>ê³±í•´ì„œ</strong> í•˜ë¥˜ë¡œ ë³´ë‚¸ë‹¤.</p>

<h2 id="sigmoid-ê³„ì¸µì˜-ìˆœì „íŒŒì™€-ì—­ì „íŒŒ">Sigmoid ê³„ì¸µì˜ ìˆœì „íŒŒì™€ ì—­ì „íŒŒ</h2>

<script type="math/tex; mode=display">y = \frac{1}{1+\exp(-x)}</script>

<h3 id="forward">Forward</h3>

<p><img src="/assets/ML/nn/NN_sigmoid_forward.png" alt="Drawing" style="width: 600px;" /></p>

<h3 id="backward">Backward</h3>

<p><img src="/assets/ML/nn/NN_sigmoid_back.png" alt="Drawing" style="width: 600px;" /></p>

<p><img src="/assets/ML/nn/NN_sigmoid_back2.png" alt="Drawing" style="width: 600px;" /></p>

<blockquote>
  <h4 id="ì—­ì „íŒŒ-1ë‹¨ê³„---">ì—­ì „íŒŒ 1ë‹¨ê³„ ( / )</h4>

  <p>â€/â€ ì—°ì‚°ì€ ì…ë ¥ë³€ìˆ˜ xë¥¼ $\dfrac{1}{x}$ ë¡œ ë°”ê¿”ì¤€ë‹¤. ì¦‰ $f_1(x) = \dfrac{1}{x}$ ê°€ ëœë‹¤.</p>

  <p>ë¯¸ë¶„ì„ í•˜ê²Œ ë˜ë©´ $\dfrac{\partial{f_1}}{\partial{x}} = -\dfrac{1}{x^2} = -y^2$ê°€ ë˜ì„œ ì…ë ¥ì‹ í˜¸ë¥¼ í•˜ë¥˜ë¡œ ë³´ë‚¸ë‹¤.</p>

  <h4 id="ì—­ì „íŒŒ-2ë‹¨ê³„---">ì—­ì „íŒŒ 2ë‹¨ê³„ ( + )</h4>

  <p>â€+â€ ì—°ì‚°ì€ ì‹ í˜¸ë¥¼ ê·¸ëŒ€ë¡œ í•˜ë¥˜ë¡œ í˜ëŸ¬ ë³´ë‚¸ë‹¤</p>

  <h4 id="ì—­ì „íŒŒ-3ë‹¨ê³„-exp">ì—­ì „íŒŒ 3ë‹¨ê³„ (exp)</h4>

  <p>â€œexpâ€ì—°ì‚°ì€ $f_2(x) = exp(x)$ ì´ë©°, ë¯¸ë¶„ë„ $\dfrac{\partial{f_2}}{\partial{x}} = exp(x)$ ë¡œ ê·¸ëŒ€ë¡œ ê³±í•´ì„œ í•˜ë¥˜ë¡œ ë³´ë‚¸ë‹¤.</p>

  <h4 id="ì—­ì „íŒŒ-4ë‹¨ê³„--x-">ì—­ì „íŒŒ 4ë‹¨ê³„ ( x )</h4>

  <p>â€$\times$â€ì—°ì‚°ì€ ì„œë¡œ ë°”ë€ ì…ë ¥ì‹ í˜¸ì˜ ê°’ì„ ê³±í•´ì„œ ë³´ë‚¸ë‹¤.</p>
</blockquote>

<p>ë”°ë¼ì„œ, ìµœì¢…ì ìœ¼ë¡œ ì‹œê·¸ëª¨ì´ë“œì˜ ì—­ì „íŒŒ ì¶œë ¥ê°’ì€ ì•„ë˜ì™€ ê°™ë‹¤.</p>

<p><img src="/assets/ML/nn/NN_sigmoid_last.png" alt="Drawing" style="width: 400px;" /></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\dfrac{\partial{L}}{\partial{y}}y^{2}\exp(-x)
&= \dfrac{\partial{L}}{\partial{y}} \dfrac{1}{[1+\exp(-x)]^2}\exp(-x) \\
&= \dfrac{\partial{L}}{\partial{y}} \dfrac{1}{1+\exp(-x)} \dfrac{\exp(-x)}{1+\exp(-x)} \\
&= \dfrac{\partial{L}}{\partial{y}}y(1-y) \\
\end{aligned} %]]></script>

<p>ì´ê²ƒì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ê²Œ ë˜ë©´</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Sigmoid(object):
    def __init__(self):
        self.out = None  # ì—­ì „íŒŒì‹œ ê³±í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì €ì¥í•´ë‘”ë‹¤

    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out

        return out

    def backward(self, dout):
        dx = dout * self.out * (1 - self.out)
</code></pre></div></div>

<h2 id="affine-ê³„ì¸µê³¼-affine-transform">Affine ê³„ì¸µê³¼ Affine Transform</h2>
<p>ê¸°í•˜í•™ì—ì„œ ì‹ ê²½ë§ ìˆœì „íŒŒ ë•Œ ìˆ˜í–‰í•˜ëŠ” í–‰ë ¬ì˜ ë‚´ì ì„ Affine Transformì´ë¼ í•˜ë©°, Affine ê³„ì¸µì€ ì–´íŒŒì¸ ë³€í™˜ì„ ìˆ˜í–‰ ì²˜ë¦¬í•˜ëŠ” ê³„ì¸µì´ë‹¤.</p>

<p>ìœ„í‚¤ë°±ê³¼[<a href="https://ko.wikipedia.org/wiki/%EC%95%84%ED%95%80_%EB%B3%80%ED%99%98">ë§í¬</a>]</p>

<h3 id="forward-1">Forward</h3>

<p>$A = X \cdot W + B$</p>

<h3 id="backward-1">Backward</h3>

<p>$\begin{cases}
    \dfrac{\partial{L}}{\partial{X}} = \dfrac{\partial{L}}{\partial{A}} \cdot \dfrac{\partial{A}}{\partial{X}} = \dfrac{\partial{L}}{\partial{A}} \cdot W^T <br />
    \dfrac{\partial{L}}{\partial{W}} = \dfrac{\partial{L}}{\partial{A}} \cdot \dfrac{\partial{A}}{\partial{W}} = X^T \cdot \dfrac{\partial{L}}{\partial{A}} <br />
    \dfrac{\partial{L}}{\partial{B}} = 1
  \end{cases}$</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Affine(object):
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.x = x
        out = np.dot(self.x, self.W) + self.b

        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(self.b, axis=0)

        return dx
</code></pre></div></div>

<h2 id="backpropogation-ì‚¬ìš©í•œ-í•™ìŠµêµ¬í˜„">Backpropogation ì‚¬ìš©í•œ í•™ìŠµêµ¬í˜„</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import collections
from layers import *

class TwoLayer(object):
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(hidden_size)

        # ê³„ì¸µ ìƒì„±
        self.layers = collections.OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['ReLu1'] = ReLu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])

        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    # x: ì…ë ¥ ë°ì´í„°, t: ì •ë‹µ ë°ì´í„°
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)

        acc = np.sum(y == t) / float(x.shape[0])
        return acc

    def gradient(self, x, t):
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()

        for layer in layers:
            dout = layer.backward(dout)

        # save
        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db

        return grads
</code></pre></div></div>

<p>TwoLayer Neural Network ë¥¼ ìƒì„±í•˜ëŠ” ê°ì²´ëŠ” ë”°ë¡œ íŒŒì¼ì— ì €ì¥í•˜ê³  ë¶ˆëŸ¬ë‚´ëŠ” ê²ƒì´ ì¢‹ë‹¤. OrderedDictì€ dictionary í˜•íƒœë¡œ ì…ë ¥ ìˆœì„œë¥¼ ê¸°ì–µí•´ì£¼ëŠ” ì¢‹ì€ í•¨ìˆ˜ë‹¤.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from dataset.mnist import load_mnist
from two_layer_nn import TwoLayer

# data_loading
(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []
train_acc_list = []
test_acc_list = []

#highper parameter
epoch_num = 10000
train_size = x_train.shape[0]
batch_size = 100
alpha = 0.01  # learning rate
epsilon = 1e-6

# 1ì—í­ë‹¹ ë°˜ë³µ ìˆ˜
iter_per_epoch = max(train_size / batch_size, 1)

start = time.time()
nn = TwoLayer(input_size=784, hidden_size=100, output_size=10, weight_init_std=0.01)
for epoch in range(epoch_num):
    # get mini batch:
    batch_mask = np.random.choice(train_size, batch_size) # shuffle íš¨ê³¼
    x_batch = x_train[batch_mask]
    y_batch = y_train[batch_mask]

    # gradient ê³„ì‚°
    grad = nn.gradient(x_batch, y_batch)

    # update
    for key in ['W1', 'b1', 'W2', 'b2']:
        nn.params[key] = nn.params[key] - alpha * grad[key]

    # record
    loss = nn.loss(x_batch, y_batch)
    train_loss_list.append(loss)

    # 1ì—í­ë‹¹ ì •í™•ë„ ê³„ì‚°
    if epoch % iter_per_epoch == 0:
        train_acc = nn.accuracy(x_train, y_train)
        test_acc = nn.accuracy(x_test, y_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print('# {0} | trian acc: {1:.5f} | test acc: {2:.5f}'.format(epoch, train_acc, test_acc))

end = time.time()
print('total time:', (end - start))

# ê²°ê³¼
# 0 | trian acc: 0.10775 | test acc: 0.10700
# 600 | trian acc: 0.10775 | test acc: 0.10700
# 1200 | trian acc: 0.10775 | test acc: 0.10700
# 1800 | trian acc: 0.10775 | test acc: 0.10700
# 2400 | trian acc: 0.10775 | test acc: 0.10700
# 3000 | trian acc: 0.10775 | test acc: 0.10700
# 3600 | trian acc: 0.10775 | test acc: 0.10700
# 4200 | trian acc: 0.10775 | test acc: 0.10700
# 4800 | trian acc: 0.10775 | test acc: 0.10700
# 5400 | trian acc: 0.10775 | test acc: 0.10700
# 6000 | trian acc: 0.10775 | test acc: 0.10700
# 6600 | trian acc: 0.10775 | test acc: 0.10700
# 7200 | trian acc: 0.10775 | test acc: 0.10700
# 7800 | trian acc: 0.10775 | test acc: 0.10700
# 8400 | trian acc: 0.10775 | test acc: 0.10700
# 9000 | trian acc: 0.10775 | test acc: 0.10700
# 9600 | trian acc: 0.10775 | test acc: 0.10700
# total time: 61.07117795944214
</code></pre></div></div>

<p>í•™ìŠµì€ ì „í˜€ ì•ˆë˜ì§€ë§Œ ìˆ˜ì¹˜ ë¯¸ë¶„ë³´ë‹¤ ë” ë¹ ë¥´ê²Œ ì§„í–‰ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ì™œ í•™ìŠµì´ ì•ˆëì„ê¹Œì— ëŒ€í•´ì„œëŠ” ë‹´ì€ ì‹œê°„ì— ì´ì•¼ê¸° í•˜ê² ë‹¤.</p>
:ET