I"ğ<h1 id="numpyë¡œ-ì§œë³´ëŠ”-neural-network-basic---8">Numpyë¡œ ì§œë³´ëŠ” Neural Network Basic - 8</h1>

<hr />
<h2 id="ì´-ì •ë¦¬">ì´ ì •ë¦¬</h2>
<p>ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” Neural Networkì˜ ê¸°ì›ë¶€í„° Feedforward ê³¼ì •, BackPropogation ê³¼ì •, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ì„ ë°°ì› ë‹¤. ì´ë“¤ì„ ì´ ì •ë¦¬í•´ì„œ Mnist ë°ì´í„°ë¥¼ ë‹¤ì‹œ í•™ìŠµ ì‹œì¼œë³´ì.</p>

<p><a href="https://github.com/simonjisu/NUMPYwithNN/tree/master/common"> ëª¨ë“  ì½”ë“œ ë§í¬</a></p>

<h3 id="package-load">Package Load</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from common.Multilayer import MLP
from dataset.mnist import load_mnist
from common.optimizer import *
import time
</code></pre></div></div>

<h3 id="data-load">Data Load</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
</code></pre></div></div>
<blockquote>
  <p>(60000, 784)<br />(60000, 10)<br />(10000, 784)<br />(10000, 10)</p>
</blockquote>

<h3 id="network--optimizer-settings">Network &amp; Optimizer settings</h3>

<p>ìš°ë¦¬ì˜ ë„¤íŠ¸ì›Œí¬ëŠ” ì´ 3ì¸µì´ë©° Input Sizeê°€ 784, Hidden nodeëŠ” ê°ê° 100, 50ê°œ, Output ì€ 10(ìˆ«ì 0~9ê¹Œì§€ì˜ ì†ê¸€ì”¨ ë¶„ë¥˜ì´ê¸° ë•Œë¬¸)ì´ë‹¤.</p>

<p>í™œì„±í™” í•¨ìˆ˜ëŠ” <strong>ReLu</strong>, ì´ˆê¸°ê°’ë„ ì´ì— ë”°ë¼ <strong>He</strong> ë¥¼ ì¨ì¤€ë‹¤. ê·¸ë¦¬ê³  ì¤‘ê°„ì— Batch Normalizationì„ ì¨ì¤€ë‹¤.</p>

<p>ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì˜µí‹°ë§ˆì´ì €ëŠ” <strong>Adam</strong> ì„ ì“°ê³ , Loss Functionì€ <strong>Cross Entropy</strong> ë¥¼ ì“°ê²Œ ëœë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nn = MLP(input_size=784, hidden_size=[100, 50], output_size=10,
         activation='relu', weight_init_std='he', use_batchnorm=True)
optimizer = Adam()
</code></pre></div></div>

<h3 id="training--test">Training &amp; test</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_loss_list = []
train_acc_list = []
test_acc_list = []
epoch_list = []

epoch_num=3000
train_size = x_train.shape[0]
batch_size = 100
epsilon = 1e-6

iter_per_epoch = max(train_size / batch_size, 1)

start = start = time.time()

for epoch in range(epoch_num):
    # get mini batch:
    batch_mask = np.random.choice(train_size, batch_size) # shuffle íš¨ê³¼
    x_batch = x_train[batch_mask]
    y_batch = y_train[batch_mask]

    grads = nn.gradient(x_batch, y_batch)

    optimizer.update(nn.params, grads)

    # 1ì—í­ë‹¹ ì •í™•ë„ ê³„ì‚°
    if epoch % iter_per_epoch == 0:
        loss = nn.loss(x_batch, y_batch)
        train_loss_list.append(loss)
        train_acc = nn.accuracy(x_train, y_train)
        train_acc_list.append(train_acc)
        test_acc = nn.accuracy(x_test, y_test)
        test_acc_list.append(test_acc)
        epoch_list.append(epoch)
        print('# {0} | loss: {1:.5f} | trian acc: {2:.5f} | test acc: {3:.5f}'.format(epoch, loss, train_acc, test_acc))
    elif epoch == (epoch_num - 1):
        loss = nn.loss(x_batch, y_batch)
        train_loss_list.append(loss)
        train_acc = nn.accuracy(x_train, y_train)
        train_acc_list.append(train_acc)
        test_acc = nn.accuracy(x_test, y_test)
        test_acc_list.append(test_acc)
        epoch_list.append(epoch)
        print('# {0} | loss: {1:.5f} | trian acc: {2:.5f} | test acc: {3:.5f}'.format(epoch, loss, train_acc, test_acc))

end = time.time()
print('total time:', (end - start))        
</code></pre></div></div>

<blockquote>
  <p># 0 | loss: 11.06622 | trian acc: 0.11408 | test acc: 0.11910<br /># 600 | loss: 0.23165 | trian acc: 0.92055 | test acc: 0.92020<br /># 1200 | loss: 0.19112 | trian acc: 0.93975 | test acc: 0.94180<br /># 1800 | loss: 0.08235 | trian acc: 0.95188 | test acc: 0.95040<br /># 2400 | loss: 0.09155 | trian acc: 0.95898 | test acc: 0.95600<br /># 2999 | loss: 0.09883 | trian acc: 0.96513 | test acc: 0.96150<br />total time: 27.169809818267822</p>
</blockquote>

<p><img src="/assets/ML/nn/train_test-graph.png" alt="Drawing" style="width=500px" /></p>

<p>ì‹œê°„ì€ 3000 Epochë¥¼ ë„ëŠ”ë° ì•½ 30ì´ˆê°€ ì•ˆê±¸ë ¸ìœ¼ë©°, í…ŒìŠ¤íŠ¸ ê²°ê³¼ë„ ìš°ìˆ˜í•˜ê²Œ ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸ëœë‹¤. CNNìœ¼ë¡œ í•˜ë©´ ë” ë†’ì•„ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.</p>

<h3 id="model-check">Model Check</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def check(x, y, model):
    pred_y = model.predict(x)
    if x.ndim != 2:
        x = x.reshape(28, 28)

    print('Predict Answer: {}'.format(np.argmax(pred_y)))
    print('Real Answer: {}'.format(np.argmax(y)))
    plt.imshow(x, cmap='binary')
    plt.grid(False)
    plt.axis('off')
    plt.show()
</code></pre></div></div>

<p>í…ŒìŠ¤íŠ¸ ë°ì´í„°ì¤‘ í•˜ë‚˜ ê³¨ë¼ì„œ ì‹¤í—˜í•´ë³´ì</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>check(x_test[45], y_test[45], nn)
</code></pre></div></div>
<blockquote>
  <p>Predict Answer: 5<br />Real Answer: 5</p>

  <p><img src="/assets/ML/nn/num5.png" alt="Drawing" height="100" width="100" /></p>
</blockquote>
:ET