I"î@<h1 id="numpyë¡œ-ì§œë³´ëŠ”-neural-network-basic---7">Numpyë¡œ ì§œë³´ëŠ” Neural Network Basic - 7</h1>

<hr />
<h2 id="í•™ìŠµê´€ë ¨-ê¸°ìˆ -part-3">í•™ìŠµê´€ë ¨ ê¸°ìˆ  Part 3</h2>

<h3 id="ë°°ì¹˜-ì •ê·œí™”-batch-normalization">ë°°ì¹˜ ì •ê·œí™” (Batch Normalization)</h3>
<p>ë°°ì¹˜ ì •ê·œí™”ë€ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ í˜•í•©ì¸ <strong>$a$</strong> ê°’ì„ ì •ê·œí™”í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, ë¯¸ë‹ˆë°°ì¹˜ì— í•œí•´ì„œ ë°ì´í„° ë¶„í¬ê°€ í‰ê· ì´ 0 ë¶„ì‚°ì´ 1ì´ ë˜ë„ë¡ í•œë‹¤. ì´ëŠ” ë°ì´í„° ë¶„í¬ê°€ ëœ ì¹˜ìš°ì¹˜ê²Œ í•˜ëŠ” íš¨ê³¼ê°€ ìˆì–´ì„œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ê°’ì˜ ì˜í–¥ì„ ëœ ë°›ê²Œí•œë‹¤. ë˜í•œ, í•™ìŠµì†ë„ë¥¼ ì¦ê°€ì‹œí‚¤ê³  regularizer ì—­í• ì„ í•˜ì—¬ Overfittingì„ ë°©ì§€í•¨ìœ¼ë¡œ Dropoutì˜ í•„ìš”ì„±ì„ ì¤„ì¸ë‹¤. <del>ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì!</del></p>

<p>Paper: <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>

<p>ê¸°ë³¸ì ì¸ ì•„ì´ë””ì–´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. $D$ ì°¨ì›ì˜ ë¯¸ë‹ˆë°°ì¹˜ ë°ì´í„° $x = (x^{(1)}, \cdots, x^{(k)}, \cdots, x^{(D)})$ì— ëŒ€í•´ì„œ ê°ê°ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•œ í›„, ì •ê·œí™”ë¥¼ í†µí•´ ìƒˆë¡œìš´ $x^{(k)}$ ($\hat{x}^{(k)}$) ë¥¼ êµ¬í•œ í›„ì— Scaling($\gamma$) ê³¼ Shifting($\beta$)ì„ ê±°ì³ ìƒˆë¡œìš´ $y$ ë¥¼ ê¸°ì¡´ì˜ ì„ í˜•í•©ì„± ê³±ì¸ $a$ ë¥¼ ëŒ€ì‹ í•´ í™œì„±í™” í•¨ìˆ˜ì— ë„£ëŠ” ê²ƒì´ë‹¤.</p>

<p><img src="/assets/ML/nn/6/batch_norm_idea.png" alt="Drawing" style="width=500px" /></p>

<p>ë”°ë¼ì„œ, í•˜ë‚˜ì˜ Hidden Layer ëŠ” $Affine \rightarrow BatchNorm \rightarrow Activation$ ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.</p>

<h3 id="ë°°ì¹˜-ì •ê·œí™”ì˜-backpropogation-ì´í•´í•˜ê¸°">ë°°ì¹˜ ì •ê·œí™”ì˜ BackPropogation ì´í•´í•˜ê¸°</h3>

<p><img src="/assets/ML/nn/NN_batchnorm.png" alt="Drawing" /></p>

<h4 id="forward">Forward:</h4>

<p>x ë¶€í„° out ê¹Œì§€ ì°¨ê·¼ì°¨ê·¼ ì§„í–‰í•´ë³´ì. í—·ê°ˆë¦¬ì§€ ë§ì•„ì•¼í•  ì ì€ ìœ„ì— ê³µì‹ì—ì„œ $i$ ëŠ” batchë¥¼ iteration í•œ ê²ƒì´ë¼ëŠ” ì ì´ë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Forward Process
# step-1: mu (D,)
mu = x.mean(axis=0)
# step-2: xmu (N, D)
xmu = x - mu
# step-3: sq (N, D)
sq = xmu**2
# step-4: var (D,)
var = np.mean(sq, axis=0)
# step-5: std (D,)
std = np.sqrt(var + 1e-6)
# step-6: invstd (D,)
invstd = 1.0 / std
# step-7: xhat (N, D)
xhat = xmu * invstd
# step-8: scale (N, D)
scale = gamma * xhat
# step-9: out (N, D)
out = scale + beta
</code></pre></div></div>

<h4 id="backward">Backward:</h4>

<p>ìš°ë¦¬ì˜ ëª©í‘œëŠ” $\dfrac{\partial L}{\partial x}, \dfrac{\partial L}{\partial \gamma}, \dfrac{\partial L}{\partial \beta}$ ë¥¼ êµ¬í•´ì„œ, $\dfrac{\partial L}{\partial x}$ ëŠ” Affine Layerë¡œ ì—­ì „íŒŒ ì‹œí‚¤ê³  $\gamma, \beta$ ëŠ” í•™ìŠµ ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.</p>

<p><strong>Step-9:</strong></p>

<p>Forward : $out(scale, \beta) = scale + \beta$</p>

<ul>
  <li>ë”í•˜ê¸° ë…¸ë“œì˜ ì—­ì „íŒŒëŠ” ê·¸ëŒ€ë¡œ í˜ëŸ¬ê°„ë‹¤.</li>
</ul>

\[\begin{cases} dscale = \dfrac{\partial L}{\partial scale} = \dfrac{\partial L}{\partial out} \dfrac{\partial out}{\partial scale} = 1 * dout \\
\\
d\beta = \dfrac{\partial L}{\partial \beta} = \dfrac{\partial L}{\partial out} \dfrac{\partial out}{\partial \beta} = 1 * \sum_i^N dout \end{cases}\]

<p><strong>Step-8:</strong></p>

<p>Forward : $scale(\gamma, \hat{x}_i) = \gamma \ * \ \hat{x}_i$</p>

<ul>
  <li>ê³±ì˜ ë…¸ë“œì˜ ì—­ì „íŒŒëŠ” ë“¤ì–´ì™”ë˜ ì‹ í˜¸ë¥¼ ì—­ìœ¼ë¡œ ê³±í•´ì„œ í˜ë ¤ ë³´ë‚¸ë‹¤.</li>
</ul>

\[\begin{cases}
d\hat{x}_i = \dfrac{\partial L}{\partial \hat{x}_i} = \dfrac{\partial L}{\partial scale} \dfrac{\partial scale}{\partial \hat{x}_i} = 1 * \sum_i^N dout \\
\\
d\gamma = \dfrac{\partial L}{\partial \gamma} = \dfrac{\partial L}{\partial scale} \dfrac{\partial scale}{\partial \gamma} = \sum_i^N dout \ * \ \hat{x}_i
\end{cases}\]

<p><strong>Step-7:</strong></p>

<p>Forward : $\hat{x}_i(xmu, invstd) = xmu \ * \ invstd$</p>

<ul>
  <li>$xmu$ëŠ” ìœ—ìª½(step-7 $\rightarrow$ step-2)ê³¼ ì•„ë˜ìª½(step-3 $\rightarrow$ step-2) ìœ¼ë¡œ ë‘ ë²ˆ ëŒì•„ê°€ê¸° ë•Œë¬¸ì— ì²¨ìë¥¼ ë‹¨ë‹¤.</li>
</ul>

\[\begin{cases}
dxmu_1= \dfrac{\partial L}{\partial xmu_1} = \dfrac{\partial L}{\partial \hat{x}_i} \dfrac{\partial \hat{x}_i}{\partial xmu_1} = d\hat{x}_i \ * \ invstd \\
\\
dinvstd = \dfrac{\partial L}{\partial \hat{x}_i} = \dfrac{\partial L}{\partial \hat{x}_i} \dfrac{\partial \hat{x}_i}{\partial invstd} = d\hat{x}_i \ * \ xmu
\end{cases}\]

<p><strong>Step-6:</strong></p>

<p>Forward : $invstd(\sigma) = \dfrac{1}{\sigma}$</p>

<ul>
  <li>$f(x) = \dfrac{1}{x}$ ì˜ ë¯¸ë¶„ì€ $fâ€™(x) = -\dfrac{1}{x^2} = -f(x)^2$ ì´ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

\[d\sigma = \dfrac{\partial L}{\partial \sigma} = \dfrac{\partial L}{\partial invstd} \dfrac{\partial invstd}{\partial \sigma} = dinvstd \ * \ (-invstd^2)\]

<p><strong>Step-5:</strong></p>

<p>Forward : $\sigma(var) = \sqrt{var + \epsilon}$</p>

<ul>
  <li>$f(x) = \sqrt{x + \epsilon}$ ì˜ ë¯¸ë¶„ì€ $fâ€™(x) = -\dfrac{1}{2}(x+\epsilon)^{-\frac{1}{2}}$ ì´ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

\[dvar = \dfrac{\partial L}{\partial var} = \dfrac{\partial L}{\partial \sigma} \dfrac{\partial \sigma}{\partial var} = d\sigma \ * \ (-\dfrac{1}{2}(var+\epsilon)^{-\frac{1}{2}})\]

<p><strong>Step-4:</strong></p>

<p>Forward : $var(sq) = \dfrac{1}{N} \sum_i^N sq$</p>

<ul>
  <li>$f(x) = \dfrac{1}{N} \sum_i^N x_i$ ì˜ ë¯¸ë¶„ì€ $fâ€™(x) = \dfrac{1}{N} \sum_i^N 1$ ì´ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ë‹¤. ë‹¨, xì˜ í˜•ìƒ(shape)ì´ ê°™ì•„ì•¼í•œë‹¤.</li>
</ul>

\[dsq = \dfrac{\partial L}{\partial sq} = \dfrac{\partial L}{\partial var} \dfrac{\partial var}{\partial sq} = \dfrac{1}{N} dvar \ * \ \begin{bmatrix} 1 &amp; \cdots &amp; 1 \\ \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; \cdots &amp; 1 \end{bmatrix}_{(N, D)} = \dfrac{1}{N} dvar \ * \ ones(N, D)\]

<p><strong>Step-3:</strong></p>

<p>Forward : $sq = xmu^2$</p>

<ul>
  <li>$f(x) = x^2$ ì˜ ë¯¸ë¶„ì€ $fâ€™(x) = 2x$ ì´ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

\[dxmu_2 = \dfrac{\partial L}{\partial xmu_2} = \dfrac{\partial L}{\partial sq} \dfrac{\partial sq}{\partial xmu_2} = dsq \ * \ 2 \ xmu\]

<p><strong>Step-2:</strong></p>

<p>Forward : $xmu = x_i - \mu$</p>

<ul>
  <li>$dxmu = dxmu_1 + dxmu_2$ ë¡œ ì •ì˜ ëœë‹¤. ê³±ì˜ ë¯¸ë¶„ ë²•ì¹™ ìƒê°í•´ë³´ë©´ ëœë‹¤. $h(x) = f(x) g(x)$ ë¥¼ $x$ ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•˜ë©´ $fâ€™(x)g(x) + f(x)gâ€™(x)$ ê¸° ë•Œë¬¸ì´ë‹¤. <br />
ë˜í•œ ì´ê²ƒë„ ë§ì…ˆê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ê·¸ëŒ€ë¡œ í˜ëŸ¬ ë³´ë‚´ëŠ”ë‹¤ ë°‘ì— ìª½ì€ -1 ì„ ê³±í•´ì„œ í˜ë ¤ ë³´ë‚¸ë‹¤.</li>
</ul>

\[\begin{cases}
dx_1= \dfrac{\partial L}{\partial x_1} = \dfrac{\partial L}{\partial xmu} \dfrac{\partial xmu}{\partial x_1} = dmu \ * \ 1 \\
\\
d\mu = \dfrac{\partial L}{\partial \mu} = \dfrac{\partial L}{\partial xmu} \dfrac{\partial xmu}{\partial \mu} = \sum_i^N dxmu \ * \ (-1)
\end{cases}\]

<p><strong>Step-1:</strong></p>

<p>Forward : $\mu = \dfrac{1}{N} \sum_i^N x_i$</p>

<ul>
  <li>step-4ì—ì„œ ì„¤ëª…í–ˆë‹¤.</li>
</ul>

\[dx_2 = \dfrac{\partial L}{\partial x_2} = \dfrac{\partial L}{\partial \mu} \dfrac{\partial \mu}{\partial x_2} = \dfrac{1}{N} d\mu \ * \ \begin{bmatrix} 1 &amp; \cdots &amp; 1 \\ \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; \cdots &amp; 1 \end{bmatrix}_{(N, D)} = \dfrac{1}{N} d\mu \ * \ ones(N, D)\]

<p><strong>Step-0:</strong></p>
<ul>
  <li>ìµœì¢…ì ìœ¼ë¡œ êµ¬í•˜ëŠ” $dx = \dfrac{\partial L}{\partial x} = dx_1 + dx_2$ ë¡œ ì •ì˜ ëœë‹¤.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Backward Process
# step-9: out = scale + beta
dbeta = dout.sum(axis=0)
dscale = dout
# step-8: scale = gamma * xhat
dgamma = np.sum(xhat * dout, axis=0)
dxhat = gamma * dscale
# step-7: xhat = xmu * invstd
dxmu1 = dxhat * invstd
dinvstd = np.sum(dxhat * xmu, axis=0)
# step-6: invstd = 1 / std
dstd = dinvstd * (-invstd**2)
# step-5: std = np.sqrt(var + 1e-6)
dvar = -0.5 * dstd * (1 / np.sqrt(var + 1e-6))
# step-4: var = sum(sq)
dsq = (1.0 / batch_size) * np.ones(input_shape) * dvar
# step-3: sq = xmu**2
dxmu2 = dsq * 2 * xmu
# step-2: xmu = x - mu
dxmu = dxmu1 + dxmu2
dmu = -1 * np.sum(dxmu, axis=0)
dx1 = dxmu * 1
# step-1: mu = mean(x)
dx2 = (1.0 / batch_size) * np.ones(input_shape) * dmu
# step-0:
dx = dx1 + dx2
</code></pre></div></div>

<h4 id="ì‹¤ì œ-êµ¬í˜„">ì‹¤ì œ êµ¬í˜„</h4>
<p>ê·¸ëŸ¬ë‚˜ ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” training ê³¼ testingì„ ë‚˜ëˆ ì„œ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤.</p>

<p><img src="/assets/ML/nn/6/batch_norm_al.png" alt="Drawing" style="width=500px" /></p>

<hr />
<h3 id="ì²¨ë¶€-backpropogation-ì „ì²´-ë¯¸ë¶„-ìˆ˜í•™ì‹">ì²¨ë¶€: Backpropogation ì „ì²´ ë¯¸ë¶„ ìˆ˜í•™ì‹</h3>

<ul>
  <li>ìˆ˜ì‹ì˜ ì´í•´ëŠ” ì´ë¶„ì˜ ë¸”ë¡œê·¸ì—ì„œ ë§ì€ ì°¸ì¡°ë¥¼ í–ˆë‹¤. Blog: [<a href="http://cthorey.github.io/backpropagation/">Clement Thorey</a>]</li>
</ul>

\[\begin{aligned}
Y &amp;= \gamma \hat{X} + \beta \\
\hat{X} &amp;= (X - \mu)(\sigma^2+\epsilon)^{-1/2}
\end{aligned}\]

<p><strong>size:</strong></p>

\[\begin{aligned}
Y, \hat{X}, X &amp;= (N, D) \\
\mu, \sigma, \gamma, \beta &amp;= (D,)
\end{aligned}\]

<p><br /></p>

<p>$N$ì€ ë¯¸ë‹ˆ ë°°ì¹˜ ì‹¸ì´ì¦ˆê³ , $D$ëŠ” ë°ì´í„°ì˜ ì°¨ì› ìˆ˜ë‹¤.</p>

<p>Matrix ë¡œ ì •ì˜í•œ ìˆ˜ì‹ì„ ë‹¤ì‹œ ì›ì†Œë³„ë¡œ í‘œê¸°ë¥¼ ì •ì˜ í•´ë³´ì. ë§¤íŠ¸ë¦­ìŠ¤ $Y, X, \hat{X}$ ì™€ ë²¡í„° $\gamma, \beta$ ê·¸ë¦¬ê³  ìœ„ì— ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ë‹¤ì‹œ ì •ì˜ í•´ë³¼ ìˆ˜ ìˆë‹¤. (ì™œ ë§¤íŠ¸ë¦­ìŠ¤ì™€ ë²¡í„°ì¸ì§€ëŠ” Forward ê³¼ì •ì— ë‚˜ì™€ìˆë‹¤. ê° ì°¨ì›ë³„ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•˜ëŠ”ê±¸ ìŠì§€ë§ì)</p>

\[\begin{aligned}
y_{kl} &amp;= \gamma_l \hat{x}_{kl} + \beta_l \\
\hat{x}_{kl} &amp;= (x_{kl} - \mu_l)(\sigma_l^2+\epsilon)^{-1/2}
\end{aligned}\]

\[where\quad \mu_l = \dfrac{1}{N} \sum_{p=1}^{N} x_{pl} , \quad \sigma_l^2 = \dfrac{1}{N} \sum_{p=1}^{N} (x_{pl}-\mu_l)^2\]

\[with\quad k = [1, \cdots, N] \ ,\  l = [1, \cdots, D]\]

<p><br /></p>

<p>ì´ì œ ìš°ë¦¬ê³  êµ¬í•˜ë ¤ê³  í•˜ëŠ” ë¯¸ë¶„ ê°’ë“¤$(\dfrac{\partial L}{\partial x}, \dfrac{\partial L}{\partial \gamma}, \dfrac{\partial L}{\partial \beta})$ì„ í•˜ë‚˜ì”© êµ¬í•´ë³´ì.</p>

<h4 id="x_ij-ì—-ëŒ€í•œ-ë¯¸ë¶„">$x_{ij}$ ì— ëŒ€í•œ ë¯¸ë¶„</h4>

\[\begin{aligned}\dfrac{\partial L}{\partial x_{ij}}
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial x_{ij}} \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \hat{x}_{kl}} \dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \cdot \gamma_l \cdot \dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} \end{aligned}\]

<p><br /></p>

<p>\(\dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} = \dfrac{\partial f}{\partial {x}_{ij}} g + f \dfrac{\partial g}{\partial {x}_{ij}} \quad where \quad \begin{cases} f = (x_{kl} - \mu_l) \\ g = (\sigma_l^2+\epsilon)^{-1/2} \end{cases}\) ì— ëŒ€í•œ ë¯¸ë¶„ì„ êµ¬í•´ë³´ì.</p>

<ul>
  <li>ìš°ì„  ë¶„ì $f = (x_{kl} - \mu_l)$ ì— ëŒ€í•œ ë¯¸ë¶„ì„ í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

\[\dfrac{\partial f}{\partial {x}_{ij}} = \delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl}\]

<p><br /></p>

\[\delta_{m,n} = \begin{cases} 1 \quad where \quad m = n \\ 0 \quad otherwise \end{cases}\]

<p><br /></p>

<p>$\delta_{m,n}$ ì€ ì•ì²¨ì $m$ ì´ ë’·ì²¨ì $n$ê³¼ ê°™ë‹¤ë©´ 1ì´ ëœë‹¤ëŠ” ëœ»ì´ë‹¤.</p>

<p>ì¦‰, ì—¬ê¸°ì„œ $i$ ê°€ $[1 \cdots k \cdots D]$ ê¹Œì§€, $j$ ê°€ $[1 \cdots l \cdots D]$ ê¹Œì§€ iteration í•  ê²ƒì¸ë°, ì˜¤ì§ $i=k, j=l$ ì¼ë•Œë§Œ ì• í•­ì¸ $\delta_{il} \delta_{jl} = 1$ ì´ ë  ê²ƒì´ê³ , $j=l$ ì¼ë•Œë§Œ ë’·í•­ì¸ $\frac{1}{N} \delta_{jl} = \frac{1}{N}$ ì´ ë  ê²ƒì´ë‹¤.</p>

<ul>
  <li>ë¶„ëª¨ $g = (\sigma_l^2+\epsilon)^{-1/2}$ ì— ëŒ€í•œ ë¯¸ë¶„ì€ ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

\[\dfrac{\partial g}{\partial {x}_{ij}} = -\dfrac{1}{2}(\sigma_l^2 + \epsilon)^{-3/2} \dfrac{\partial \sigma_l^2}{\partial x_{ij}}\]

<p><br /></p>

\[\begin{aligned} where \quad \sigma_l^2
&amp;= \dfrac{1}{N} \sum_{p=1}^{N} (x_{pl}-\mu_l)^2 \\
\dfrac{\partial \sigma_l^2}{\partial x_{ij}}
&amp;= \dfrac{1}{N} \sum_{p=1}^{N} 2(x_{pl}-\mu_l)(\delta_{ip} \delta_{jl} - \frac{1}{N} \delta_{jl}) \\
&amp;= \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl} - \dfrac{2}{N^2} \sum_{p=1}^N (x_{pl}-\mu_l) \delta_{jl} \\
&amp; = \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl} - \dfrac{2}{N} \delta_{jl} (\dfrac{1}{N}  \sum_{p=1}^N  (x_{pl}-\mu_l)) \cdots (1) \\
&amp; = \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl}
\end{aligned}\]

<p><br /></p>

<p>(1) ë²ˆ ì‹ì„ ì ê¹ ì´ì•¼ê¸° í•˜ë©´ $\dfrac{1}{N} \sum_{p=1}^N  (x_{pl}-\mu_l) = 0$ ì¸ê²ƒì€ ì–´ë–¤ ê°’ë“¤ì„ í‰ê· ì„ ë¹¼ê³  ë‹¤ì‹œ í‰ê·  ì‹œí‚¤ë©´ 0ì´ ëœë‹¤.</p>

<p>$e.g)\quad \frac{(1-2)+(2-2)+(3-2)}{3}=0$</p>

<p>ì´ì œ ë“œë””ì–´ \(\dfrac{\hat{x}_{kl}}{\partial {x}_{ij}}\) ì— ëŒ€í•´ êµ¬í• ìˆ˜ ìˆë‹¤. ê³±ì˜ ë¯¸ë¶„ ë²•ì¹™ì„ ì‚¬ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì „ê°œ ëœë‹¤.</p>

\[\begin{aligned} \dfrac{\hat{x}_{kl}}{\partial {x}_{ij}}
&amp;= (\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2}  -\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl} \\
\end{aligned}\]

<p>ìµœì¢…ì ìœ¼ë¡œ ìš°ë¦¬ì˜ ëª©ì  \(\dfrac{\partial L}{\partial x_{ij}}\) ë¥¼ êµ¬í•´ë³´ì.</p>

<p><br /></p>

\[\begin{aligned}\dfrac{\partial L}{\partial x_{ij}}
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \cdot \gamma_l \cdot [(\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2} - \dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [(\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2}] - \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l (\delta_{ik} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l \delta_{jl}(\sigma_l^2+\epsilon)^{-1/2} - \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&amp;= \dfrac{\partial L}{\partial y_{ij}} \gamma_l \delta_{ii} \delta_{jj} (\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_k \dfrac{\partial L}{\partial y_{kj}} \gamma_l \delta_{jj}(\sigma_j^2+\epsilon)^{-1/2} - \dfrac{1}{N} \sum_{k} \dfrac{\partial L}{\partial y_{kj}} \gamma_l [ (x_{kj} - \mu_j)(\sigma_j^2 + \epsilon)^{-3/2} (x_{ij}-\mu_j) \delta_{jj}] \cdots (2) \\
&amp;= \dfrac{\partial L}{\partial y_{ij}} \gamma_l (\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_k \dfrac{\partial L}{\partial y_{kj}} \gamma_l (\sigma_j^2+\epsilon)^{-1/2} - \dfrac{1}{N} \sum_{k} \dfrac{\partial L}{\partial y_{kj}} \gamma_l (x_{kj} - \mu_j)(\sigma_j^2 + \epsilon)^{-3/2} (x_{ij}-\mu_j) \\
&amp;= \dfrac{1}{N} \gamma_l (\sigma_l^2+\epsilon)^{-1/2} [N \dfrac{\partial L}{\partial y_{ij}} - \sum_k \dfrac{\partial L}{\partial y_{kj}} - (x_{ij}-\mu_j)(\sigma_j^2 + \epsilon)^{-1} \sum_{k} \dfrac{\partial L}{\partial y_{kj}}(x_{kj} - \mu_j)]
\end{aligned}\]

<p><br /></p>

<p>(2) ë²ˆ ì‹ìœ¼ë¡œ ë„ì¶œ ë˜ëŠ” ê³¼ì •ì„ ì˜ ì‚´í´ë³´ë©´, ê° í•­ë§ˆë‹¤ ê³±ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì²«ë²ˆì§¸ í•­ì€ $\sum_{k, l}$ ì—ì„œ ì˜¤ì§ $k=i, l=j$ ì¼ë•Œ ë‚¨ì•„ ìˆê³  ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ë‹¤ 0 ì´ê³ , ë‘ë²ˆì§¸ í•­ì€ ì˜¤ì§ $l=j$ ì¼ë•Œ ë‚¨ì•„ìˆê³  ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ë‹¤ 0 ì´ë‹¤. ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ë„ ë§ˆì¹œê°€ì§€ë¡œ $l=j$ ì¼ë•Œë§Œ ë‚¨ì•„ìˆëŠ”ë‹¤.</p>

<h4 id="gamma_j-ì—-ëŒ€í•œ-ë¯¸ë¶„">$\gamma_j$ ì— ëŒ€í•œ ë¯¸ë¶„</h4>

<p>ìœ„ì— ê¹Œì§€ ì´í•´í–ˆìœ¼ë©´ $\gamma_l$ ì— ëŒ€í•œ ë¯¸ë¶„ì€ ê°„ë‹¨í•˜ë‹¤.</p>

\[\begin{aligned}\dfrac{\partial L}{\partial \gamma_j}
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \gamma_j} \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \hat{x}_{kl} \delta_{jl} \\
&amp;= \sum_k \dfrac{\partial L}{\partial y_{kj}} \hat{x}_{kj} \\
&amp;= \sum_k \dfrac{\partial L}{\partial y_{kj}} (x_{kj} - \mu_j)(\sigma_j^2+\epsilon)^{-1/2}
\end{aligned}\]

<h4 id="beta_j-ì—-ëŒ€í•œ-ë¯¸ë¶„">$\beta_j$ ì— ëŒ€í•œ ë¯¸ë¶„</h4>

\[\begin{aligned}\dfrac{\partial L}{\partial \beta_j}
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \gamma_j} \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \delta_{jl} \\
&amp;= \sum_k \dfrac{\partial L}{\partial y_{kj}}
\end{aligned}\]

<p><br /></p>

<p>ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì™œ ìœ„ì— step-9, 8 ì½”ë“œ êµ¬í˜„ì—ì„œ dgammaì™€ dbetaë¥¼ summation í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ë‹¤ìŒ ë§ˆì§€ë§‰ ì‹œê°„ì—ëŠ” ëª¨ë“ ê±¸ ì¢…í•©í•´ì„œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì½”ë“œë¡œ ì‚´í´ë³´ì.</p>
:ET