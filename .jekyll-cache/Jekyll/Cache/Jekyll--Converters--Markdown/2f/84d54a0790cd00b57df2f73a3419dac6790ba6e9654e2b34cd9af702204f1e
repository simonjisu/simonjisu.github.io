I"£=<h1 id="numpyë¡œ-ì§œë³´ëŠ”-neural-network-basic---2">Numpyë¡œ ì§œë³´ëŠ” Neural Network Basic - 2</h1>
<hr />

<p>ì €ë²ˆ ì‹œê°„ì—ëŠ” Neural Networkì˜ ê¸°ë³¸ì´ ë˜ì—ˆë˜ XOR ë¬¸ì œë¥¼ í’€ì–´ë³´ì•˜ë‹¤. ê·¸ ê³¼ì •ì—ì„œ Feedforward í¼ì…‰íŠ¸ë¡ ì´ë¼ëŠ” ê°œë…ì´ ë“±ì¥í–ˆê³  ì˜¤ëŠ˜ì€ Feedforward ê³¼ì •ì´ ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ ì•Œì•„ë³´ì.</p>

<h2 id="í™œì„±í™”-í•¨ìˆ˜activation-function">í™œì„±í™” í•¨ìˆ˜(Activation Function)</h2>

<p><img src="/assets/ML/nn/NN.jpg" alt="Drawing" style="width: 350px;" /></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} a &=b + w_1x_1 + w_2x_2 \\ y &= h(a)
\end{aligned} %]]></script>

<p>ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì˜ ê³¼ì •ì„ ë‹´ì€ ê·¸ë¦¼ê³¼ ìˆ˜ì‹ì´ë‹¤. ê° ë‰´ëŸ°(ë™ê·¸ë¼ë¯¸ë“¤)ì—ì„œ ë‹¤ìŒ ì¸µì˜ ë‰´ëŸ°(ì˜¤ë¥¸ìª½ í° ë™ê·¸ë¼ë¯¸)ë¡œ ì§„í–‰í•˜ëŠ”ë° ìš°ì„  ê° $x$ ë¥¼ ê°€ì¤‘í•© $a$ ë¥¼ êµ¬í•˜ê³ , ê·¸ í•©ì„ ë‹¤ì‹œ ì–´ë–¤ í•¨ìˆ˜ $h$ ë¥¼ ê±°ì³ Outputì¸ $y$ ê°€ ë‚˜ì˜¤ê²Œ ëœë‹¤.</p>

<p>ì €ë²ˆ ì‹œê°„ì— ì´ì•¼ê¸° í–ˆë˜ ANDë¥¼ ì ìš©í•´ë³´ë©´ $h$ ëŠ” 0ê³¼ 1ì„ ë°˜í™˜í•˜ëŠ” ì•„ë˜ì™€ ê°™ì€ í•¨ìˆ˜ì¼ ê²ƒì´ë‹¤.</p>

<script type="math/tex; mode=display">y = h(a) =
  \begin{cases}
  0\ \ (a = b + w_1x_1 + w_2x_2 \leq 0) \\
  1\ \ (a = b + w_1x_1 + w_2x_2 > 0)
  \end{cases}</script>

<p>ì´ëŸ° $h$ í•¨ìˆ˜ë¥¼ <strong>í™œì„±í™” í•¨ìˆ˜</strong> ë¼ê³  ë¶€ë¥´ë©°, ë³´í†µ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì“´ë‹¤.</p>

<h3 id="ì™œ-ë¹„ì„ í˜•ì„-ì“°ëŠ”ê°€">ì™œ ë¹„ì„ í˜•ì„ ì“°ëŠ”ê°€?</h3>
<p>ìš°ë¦¬ëŠ” í™œì„±í™” í•¨ìˆ˜ë¥¼ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì¸ë° ë¹„ì„ í˜•í•¨ìˆ˜ê°€ ì í•©í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì„ í˜•í•¨ìˆ˜ì˜ íŠ¹ì§•ì€ ë” í•´ë„ ì„ í˜•ì´ë¼ëŠ” ê²ƒì¸ë°, ì˜ˆë¥¼ ë“¤ì–´</p>

<script type="math/tex; mode=display">y=3x</script>

<p>ë¼ëŠ” ì„ í˜• í•¨ìˆ˜ì™€ ê°„ë‹¨í•œ ì‚°ìˆ˜ì¸ $1+5=6$ ì´ë€ ê²ƒì„ ìƒê°í•´ë³´ì. ì¢Œë³€ì— $1$ê³¼ $5$ë¥¼ ì„ í˜•í•¨ìˆ˜ì— ë„£ì–´ì„œ ë”í•œ ê°’ì¸ $3 + 15 = 18$ ì´ë€ ê°’ì„ ìš°ë¦¬ëŠ” ìš°ë³€ì˜ $6$ ì„ ì„ í˜•í•¨ìˆ˜ì— ë„£ì—ˆì„ ë•Œ ê°’ì´ë‘ ê°™ë‹¤ëŠ” ê²ƒì„ ì¶©ë¶„íˆ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë¥¼ â€œ<strong>ì˜ˆì¸¡</strong> í•  ìˆ˜ ìˆë‹¤.â€ ë¼ê³  ì´ì•¼ê¸° í•œë‹¤. ì´ì²˜ëŸ¼ ë‘ ê°œì˜ ì„ í˜•í•¨ìˆ˜ë¥¼ ë”í•˜ë©´ ì„ í˜•ì´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>í•˜ì§€ë§Œ ì–´ë–¤ ë¹„ì„ í˜• í•¨ìˆ˜</p>

<p><script type="math/tex">y=3x^2</script>
ë¡œ ì•„ê¹Œì˜ ê³¼ì •ì„ ë˜‘ê°™ì´ í•´ë³´ì, ì¢Œë³€ì˜ ê°’ì„ ë„£ì–´ì„œ ë”í•˜ë©´ $3 + 75 = 78$ ì¸ë°, ìš°ë³€ì˜ ê°’ì„ ë„£ìœ¼ë©´ $108$ ì´ ëœë‹¤. ë”°ë¼ì„œ ë¹„ì„ í˜• í•¨ìˆ˜ëŠ” ë”í•´ë„ ê°™ì€ ë¹„ì„ í˜•ì´ ì•„ë‹ˆë©° ë‹¤ë¥¸ ê°’ì´ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ë‹¤ ë¼ê³  ë§ í•  ìˆ˜ ìˆë‹¤.</p>

<p>ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ í†µê³¼í•¨ìœ¼ë¡œì„œ ë‰´ëŸ°ì´ <strong>í™œì„±í™”</strong> ëœë‹¤ë¼ê³  ì´ì•¼ê¸° í•œë‹¤.</p>

<p>ë˜ í•œ ê°€ì§€ ì´ìœ ë¥¼ ë“¤ìë©´, ì„ í˜•ì¼ ê²½ìš°ì— ì—¬ëŸ¬ ì¸µì„ ìŒ“ëŠ” ì´ìœ ê°€ ì—†ì–´ì§„ë‹¤. ë§Œì•½ì— $h=c\cdot a$ ê°€ ì„ í˜•ì¸ í•¨ìˆ˜ ì˜€ë‹¤ë©´ ì—¬ëŸ¬ ì¸µì„ ê±°ì¹˜ê²Œ ë˜ë©´ $h(h(h(a))) = c\cdot c\cdot c\cdot a$ ì¸ë° ì´ëŠ” ê²°êµ­ $b\cdot a=c^3\cdot a$ ë¼ëŠ” ì„ í˜•í•¨ìˆ˜ë¡œ ë°”ê¿€ ìˆ˜ ìˆì–´ì„œ ì—¬ì „íˆ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<p>ë” ìì„¸í•œ ë¹„ì„ í˜•í•¨ìˆ˜ì™€ ì„ í˜•í•¨ìˆ˜ì˜ ì°¨ì´ëŠ” ë§í¬ì˜ ë¸”ë¡œê·¸ë¥¼ ì°¸ì¡°í•´ë³´ì [<a href="http://sdolnote.tistory.com/entry/LinearityNonlinearityFunction">ë§í¬</a>]</p>

<h3 id="ìì£¼-ì“°ëŠ”-í™œì„±í™”-í•¨ìˆ˜ë“¤">ìì£¼ ì“°ëŠ” í™œì„±í™” í•¨ìˆ˜ë“¤</h3>

<h4 id="ê³„ë‹¨-í•¨ìˆ˜step-function">ê³„ë‹¨ í•¨ìˆ˜(Step Function)</h4>

<p><script type="math/tex">h(x) =
  \begin{cases}
  1\ \ (x > 0) \\
  0\ \ (x  \leq 0)
\end{cases}</script></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def step_function(x):
    y = x &gt; 0
    return y.astype(np.int)
</code></pre></div></div>
<p><img src="/assets/ML/nn/step.png" alt="Drawing" style="width: 350px;" /></p>

<h4 id="ì‹œê·¸ëª¨ì´ë“œ-í•¨ìˆ˜sigmoid-function">ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜(Sigmoid Function)</h4>

<p><script type="math/tex">h(x) = \frac{1}{1+exp(-x)}</script></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def sigmoid(x):
    return 1 / (1 + np.exp(-x))
</code></pre></div></div>
<p><img src="/assets/ML/nn/sigmoid.png" alt="Drawing" style="width: 350px;" /></p>

<h4 id="relu-í•¨ìˆ˜relu-function">ReLu í•¨ìˆ˜(ReLu Function)</h4>

<p><script type="math/tex">h(x) =
  \begin{cases}
  x\ \ (x > 0) \\
  0\ \ (x \leq 0)
  \end{cases}</script></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def ReLu(x):
    return np.maximum(0, x)
</code></pre></div></div>
<p><img src="/assets/ML/nn/relu.png" alt="Drawing" style="width: 350px;" /></p>

<h2 id="feedforward-ê³¼ì •">Feedforward ê³¼ì •</h2>

<p><img src="/assets/ML/nn/NN_2.jpg" alt="Drawing" style="width: 500px;" /></p>

<p>(ì‚¬ì§„ì¶œì²˜: ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹)</p>

<p>ìœ„ì™€ ê°™ì€ Neural Networkì˜ ê³¼ì •ì„ í•œë²ˆ ì‚´í´ ë³´ì, ì—¬ëŸ¬ê°œì˜ Perceptronì„ ìŒ“ìœ¼ë©´ ì´ëŸ° ëª¨ì–‘ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì œì¼ ì™¼ìª½ì— ìˆëŠ” $x_1, x_2$ 2ê°œì˜ ë‰´ëŸ°ì„ í•œ ì¸µìœ¼ë¡œ ë³´ë©°, ì´ë¥¼ ì…ë ¥ì¸µ(Input Layer)ë¼ê³  í•œë‹¤. (1 ì´ë€ ë‰´ëŸ°ì€ ë§¤ë²ˆ ë‰´ëŸ°ì„ ê±°ì¹  ë•Œë§ˆë‹¤ ë” í•´ì£¼ëŠ” ìˆ«ìê¸° ë•Œë¬¸ì— ì•ìœ¼ë¡œë„ í•œë²ˆë§Œ ì“°ë„ë¡ í•œë‹¤.) ë§ˆì°¬ê°€ì§€ë¡œ ì¤‘ê°„ì— ë‘ ê°œì˜ ì¸µì„ ì€ë‹‰ì¸µ(Hidden Layer)ì´ë¼ê³  í•˜ë©° ë§ˆì§€ë§‰ì„ ì¶œë ¥ì¸µ(Output Layer)ì´ë¼ê³  í•œë‹¤. ë³´í†µ ì…ë ¥ì¸µì€ ê°¯ìˆ˜ë¡œ ì•ˆì„¸ë©° ìœ„ ê·¸ë¦¼ì€ ì´ 3ì¸µì¸ Neural Network ë¼ê³  í•  ìˆ˜ ìˆë‹¤.</p>

<p>ë‹¨ê³„ë³„ë¡œ ì‚´í´ë³´ì.</p>

<h3 id="input-rightarrow-hidden-1">Input $\rightarrow$ Hidden 1</h3>

<p>Inputì—ì„œ Hidden1 ì¸µìœ¼ë¡œ ê°€ëŠ” ê³¼ì •ì„ í–‰ë ¬ë¡œ í‘œì‹œí•´ë³¼ ê²ƒì´ë‹¤.</p>

<p>ê°€ì¤‘ì¹˜ $w$ ì˜ í‘œê¸°ë²•ì€ $w_{ì˜¤ë¥¸ìª½\ ë‰´ëŸ°ìœ„ì¹˜,\ ì™¼ìª½\ ë‰´ëŸ°ìœ„ì¹˜}^{ëª‡ë²ˆì§¸\ ì¸µ}$ ë¡œì¨,</p>

<p>ì²«ë²ˆì§¸ ì¸µì—ì„œ ì…ë ¥ì¸µ $x_1$ ë‰´ëŸ°ì—ì„œ íˆë“ ì¸µ1 $a_2^{(1)}$ ë°©í–¥ì¸ ê°€ì¤‘ì¹˜ëŠ” $w_{21}^{(1)}$ ì´ë¼ê³  í‘œê¸°í•œë‹¤.</p>

<p>ë”°ë¼ì„œ ê°ê°ì˜ ê°€ì¤‘ì¹˜ í•©ì„ êµ¬í•˜ë©´,</p>

<script type="math/tex; mode=display">A = \begin{bmatrix}
    a_1^{(1)} \\
    a_2^{(1)} \\
    a_3^{(1)}
    \end{bmatrix} =
    \begin{bmatrix}
    w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)} \\
    w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)} \\
    w_{31}^{(1)}x_1 + w_{32}^{(1)}x_2 + b_3^{(1)}
\end{bmatrix}</script>

<p>ê°€ ë˜ëŠ”ë°, ì´ë¥¼ ë‹¤ì‹œ ê°„ë‹¨í•˜ê²Œ ì“°ë©´</p>

<script type="math/tex; mode=display">X =
  \begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}</script>

<script type="math/tex; mode=display">% <![CDATA[
W^{(1)} =
      \begin{bmatrix}
      w_{11}^{(1)} & w_{12}^{(1)} \\
      w_{21}^{(1)} & w_{22}^{(1)} \\
      w_{31}^{(1)} & w_{32}^{(1)}
      \end{bmatrix} %]]></script>

<script type="math/tex; mode=display">B^{(1)} =
  \begin{bmatrix}
  b_1^{(1)} \\
  b_2^{(1)} \\
  b_3^{(1)}
  \end{bmatrix}</script>

<p>$A = W^{(1)} \cdot X + B^{(1)}$ ê°€ ë˜ë©°, í˜•íƒœëŠ” $(3, 1) = (3, 2) \times (2, 1) + (3, 1)$ ë¡œ ëœë‹¤. ì´ëŠ” ê°„ë‹¨í•œ ë‚´ì  ì—°ì‚°ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤.</p>

<p>ê°€ì¤‘ì¹˜ì˜ í•©ì„ êµ¬í•˜ë©´ ì´ì œ ë¹„í™œì„±í•¨ìˆ˜ì— ëŒ€ì…í•´ì„œ ë‰´ëŸ°ì„ í™œì„±í™” ì‹œí‚¨ë‹¤.</p>

<script type="math/tex; mode=display">Z^{(1)} =
    \begin{bmatrix}
    z_1^{(1)} \\
    z_2^{(1)} \\
    z_3^{(1)}
    \end{bmatrix} =
    \begin{bmatrix}
    h(a_1^{(1)}) \\
    h(a_2^{(1)}) \\
    h(a_3^{(1)})
    \end{bmatrix}</script>

<p>ì´ë ‡ê²Œ ë‚˜ì˜¨ $Z^{(1)}$ ê°’ë“¤ì€ ë‹¤ìŒ ì¸µì—ì„œ ì…ë ¥ìœ¼ë¡œ ì“°ì´ê²Œ ëœë‹¤.</p>

<p>ì•„ë˜ ì½”ë“œì˜ Shapeë„ ê°™ì´ ì˜ ì‚´í´ë³´ì.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.2],
               [0.3, 0.4],
               [0.5, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print('X:', X.shape)
print('W1:', W1.shape)
print('B1:', B1.shape)
# Input -&gt; Hidden 1
print('=================')
print('Input -&gt; Hidden1')
print('=================')
# linear sum
A1 = np.dot(W1, X) + B1
print('A1:', A1.shape)
print(A1)
# activation
Z1 = sigmoid(A1)
print('Z1:', Z1.shape)
print(Z1)
</code></pre></div></div>
<blockquote>
  <p>W1: (3, 2)</p>

  <p>B1: (3,)</p>

  <p>=================</p>

  <p>Input -&gt; Hidden1</p>

  <p>=================</p>

  <p>A1: (3,)</p>

  <p>[ 0.3  0.7  1.1]</p>

  <p>Z1: (3,)</p>

  <p>[ 0.57444252  0.66818777  0.75026011]</p>
</blockquote>

<h3 id="hidden-1-rightarrow-hidden-2">Hidden 1 $\rightarrow$ Hidden 2</h3>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W2 = np.array([[0.1, 0.2, 0.3],
               [0.4, 0.5, 0.6]])
B2 = np.array([0.1, 0.2])

print('Z1:', Z1.shape)
print('W2:', W2.shape)
print('B2:', B2.shape)
# Hidden 1 -&gt; Hidden 2
print('=================')
print('Hidden 1 -&gt; Hidden 2')
print('=================')
# linear sum
A2 = np.dot(W2, Z1) + B2
print('A2:', A2.shape)
print(A2)
# activation
Z2 = sigmoid(A2)
print('Z2:', Z2.shape)
print(Z2)
</code></pre></div></div>
<blockquote>
  <p>Z1: (3,)</p>

  <p>W2: (2, 3)</p>

  <p>B2: (2,)</p>

  <p>=================</p>

  <p>Hidden 1 -&gt; Hidden 2</p>

  <p>=================
A2: (2,)</p>

  <p>[ 0.51615984  1.21402696]</p>

  <p>Z2: (2,)</p>

  <p>[ 0.62624937  0.7710107 ]</p>
</blockquote>

<h3 id="hidden-2-rightarrow-output">Hidden 2 $\rightarrow$ Output</h3>

<p>ë§ˆì§€ë§‰ ì¶œë ¥ ì¸µì—ì„œëŠ” ì´ì „ ì¸µì— ì¶œë ¥ëœ $Z$ ê°’ë“¤ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def identity_function(x):
    return x

W3 = np.array([[0.1, 0.2],
               [0.3, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(W3, Z2) + B3
Y = identity_function(A3)
print(Y)
</code></pre></div></div>
<blockquote>
  <p>[ 0.31682708  0.69627909]</p>
</blockquote>

<p>í˜¹ì€ Softmaxë¼ëŠ” í•¨ìˆ˜ë¥¼ ì¨ì„œ ê° Outputì˜ í™•ë¥ ë¡œì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ë³´í†µì„ ì´ê±¸ ì“´ë‹¤.</p>

<h4 id="softmax">Softmax</h4>

<p><script type="math/tex">y_k = \frac{exp(a_k)}{\sum_{i=1}^{n}{exp(a_i)}}</script></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def softmax(x):
    return np.exp(x) / np.sum(np.exp(x))
</code></pre></div></div>
<p>ì–¼í• ì˜ ë§Œë“  ê²ƒ ê°™ì§€ë§Œ ì»´í“¨í„°ì—ì„œ ì•„ì£¼ í° ìˆ˜ë¥¼ ê³„ì‚°ì‹œ Overflowë¬¸ì œê°€ ë°œìƒí•œë‹¤. ì˜¤ë²„í”Œë¡œìš°ë€, ì‚¬ìš© ê°€ëŠ¥í•œ í•˜ë“œì›¨ì–´(ì¦‰, 32bit ë‹¨ìœ„ ì›Œë“œì˜ í•˜ë“œì›¨ì–´, ë ˆì§€ìŠ¤í„° ë“±)ë¡œ ì—°ì‚° ê²°ê³¼ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ì„ ë•Œ ì˜¤ë²„í”Œë¡œìš°ê°€ ë°œìƒí•œë‹¤ê³  í•œë‹¤. (ì˜¤ë²„í”Œë¡œìš° ê°œë… ì¶œì²˜: [<a href="https://m.blog.naver.com/PostView.nhn?blogId=osw5144&amp;logNo=120206206420&amp;proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F">ë§í¬</a>])</p>

<p>ê°„ë‹¨íˆ ì˜ˆë¥¼ ë“¤ì–´ë³´ë©´ ì•„ë˜ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ë©´ ê¸ˆë°© ì•Œ ìˆ˜ ìˆë‹¤.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = np.array([1010, 1000, 990])
softmax(a)
</code></pre></div></div>
<blockquote>
  <p>/Users/user/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp
from ipykernel import kernelapp as app</p>

  <p>/Users/user/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: RuntimeWarning: invalid value encountered in true_divide
  from ipykernel import kernelapp as app</p>

  <p>array([ nan,  nan,  nan])</p>
</blockquote>

<p>ê²½ê³ ê°€ ëœ¨ë©´ì„œ NaN ê°’ë“¤ë§Œ ë‚˜ì˜¨ë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ ì…ë ¥ ì‹ í˜¸ ì¤‘ ìµœëŒ€ê°’ì„ ì´ìš©í•˜ëŠ”ê²Œ ì¼ë°˜ì ì´ë‹¤. ì•„ë˜ëŠ” ë¶„ëª¨, ë¶„ì ë³€ìˆ˜ì— ì–´ë–¤ ìƒìˆ˜ Câ€™ë¥¼ ë”í•´ë„ ê²°êµ­ì—” Softmaxê°€ ë˜ëŠ” ê²ƒì„ ì¦ëª… í•œ ì‹ì´ë‹¤.</p>

<p><script type="math/tex">% <![CDATA[
\begin{aligned}
y_k &= \frac{exp(a_k)}{\sum_{i=1}^{n}{exp(a_i)}} \\
&= \frac{Cexp(a_k)}{C\sum_{i=1}^{n}{exp(a_i)}} \\
&= \frac{exp(a_k+\log{C})}{\sum_{i=1}^{n}{exp(a_i+\log{C})}}\\
&= \frac{exp(a_k+C^{'})}{\sum_{i=1}^{n}{exp(a_i+C^{'})}}
\end{aligned} %]]></script></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>c = np.max(a)
print(a - c)
print(softmax(a-c))
</code></pre></div></div>
<blockquote>
  <p>[  0 -10 -20]</p>

  <p>[  9.99954600e-01   4.53978686e-05   2.06106005e-09]</p>
</blockquote>

<p>ì´ë²ˆì—ëŠ” ê²½ê³  ì—†ì´ ì‹¤í–‰ì´ ì˜ ëœë‹¤. ì´ì œ ìµœì¢… SoftmaxëŠ” ì•„ë˜ì™€ ê°™ë‹¤. ì´ë¥¼ ì¶œë ¥ì¸µì— ì ìš©í•˜ë©´ yê°’ì— ëŒ€í•œ í™•ë¥ ì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë¥¼ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë§Œë“œëŠ” ì´ìœ ê°€ ìˆëŠ”ë° í–¥í›„ í•™ìŠµì‹œì— í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤. (ë„¤íŠ¸ì›Œí¬ í•™ìŠµì—ì„œ ì„¤ëª…)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def softmax(a):
    c = np.max(a)
    return np.exp(a - c) / np.sum(np.exp(a - c))
</code></pre></div></div>
<h2 id="feedforward-ì‹¤ìŠµ">Feedforward ì‹¤ìŠµ</h2>

<p>ì—¬íƒœ ë³´ì•˜ë˜ 3ì¸µ Neural Networkë¥¼ ë§Œë“¤ì–´ ë³´ì, ì–´ë ¤ìš´ ê²ƒì€ ì—†ê³  ì•„ê¹Œ ë§Œë“¤ì—ˆë˜ ê²ƒì„ ë‚˜ì—´í•´ë³´ë©´ ì‰½ë‹¤.</p>

<p>ì‹¤ìŠµí•  ë°ì´í„°ëŠ” mnist ë°ì´í„° ì´ë©°, ì•„ë˜ ë§í¬ë¡œ ë°›ì„ ìˆ˜ ìˆë‹¤.</p>

<p>&lt;ë°‘ë°”ë‹¥ ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹&gt; ì±…ì˜ Github: [<a href="https://github.com/WegraLee/deep-learning-from-scratch">ë§í¬</a>]</p>

<p>ì…ë ¥ì¸µì—ëŠ” 784 ê°œì˜ ë‰´ëŸ°, ì€ë‹‰ì¸µ1ì—ëŠ” 50ê°œ, ì€ë‹‰ì¸µ2ì—ëŠ” 100ê°œ, ë§ˆì§€ë§‰ ì¸µì—ëŠ” 10ê°œì˜ ë‰´ëŸ°ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ë‹¤. í™œì„±í™” í•¨ìˆ˜ëŠ” sigmoidë¥¼ ì“°ê³ , ë§ˆì§€ë§‰ì— Softmaxë¡œ í™•ë¥ ì„ êµ¬í–ˆë‹¤. ì‹¤í–‰ë‹¨ê³„ì—ì„œ batchë¼ëŠ” ê²ƒì´ ìˆëŠ”ë°, í•œë²ˆì— ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ë©´ ëŠë¦¬ë‹ˆ, ì¡°ê¸ˆì”© ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># ë„¤íŠ¸ì›Œí¬ ë§Œë“¤ê¸°
from dataset.mnist import load_mnist
import numpy as np

class NN(object):
    def __init__(self):
        # W1(50, 784) X(784, batch_size)
        # W2(100, 50) Z1(50, batch_size)
        # W3(10, 100) Z2(100, batch_size)
        # B1(50, batch_size)
        # B2(100, batch_size)
        # B3(10, batch_size)
        self.W = {'W1': np.random.normal(size=(50, 784)),  
                  'W2': np.random.normal(size=(100, 50)),  
                  'W3': np.random.normal(size=(10, 100)),}  
        self.B = {'B1': np.random.normal(size=(50, batch_size)),  
                  'B2': np.random.normal(size=(100, batch_size)),  
                  'B3': np.random.normal(size=(10, batch_size)),}  

    def get_data(self):
        (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True,
                                                          normalize=True,
                                                          one_hot_label=False)
        return x_train, t_train, x_test, t_test

    def predict(self, X):
        W1, W2, W3 = self.W['W1'], self.W['W2'], self.W['W3']
        B1, B2, B3 = self.B['B1'], self.B['B2'], self.B['B3']
        # Input -&gt; Hidden 1
        A1 = np.dot(W1, X) + B1
        Z1 = sigmoid(A1)
        # Hidden 1 -&gt; Hidden 2
        A2 = np.dot(W2, Z1) + B2
        Z2 = sigmoid(A2)
        # Hidden 2 -&gt; Output
        A3 = np.dot(W3, Z2) + B3
        Y = softmax(A3)

        return Y

# ì‹¤í–‰ ë‹¨ê³„
model_mnist = NN()
x_train, t_train, x_test, t_test = model_mnist.get_data()
acc_count = 0
batch_size = 100
for i in range(0, len(x_train), batch_size):
    x_batch = x_train[i:i+batch_size].T  # (784, 100)
    y_batch = model_mnist.predict(x_batch) # (10, 100)
    p = np.argmax(y_batch, axis=0)
    acc_count += np.sum(p == t_train[i:i+batch_size])
print("accuracy:", acc_count / len(x_train))
</code></pre></div></div>

<blockquote>
  <p>accuracy: 0.0857833333333</p>
</blockquote>

<p>ì •í™•ë„ë€ ë°ì´í„° ì¤‘ì—ì„œ ì–¼ë§Œí¼ ë¼ë²¨ ë§ì·ƒëŠ”ì§€ ì¸¡ì •í•˜ëŠ” ê²ƒì¸ë°, ë‹¹ì—°í•˜ì§€ë§Œ ê²°ê³¼ê°€ ì•„ì£¼ í˜•í¸ì´ ì—†ë‹¤. ì´ì œ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµì‹œí‚¤ë©´ì„œ ì´ë¥¼ í–¥ìƒ ì‹œí‚¬ ê²ƒì´ë‹ˆê¹Œ ë„ˆë¬´ ê±±ì •í•˜ì§€ ë§ì.</p>
:ET