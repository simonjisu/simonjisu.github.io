---
layout: post
title: "Big Little Data 참석후기"
date: "2018-04-21 12:47:28 +0900"
categories: "DataScience"
author: "Soo"
comments: true
---
# Little Big Data 참석 후기

<img src="/assets/ds/littlebigdata.png">

주최: ZEPL
링크: [festa](https://festa.io/events/21)

## 후기

발표자분들께서 자신들이 겪은 다양한 경험을 들었다. 모든 일이 다 그렇지만, 문제를 파악하고 정의를 어떻게 하며, 방법을 모색하고 해결 후 결과를 다시 한번 정리해보는 사고 프로세스를 배운 것 같다.

발표 세션을 듣고 일이 있어서 나와야했지만 유익했던 자리.

---

## 상세

발표자의 내용이 완벽하게 일치하지 않으며, 제가 중간중간 생각나서 제 생각을 기록한 것도 있습니다. (거의 없긴 하지만)

---

### 극한직업: 한국어 채팅 데이터로 머신러닝 하기

**[Scatter Lab](http://www.scatterlab.co.kr/) 조한석 님**

한글 데이터는 문제가 많음:
* Hell 조사
* 자유로운 언어 변형
* 혀꼬인 소리
* 맞춤법 및 띄어쓰기 오류
* 챗팅에서 쓰이는 단어

한국어 데이터는 전처리(Preprocessing)이 80%다.

일반 오픈소스 형태소 분석기의 한게점: 학습에 사용된 corpus가 잘 정돈된 데이터를 학습했기 때문에, 잘 안되는 경향이 있음.

그래서 데이터로부터 학습하자! 김현중 님의 [soynlp](https://github.com/lovit/soynlp)

**다양한 문제들과 문제 정의 및 해결:**

1. Normalize:
    * 아이디어: 오류가 적다고 생각하는 데이터를 선택 후, 전체 데이터에서 조금 등장한 패턴을 자주 등장하는 것으로 수정하는 방법
2. 띄어쓰기 교정:
    * 다음 글자가 띄어쓸지 아닐지 binary Classification  
3. Tokenizing:
    * 단어 추출 process
4. Word Embedding:
    * oov 문제 (학습되지 않는 단어는 inference 단계에서 문제)
    * Fasttext 사용: substring 정보 활용 [paper](https://arxiv.org/abs/1607.01759)
    * 한글의 경우: ngam 단위를 글자 / 자음모음으로 하게됨
5. Sentence Similarity
    * BOW + Word Embedding 방식: 임베딩에 너무 의존하게 됨, 학습된 데이터에 따라서 원하는 결과가 안나올 수도 있음, 따라서 다른 방법을 추가해서 쓰게됨.
    * 참고한 논문: [Unsupervised Sparse Vector Densification for Short Text Similarity](http://cogcomp.org/papers/SongRo15.pdf)
6. 팁
    * 전처리 단계에서 [힙의 법칙(Heap's Law)](https://en.wikipedia.org/wiki/Heaps%27_law) 에 따라서 빈도수가 너무 적은 단어는 과감하게 쳐내기
    * 문제 정의를 잘하기, Countbase 모델이 오히려 더 잘 될 수도 있다.
    * unlabel 데이터에 label 을 달아서 인사이트를 얻어보자!

---

### 딥러닝에 필요한 로그 기깔나게 잘 디자인하는 법

**구글 클라우드 엔지니어 백정상 님**

탐색적 데이터 분석(EDA)을 잘 하기위해 어떻게 로그를 쌓는 것이 좋았는가?

* 로그 디자인
* 명확한 데이터
* 원시 데이터에서 분석 쿼리
* 등등

그러나 딥러닝은 조금 달랐다!!

통계적으로 풀지 못하는 문제 생겼는데, 비정상적인 데미지를 만드는 플레이어가 핵유저인지 아닌지? 이상 탐지 문제 (정확히 기억이 안남)

고전적인 머신러닝 기법으로 풀려고 보니 바운더리 필요하고, 어느정도 데미지가 비정상적인 플레이고, 정상적인 플레이인지 알수 없었음.

1. 정상적인 데이터가 충분히 많다면, 학습후 비정상 데이터 잡아내기 **(지도학습)**
2. 정상인지 아닌지 확신할 데이터 충분치 않다면, 학습후 클러스터링 해서 아웃라이어 잡기 **(비지도학습)**

데이터가 충분하지 않았기 때문에 2번으로 선택후 가설을 세움.

> 가설: 유저들이 평균적으로 내는 데미지에 비해 엄청 크면? --> 비정상

Feature engineering: feature selection

* 유저 인덱스 들어가면 분류 너무 세분화,
* 페이즈 별로 데미지 얼마 넣었는지 보다 스테이지 완료시 데미지만,
* 캐릭터 직업별로 데미지 넣는 양이 다르니, 직업은 넣자

여기까지 **<span style="color: #e87d7d">행복회로! </span>** 상상의 나래였던거임.

**-현실-**

무슨 모델을 쓸 것인가? 꼭 딥러닝을 써야되나?

오코인코더 써서 대다수 유저와 loss 차이가 많이 나는지 확인! (위에 가설을 검정확인함)
* Deep AutoEncoder - Compressed Feature Vector: 데이터 복원하는 속성 학습
* train, 평가 쉬움
* 주의할 것은 **가설** 이 참이여야만 모델이 정확하다는 것

만들면서 생긴 문제들:
1. input data 에 따라서 달라짐 > 어떤 feature 를 쓸 것인가?
    * 꼭 필요함.
2. 훈련방식: 온라인, 배치, 저장된 데이터 > 3개 다해야함
    * 이유: 왜냐면 많은 사람들이 어뷰징을 쓰게되면 바이어스가 정상으로 학습 될 수도
3. 피쳐선택:
    * 꼭 필요함. 피쳐 늘어날 수록 더 많은 데이터가 필요함
    * EDA: 게임상에서 일어나는 특징적인 패턴을 찾는 데 주력 > 어떤 피쳐가 상관관계가 높지?

따라서,

* 로그 디자인: json 으로 일단 저장, nested repeated 정보 어떻게 저장?
* 관리: 용량이 더 커질 수 밖에 없음, 트레이닝 데이터 사이즈를 줄여보는 것도 방법, 콜드데이터는 비용 절감에 주력, 머신러닝에 들어가는 피쳐는 최대한 줄이고 차원을 축소해서 트레이닝 비용을 줄여라.
* 데이터 검증: 관리해야할 로그의 종류와 데이터 타입이 너무 많아짐, 테스트 기반 검증을 진행해야함, 모든 로그 데이터는 검증 로직 테스트를 통과해야 게임 업데이트가 가능 하도록 해야함

**결론:** 피쳐가 생명임, 즉 구하려고하는 것과 상관관계가 높은 데이터를 한번 찾아보자

**협업 방식:** 처음에는 각 로그별로 정의 문서를 만듬, 나중에는 QA 후 valid 함, 오류나면 커뮤니케이션

---

### 바닥부터 시작하는 데이터 인프라

**(전) 레트리카, 변성윤 님** [(블로그)](https://zzsza.github.io/)

**1. 대시보드 만들기**

* 목표: 팀원들이 조회하고 싶은 데이터를 볼 수 있는 대쉬보드 만들기
* 단순 반복적인 작업을 줄이는 것
* 직접 구현하기 힘들면 오픈소스 툴을 사용하자! (superset)

**2. 데이터 파이프라인 생성**

* 목표 이벤트 레벨까지 데이터를 조회할 수 있는 대시보드
* 문제: 이벤트로그 정리되어 있는가? 테이블 형태가 아니라면 못씀
* 해결: 테이블 형태로 변환
* 문제2: 빅쿼리비용이 너무 크게 나옴
* 해결: 이재광 님(NBT), 데이터를 최대한 줄여라 > flatten table 만들고, 목적에 맞게 데이터 구성(중복하지 않게)
* 기타:
    * bigquery vs dataflow
    * task management tool - airflow 도입

**3. 음란사진 올라오면?**

* 처리 프로세스: user > scheduler > docker (NSFW score) > block / unblock
* 사진 말고 비디오, 움짤의 경우? webp 전환후 비디오/움짤의 일부분 만 input으로 집어 넣기
* 콜라보사진: 사람 얼굴 갯수가 많아졌을 때, score가 높음 > 사람 얼굴 갯수로 threshold (정확히 기억이 안남 ㅠㅠ)

**4. 팁**
* 모든것을 만들 필요는 없다.
* 선인의 지혜를 빌리자.

---

### 게임회사 주니어 웹 개발자가 바라본 데이터 분석 이야기

**넥슨 이준범 님**

**1. 대시보드**

DB에서 데이터 불러올 때, ORM 굉장히 느리다. (몰랐는데 처음알아따...) 그런데, Pure SQL 썼는데도 느리다 > **"인덱스 타고 있니??"**

* DB를 full-scan 하지 않게 만들어야 함

**2. 정량적 접근 vs 정성적 접근**

case1 신규 게임: 데이터가 없는 상황
* 모든 게임에 다 남고 있는 데이터 (접속기록, 계졍명...)
* 게임 유형에 맞는 공통 형식의 로그 -> 쉬움 -> 전부다 잡아낼까?

case2 게임 특성에 따른 어뷰징
* 어떤 어뷰징 존재한가? 매크로 커뮤니티, 유저들의 신고
* 로그 속에서 패턴 찾아내기

등등...(나머지 10분 뒤에 내용은 들어도 잘 모르겠어서 정리를 못했다.)

---

그 뒤에 패널 토크에는 어마어마하신 분들이 오신거 같았는데, 못들었다... 다음엔 끝까지 들을 수 있기를~
