---
layout: post
title: "Machine Learning: Evaluation"
date: "2021-08-28 12:13:01 +0900"
categories: machinelearning
author: "Soo"
comments: true
toc: true
---

{% include image.html id="1rxOWFSNawoCtGJQLLpQzE90tYWCAtEEI" desc="Reference: Pixabay" width="100%" height="auto" %}

# Confusion Matrix

**Confusion Matrix** 는 보통 supervised learning 관련 머신러닝 알고리즘의 퍼포먼스를 측정하기 위한 도구로 사용된다. 자주 사용되는 것인데 계속 잊어버려서 한 번 정리해보고 어떤 상황에서 사용되는지 알아보자.

---

# 개념

분류 문제에서 정확도를 평가지표로 사용할 때, 클래스의 불균형 문제가 있으면 잘못된 결과를 낳다. 예를 들어, 개/고양이를 분류하는 문제에서 95개의 고양이와 5개의 개 사진이 있고, 어떤 모델이 전부 고양이를 예측했다. 최종 정확도는 95%지만, 고양이의 검출율 은 100% (95개 중 95개 예측)이고 개의 검출율를 0% (5개 중 0개 예측)이다. 

정확도로 탐지할 수 없는 모델의 문제점을 정보(informedness)로 표현을 하게되자 어떤 문제가 있는지 알게 되었다. 이 예시에서는 모델이 항상 고양이만 예측한다는 것이다.

두 개의 클래스가 있을 때 Confusion Matrix는 다음과 같이 정의 된다.

|  | **Predicted** | **Positive** | **Negative** |
|---|---|---|---|
| **Actual** | **Positive** | True Positive | False Negative |
|            | **Negative**| False Positive | True Negative |

여기서 행은 실제(Actual) 데이터의 레이블이고, 칼럼은 모델이 예측(Predict)한 레이블을 뜻한다. 표의 내용은 `(앞)진위여부 (뒤)예측`과 같이 해석하는게 좋다. 즉, True Positive는 예측(Predicted)는 Positive인데 실제(Actual)로 Positive라서 진위 여부는 True다. 하나더 예를 보면 False Negative면 예측은 Negative인데 실제로 Negative라 진위 여부는 False다.

Confusion Matrix에서 파생되는 여러가지 지표들이 있는데 지금부터 알아보겠다.

|지표|공식|설명|
|---|:-:|---|
|Accuracy|$\dfrac{(TP + TN)}{(TP + TN + FN + FP)}$|전체 맞게 예측한 비율|
|Precision|$\dfrac{TP}{(TP + FP)}$|예측한 Positive 중에 맞게(True) 예측한 비율|
|Recall / Sensitivity|$\dfrac{TP}{(TP + FN)}$|실제 Positive 중에 맞게(True) 예측한 비율|
|Fall-out|$\dfrac{FP}{(FP + TN)}$|실제 Negative 중에 틀리게(False) 예측한 비율|
|F1 Score|$\dfrac{2}{(1/Precision + 1/Recall)} = \dfrac{2\times Precision\times Recall}{Precision + Recall}$| Precision 과 Recall의 조화 평균, 데이터 분포가 불균형 일때 사용, 큰 비중이 끼치는 bias 가 줄어듦|

---

# 예시

예를 들어, 쇼핑몰의 머신러닝 개발자가 조금더 편한 태깅을 위해, 의류 사진를 보고 어떤 종류인지 예측해서 분류하는 머신러닝 모델을 만들었다고 생각해보자. 의류는 총 3가지 class 이며 다음과 같다.

```python
cls_dict = {0: "상의", 1: "하의", 2: "신발"}
```

그리고 신규 물품 사진에 대한 모델의 에측 결과를 다음과 같이 저장했다.

```python
target = [
    0, 0, 0, 0, 0, 0,  # 상의 6개
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  # 하의 10개
    2, 2, 2, 2, 2, 2, 2, 2, 2  # 신발 9개
]
pred = [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2]
```

그러면 Confusion matrix를 다음과 같이 만들 수 있다.

```python
K = len(np.unique(target))
confusion_matrix = np.zeros((K, K), dtype=np.int32)
for t, p in zip(target, pred):
    confusion_matrix[t, p] += 1

print(confusion_matrix)
#         Predict
#        [[4 1 1]
# Target  [6 2 2]
#         [3 0 6]]
```

## Accuracy

정확도는 전체에서 옳게 맞춘 비율임으로 다음과 같이 계산된다.

$$\text{Accuracy} =\dfrac{4+2+6}{25}$$

```python
total = len(target)
true_positive = (np.eye(3) * confusion_matrix).sum(0)
print(true_positive.sum() / total)
# 0.48
```

## Precision

**정밀도(Precision)** 는 Positive라고 예측한 것들 중에서 실제로 맞게 예측한 비율임으로 다음과 같이 계산된다. 

$$\begin{aligned} \text{Class 0} &= \dfrac{4}{4+6+3} \\ \text{Class 1} &= \dfrac{2}{1+2+0} \\ \text{Class 2} &= \dfrac{6}{1+2+6} \end{aligned}$$

```python
predicted = confusion_matrix.sum(0)
precisions = true_positive / predicted
print(precisions.round(4))
print(f"Macro Precision {(precisions / len(precisions)).sum():.4f}")
# [0.3077 0.6667 0.6667]
# Macro Precision 0.5470
```

## Recall / Sensitivity

**재현율(Recall/Sensitivity)** 은 실제 Positive 인것들 중에서 맞게 예측한 비율임으로 다음과 같이 계산된다. 

$$\begin{aligned} \text{Class 0} &= \dfrac{4}{4+1+1} \\ \text{Class 1} &= \dfrac{2}{6+2+2} \\ \text{Class 2} &= \dfrac{6}{3+0+6} \end{aligned}$$

```python
actual_count = confusion_matrix.sum(1)
recalls = true_positive / actual_count
print(recalls.round(4))
print(f"Macro Recall {(recalls / len(recalls)).sum():.4f}")
# [0.6667 0.2    0.6667]
# Macro Recall 0.5111
```

## Fallout

**Fallout** 은 Negative라고 예측했는데 실제로 Positive라고 맞춘 비율임으로 다음과 같이 계산된다.

$$\begin{aligned} \text{Class 0} &= \dfrac{6+3}{6+2+2+3+0+6} \\ \text{Class 1} &= \dfrac{1+0}{4+1+1+3+0+6} \\ \text{Class 2} &= \dfrac{1+2}{4+1+1+6+2+2} \end{aligned}$$

```python
from itertools import combinations

fallouts = []
for cls_idx, neg_idx in enumerate(reversed(list(combinations(range(3), 2)))):
    false_positive = confusion_matrix[neg_idx, cls_idx]
    negative = confusion_matrix[neg_idx, :]
    fpr = false_positive.sum() / negative.sum()
    fallouts.append(fpr)
    
fallouts = np.array(fallouts)
print(fallouts.round(4))
print(f"Macro Fallout {(fallouts / len(fallouts)).sum():.4f}")
# [0.4737 0.0667 0.1875]
# Macro Fallout 0.2426
```

## F1 Score

**F1 Score** 는 다음과 같이 계산된다.

```python
f1 = 2 * precisions * recalls / (precisions + recalls)
print(f1.round(4))
print(f"Total F1 {(f1 / len(f1)).sum():.4f}")
# [0.4211 0.3077 0.6667]
# Total F1 0.4651
```

# Scikit-Learn

Scikit-learn 패키지중 이를 한번에 구해주는 패키지가 있다. 이 표를 보고 모델의 성능을 한번 평가해보자.

```python
from sklearn.metrics import classification_report
print(classification_report(target, pred, target_names=['상의', '하의', '신발']))
#               precision    recall  f1-score   support

#         상의       0.31      0.67      0.42         6
#         하의       0.67      0.20      0.31        10
#         신발       0.67      0.67      0.67         9

#     accuracy                           0.48        25
#    macro avg       0.55      0.51      0.47        25
# weighted avg       0.58      0.48      0.46        25
```

### Precision

정밀도는 Type-I Error와 연관있다. Type-I Error는 실제 Negative를 Positive라고 예측한 경우인데, 예를 들어 재판의 경우 죄를 짓지 않았는데 유죄 판결을 내리는 경우 피고인은 무고할 수가 있다.  Precision이 높을 수록 Type-I Error를 줄일 수 있다.

의류분류기 모델의 경우 "상의"에 대해서는 상의가 아닌데 상의라고 판별할 가능성이 높다는 것이다. 

### Recall

재현율은 Type-II Error와 연관있다. Type-II Error는 실제 Positive를 Negative라고 예측한 경우인데, 암 판정의 예시를 들면, 암이 있는데 암이 없다고 판정한 것이며, 이는 환자에게 치명적일 수가 있다. Recall이 높을 수록 Type-II Error를 줄 일 수 있다.

의류분류기 모델의 경우 "하의"에 대해서는 실제로 "하의"인데 다른 클래스라고 판별할 가능성이 높다는 것이다. 

### F1 Score

조화 평균인 F1 Score은 데이터 분포가 불균형 일때 사용되는데, 큰 비중이 끼치는 bias 가 줄어들게 된다. 예를 들어 recall이 $0.9$ 이고 precision이 $0.01$ 인 경우, 일반 평균을 구하면 $(0.9+0.01)/2 = 0.455$가 나오지만 조화 평균을 사용하게 되면, $2\times 0.9 \times 0.01 / (0.9 + 0.01) = 0.020$으로 굉장히 낮게 나온다.

의류분류기 모델의 경우 "신발"의 성능이 상대적으로 "하의", "상의"보다는 더 나은 퍼포먼스를 보인다.