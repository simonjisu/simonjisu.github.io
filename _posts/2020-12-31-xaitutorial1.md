---
layout: post
title: "[XAI] Explainable Artificial Intelligence (XAI) - 1 "
date: "2020-12-31 11:38:38 +0900"
categories: paper
author: "Soo"
comments: true
toc: true
---

# Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI

Paper Link: [https://arxiv.org/abs/1910.10045](https://arxiv.org/abs/1910.10045)

XAIì— ëŒ€í•œ ì „ë°˜ì ì¸ ì†Œê°œë¥¼ ì •ë¦¬í•œ ë…¼ë¬¸ì´ ë‚˜ì™€ì„œ ì°¨ê·¼ ì°¨ê·¼ ìš”ì•½ ì •ë¦¬í•´ë³´ë ¤ê³  í•œë‹¤(ë¬´ë ¤ 115í˜ì´ì§€, referenceë§Œ 6í˜ì´ì§€). ì•½ê°„ì˜ ë²ˆì—­ ì–´íˆ¬ì™€ ìƒëµëœ ê²ƒë„ ìˆìœ¼ë‹ˆ ì˜ì–´ ì›ë¬¸ì„ ì°¸ê³ í•˜ê¸¸ ë°”ë€ë‹¤.

1. [<span style="color:#e25252">Introduction(ì´ë²ˆí¸)</span>](https://simonjisu.github.io/paper/2020/12/31/xaitutorial1.html)
2. [Explainability: What, why, what for and how?](https://simonjisu.github.io/paper/2021/01/14/xaitutorial2.html)
3. [Transparent machine learning models](https://simonjisu.github.io/paper/2021/01/23/xaitutorial3.html)
4. Post-hoc explainability techniques for machile learning models: Taxonomy, shallow models and deep learning
5. XAI: Opportunities, challenges and future research needs
6. Toward responsible AI: Principles of artificial intelligence, fairness, privacy and data fusion
7. Conclusions and outlook

# 1. Introduction 

[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ 

Artificial Intelligence (AI) lies at the core of many activity sectors that have embraced new information technologies [1]. While the roots of AI trace back to several decades ago, there is a clear consensus on the paramount importance featured nowadays by intelligent machines endowed with learning, reasoning and adaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedented levels of performance when learning to solve increasingly complex computational tasks, making them pivotal for the future development of the human society [2]. The sophistication of AI-powered systems has lately increased to such an extent that almost no human intervention is required for their design and deployment. When decisions derived from such systems ultimately affect humansâ€™ lives (as in e.g. medicine, law or defense), there is an emerging need for understanding how such decisions are furnished by AI methods [3].

[/expand]

<span style="color:#e25252">ìš”ì•½:</span> ì¸ê³µì§€ëŠ¥ì´ ì •êµí•´ì§€ë©´ì„œ ê³„ì‚°ì´ ì ì  ë³µì¡í•´ì§€ëŠ” ë°˜ë©´, ê¶ê·¹ì ìœ¼ë¡œ ì¸ê°„ì˜ ì‚¶ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”(ì˜í•™, ë²•ë¥ , êµ­ë°©) ì‹œìŠ¤í…œ(ê¸°ê³„)ì˜ ê²°ì •ì´ ì–´ë–»ê²Œ ë‚´ë ¤ì¡ŒëŠ”ì§€, ìš°ë¦¬ëŠ” ì´í•´í•  í•„ìš”ê°€ ìˆë‹¤.

[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ 

While the very first AI systems were easily interpretable, the last years have witnessed the rise of opaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning (DL) models such as DNNs stems from a combination of efficient learning algorithms and their huge parametric space. The latter space comprises hundreds of layers and millions of parameters, which makes DNNs be considered as complex black-box models [4]. The opposite of black-box-ness is transparency, i.e., the search for a direct understanding of the mechanism by which a model works [5].

[/expand]

<span style="color:#e25252">ìš”ì•½:</span> ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ íš¨ìœ¨ì ì¸ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ê³¼ ê±°ëŒ€í•œ íŒŒë¼ë¯¸í„° ê³µê°„ì˜ ê²°í•©ì—ì„œ ë¹„ë¡¯ëœë‹¤. ê·¸ë¦¬ê³  black-box ëª¨ë¸ë¡œ ê°„ì£¼ ëœë‹¤. ì´ì˜ ë°˜ëŒ€ëŠ” **íˆ¬ëª…ì„±(transparency)**ì´ë‹¤.

[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ 

As black-box Machine Learning (ML) models are increasingly being employed to make important predictions in critical contexts, the demand for transparency is increasing from the various stakeholders in AI [6]. The danger is on creating and using decisions that are not justifiable, legitimate, or that simply do not allow obtaining detailed explanations of their behaviour [7]. Explanations supporting the output of a model are crucial, e.g., in precision medicine, where experts require far more information from the model than a simple binary prediction for supporting their diagnosis [8]. Other examples include autonomous vehicles in transportation, security, and finance, among others.

[/expand]

<span style="color:#e25252">ìš”ì•½:</span> Machine Learning ëª¨ë¸ì´ ì ì  ë§ì´ í™œìš©ë˜ë©´ì„œ, ì´í•´ê´€ê³„ìë“¤ë¡œë¶€í„° íˆ¬ëª…ì„±ì˜ ìš”êµ¬ê°€ ë†’ì•„ì§€ê³  ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜ë£Œ(ì§„ë‹¨), êµí†µ(ììœ¨ì£¼í–‰), ë³´ì•ˆ, ê¸ˆìœµë“± ì´ ìˆë‹¤.

[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ 

In general, humans are reticent to adopt techniques that are not directly interpretable, tractable and trustworthy [9], given the increasing demand for ethical AI [3]. It is customary to think that by focusing solely on performance, the systems will be increasingly opaque. This is true in the sense that there is a trade-off between the performance of a model and its transparency [10]. However, an improvement in the understanding of a system can lead to the correction of its deficiencies. When developing a ML model, the consideration of interpretability as an additional design driver can improve its implementability for 3 reasons:

* Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correct from bias in the training dataset.
* Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbations that could change the prediction.

* Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeing that an underlying truthful causality exists in the model reasoning.

All these means that the interpretation of the system should, in order to be considered practical, provide either an understanding of the model mechanisms and predictions, a visualization of the modelâ€™s discrimination rules, or hints on what could perturb the model [11].

[/expand]

<span style="color:#e25252">ìš”ì•½:</span> í†µìƒì ìœ¼ë¡œ ì„±ê³¼ì—ë§Œ ì¹˜ì¤‘í•  ìˆ˜ë¡ ì‹œìŠ¤í…œì€ ì ì  ë¶ˆíˆ¬ëª…í•´ì§ˆ ê²ƒì´ë¼ ìƒê°í•œë‹¤. ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ íˆ¬ëª…ì„± ì‚¬ì´ì— trade-offê°€ ìˆë‹¤ëŠ” ì ì€ ì‚¬ì‹¤ì´ë‚˜, ëª¨ë¸ì— ëŒ€í•œ ì´í•´ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ ë‚¼ ìˆ˜ë„ ìˆë‹¤. ì¶”ê°€ë¡œ MLëª¨ë¸ì„ ê°œë°œí•  ë•Œ, í•´ì„ ê°€ëŠ¥ì„±ì„ ëª¨ë“ˆë¡œ ë„£ìœ¼ë©´ ì„¸ ê°€ì§€ ì´ìœ ë¡œ êµ¬í˜„ ê°€ëŠ¥ì„±ì„ í–¥ìƒ ì‹œí‚¬ ìˆ˜ ìˆë‹¤.

- í•´ì„ê°€ëŠ¥ì„±ì€ ì˜ì‚¬ê²°ì •ì—ì„œ ê³µì •ì„±ì„ ë³´ì¥í•˜ëŠ”ë° ë„ì›€ì´ ëœë‹¤. ì¦‰, êµìœ¡ ë°ì´í„° ì§‘í•©ì˜ í¸í–¥ì„±ì„ íƒì§€í•˜ê³  ê²°ê³¼ì ìœ¼ë¡œ ìˆ˜ì •í•œë‹¤.
- í•´ì„ê°€ëŠ¥ì„±ì€ ì˜ˆì¸¡ì„ ë°”ê¿€ ìˆ˜ ìˆëŠ” ì ì¬ì  ì ëŒ€ì  ì„­ë™ì„ ê°•ì¡°í•¨ìœ¼ë¡œì¨ ê±´ì „ì„±ì˜ ì œê³µì„ ì´‰ì§„í•œë‹¤.
- í•´ì„ê°€ëŠ¥ì„±ì€ ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ë§Œìœ¼ë¡œ ì‚°ì¶œë¬¼ì„ ìœ ì¶”í•˜ëŠ” ë³´í—˜ìœ¼ë¡œì„œ, ì¦‰ ëª¨í˜• ì¶”ë¡ ì—ì„œ ê·¼ë³¸ì ì¸ ì§„ì‹¤ì  ì¸ê³¼ê´€ê³„ê°€ ì¡´ì¬í•¨ì„ ë³´ì¦í•˜ëŠ” ë³´í—˜ìœ¼ë¡œ ì‘ìš©í•  ìˆ˜ ìˆë‹¤.

ì¦‰, í•´ì„ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì€ ëª¨ë¸ ë§¤ì»¤ë‹ˆì¦˜ê³¼ ì˜ˆì¸¡ì— ëŒ€í•œ ì´í•´, ëª¨ë¸ì˜ íŒê²° ê·œì¹™ ì‹œê°í™”, ë˜ëŠ” ëª¨ë¸ì„ ë°©í•´í•˜ëŠ” ê²ƒì— ëŒ€í•œ íŒíŠ¸ ë“±ì„ ì œê³µí•´ì•¼í•œë‹¤.

[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ 

In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI (XAI) [7] proposes creating a suite of ML techniques that 1) produce more explainable models while maintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners. XAI draws as well insights from the Social Sciences [12] and considers the psychology of explanation.

[/expand]

<span style="color:#e25252">ìš”ì•½:</span> í˜„ì¬ì˜ íš¨ê³¼ì ì¸ AI ì‹œìŠ¤í…œì„ ì œí•œì‹œí‚¤ì§€ ì•ŠëŠ” ì„ ì—ì„œ, eXplainable AI(XAI)ì€ 1) í•™ìŠµ í¼í¬ë¨¼ìŠ¤ëŠ” ìµœëŒ€í•œìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ ì„¤ëª…ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë§Œë“¤ê²ƒì„ ì œì•ˆ 2) ì‚¬ëŒì´ ì´í•´í•˜ê³ , ì ì ˆí•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ 

Fig. 1 displays the rising trend of contributions on XAI and related concepts. This literature outbreak shares its rationale with the research agendas of national governments and agencies. Although some recent surveys [8], [10], [13], [14], [15], [16], [17] summarize the upsurge of activity in XAI across sectors and disciplines, this overview aims to cover the creation of a complete unified framework of categories and concepts that allow for scrutiny and understanding of the field of XAI methods. Furthermore, we pose intriguing thoughts around the explainability of AI models in data fusion contexts with regards to data privacy and model confidentiality. This, along with other research opportunities and challenges identified throughout our study, serve as the pull factor toward Responsible Artificial Intelligence, term by which we refer to a series of AI principles to be necessarily met when deploying AI in real applications. As we will later show in detail, model explainability is among the most crucial aspects to be ensured within this methodological framework. All in all, the novel contributions of this overview can be summarized as follows:

1. Grounded on a first elaboration of concepts and terms used in XAI-related research, we propose a novel definition of explainability that places audience (Fig. 2) as a key aspect to be considered when explaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques, from trustworthiness to privacy awareness, which round up the claimed importance of purpose and targeted audience in model explainability.
2. We define and examine the different levels of transparency that a ML model can feature by itself, as well as the diverse approaches to post-hoc explainability, namely, the explanation of ML models that are not transparent by design.
3. We thoroughly analyze the literature on XAI and related concepts published to date, covering approximately 400 contributions arranged into two different taxonomies. The first taxonomy addresses the explainability of ML models using the previously made distinction between transparency and post-hoc explainability, including models that are transparent by themselves, Deep and non-Deep (i.e., shallow) learning models. The second taxonomy deals with XAI methods suited for the explanation of Deep Learning models, using classification criteria closely linked to this family of ML methods (e.g. layerwise explanations, representation vectors, attention).
4. We enumerate a series of challenges of XAI that still remain insufficiently addressed to date. Specifically, we identify research needs around the concepts and metrics to evaluate the explainability of ML models, and outline research directions toward making Deep Learning models more understandable. We further augment the scope of our prospects toward the implications of XAI techniques in regards to confidentiality, robustness in adversarial settings, data diversity, and other areas intersecting with explainability.
5. After the previous prospective discussion, we arrive at the concept of Responsible Artificial Intelligence, a manifold concept that imposes the systematic adoption of several AI principles for AI models to be of practical use. In addition to explainability, the guidelines behind Responsible AI establish that fairness, accountability and privacy should also be considered when implementing AI models in real environments.
6. Since Responsible AI blends together model explainability and privacy/security by design, we call for a profound reflection around the benefits and risks of XAI techniques in scenarios dealing with sensitive information and/or confidential ML models. As we will later show, the regulatory push toward data privacy, quality, integrity and governance demands more efforts to assess the role of XAI in this arena. In this regard, we provide an insight on the implications of XAI in terms of privacy and security under different data fusion paradigms.

[/expand]

{% include image.html id="119QnRBvYV4gHiuKz7kpaOVo_2b2tlhz5" desc="Fig 1. í•™ê³„ì—ì„œ XAI ë° ì—°ê´€ëœ ê°œë…ì˜ ê¸°ì—¬ë„ ì¶”ì„¸" width="100%" height="auto" %}

<span style="color:#e25252">ìš”ì•½:</span> `Fig 1`ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ êµ­ê°€ ì •ë¶€ ë° ê¸°ê´€ì˜ ì—°êµ¬ì˜ì œì˜ í‚¤ì›Œë“œ ì¶”ì„¸ë¥¼ ì‚´í´ë³´ë©´ XAIê´€ë ¨ í™œë™ì´ ìµœê·¼ ê¸‰ì¦í–ˆì§€ë§Œ, í†µì¼ëœ í”„ë ˆì„ì›Œí¬ê°€ ì—†ë‹¤. ì´ë²ˆ ë…¼ë¬¸ì—ì„œëŠ” í†µì¼ëœ í”„ë ˆì„ì›Œí¬ì˜ ì‘ì„±í•˜ê³ , ê°œì¸ì •ë³´ ë³´í˜¸ ë° ëª¨ë¸ ê¸°ë°€ì„±ì— ëŒ€í•´ì„œ ì˜ê²¬ì„ ì œì‹œí•  ê²ƒì´ë‹¤. 

1. ì§€ê¸ˆê¹Œì§€ XAI ê´€ë ¨ ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ ê°œë…ê³¼ ìš©ì–´ì˜ ê¸°ì´ˆí•˜ì—¬, ML ëª¨ë¸ì„ ì„¤ëª…í•  ë•Œ ì²­ì¤‘(audience)ì„ í•µì‹¬ìœ¼ë¡œ ê³ ë ¤í•  ê²ƒì´ë‹¤(ê·¸ë¦¼ 2). ë˜í•œ XAI ê¸°ë²•ì„ ì‚¬ìš©í•  ë•Œ ì¶”êµ¬í•˜ëŠ” ë‹¤ì–‘í•œ ëª©ì ì— ë”°ë¼ ì„¸ë¶„í™”í•  ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì„¤ëª…ê°€ëŠ¥ì„±ì—ì„œ ëª©ì ê³¼ íƒ€ê²Ÿ ì²­ì¤‘ì˜ ì¤‘ìš”í•¨ì„ ì´ì•¼ê¸° í•œë‹¤.
2. ë‹¤ì–‘í•œ ë ˆë²¨ì˜ íˆ¬ëª…ì„±ì„ ì •ì˜í•˜ê³  ê²€í† í•œë‹¤. ëŒ€ìƒì—ëŠ” ì‚¬í›„(post-hoc) ì„¤ëª…ì´ ê°€ëŠ¥í•œ, ìì²´ ì„¤ëª…ê°€ëŠ¥í•œ í˜¹ì€ ì„¤ê³„ì— ì˜í•´ ì„¤ëª…ì´ ë¶ˆê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ë“±ì´ ìˆë‹¤.
3. XAIì— ê´€í•œ ë¬¸í—Œê³¼ ì§€ê¸ˆê¹Œì§€ ì¶œíŒëœ ê´€ë ¨ ê°œë…ë“¤ì„ ì² ì €í•˜ê²Œ ë¶„ì„í•˜ì—¬, ëŒ€ëµ 400ê°œì˜ ê¸°ì—¬ë¥¼ ë‘ ê°œì˜ ë‹¤ë¥¸ ë¶„ë¥˜ë²•ìœ¼ë¡œ ë°°ì—´í•˜ì˜€ë‹¤.Â ì²« ë²ˆì§¸ ë¶„ë¥˜ë²•ì€ ì´ì „ì— ë§Œë“  íˆ¬ëª…ì„±(transparency)ê³¼ ì‚¬í›„ ì„¤ëª…ì„±(post-hoc explainability) ì‚¬ì´ì˜ êµ¬ë³„ì„ ì‚¬ìš©í•˜ì—¬ ML ëª¨ë¸ì˜ ì„¤ëª…ê°€ëŠ¥ì„±ì„ ë‹¤ë£¨ê³  ìˆìœ¼ë©°, ì—¬ê¸°ì—ëŠ” ìŠ¤ìŠ¤ë¡œ íˆ¬ëª…í•˜ê³  ê¹Šì§€ ì•Šì€(ì¦‰,Â shallow ì–‰ì€) í•™ìŠµ ëª¨ë¸ì´ í¬í•¨ëœë‹¤.Â ë‘ ë²ˆì§¸ ë¶„ë¥˜ë²•ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„¤ëª…ì— ì í•©í•œ XAI ë°©ë²•ì„ ë‹¤ë£¨ë©°, ì´ ML ë°©ë²• ê³„ì—´ê³¼ ë°€ì ‘í•˜ê²Œ ì—°ê³„ëœ ë¶„ë¥˜ ê¸°ì¤€(ì˜ˆ: ê³„ì¸µì  ì„¤ëª… layer-wise explanations, í‘œí˜„ ë²¡í„° representation vectors, ì–´í…ì…˜ attention)ì„ ì‚¬ìš©í•œë‹¤.
4. ì§€ê¸ˆê¹Œì§€ë„ ë¶ˆì¶©ë¶„í•˜ê²Œ ë‹¤ë£¨ì–´ì§€ì§€ ì•Šê³  ìˆëŠ” XAIì˜ ì¼ë ¨ì˜ ê³¼ì œë¥¼ ì—´ê±°í•œë‹¤.Â êµ¬ì²´ì ìœ¼ë¡œëŠ” ML ëª¨ë¸ì˜ ì„¤ëª… ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê°œë… ë° ë©”íŠ¸ë¦­ìŠ¤ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì—°êµ¬ ìš”êµ¬ë¥¼ íŒŒì•…í•˜ê³ , ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë³´ë‹¤ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì—°êµ¬ ë°©í–¥ì„ ì •ë¦¬í•œë‹¤.Â ê¸°ë°€ì„±, ì ëŒ€ì  ì„¤ì •ì˜ ê²¬ê³ ì„±, ë°ì´í„° ë‹¤ì–‘ì„± ë° ì„¤ëª… ê°€ëŠ¥ì„±ê³¼ êµì°¨í•˜ëŠ” ê¸°íƒ€ ì˜ì—­ì— ê´€í•œ XAI ê¸°ë²•ì˜ í•¨ì¶•ì„±ì„ í–¥í•œ ì „ë§ì˜ ë²”ìœ„ë¥¼ ë”ìš± í™•ëŒ€í•©ë‹ˆë‹¤.
5. ì•ì„œì˜ ì¥ë˜ì˜ ë…¼ì˜ë¥¼ ê±°ì³, AI ëª¨ë¸ì´ ì‹¤ìš©í™”í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°€ì§€ AI ì›ë¦¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì±„íƒí•˜ëŠ” ë§¤ë‹ˆí´ë“œ ê°œë…ì¸ ì±…ì„ê° ìˆëŠ” ì¸ê³µì§€ëŠ¥ì˜ ê°œë…ì— ë„ë‹¬í•œë‹¤.Â ì±…ì„ AIë¥¼ ë’·ë°›ì¹¨í•˜ëŠ” ê°€ì´ë“œë¼ì¸ì€ ì„¤ëª…ê°€ëŠ¥ì„± ì™¸ì—ë„ ì‹¤ì œ í™˜ê²½ì—ì„œ AI ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ ê³µì •ì„±, ì±…ì„ì„±, í”„ë¼ì´ë²„ì‹œ ë“±ë„ ê³ ë ¤í•´ì•¼ í•œë‹¤ê³  ê·œì •í•˜ê³  ìˆë‹¤.
6. ì±…ì„ ìˆëŠ” AIëŠ” ëª¨ë¸ ì„¤ëª… ê°€ëŠ¥ì„±ê³¼ ê°œì¸ ì •ë³´ ë³´í˜¸/ë³´ì•ˆì„±ì„ ì„¤ê³„ë³„ë¡œ í˜¼í•©í•˜ë¯€ë¡œ, ë¯¼ê°í•œ ì •ë³´ ë°/ë˜ëŠ” ê¸°ë°€ ML ëª¨ë¸ì„ ë‹¤ë£¨ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ XAI ê¸°ë²•ì˜ ìœ ìµì„±ê³¼ ìœ„í•´ì„±ì— ëŒ€í•´ ì‹¬ì˜¤í•œ ë°˜ì„±ì„ ìš”êµ¬í•œë‹¤.Â ë‚˜ì¤‘ì— ë³´ì—¬ë“œë¦¬ê² ì§€ë§Œ, ë°ì´í„° ê°œì¸ ì •ë³´ ë³´í˜¸, í’ˆì§ˆ, ë¬´ê²°ì„± ë° ê±°ë²„ë„ŒìŠ¤ë¥¼ í–¥í•œ ê·œì œëŠ” ì´ ë¶„ì•¼ì—ì„œ XAIì˜ ì—­í• ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë” ë§ì€ ë…¸ë ¥ì„ ìš”êµ¬í•©ë‹ˆë‹¤.Â ì´ì™€ ê´€ë ¨í•˜ì—¬, ìš°ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ìœµí•© íŒ¨ëŸ¬ë‹¤ì„ í•˜ì—ì„œì˜ í”„ë¼ì´ë²„ì‹œ ë° ë³´ì•ˆ ì¸¡ë©´ì—ì„œ XAIì˜ ì˜ë¯¸ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•œë‹¤.

[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ 

The remainder of this overview is structured as follows: first, Section 2 and subsections therein open a discussion on the terminology and concepts revolving around explainability and interpretability in AI, ending up with the aforementioned novel definition of interpretability (Section 2.1 and 2.2), and a general criterion to categorize and analyze ML models from the XAI perspective. Sections 3 and 4 proceed by reviewing recent findings on XAI for ML models (on transparent models and post-hoc techniques respectively) that comprise the main division in the aforementioned taxonomy. We also include a review on hybrid approaches among the two, to attain XAI. Benefits and caveats of the synergies among the families of methods are discussed in Section 5, where we present a prospect of general challenges and some consequences to be cautious about. Finally, Section 6 elaborates on the concept of Responsible Artificial Intelligence. Section 7 concludes the survey with an outlook aimed at engaging the community around this vibrant research area, which has the potential to impact society, in particular those sectors that have progressively embraced ML as a core technology of their activity.

[/expand]

<span style="color:#e25252">ìš”ì•½:</span> ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆë‹¤: 

- Section 2: ì„¤ëª…ê°€ëŠ¥ì„±(explainability)ì™€ í•´ì„ê°€ëŠ¥ì„±(interpretability)ì˜ ìƒˆë¡œìš´ ì •ì˜, XAI ê´€ì ì—ì„œ ML ëª¨ë¸ ë¶„ë¥˜ ë° ë¶„ì„ì„ ìœ„í•œ ìš©ì–´ ë° ê°œë…ì— ëŒ€í•œ ì´ì•¼ê¸°
- Section 3, 4: ìµœê·¼ ì—°êµ¬ ê²°ê³¼ì™€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•
- Section 5: í•´ë‹¹ ë°©ë²•ë“¤ì— ëŒ€í•œ ì¥ë‹¨ì  ë° ì£¼ì˜í•´ì•¼í•  ëª‡ ê°€ì§€ ê²°ê³¼ë“¤ ì œì‹œ
- Section 6: "ì±…ì„ê° ìˆëŠ” ì¸ê³µì§€ëŠ¥" ê°œë…ì— ëŒ€í•œ ì„¤ëª…
- Section 7: ì‚¬íšŒì— ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì—°êµ¬ ì˜ì—­ì¸ ë§Œí¼, ML ê¸°ìˆ ì„ ì±„íƒí•œ ì‚¬ëŒë“¤ì„ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì°¸ì—¬ì‹œí‚¤ëŠ” ëª©í‘œë¡œ ê²°ë¡ ì„ ë‚´ë¦¬ê³ ì í•œë‹¤.