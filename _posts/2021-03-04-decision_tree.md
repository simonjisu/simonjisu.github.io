---
layout: post
title: "Decision Tree"
date: "2021-03-04 00:00:01 +0900"
categories: machinelearning
author: "Soo"
comments: true
toc: true
---

ê°€ì¥ ê³ ì „ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ì§€ë§Œ ì •í™•í•˜ê²Œ ì•Œê³  ë„˜ì–´ê°€ì•¼í•  ê²ƒ ê°™ì•„ ì •ë¦¬í•œë‹¤.

# References

- [ìœ„í‚¤ë°±ê³¼: ê²°ì • íŠ¸ë¦¬ í•™ìŠµë²•](https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95)
- [ratsgo blog: ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree)](https://ratsgo.github.io/machine%20learning/2017/03/26/tree/)
- [ë¼ì˜¨í”¼í”Œ blog: Decision Tree](https://m.blog.naver.com/laonple/220861527086)
- [Scikit-Learn: Decision Tree](https://scikit-learn.org/stable/modules/tree.html)
- [Scikit-Learn: Post Pruning](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)
- [Scikit-Learn: Permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html)
- [Feature selection : feature importance vs permutation importance](https://hwi-doc.tistory.com/entry/Feature-selection-feature-importance-vs-permutation-importance)

# ì˜ì‚¬ê²°ì • ë‚˜ë¬´(Decision Tree)

ì˜ì‚¬ê²°ì • ë‚˜ë¬´ëŠ” ë¶„ë¥˜ ë° íšŒê·€ì— ì‚¬ìš©ë˜ëŠ” ì „í†µì ì¸ **ë¹„ëª¨ìˆ˜(non-parametric)** ì§€ë„í•™ìŠµ ë°©ë²•ì´ë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì˜ ëª©ì ì€ ì…ë ¥ ë°ì´í„° í”¼ì²˜(features)ë¡œë¶€í„° íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ê·œì¹™ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. ìœ„í‚¤ë°±ê³¼ì— ë‚˜ì˜¨ íƒ€ì´íƒ€ë‹‰ íƒ‘ìŠ¹ê° ìƒì¡´ ì—¬ë¶€ë¥¼ ë‚˜íƒœë‚´ëŠ” íŠ¸ë¦¬ë¡œ ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

{% include image.html id="1U_uzhLK2KzUYOyxjCLoHcVjzVneLOQBn" desc="ì¶œì²˜: ìœ„í‚¤ë°±ê³¼" width="50%" height="auto" %}

ë¦¬í”„ ë…¸ë“œ(leaf node)ê°€ ì•„ë‹Œ ë…¸ë“œë“¤(ë§ˆë¦„ëª¨ ëª¨ì–‘)ì€ ê° ì…ë ¥ ë°ì´í„° í”¼ì²˜ì— ëŒ€í•œ ê·œì¹™ì„ ë‚˜íƒ€ë‚´ê³  ìˆìœ¼ë©°, ì´ ê·¸ë¦¼ì—ì„œëŠ” ìœ„ì—ì„œ ë¶€í„° 
1. ì„±ë³„
2. ë‚˜ì´
3. íƒ‘ìŠ¹í•œ ë°°ìš°ìì™€ ìë…€ì˜ ìˆ˜

ì´ë‹¤. ë°˜ë©´ ë¦¬í”„ ë…¸ë“œ(ìƒì¡´/ì‚¬ë§)ëŠ” ê° ê·œì¹™ì— ì´ì–´ì§€ëŠ” ê²½ë¡œë¥¼ ë”°ëì„ ë•Œ, íƒ€ê²Ÿ ë°ì´í„°ì˜ ê°’ì´ë‹¤. ì´ ê·¸ë¦¼ì—ì„œëŠ” 
- ìƒì¡´ í™•ë¥  = í•´ë‹¹ ë¦¬í”„ ë…¸ë“œì— ì†í•˜ëŠ” ìƒì¡´ ìˆ˜ / í•´ë‹¹ í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ì „ì²´ ìŠ¹ê° ìˆ˜
- ë¦¬í”„ë…¸ë“œì— í•´ë‹¹í•  í™•ë¥  = ì‚¬ë§ í˜¹ì€ ìƒì¡´ ìˆ˜ / ì „ì²´ íƒ‘ìŠ¹ê° ìˆ˜

ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤.

---

## í•™ìŠµì˜ ê¸°ì¤€: ë¶ˆí™•ì‹¤ì„±

í•™ìŠµì˜ ê¸°ì¤€ì€ ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” **ì—”íŠ¸ë¡œí”¼(entropy)** í˜¹ì€ **ë¶ˆìˆœë„(impurity)**ê°€ ìµœëŒ€ë¡œ ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ëœë‹¤(ì—”íŠ¸ë¡œí”¼ í•¨ìˆ˜ í˜¹ì€ Giniê³„ìˆ˜ë¥¼ ì“°ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì—”íŠ¸ë¡œí”¼ë¥¼ ì‚¬ìš©í•œë‹¤). ì´ì „ ë‹¨ê³„ì™€ í˜„ì¬ ë‹¨ê³„ì˜ ë¶ˆí™•ì‹¤ì„±ì˜ ì°¨ì´ë¥¼ **ì •ë³´íšë“(information gain, ì´í•˜ IG)**ì´ë¼ê³  í•˜ë©°, ì •ë³´íšë“ì´ ë§ì€ ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤ê³  ë§ í•  ìˆ˜ ìˆë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼, íŠ¹ì • ì˜ì—­ë‚´ì— ë‘ ìƒ‰ìƒì˜ ê³µì„ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œê°€ ìˆê³ , ì‚¬ê°í˜•ì˜ ìˆ˜í‰ í˜¹ì€ ìˆ˜ì§ ë³€ì—ì„œ íŠ¹ì • ì§€ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë°˜ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ê·œì¹™ì´ë¼ê³  í•´ë³´ì. ìš°ë¦¬ì˜ ëª©ì ì€ 10ê°œì˜ ê³µì„ ì˜ ë‚˜ëˆ„ëŠ” ê·œì¹™ë“¤ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë©°, ë¹¨ê°„ê³µì„ 1, ì´ˆë¡ê³µì„ 2ë¡œ í‘œê¸°í•œë‹¤.

{% include image.html id="17jk7c6NbX0ckSKhZqebV5M7m-a3bHyeV" desc="ì˜ì—­ A" width="50%" height="auto" %}

ì´ë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ ì—”íŠ¸ë¡œí”¼ëŠ” ë‹¤ìŒê³¼ ê°™ìœ¼ë©° íŠ¹ì • ì˜ì—­ $A$ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì—¬ê¸°ì„œ $p_k$ëŠ” íŠ¹ì • k í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ë‚˜íƒ€ë‚¸ë‹¤($p_k$ = `k-class ê³µì˜ ê°œìˆ˜` / `ì „ì²´ ê³µì˜ ê°œìˆ˜`).

$$Entropy(A)= - \sum_{k=1}^{K} p_k \log_{2}(p_k)$$

í˜„ì¬ ìƒíƒœì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ë©´, ë‹¤ìŒê³¼ ê°™ë‹¤. 

$$Entropy(A)= - \Big( \dfrac{6}{10}\log_{2}(\dfrac{6}{10}) + \dfrac{4}{10}\log_{2}(\dfrac{4}{10}) \Big) = 0.9710$$

```python
import numpy as np

def calculate_entropy(pks):
    log = np.log2(pks)
    return - (pks * log).sum()

pks = np.array([6/10, 4/10])  # A: p_red, p_greem
entropy = calculate_entropy(pks)
print(f"Entropy is {entropy:.4f}")
# Entropy is 0.9710
```

{% include image.html id="133G4Y4HmuHBGTeSisfZw76h3CoXhD-5B" desc="ì¢Œìš° ì˜ì—­ A1, A2 ë‚˜ëˆŒ ê²½ìš°" width="50%" height="auto" %}

ì´ì œ ê°€ë¡œë³€ì˜ ì„ì˜ë¡œ í•œ ê³³ì„ ë‚˜ëˆ ì„œ ë‘ ê°œì˜ ì˜ì—­(ì¢Œ: $A_1$, ìš°: $A_2$)ìœ¼ë¡œ ë‚˜ëˆ ë³¸ë‹¤. ì´ë•Œì˜ ì—”íŠ¸ë¡œí”¼ëŠ” ë‘ ì—”íŠ¸ë¡œí”¼ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ë‹¤. 

$$\begin{aligned} Entropy(A) &= \sum_{i=1}^{m} \dfrac{1}{m} Entropy(A_m) = \dfrac{1}{2} Entropy(A_1) + \dfrac{1}{2} Entropy(A_2) \\
&= - \sum_{i=1}^{2} \dfrac{1}{2} \sum_{k=1}^K p_k^{(i)} \log_2 (p_k^{(i)}) \\
&= - \dfrac{1}{2} \Big( \dfrac{5}{6}\log_{2}(\dfrac{5}{6}) + \dfrac{1}{6}\log_{2}(\dfrac{1}{6}) \Big) - \dfrac{1}{2} \Big( \dfrac{1}{4}\log_{2}(\dfrac{1}{4}) + \dfrac{3}{4}\log_{2}(\dfrac{3}{4}) \Big) \\
&= 0.7307
\end{aligned}$$

```python
def calculate_entropy(pks):
    if pks.ndim == 1:
        pks = np.expand_dims(pks, 0)
    log = np.log2(pks + 1e-10)
    entropy = - 1/len(pks) * (pks * log).sum()
    return entropy.round(6)

pks = np.array([
    [5/6, 1/6],  # A_1: p_red, p_greem
    [1/4, 3/4]]  # A_2: p_red, p_greem
)
entropy2 = calculate_entropy(pks)
print(f"Entropy is {entropy2:.4f}")
# Entropy is 0.7307
```

ì´ì œ ìš°ë¦¬ëŠ” ì •ë³´íšë“(IG)ì„ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ì´ì „ ë‹¨ê³„ê³¼ í˜„ì¬ ìƒíƒœì˜ ë¶ˆí™•ì‹¤ì„± ì°¨ì´ì¸ $IG_1 = 0.9710 - 0.7307 = 0.2403$ê°€ ì •ë³´íšë“ëŸ‰ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. 

ê·¸ë ‡ë‹¤ë©´ ë‹¤ë¥¸ ê²½ìš°ì—ëŠ” ì–´ë–¨ê¹Œ? ì˜ˆë¥¼ ë“¤ì–´ ê°€ë¡œë¡œ ì„ ì„ ê·¸ì–´ ë‘ ì˜ì—­(ìƒ: $A_1$, í•˜: $A_2$)ì„ ë‚˜ëˆ ë³´ê³  ì •ë³´íšë“ëŸ‰ì„ ê³„ì‚°í•´ë³´ì.

{% include image.html id="1VvzOYsLNMEoM2mOtCi9FbHxIo5e-bUVM" desc="ìƒí•˜ ì˜ì—­ A1, A2 ë‚˜ëˆŒ ê²½ìš°" width="50%" height="auto" %}

```python
pks = np.array([
    [4/5, 1/5],  # A_1: p_red, p_greem
    [2/5, 3/5]]  # A_2: p_red, p_greem
)
entropy3 = calculate_entropy(pks)
print(f"Entropy is {entropy3:.4f}")
# Entropy is 0.8464
```

ê³„ì‚°í•´ë³´ë‹ˆ ì •ë³´íšë“ëŸ‰ì€ $IG_2 = 0.9710 - 0.8464 = 0.1246$ì´ ë˜ê¸° ë•Œë¬¸ì—, ì„¸ë¡œë¡œ ì„ ì„ ê¸‹ëŠ” ë°©ë²• ë³´ë‹¤ ì„ í˜¸í•˜ì§€ ì•ŠëŠ” ê·œì¹™ì´ ë  ê²ƒì´ë‹¤. 

---

## í•™ìŠµ ë°©ë²•: ì¬ê·€ì  ë¶„ê¸° & ê°€ì§€ì¹˜ê¸°

### ì¬ê·€ì  ë¶„ê¸°(recursive partitioning)

ì˜ˆë¥¼ ë“¤ì–´ ëŒ€ì¶œ ì‹¬ì‚¬ë¥¼ í•˜ëŠ” ë°ì´í„°ê°€ ìˆë‹¤ë©´, ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ ì•Œì•„ë³¸ë‹¤.

| ìë™ì°¨ ì†Œìœ  | ì†Œë“ | ë³´ìœ  ëŒ€ì¶œ ê±´ìˆ˜| ëŒ€ì¶œì—¬ë¶€ |
|:-:|:-:|:-:|:-:|
|0|650|1| yes|
|0|200|0| no|
|1|700|3| no|
|0|500|0| no|
|0|425|1| no|
|1|900|1| yes|

ë¨¼ì € ì²˜ìŒ ì‹œì ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$Entropy(A)= - \Big( \dfrac{4}{6}\log(\dfrac{4}{6}) + \dfrac{2}{6}\log(\dfrac{2}{6}) \Big) = 0.9183$$

```python
pks = np.array([4/6, 2/6])
entropy = calculate_entropy(pks)
print(f"Entropy is {entropy:.4f}")
# Entropy is 0.9183
```

ê·¸ í›„ íŠ¹ì • í”¼ì²˜ë¥¼ ì„ ì •í•´ì„œ ì •ë ¬ í›„ì— ê° ë¶„ê¸°ì ì—ì„œ í•œë²ˆì”© ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ê³  IGë¥¼ êµ¬í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ í‘œì²˜ëŸ¼ `ì†Œë“ <= 200`ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ ì„œ ê³„ì‚°í•˜ê³ , ë‹¤ìŒ ê¸°ì¤€ì„ ì„ ì •í•´ì„œ ê³„ì† ê³„ì‚°í•œë‹¤. 

| ìë™ì°¨ ì†Œìœ  | ì†Œë“ | ë³´ìœ  ëŒ€ì¶œ ê±´ìˆ˜ | ëŒ€ì¶œì—¬ë¶€ |
|:-:|:-:|:-:|:-:|
|<span style="color: #7d7ee8">0</span>|<span style="color: #7d7ee8">200</span>|<span style="color: #7d7ee8">0</span>|<span style="color: #7d7ee8">no</span>|
|0|425|1| no|
|0|500|0| no|
|0|650|1| yes|
|1|700|3| no|
|1|900|1| yes|

---

|Colname|Value|MaxIG|ClsCount|
|---|---|---|---|
|income|700|0.677653| {False: {0: 0, 1: 1}, True: {0: 4, 1: 1}}
|existloan|0|0.594646| {False: {0: 1, 1: 0}, True: {0: 3, 1: 2}}
|car|0|-0.018797|{False: {0: 1, 1: 1}, True: {0: 3, 1: 1}}

[expand]summary:ì „ì²´ì½”ë“œ ë³´ê¸° ğŸ‘ˆ

```python
import numpy as np
import pandas as pd

def calculate_entropy(counts):
    if counts.ndim == 1:
        counts = np.expand_dims(counts, 0)
    pks = counts / counts.sum(1, keepdims=True)
    log = np.log2(pks + 1e-10)
    e = -(pks * log).sum(1)
    return e

def get_info_gain(df, E_base, x_col):
    df = df.sort_values(x_col).reset_index(drop=True)
    unique_x_values = df[x_col].unique()
    print(f"[Start] Processing column: {x_col}: {list(unique_x_values)}")
    
    IG_candidates = []
    IG_counts = []
    for v in unique_x_values[:-1]:
        df["group"] = (df[x_col] <= v)
        counts = df.groupby(["group", "loan"]).size().unstack(fill_value=0)
        E_x_col = calculate_entropy(counts.values)
        if len(E_x_col) >= 2:
            weights = counts.values.sum(0) / counts.values.sum()
            E_x_col = (E_x_col * weights).sum()
        else:
            E_x_col = E_x_col.sum()
        info_gain = E_base - E_x_col
        IG_candidates.append(info_gain)
        IG_counts.append(counts.T.to_dict())
        print(f"  Testing Value <= {v} | Entropy: {E_x_col:.4f} | IG: {info_gain:.4f}")
        print("  Counts:", counts.T.to_dict())
    IG_candidates = np.array(IG_candidates)
    IG_max = np.max(IG_candidates)
    idx_IG_max = np.argmax(IG_candidates)
    IG_value = df.loc[idx_IG_max, x_col]
    print(f"[Result] Max information gain is {IG_max:.4f} value: {IG_value}")
    print()
    del df["group"]
    return (IG_value, IG_max, IG_counts[idx_IG_max])

df = pd.DataFrame(
    dict(
        car=[0, 0, 1, 0, 0, 1],
        income=[650, 200, 700, 500, 425, 900],
        existloan=[1, 0, 3, 0, 1, 1],
        loan=[1, 0, 0, 0, 0, 1]
    )
)
E_base = calculate_entropy(df["loan"].value_counts().values).sum()
IGs = []
for x_col in ["car", "income", "existloan"]:
    IG_value, IG_max, IG_counts = get_info_gain(df, E_base, x_col)
    IGs.append((x_col, IG_value, IG_max, IG_counts))
df_res = pd.DataFrame({col: x for col, x in zip(
    ["Colname", "Value", "MaxIG", "ClsCount"], list(zip(*IGs)))})
df_res.sort_values("MaxIG", ascending=False)

# [Start] Processing column: car: [0, 1]
# 0.9370927078645052
#   Testing Value <= 0 | Entropy: 0.9371 | IG: -0.0188
#   Counts: {False: {0: 1, 1: 1}, True: {0: 3, 1: 1}}
# [Result] Max information gain is -0.0188 value: 0

# [Start] Processing column: income: [200, 425, 500, 650, 700, 900]
# 0.6473003960626632
#   Testing Value <= 200 | Entropy: 0.6473 | IG: 0.2710
#   Counts: {False: {0: 3, 1: 2}, True: {0: 1, 1: 0}}
# 0.6666666664262174
#   Testing Value <= 425 | Entropy: 0.6667 | IG: 0.2516
#   Counts: {False: {0: 2, 1: 2}, True: {0: 2, 1: 0}}
# 0.6121972224625437
#   Testing Value <= 500 | Entropy: 0.6122 | IG: 0.3061
#   Counts: {False: {0: 1, 1: 2}, True: {0: 3, 1: 0}}
# 0.9370927078645052
#   Testing Value <= 650 | Entropy: 0.9371 | IG: -0.0188
#   Counts: {False: {0: 1, 1: 1}, True: {0: 3, 1: 1}}
# 0.2406426981034281
#   Testing Value <= 700 | Entropy: 0.2406 | IG: 0.6777
#   Counts: {False: {0: 0, 1: 1}, True: {0: 4, 1: 1}}
# [Result] Max information gain is 0.6777 value: 700

# [Start] Processing column: existloan: [0, 1, 3]
# 0.6666666664262174
#   Testing Value <= 0 | Entropy: 0.6667 | IG: 0.2516
#   Counts: {False: {0: 2, 1: 2}, True: {0: 2, 1: 0}}
# 0.32365019795919686
#   Testing Value <= 1 | Entropy: 0.3237 | IG: 0.5946
#   Counts: {False: {0: 1, 1: 0}, True: {0: 3, 1: 2}}
# [Result] Max information gain is 0.5946 value: 0

```

[/expand]

ìœ„ ì½”ë“œì™€ í‘œëŠ” ê° ì…ë ¥ í”¼ì²˜ë¡œ í•˜ë‚˜ì”© ê·œì¹™ì„ ì°¾ì€ ê²°ê³¼ë‹¤. ì‚´í´ë³´ë©´ ìµœëŒ€ ì •ë³´íšë“±ëŸ‰(MaxIG)ì€ $0.068056$ ìœ¼ë¡œ `ì†Œë“ <= 700` ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì²«ë²ˆì§¸ ë…¸ë“œì˜ ê·œì¹™ì´ ëœë‹¤. ë§Œì•½ ì†Œë“ì´ ê·œì¹™ì¸ 700ë³´ë‹¤ ì‘ì€ ê°’ì´ë©´, ëŒ€ì¶œì—¬ë¶€ê°€ 0ì¸ ê°’ì€ 4ê°œ, 1ì¸ ê°’ì€ 1ê°œê°€ ë˜ê³ , 700 ë³´ë‹¤ í¬ë‹¤ë©´ ëŒ€ì¶œì—¬ë¶€ê°€ 0ì¸ ê°’ì€ 0ê°œ, 1ì¸ ê°’ì€ 1ê°œê°€ ëœë‹¤. ë‹¤ì‹œ í’€ì–´ì„œ ë§í•˜ë©´, ë§Œì•½ì— ë‹¹ì‹ ì˜ ì†Œë“ì´ 700 ë³´ë‹¤ ì ë‹¤ë©´, í•™ìŠµëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ, ëŒ€ì¶œê°€ëŠ¥í•œ í™•ë¥ ì€ 20%(1/5) ì •ë„ê°€ ë ê²ƒì´ë‹¤. 

ì´ë ‡ê²Œ ì²«ë²ˆì§¸ ê·œì¹™ì´ ì •í•´ì§€ë©´, ì‚¬ëŒì´ ì •í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ ë‚˜ë¬´ì˜ ìµœëŒ€ ê¹Šì´(max depth)ì— ë”°ë¼ì„œ ê³„ì† ì°¾ì„ ê²ƒì¸ì§€ ì•„ë‹ˆë©´ ë©ˆì¶œì§€ë¥¼ ê²°ì •í•œë‹¤. ê·¸ë¦¬ê³  ì—”íŠ¸ë¡œí”¼ê°€ 0ì´ ë˜ë©´ í•™ìŠµì„ ìµœëŒ€ ê¹Šì´ê°€ ì•„ë‹ˆë”ë¼ë„ ì•Œì•„ì„œ ë©ˆì¶”ê²Œ ëœë‹¤. ë‹¤ë§Œ, ì˜ì‚¬ê²°ì • ë‚˜ë¬´ ëª¨ë¸ì´ ê¹Šê²Œ ë“¤ì–´ê°ˆ ìˆ˜ë¡ ê³¼ì í•©(overfitting)ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.

scikit-learn íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•˜ë©´ ì†ì‰½ê²Œ ì˜ì‚¬ê²°ì • ë‚˜ë¬´ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ í”¼ì²˜ì˜ ì •ë ¬í•˜ì—¬ íŠ¹ì •ê°’ì„ ê¸°ì¤€ê°’ìœ¼ë¡œ ì •í•˜ì§€ ì•Šê³  ì•½ê°„ ë‹¤ë¥´ê²Œ ì ìš©í•œë‹¤.

```python
import pandas as pd
from sklearn import tree

df = pd.DataFrame(
    dict(
        car=[0, 0, 1, 0, 0, 1],
        income=[650, 200, 700, 500, 425, 900],
        existloan=[1, 0, 3, 0, 1, 1],
        loan=[1, 0, 0, 0, 0, 1]
    )
)

X, y = df.iloc[:, :-1], df.iloc[:, -1]

clf = tree.DecisionTreeClassifier(criterion="entropy", max_depth=3)
clf = clf.fit(X, y)
```

ë§Œì•½ graphvizë¥¼ ì„¤ì¹˜í–ˆë‹¤ë©´, ì˜ì‚¬ê²°ì • ë‚˜ë¬´ ëª¨ë¸ì„ ì‹œê°í™” í•´ë³¼ ìˆ˜ë„ ìˆë‹¤.

```python
import graphviz
dot_data = tree.export_graphviz(
    clf, out_file=None, 
    feature_names=["car", "income", "existloan"], 
    class_names=["ëŒ€ì¶œ ë¶ˆê°€ëŠ¥", "ëŒ€ì¶œ ê°€ëŠ¥"], 
    filled=True, rounded=True, 
    special_characters=True
)
graph = graphviz.Source(dot_data)
graph
```

{% include image.html id="1B8ZxPjkeFn_TCkG-ll5JSPu0JBGsLuHQ" desc="Graphvizë¡œ ì˜ì‚¬ê²°ì • ë‚˜ë¬´ ëª¨ë¸ ì‹œê°í™”" width="90%" height="auto" %}

ì´ ëª¨ë¸ ê·¸ë˜í”„ì— ë”°ë¥´ë©´ ëŒ€ì¶œì´ ê°€ëŠ¥í•œ ì‚¬ëŒì€ ìˆ˜ì…(income)ì´ 575 ì´ìƒì´ì–´ì•¼ í•˜ê³ , ëŒ€ì¶œ ë³´ìœ  ëŒ€ì¶œ ê±´ìˆ˜(existloan)ê°€ 2ê°œ ì´í•˜ì—¬ì•¼ ëŒ€ì¶œì´ ê°€ëŠ¥í•˜ë‹¤. 

<br>

### ê°€ì§€ì¹˜ê¸°(pruning)

ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆë‹¤. scikit-learnì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- `min_samples_leaf`: ë¦¬í”„ ë…¸ë“œê°€ ë˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œì˜ ê°œìˆ˜ë¥¼ ë§í•˜ë©°, ì´ ê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡ ëª¨ë¸ì´ ê°„ê²°í•´ì§€ë‚˜, í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•  ê²½ìš°, ì „ì²´ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ê°€ ìˆë‹¤. 
- `max_depth`: ìµœëŒ€ ê¹Šì´, ê¹Šì´ë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ ëª¨ë¸ì„ ê°„ê²°í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆì§€ë§Œ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆë‹¤. 

ì—¬ê¸°ì„œ ë§í•˜ëŠ” ê°€ì§€ì¹˜ê¸°(purning)ëŠ” **ë¹„ìš©ë³µì¡ë„ ê°€ì§€ì¹˜ê¸°(Cost Complexity Pruning)**ì„ ë§í•˜ë©°, ë³µì¡ë„ íŒŒë¼ë¯¸í„°ë¼ê³  ë¶ˆë¦¬ëŠ” $\alpha$ë¡œ ì¡°ì ˆ í•  ìˆ˜ ìˆë‹¤. ì˜ì‚¬ê²°ì • ë‚˜ë¬´ ëª¨ë¸ $T$ê°€ ì£¼ì–´ ì¡Œì„ ë•Œ, ë¹„ìš©ë³µì¡ë„ í•¨ìˆ˜$R_{\alpha}(T)$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê²°ì •ëœë‹¤.

$$R_{\alpha}(T) = R(T) + \alpha \vert \hat{T} \vert$$

ì—¬ê¸°ì„œ $\vert \hat{T} \vert$ëŠ” ë¦¬í”„ ë…¸ë“œì˜ ê°œìˆ˜, $R(T)$ëŠ” ì „ì²´ ë¦¬í”„ ë…¸ë“œì—ì„œ ê³„ì‚°ëœ ì˜¤ë¶„ë¥˜ìœ¨ì´ë‹¤. Scikit-learnì—ì„œëŠ” $R(T)$ë¥¼ ì „ì²´ ìƒ˜í”Œë¡œ ê°€ì¤‘ì¹˜í™”ëœ ë¦¬í”„ ë…¸ë“œì˜ ë¶ˆìˆœë„(impurity)ë¡œ ëŒ€ì‹  ê³„ì‚°í•œë‹¤. $\alpha$ê°’ì´ ì˜¬ë¼ ê°ˆ ìˆ˜ë¡, í›ˆë ¨ ë°ì´í„°ì—ì„œ ì˜ì‚¬ ê²°ì • ë‚˜ë¬´ ëª¨ë¸ì˜ ê¹Šì´ì™€ ë…¸ë“œì˜ ê°œìˆ˜ê°€ ì ì  ë–¨ì–´ì§„ë‹¤. ë”°ë¼ì„œ, $\alpha$ê°’ì„ ì˜ ì¡°ì ˆí•˜ë©´, ê²€ì¦ ë°ì´í„°ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ” ìµœì ì˜ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

{% include image.html id="1Q_0g6iCs1sS7dvG1c7OA451PPrDUIMEF" desc="Alphaê°’ì— ë”°ë¥¸ Train/Test ë°ì´í„°ì—ì„œ ì •í™•ë„ ë³€í™”" width="90%" height="auto" %}


<br>

---

## Feature Importance

**í”¼ì²˜ ì¤‘ìš”ë„(Feature Improtance)**ë€ ì˜ì‚¬ê²°ì • ë‚˜ë¬´ë¥¼ ë§Œë“œëŠ”ë° ê¸°ì—¬í•œ í”¼ì²˜ì˜ ì •ëŸ‰ì  í‰ê°€ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì¬ê·€ì  ë¶„ë¦¬ì™€ ê°€ì§€ì¹˜ê¸°ë¥¼ í†µí•´ ì˜ì‚¬ê²°ì • ë‚˜ë¬´ë¥¼ ìƒì„±í•  ë•Œ, ë¶ˆìˆœë„ë¥¼ ê°€ì¥ ë§ì´ ì¤„ì´ëŠ” í”¼ì²˜ê°€ ê³§ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ”ë° í° ê³µí—Œì„ ìƒˆìš´ í”¼ì²˜ë¼ê³  í•  ìˆ˜ ìˆìœ¼ë©°, ì¤‘ìš”ë„ê°€ ë†’ë‹¤ê³  ë§ í•  ìˆ˜ ìˆë‹¤. 

### Feature Importance êµ¬í•˜ëŠ” ë°©ë²•

ì¼ë°˜ì ì¸ Feature Importanceë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì€, ë‹¤ë¦„ ì˜ì‚¬ê²°ì • ë‚˜ë¬´ë¥¼ ë³´ê³  ê³„ì‚°í•˜ë©´ì„œ ì•Œì•„ë³´ì.

[expand]summary:ì „ì²´ì½”ë“œ ë³´ê¸° ğŸ‘ˆ

```python
import pandas as pd
from sklearn import tree
import graphviz

df = pd.DataFrame(
    dict(
        car=[0, 0, 1, 0, 0, 1, 1],
        income=[650, 200, 700, 500, 425, 900, 550],
        existloan=[1, 0, 3, 0, 1, 1, 0],
        loan=[1, 0, 0, 0, 0, 1, 1]
    )
)

X, y = df.iloc[:, :-1], df.iloc[:, -1]

clf = tree.DecisionTreeClassifier(criterion="entropy", max_depth=3)
clf = clf.fit(X, y)

dot_data = tree.export_graphviz(
    clf, out_file=None, 
    feature_names=["car", "income", "existloan"], 
    class_names=["ëŒ€ì¶œ ë¶ˆê°€ëŠ¥", "ëŒ€ì¶œ ê°€ëŠ¥"], 
    filled=True, rounded=True, 
    special_characters=True
)
graph = graphviz.Source(dot_data)
```

[/expand]

{% include image.html id="1KZS2M8IQDzULdVkRTIbkK1SlkkGBUWQu" desc="ìƒˆë¡œìš´ Tree" width="90%" height="auto" %}

$i$ë²ˆì§¸ ê°€ì§€(feature)ì—ì„œ ë…¸ë“œê°€ $L$ê³¼ $R$ë¡œ ë¶„ë¦¬ ë˜ì—ˆ ë‹¤ë©´ information gain ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•œë‹¤. ì—¬ê¸°ì„œ $N$ì€ ì „ì²´ ìƒ˜í”Œì˜ ê°œìˆ˜, $N_i$ëŠ” ë¶„ë¦¬ ì´ì „ì˜ í•´ë‹¹ ë…¸ë“œì—ì„œ ë³´ìœ í•˜ê³  ìˆëŠ” ìƒ˜í”Œì˜ ê°œìˆ˜, $N_{(i, L)}$ëŠ” ì¢Œì¸¡ìœ¼ë¡œ ë¶„ë¦¬ëœ ìƒ˜í”Œì˜ ê°œìˆ˜, $N_{(i, R)}$ì€ ìš°ì¸¡ìœ¼ë¡œ ë¶„ë¦¬ëœ ìƒ˜í”Œì˜ ê°œìˆ˜, $E_i$ëŠ” ë¶„ë¦¬ ì´ì „ì˜ ì—”íŠ¸ë¡œí”¼, $E_{(i, L)}$ê³¼, $E_{(i, R)}$ì€ ê°ê° ì¢Œì¸¡ê³¼ ìš°ì¸¡ì˜ ì—”íŠ¸ë¡œí”¼ë‹¤.

$$IG_{i} = \dfrac{N_i}{N} E_{i} - \dfrac{N_{(i, L)}}{N_i} E_{(i, L)} - \dfrac{N_{(i, R)}}{N_i} E_{(i, R)}$$

ì§€ê¸ˆ ê·¸ë˜í”„ëŠ” ë‘ ê°œì˜ í”¼ì²˜(income, existloan)ìœ¼ë¡œ ì¸í•´ ë‚˜ëˆ ì¡Œê³ , ë¨¼ì € incomeì˜ ì •ë³´íšë“ëŸ‰ì„ êµ¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$\begin{aligned}
IG_{income} &= \dfrac{N_{income}}{N} E_{income} - \dfrac{N_{(income, L)}}{N_{income}} E_{(income, L)} - \dfrac{N_{(income, R)}}{N_{income}} E_{(income, R)} \\
&= \dfrac{7}{7} \times 0.9852 - \dfrac{3}{7} \times 0.0 - \dfrac{4}{7} \times 0.8113 \\
&= 0.5216
\end{aligned}$$

ë‚˜ë¨¸ì§€ existloanì˜ ì •ë³´íšë“ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$\begin{aligned}
IG_{existloan} &= \dfrac{N_{existloan}}{N} E_{existloan} - \dfrac{N_{(existloan, L)}}{N_{existloan}} E_{(existloan, L)} - \dfrac{N_{(existloan, R)}}{N_{existloan}} E_{(existloan, R)} \\
&= \dfrac{4}{7} \times 0.8113 - \dfrac{1}{4} \times 0.0 - \dfrac{3}{4} \times 0.0 \\
&= 0.4636
\end{aligned}$$

car ì¹¼ëŸ¼ì€ ì“°ì´ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— í”¼ì²˜ ì¤‘ìš”ë„ëŠ” 0ì´ ëœë‹¤. ë”°ë¼ì„œ ê°ê°ì˜ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì¼ë°˜í™”(normalize) ì‹œí‚¤ë©´ `(car, income, existloan) = (0, 0.5295, 0.4705)`ê°€ ëœë‹¤.

[expand]summary:ê³„ì‚°ì½”ë“œ ë³´ê¸° ğŸ‘ˆ

```python
from scipy.stats import entropy

def calculate_ig(x, x_left, x_right, n_samples):
    e_base = entropy(x, base=2)
    e_left = entropy(x_left, base=2)
    e_right = entropy(x_right, base=2)
    IG = x.sum()/n_samples * e_base - x_left.sum()/x.sum() * e_left - x_right.sum()/x.sum() * e_right
    print(f"E Base: {e_base:.4f} | E Left {e_left:.4f} | E Right {e_right:.4f}")
    print(f"Information Gain = {IG:.4f}")
    return IG

n_samples = 7
# Income
print("[Feature] Income")
x_income = np.array([4, 3])
x_income_left = np.array([3, 0])
x_income_right = np.array([1, 3])
IG_income = calculate_ig(x_income, x_income_left, x_income_right, n_samples)
# Existloan
print("[Feature] Existloan")
x_existloan = np.array([1, 3])
x_existloan_left = np.array([0, 3])
x_existloan_right = np.array([1, 0])
IG_existloan = calculate_ig(x_existloan, x_existloan_left, x_existloan_right, n_samples)
print("[Feature Importance] Normalized (Car, Income, Existloan)")
IG_car = 0
x = np.array([IG_car, IG_income, IG_existloan])
normed = x / x.sum()
print(normed.round(4))
print("[Feature Importance] in scikit-learn")
print(clf.feature_importances_.round(4))

# [Feature] Income
# E Base: 0.9852 | E Left 0.0000 | E Right 0.8113
# Information Gain = 0.5216
# [Feature] Existloan
# E Base: 0.8113 | E Left 0.0000 | E Right 0.0000
# Information Gain = 0.4636
# [Feature Importance] Normalized (Car, Income, Existloan)
# [0.     0.5295 0.4705]
# [Feature Importance] in scikit-learn
# [0.     0.5295 0.4705]
```

[/expand]

### ì‚¬ìš©ì‹œ ì£¼ì˜í•  ì 

ì‚¬ìš©ì‹œì— ì£¼ì˜í•  ì ì´ ìˆëŠ”ë°, í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì ˆëŒ€ì ì¸ ì§€í‘œë¡œ ì‚¬ìš©í•˜ë©´ ì•ˆ ëœë‹¤. ê·¸ ì´ìœ ëŠ” í›ˆë ¨ ë°ì´í„°ì— ìµœì í™”ëœ ëª¨ë¸ì—ì„œ ë³´ì—¬ì£¼ëŠ” ì¤‘ìš”ë„ì´ê¸° ë•Œë¬¸ì—, íŠ¹ì • í”¼ì²˜ê°€ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤. ìœ„ì—ì„œ ì„¤ëª…í•œ ìë™ì°¨ ë³´ìœ  ì—¬ë¶€ì¸ í”¼ì²˜ carì˜ ê²½ìš° ëª¨ë¸ì— ê³ ë ¤ë˜ì§€ ì•Šì•˜ë‹¤ê³  í•´ì„œ ëŒ€ì¶œì˜ ì¤‘ìš”í•œ ì²™ë„ê°€ ì•„ë‹ˆë‹¤. ì‹¤ì œë¡œ ì–´ë–¤ ì‚¬ëŒì´ ì°¨ë¥¼ ì†Œìœ í–ˆë‹¤ë©´, ë³´í†µì€ ê·¸ ìœ ì§€ë¹„ìš©ì„ ê°ë‹¹í•  ìˆ˜ ìˆì–´ì„œ(ì¦‰, ì–´ëŠì •ë„ì˜ í˜„ê¸ˆ íë¦„ì´ ìˆë‹¤)ì°¨ë¥¼ ìƒ€ë‹¤ê³  ìƒê°í•˜ê¸° ë•Œë¬¸ì— ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  ë³´ê¸°ëŠ” í˜ë“¤ë‹¤. í•˜ì§€ë§Œ ìƒëŒ€ì ìœ¼ë¡œ ì¤‘ìš”í•˜ë‹¤ê³ ëŠ” ë§ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ë§Œë“¤ê³  ë¶„ì„ì‹œì— ìœ ìš©í•˜ê²Œ ì“°ì¸ë‹¤.

## Permutation Feature Importance 

Permutation Feature ImportanceëŠ” featureì˜ ê°’ì„ ì„ì˜ë¡œ ì¹˜í™˜í–ˆì„ ë•Œ ì„±ëŠ¥ì˜ ë³€í™”ë¥¼ ë³¸ë‹¤. ë§Œì•½ í•´ë‹¹ featureê°€ ëª¨ë¸ì—ì„œ í¬ê²Œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆë‹¤ë©´ ê°’ì„ ì¹˜í™˜í–ˆì„ ë•Œ ì„±ëŠ¥ì´ í¬ê² ë–¨ì–´ì§„ë‹¤ëŠ” ì•„ì´ë””ì–´ì—ì„œ ì‹œì‘í•œë‹¤. ì…ë ¥ ë°ì´í„° $X$, íƒ€ê²Ÿ ë°ì´í„° $y$, ëª¨ë¸ $f$ê³¼ ì†ì‹¤í•¨ìˆ˜ $L$ë¡œ ì£¼ì–´ ì¡Œì„ ë•Œ, ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ ì¸¡ì •: $e^{original} = L\big(y, f(X)\big)$
2. ë°ì´í„°ì˜ ê° í”¼ì²˜ $j$ì— ëŒ€í•´ì„œ 
   1. $K$ë²ˆ ë°˜ë³µí•œë‹¤. ($k = 1, \cdots, K$)
      1. ëœë¤í•˜ê²Œ í”¼ì²˜ $j$ì˜ ë°ì´í„°ë¥¼ ì…”í”Œí•˜ì—¬ ìƒˆë¡œìš´ ë³€í˜•ëœ ë°ì´í„° ì„¸íŠ¸ $\hat{X}^{(j)}_k$ë¥¼ ë§Œë“ ë‹¤
      2. ë³€í˜•ëœ ë°ì´í„° ì„¸íŠ¸ë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤. $e^{(j)}_k = L\big(y, f(\hat{X}^{(j)}_k)\big)$
   2. í”¼ì²˜ $k$ì˜ ì¤‘ìš”ë„ $I^{(j)}$ë¥¼ ê³„ì‚°í•œë‹¤. $I^{(j)} = e^{original} - \dfrac{1}{K} \sum_{k=1}^{K} e^{(j)}_k$

Scikit-learnì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì œê³µí•˜ê³  ìˆë‹¤. ë‹¤ë§Œ ëª¨ë¸ì´ ì»¤ì§ˆ ê²½ìš° ì‹¤í–‰ì‹œê°„ì´ ê½¤ ì˜¤ë˜ ê±¸ë¦°ë‹¤.

```python
from sklearn.inspection import permutation_importance

r = permutation_importance(clf, X, y, n_repeats=30, random_state=0)

for i in r.importances_mean.argsort()[::-1]:
    print(f"{df.columns[i]}  {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}")

# income  0.462 +/- 0.180
# existloan  0.195 +/- 0.078
# car  0.000 +/- 0.000
```

---

# ì¥ë‹¨ì 

ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ, ì™œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ê³  í•˜ëŠ”ì§€ ì´í•´í•˜ê³  ì“°ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ê¸°ì—, ì˜ì‚¬ê²°ì • ë‚˜ë¬´ ëª¨ë¸ì˜ ì¥ë‹¨ì ì„ ìš”ì•½í•´ì„œ ì •ë¦¬í•´ë³´ì•˜ë‹¤. 

## ì¥ì 

- ì‚¬ëŒì´ í•´ì„í•˜ê³ , ì´í•´í•˜ê¸° ì‰½ê²Œ ì‹œê°í™”ê°€ ê°€ëŠ¥í•˜ë‹¤.
- ì¼ë°˜í™”(normalization), ë”ë¯¸ë³€ìˆ˜(dummy variables), ê²°ì¸¡ì¹˜(missing values)ì— ëŒ€í•œ ì „ì²˜ë¦¬ê°€ ê±°ì˜ í•„ìš”ì—†ë‹¤.
- ìˆ˜ì¹˜í˜•ê³¼ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë‹¤ë£° ìˆ˜ê°€ ìˆë‹¤. 
- ì¡°ì ˆí•´ì•¼í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ê¸± ë•Œë¬¸ì—, ë¹ ë¥´ê²Œ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆë‹¤.

## ë‹¨ì 

- ì¡°ì ˆì„ ëª»í•˜ë©´ ê³¼ì í•©ëœ ëª¨ë¸ì„ ìƒì„±í•  ê°€ëŠ¥ì„±ì´ í¬ë‹¤. 
- ë°ì´í„°ì˜ ì‘ì€ ë³€ë™ìœ¼ë¡œ ì¸í•´ ì™„ì „íˆ ë‹¤ë¥¸ íŠ¸ë¦¬ê°€ ìƒì„±ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ëŠ” ë¶ˆì•ˆì •í•  ìˆ˜ ìˆë‹¤. ì´ ë¬¸ì œëŠ” ì•™ìƒë¸” ë‚´ì—ì„œ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì™„í™”í•  ìˆ˜ ìˆë‹¤.
- ì˜ˆì¸¡ê°’ì´ ë§¤ë„ëŸ½ì§€ë„ ì•Šê³  ì—°ì†ì ì´ì§€ë„ ì•Šê³  ë‹¨í¸ì ìœ¼ë¡œ ì¼ì •í•œ ê·¼ì‚¬ì¹˜ì´ë‹¤. ë³´ì™¸ë²•(extrapolation)ì„ ìˆ˜í–‰í•˜ê¸°ê°€ ì–´ë µë‹¤. ì¦‰, ì¼ë°˜í™”ê°€ ì•ˆë  ìˆ˜ ìˆë‹¤.
- íŠ¹ì • í´ë˜ìŠ¤ì˜ ê°’ì— ì§€ë°°ì ìœ¼ë¡œ í¸í–¥ë˜ëŠ” ê²½ìš°, í¸í–¥ëœ íŠ¸ë¦¬ë¥¼ ë§Œë“ ë‹¤. ë”°ë¼ì„œ í•™ìŠµ ì „ì— ê· í˜• ìˆëŠ” ë°ì´í„° ì„¸íŠ¸ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.