---
layout: post
title: "FARM tutorial"
date: "2021-03-31 01:30:01 +0900"
categories: nlp
author: "Soo"
comments: true
toc: true
---

{% include image.html id="1hbtUClFoXg45IbViZoFRLnnDGVlr9Dlb" desc="" width="50%" height="auto" %}

# FARM

> Framework for Adapting Representation Models

ì´ íŒ¨í‚¤ì§€ë¥¼ í•œ ë§ˆë””ë¡œ ìš”ì•½í•˜ë©´ Fine-tuningì— ìµœì í™”ëœ ë„êµ¬ë‹¤.

ìµœê·¼ì˜ ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ëŠ” Transformerì™€ ê·¸ ë³€í˜•ì˜ ë“±ì¥ìœ¼ë¡œ ì¸í•´, ë³´í†µ 2ë‹¨ê³„ë¡œ ë‚˜ëˆ ì„œ í•™ìŠµì´ ì§„í–‰ëœë‹¤. 
1. **Pretrained Language Modeling**
    
   ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë¹„ì§€ë„í•™ìŠµ(unsupervised learning)ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ë§ì€ ì§„í–‰í•œë‹¤. ì–¸ì–´ ëª¨ë¸ë§ì´ë€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ë¡œ ëª¨ë¸ë§í•˜ëŠ” ê³¼ì •ì´ë‹¤. ì‰½ê²Œ ë§í•˜ë©´, ëª¨ë¸ì—ê²Œ ë‹¨ì–´ë“¤ì„ ì…ë ¥í–ˆì„ ë•Œ, ì œì¼ ë§ì´ ë˜ëŠ” ë‹¨ì–´(í† í°)ì„ ë±‰ì–´ë‚´ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. ê³¼ê±°ì—ëŠ” ë‹¨ì–´(í† í°)ì˜ ìˆœì„œê°€ ì¤‘ìš”í–ˆì—ˆë‹¤. ì¦‰, ì¼ì • ë‹¨ì–´ë“¤ì˜ ì‹œí€€ìŠ¤ $x_{1:t-1}$ê°€ ì£¼ì–´ì§€ë©´, $t$ë²ˆì§¸ ë‹¨ì–´ì¸ $x_t$ë¥¼ ì˜ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ì—ˆë‹¤. ì´ë¥¼ **Auto Regressive Modeling**ì´ë¼ê³ ë„ í•œë‹¤. ê·¸ëŸ¬ë‚˜, **Masked Language Modeling** ë°©ë²•ì´ ë“±ì¥í–ˆëŠ”ë°, ì´ëŠ” ëœë¤ìœ¼ë¡œ ë§ì¶°ì•¼í•  ë‹¨ì–´ë¥¼ ê°€ë¦° ë‹¤ìŒì— ê°€ë ¤ì§„ ë‹¨ì–´ $x_{mask}$ê°€ í¬í•¨ëœ ì‹œí€€ìŠ¤ $x_{1:t}$ ë¥¼ ëª¨ë¸ì—ê²Œ ì…ë ¥í•˜ì—¬ ë§ì¶”ëŠ” í•™ìŠµ ë°©ë²•ì´ë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì´ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ë©´ì„œ, ìµœê·¼ì—ëŠ” ëª¨ë“  ì–¸ì–´ëª¨ë¸ë§ ê¸°ë²•ë“¤ì´ MLMì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆë‹¤. 
   
   ë‹¤ë§Œ, ì–¼ë§Œí¼ì˜ í™•ë¥ ë¡œ ì ì ˆí•˜ê²Œ ê°€ë¦´ì§€, transformerê°€ ê°€ì§€ê³  ìˆëŠ” íƒœìƒì ì¸ ë‹¨ì ì¸ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í† í°ì˜ ê°œìˆ˜ ì œì•½ ë“± í•´ê²°í•˜ë ¤ëŠ” ì‹œë„ê°€ ë§ì´ ìˆê³ , ì•ìœ¼ë¡œë„ í•´ê²°í•´ì•¼í•  ë¬¸ì œë“¤ì´ë‹¤.
   
2. **Fine-tuning**

    **PLM(Pretrained Language Model)**ì„ ë§Œë“¤ê³  ë‚˜ë©´, ê°ê¸° ë‹¤ë¥¸ downstream taskì— ë”°ë¼ì„œ fine-tuningì„ í•˜ê²Œ ëœë‹¤. Downstream taskì€ êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë¥¼ ë§í•˜ë©°, ì£¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ì´ë‹¤.
    * **í…ìŠ¤íŠ¸ ë¶„ë¥˜ Text Classification** - ì˜ˆì‹œ: ì˜í™” ëŒ“ê¸€ ê¸ì •/ë¶€ì • ë¶„ë¥˜í•˜ê¸°
    * **ê°œì²´ëª…ì¸ì‹ NER(Named Entity Recognition)** - ì˜ˆì‹œ: íŠ¹ì • ê¸°ê´€ëª…, ì¸ëª… ë° ì‹œê°„ ë‚ ì§œ ë“± í† í°ì— ì•Œë§ëŠ” íƒœê·¸ë¡œ ë¶„ë¥˜í•˜ê¸°
    * **ì§ˆì˜ì‘ë‹µ Question and Answering** - ì˜ˆì‹œ: íŠ¹ì • ì§€ë¬¸ê³¼ ì§ˆì˜(query)ê°€ ì£¼ì–´ì§€ë©´ ëŒ€ë‹µí•˜ê¸°

ì˜¤ëŠ˜ ì†Œê°œí•  FARM íŒ¨í‚¤ì§€ëŠ” 2ë²ˆì§¸ ë‹¨ê³„ì¸ Fine-tuningì„ ë³´ë‹¤ ì†ì‰½ê²Œ ë§Œë“¤ì–´ ë†“ì€ íŒ¨í‚¤ì§€ë‹¤. 

- **Tutorial github:** [https://github.com/simonjisu/FARM_tutorial](https://github.com/simonjisu/FARM_tutorial)
- **Colab Tutorial:** [ë§í¬](https://colab.research.google.com/github/simonjisu/FARM_tutorial/blob/main/notebooks/FARM_colab.ipynb)

## Core Features

- **Easy fine-tuning of language models**Â to your task and domain language
- **Speed**: AMP(Automatic Mixed Precision) optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)
- **Modular design**Â of language models and prediction heads
- Switch between heads or combine them forÂ **multitask learning**
- **Full Compatibility**Â with HuggingFace Transformers' models and model hub
- **Smooth upgrading**Â to newer language models
- Integration ofÂ **custom datasets**Â via Processor class
- PowerfulÂ **experiment tracking**Â & execution
- **Checkpointing & Caching**Â to resume training and reduce costs with spot instances
- SimpleÂ **deployment**Â andÂ **visualization**Â to showcase your model

[expand]summary:What is AMP? ğŸ‘ˆ 

**Reference**
- [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex)
- [https://forums.fast.ai/t/mixed-precision-training/20720](https://forums.fast.ai/t/mixed-precision-training/20720)

**mixed precision trainingì´ë€**
- ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ FP16(16bit floating point)ì—°ì‚°ê³¼ ì •í™•ë„ ìœ ì§€ë¥¼ ìœ„í•œ FP32 ì—°ì‚°ì„ ì„ì–´ í•™ìŠµí•˜ëŠ” ë°©ë²•
- Tensor Coreë¥¼ í™œìš©í•œ FP16ì—°ì‚°ì„ ì´ìš©í•˜ë©´ FP32ì—°ì‚° ëŒ€ë¹„Â ì ˆë°˜ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ 8ë°°ì˜ ì—°ì‚° ì²˜ë¦¬ëŸ‰ & 2ë°°ì˜ ë©”ëª¨ë¦¬ ì²˜ë¦¬ëŸ‰ íš¨ê³¼ê°€ ìˆë‹¤.

[/expand]

---

# NSMC ë°ì´í„°ë¡œ FARM ì•Œì•„ë³´ê¸°

## NSMC ë°ì´í„°

**NSMC(Naver Sentiment Movie Corpus)**ëŠ” í•œêµ­ì–´ë¡œ ëœ ì˜í™” ëŒ“ê¸€ ë°ì´í„° ì„¸íŠ¸ë‹¤. í•´ë‹¹ TaskëŠ” íƒ€ê²Ÿ ê°’ì´ ê¸ì •(1)/ë¶€ì •(0)ì´ ë˜ëŠ” Binary Text Classification ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆë‹¤. [https://github.com/e9t/nsmc](https://github.com/e9t/nsmc)ì—ì„œ ë°›ì„ ìˆ˜ ìˆë‹¤(ì•„ë˜ ê·¸ë¦¼ì€ labelì„ badì™€ goodìœ¼ë¡œ ì²˜ë¦¬í•´ë†“ì€ ìƒíƒœ). 

{% include image.html id="1FIGIBtZxtuKD5Prps5vPOPldBb0xHwzH" desc="[ê·¸ë¦¼1] NSMC Dataset" width="100%" height="auto" %}

## Fine-tuning Process

{% include image.html id="1j9pn8Lpg7sy6S8Ubvq3E7JLWf28KvRt4" desc="[ê·¸ë¦¼2] Fine-tuning Process" width="50%" height="auto" %}

Fine-tuning ProcessëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.

* Load Data: ë°ì´í„°ë¥¼ ì•Œë§ëŠ” í˜•ì‹(json, csv ë“±)ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ë‹¤.
* Create Dataset: ë°ì´í„°ì„¸íŠ¸(Dataset) ë§Œë“¤ê¸°
    * Tokenization: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ë‹¨ì–´ì¥(vocab)ì„ ìƒì„±í•œë‹¤.
    * ToTensor: vocabì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ìˆ˜ì¹˜í™”í•˜ëŠ” ê³¼ì • (transformers íŒ¨í‚¤ì§€ `input_ids`ì— í•´ë‹¹)
    * Attention Mask: íŒ¨ë”©ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ Attention í•´ì•¼í•  í† í°ë§Œ masking(transformers íŒ¨í‚¤ì§€ `attention_mask` ì— í•´ë‹¹)
* Create Dataloader: í›ˆë ¨, í‰ê°€ì‹œ ë°°ì¹˜í¬ê¸° ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê°ì²´
* Create Model:
    * Pretrained Language Model: ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ì „ì— í›ˆë ¨ëœ ëª¨ë¸ 

        $$\underset{\theta}{\arg \max} P(x_{mask} \vert x_{1:t})$$

    * Fine-tuninig Layer: Downstream Taskì— ë§ì¶°ì„œ í•™ìŠµí•œë‹¤. 
      
        $$\underset{\theta}{\arg \max}P(y\vert x_{1:t})$$

        ì˜ˆë¥¼ ë“¤ì–´, ì˜í™” ê¸ì •/ë¶€ì • ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš°

        $$\underset{\theta}{\arg \max} P(y=\text{ê¸ì •/ë¶€ì •} \vert x_{1:t})$$

* Train Model: ëª¨ë¸ í›ˆë ¨
* Eval Model: ëª¨ë¸ í‰ê°€
* Inference: ëª¨ë¸ ì„œë¹„ìŠ¤

## Processor & Data Silo

{% include image.html id="1XCc0AJpPBMFcC81NW0A6w0mpswZ2KU7h" desc="[ê·¸ë¦¼3] Fine-tuning Process" width="90%" height="auto" %}

* **Processor**ëŠ” file í˜¹ì€ requestë¥¼ PyTorch Datsetë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” ì—­í• ì´ë‹¤. ìì„¸í•œ ì¸ìê°’ì€ ë‹¤ìŒ ì½”ë“œ ë¸”ë¡ì—ì„œ ì„¤ëª…í•œë‹¤.
* **Data Silo**ëŠ” train, dev, test setsë¥¼ ê´€ë¦¬í•˜ê³ , Processorì˜ functionë“¤ ì´ìš©í•´ ê° setë¥¼ DataLoaderë¡œ ë³€í™˜í•œë‹¤.
* **Processor**ëŠ” ê° ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ, **Samples**, **SampleBasket**ì— ë‹´ê²Œ ë˜ëŠ”ë°, ì´ë“¤ì€ raw documentë¥¼ ê´€ë¦¬í•˜ëŠ” ê°ì²´ì´ë©° tokenized, featuresë“± ë°ì´í„°ì™€ ê° ìƒ˜í”Œì„ ê´€ë¦¬í•˜ëŠ” idë¥¼ ì €ì¥í•˜ê³  ìˆë‹¤. ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” í•˜ë‚˜ì˜ ì†ŒìŠ¤ í…ìŠ¤íŠ¸(raw text)ì—ì„œ ì—¬ëŸ¬ê°œì˜ ìƒ˜í”Œì„ ìƒì„±í•  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ë‹¤
  ì—¬ë‹´ì´ì§€ë§Œ huggingfaceì˜ SquadProcessorëŠ” 512ê°œ í† í°ì´ ë„˜ì–´ê°€ë©´, ë’¤ì—ì„œ ë¶€í„° 512í† í°ì„ ì„¸ì„œ í•˜ë‚˜ì˜ ë°ì´í„°ë¥¼ ë‘ ê°œì˜ ìƒ˜í”Œë¡œ ë§Œë“ ë‹¤.
    ```python
    def dataset_from_dicts(self, ...)
        # ...
        for dictionary, input_ids, segment_ids, padding_mask, tokens in zip(
                dicts, input_ids_batch, segment_ids_batch, padding_masks_batch, tokens_batch
        ):
            # ...
            # Add Basket to self.baskets
            curr_sample = Sample(
                id=None,
                clear_text=dictionary,
                tokenized=tokenized,
                features=[feat_dict]
            )
            curr_basket = SampleBasket(
                id_internal=None,
                raw=dictionary,
                id_external=None,
                samples=[curr_sample]
            )
            self.baskets.append(curr_basket)

        # ...
    ```

ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

[expand]summary: ì½”ë“œë³´ê¸° ğŸ‘ˆ 

```python
# Reference: https://github.com/Beomi/KcBERT
PRETRAINED_MODEL_NAME_OR_PATH = "beomi/kcbert-base"  
MAX_LENGTH = 150
LABEL_LIST = ["bad", "good"]
TRAIN_FILE = "train.tsv"
TEST_FILE = "test.tsv"
TASK_TYPE = "text_classification"

# Tokenizer
tokenizer = Tokenizer.load(
    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,
    do_lower_case=False,
)
# Processor
processor = TextClassificationProcessor(
    tokenizer=tokenizer,  # tokenizer 
    train_filename=TRAIN_FILE,  # training data íŒŒì¼ëª…
    dev_filename=None,  # development data íŒŒì¼ëª…, ì—†ìœ¼ë©´, dev_split ë¹„ìœ¨ë§Œí¼ training dataì—ì„œ ìë¥¸ë‹¤ 
    test_filename=TEST_FILE,  # test data íŒŒì¼ëª…
    dev_split=0.1,  # development dataë¡œ ì„¤ì •í•  ë¹„ìœ¨
    header=0,  # csv, tsv, excel ë“± tabularí˜•íƒœ ë°ì´í„°ì—ì„œ ì²«í–‰(ë³´í†µì€ ì»¬ëŸ¼ëª…)ì˜ ìœ„ì¹˜
    max_seq_len=MAX_LENGTH,  # ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´
    data_dir=str(DATA_PATH),  # ë°ì´í„°ì˜ ë””ë ‰í† ë¦¬
    label_list=LABEL_LIST,  # ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸(string í•„ìš”)
    metric="acc",  # í‰ê°€ì§€í‘œ
    label_column_name="label",  # tabularí˜•íƒœ ë°ì´í„°ì—ì„œ ë ˆì´ë¸”ì˜ ì»¬ëŸ¼ëª…
    text_column_name="document",  # tabularí˜•íƒœ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ì˜ ì»¬ëŸ¼ëª…
    delimiter="\t"
)

data_silo = DataSilo(
    processor=processor,
    batch_size=8,
    eval_batch_size=8,
    caching=True
)
```

[/expand]

ì½”ë“œ ì‹¤í–‰ í›„, ë‹¤ìŒê³¼ ê°™ì´ tokenization ë˜ë©°, sample ê°ì²´ì— ì €ì¥ëœë‹¤. 

{% include image.html id="1DVPT_Rjv_SI4ggJZzqfPh0MgsMa1Q9El" desc="[ê·¸ë¦¼4] ì‹¤í–‰í™”ë©´" width="100%" height="auto" %}

í•˜ë‚˜ë¥¼ í™•ëŒ€í•´ì„œ ì‚´í´ë³´ë©´ Sampleê°ì²´ ì•ˆì— ë‹¤ì–‘í•œ ì •ë³´ë“¤ì´ ë“¤ì–´ ìˆë‹¤. 

```plaintext
03/28/2021 22:12:15 - INFO - farm.data_handler.processor -   

      .--.        _____                       _      
    .'_\/_'.     / ____|                     | |     
    '. /\ .'    | (___   __ _ _ __ ___  _ __ | | ___ 
      "||"       \___ \ / _` | '_ ` _ \| '_ \| |/ _ \ 
       || /\     ____) | (_| | | | | | | |_) | |  __/
    /\ ||//\)   |_____/ \__,_|_| |_| |_| .__/|_|\___|
   (/\||/                             |_|           
______\||/___________________________________________                     

ID: 437-0
Clear Text: 
 	text_classification_label: good
 	text: ì´ ì˜í™”ë¥¼ ë³´ê³  ë‘í†µì´ ë‚˜ì•˜ìŠµë‹ˆë‹¤. ã…  ã… 
Tokenized: 
 	tokens: ['ì´', 'ì˜í™”ë¥¼', 'ë³´ê³ ', 'ë‘', '##í†µì´', 'ë‚˜', '##ì•˜ìŠµë‹ˆë‹¤', '.', '[UNK]', '[UNK]']
 	offsets: [0, 2, 6, 9, 10, 13, 14, 18, 20, 22]
 	start_of_word: [True, True, True, True, False, True, False, False, True, True]
Features: 
 	input_ids: [2, 2451, 25833, 8198, 917, 11765, 587, 21809, 17, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 	padding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 	segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 	text_classification_label_ids: [1]
```

## Modeling Layers: AdaptiveModel = LanguageModel + PredictionHead

{% include image.html id="1OLWdr8rh7ucpF9t55gzVeMawMBJbRiEC" desc="[ê·¸ë¦¼5] Modeling Layers" width="90%" height="auto" %}

* **LanguageModel**ì€ pretrained language models(BERT, XLNet ...)ì˜ í‘œì¤€ í´ë˜ìŠ¤ 
* **PredictionHead**ëŠ” ëª¨ë“  down-stream tasks(NER, Text classification, QA ...)ë¥¼ í‘œì¤€ í´ë˜ìŠ¤
* **AdaptiveModel**ì€ ìœ„ ë‘ ê°€ì§€ ëª¨ë“¤ì˜ ê²°í•©, í•˜ë‚˜ì˜ LanguageModelê³¼ ì—¬ëŸ¬ ê°œì˜ PredictionHeadë¥¼ ê²°í•©í•  ìˆ˜ ìˆë‹¤.

[expand]summary: ì½”ë“œë³´ê¸° ğŸ‘ˆ 

```python
# LanguageModel: Build pretrained language model
EMBEDS_DROPOUT_PROB = 0.1
TASK_NAME = "text_classification"

language_model = LanguageModel.load(PRETRAINED_MODEL_NAME_OR_PATH, language="korean")
# PredictionHead: Build predictor layer
prediction_head = TextClassificationHead(
    num_labels=len(LABEL_LIST), 
    class_weights=data_silo.calculate_class_weights(
        task_name=TASK_NAME
    )
)
model = AdaptiveModel(
    language_model=language_model,
    prediction_heads=[prediction_head],
    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,
    lm_output_types=["per_sequence"],
    device=device
)
```

ì‹¤ì œ ëª¨ë¸ì˜ êµ¬ì„±ì„ ì‚´í´ë³´ë©´ classificationì„ ìœ„í•œ bertì™€ ìœ ì‚¬í•˜ê²Œ `PredictionHead`ì—ì„œëŠ” `pooler`ì—ì„œ ë‚˜ì˜¨ `pooled_output`ì„ `dropout`ì¸µì„ í†µê³¼í•œ í›„ì— `FeedForwardBlock`ìœ¼ë¡œ ë³´ë‚´ì„œ ìµœì¢… logitsì„ ìƒì„±í•œë‹¤. `AdaptiveModel` classì—ì„œ `embeds_dropout_prob`ë¥¼ ë°”ê¾¸ë©´, dropout í™•ë¥ ì„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤.

[/expand]

## Train & Eval & Inference

{% include image.html id="1bD54igqAn7T96gDCFZ2uxzFHpZIL5GOh" desc="[ê·¸ë¦¼6] Modeling Layers" width="90%" height="auto" %}

ì—¬íƒ€ ë‹¤ë¥¸ íŒ¨í‚¤ì§€ì™€ ë§ˆì°¬ê°€ì§€ë¡œ TrainerëŠ” ëª¨ë¸ê³¼ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤. FARMì—ëŠ” EarlyStopping callbackì„ ì§€ì›í•œë‹¤. í›ˆë ¨ ì§„í–‰ë„ì¤‘ ì •í•´ì§„ ìŠ¤í…ë§ˆë‹¤ í‰ê°€ë¥¼ í•˜ëŠ”ë°, ì´ë•Œ callbackì´ ì‘ë™í•œë‹¤.

### Train & Eval

[expand]summary: ì½”ë“œë³´ê¸° ğŸ‘ˆ 

```python
LEARNING_RATE = 2e-5
N_EPOCHS = 1
N_GPU = 1
checkpoint_path = "./ckpt/NSMC"

# Initialize Optimizer
model, optimizer, lr_schedule = initialize_optimizer(
    model=model,
    device=device,
    learning_rate=LEARNING_RATE,
    n_batches=len(data_silo.loaders["train"]),
    n_epochs=N_EPOCHS
)
# EarlyStopping
earlymetric = "f1" if args.task_name == "question_answering" else "acc" 
mode = "max" if args.task_name in ["text_classification", "question_answering"] else "min"
earlystop = EarlyStopping(
    save_dir=checkpoint_path,
    metric=earlymetric,
    mode=mode,
    patience=3,
)

# Trainer
trainer = Trainer(
    model=model,
    optimizer=optimizer,
    lr_schedule=lr_schedule,
    data_silo=data_silo,
    early_stopping=earlystop,
    evaluate_every=20,
    checkpoints_to_keep=3,
    checkpoint_root_dir=checkpoint_path,
    checkpoint_every=200,
    epochs=N_EPOCHS,
    n_gpu=N_GPU,
    device=device, 
)
# now train!
model = trainer.train()
```

[/expand]

í›ˆë ¨ ê³¼ì •ì— ê³„ì† Logê°€ ì°íˆê³ , Processorë‹¨ê³„ì—ì„œ ì…ë ¥í•´ë‘” `test_filename`ë¡œ í‰ê°€ë„ í•´ì¤€ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì€ 430ê°œì˜ ë°°ì¹˜ ë°ì´í„°(`batch_size=256`)ë¥¼ ëŒë ¸ì„ ë•Œ earlystoppingí•œ ê²°ê³¼ë‹¤.

{% include image.html id="1m1K9CjBNulC4dzSxC1vKjLb94p9BQu26" desc="[ê·¸ë¦¼7] logging ë‚´ìš©" width="90%" height="auto" %}

### Inference

ì‹¤ì œ ë„¤ì´ë²„ ì˜í™” ë‘ ê³³ì—ì„œ ê°ê¸° ë‹¤ë¥¸ í‰ì ì„ ê°€ì ¸ì™€ì„œ í…ŒìŠ¤íŠ¸ í•´ë³´ì•˜ë‹¤.

[expand]summary: ì½”ë“œë³´ê¸° ğŸ‘ˆ 

```python
import termcolor

from farm.infer import Inferencer
from pprint import PrettyPrinter

# https://movie.naver.com/movie/bi/mi/basic.nhn?code=161967
# https://movie.naver.com/movie/bi/mi/point.nhn?code=196051

basic_texts = [
    {"text": "ë¹„ì— ì –ì§€ ì•ŠëŠ” ê³ ê¸‰ ì¥ë‚œê° í…íŠ¸ì™€, ë¹„ì— ì –ë‹¤ ëª»í•´ ì ê²¨ë²„ë¦¬ëŠ” ë°˜ì§€í•˜ ê°€êµ¬"},  # 161967 / í‰ì  10
    {"text": """ë‚¨ë“¤ì´ ë‚œí•´í•˜ë‹¨ê±° ë³´ê³  í˜¼ì ì´í•´í–ˆë‹¤ë©° ì‹¬ì˜¤í•œì²™ í•˜ê³ í”Œë•Œë‚˜ ë³´ë©´ ë”±ì¸ ì˜í™”. í†µì°°ë„ ì‹œì‚¬ì ë„ ì¬ë¯¸ë„ ì˜ë¯¸ë„ ê°ë™ë„ ì—†ëŠ”... 
ì§„ì •í•œ í‚¬ë§íƒ€ì„. ê°€ë‚œí•œ ì‚¬ëŒë“¤ ë‹¤ ê¸°ìƒì¶©ì— ë¹„ìœ í•œê±°ì•¼? ê·¸ë ‡ë‹¤ë©´ ê°ë… ê°œë˜¥ì² í•™ ì™„ì „ê½ì´ê³ ..."""},  # 161967 / í‰ì  1
    {"text": "ì™€ ì´ê±° ì•ˆë³´ë©´ ì¸ìƒ ì ˆë°˜ í›„íšŒí•œê²ë‹ˆë‹¤ ì—¬ëŸ¬ë¶„"},  # 196051 / í‰ì  10
    {"text": "ì ˆë ˆì ˆë ˆ ëˆì£¼ê³  ë³´ì§€ë§ˆì…ˆã…‹ã…‹ã…‹ã…‹"}  # 196051 / í‰ì  1
]

infer_model = Inferencer.load(
    model_name_or_path="./ckpt/best_nsmc",
    task_type="text_classification"
)
result = infer_model.inference_from_dicts(dicts=basic_texts)

for p, mid, star in zip(
        result[0]["predictions"], [161967, 161967, 196051, 196051], [10, 1, 10, 1]
    ):
    context = p["context"]
    label = p["label"]
    probability = p["probability"]
    star = termcolor.colored(str(star), "blue", attrs=["bold"])
    if label == "bad":
        label = termcolor.colored(label, "red", attrs=["bold"])
    else:
        label = termcolor.colored(label, "green", attrs=["bold"])
    print(termcolor.colored(f"[Movie: {mid}] Context:", attrs=["bold"]))
    print(context)
    print(f"Probability {probability*100:.2f}% | Predict: {label} | Real Star: {star}")
    print()
```

[/expand]

{% include image.html id="1fI8ME4YexqN75CumIcCO32jUWl3BB86U" desc="[ê·¸ë¦¼8] í…ŒìŠ¤íŠ¸ ê²°ê³¼" width="100%" height="auto" %}

ë‘ ì˜í™”ëŠ” ë´‰ì¤€í˜¸ ê°ë…ë‹˜ì˜ 'ê¸°ìƒì¶©(id=161967)', ìµœê·¼ ì¸ê¸°ê°€ë„ë¥¼ ë‹¬ë¦¬ê³  ìˆëŠ” 'ê·¹ì¥íŒ ê·€ë©¸ì˜ ì¹¼ë‚ : ë¬´í•œì—´ì°¨í¸(id=196051)'ë¥¼ ì„ ì •í–ˆë‹¤. í•˜ë‚˜ë¥¼ ì œì™¸í•˜ê³  ì˜ ë§ì¶˜ ëª¨ìŠµì„ ë³´ì—¬ì¤¬ëŠ”ë°, ì²«ë²ˆì§¸ ìƒ˜í”Œì˜ ê²½ìš° ì‚¬ì‹¤ ì˜í™”ì˜ ì¥ë©´ì„ ë¬˜ì‚¬í•œ ê²ƒìœ¼ë¡œ, ê·¸ë§Œí¼ ì¸ìƒê¹Šì—ˆë˜ ì¥ë©´ë“¤ì„ ë‹¬ë©´ì„œ í‰ì ì€ 10ì ìœ¼ë¡œ ë‹¬ì•˜ë‹¤. ì‚¬ëŒìœ¼ë¡œì¨ ì´ ì˜í™”ì„ ë³¸ ê´€ê°ì´ë¼ë©´ ì´ í‰ê°€ê°€ 10ì ì— ì•Œë§ëŠ” í‰ì (í˜¹ì€ ê¸ì •)ì´ì§€ë§Œ, ê¸°ê³„ì—ê²ŒëŠ” ì•„ì§ ì–´ë ¤ìš´ ì  ì¤‘ì— í•˜ë‚˜ë¼ê³  ìƒê°í•œë‹¤. 

---

# MLflow

MLflowë¥¼ ì´ìš©í•˜ ë¹ ë¥´ê³  ì‰½ê²Œ ì‹¤í—˜ì„ ê´€ë¦¬í•˜ê³ , ê´€ë ¨ í‰ê°€ì§€í‘œë„ í•¨ê»˜ ë³¼ ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ê·¸ë¦¼ë“¤ì€ TITAN RTX 4ëŒ€ì—ì„œ ë°°ì¹˜í¬ê¸°ë¥¼ 256ìœ¼ë¡œ í›ˆë ¨ ì‹œí‚¨ ê²°ê³¼ë‹¤(440 batches ì—ì„œ Early Stoppingí–ˆë‹¤.).

* public mlflow([ë§í¬](https://public-mlflow.deepset.ai/#/experiments/313/runs/05e7e3d4945642f9ab3e296637d57c26))ì—ì„œ í™•ì¸í•˜ê¸°

{% include image.html id="13Cg8eziHBgA3JLwZJ3Bo8YzeySWPRmiP" desc="[ê·¸ë¦¼9] Parameters" width="50%" height="auto" %}

Trainê³¼ Dev ì„¸íŠ¸ì˜ lossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

{% include image.html id="1cpFWVvjkSqshvN0hS_CuPk4RjyEyM0AV" desc="[ê·¸ë¦¼10] Loss Graph" width="100%" height="auto" %}

Dev ì„¸íŠ¸ì˜ ì •í™•ë„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

{% include image.html id="1VPso9Gx60V8_dgE4as054n7kymCoQ9w5" desc="[ê·¸ë¦¼11] Dev Accuracy Graph" width="100%" height="auto" %}

---

# TASK Supported

í˜„ì¬ ì§€ì›ë˜ëŠ” ëª¨ë¸ê³¼ SubTaskëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

|Task|BERT|RoBERTa*|XLNet|ALBERT|DistilBERT|XLMRoBERTa|ELECTRA|MiniLM|
|---|---|---|---|---|---|---|---|---|
|Text classification|x|x|x|x|x|x|x|x|
|NER|x|x|x|x|x|x|x|x|
|Question Answering|x|x|x|x|x|x|x|x|
|Language Model Fine-tuning|x||||||||
|Text Regression|x|x|x|x|x|x|x|x|
|Multilabel Text classif.|x|x|x|x|x|x|x|x|
|Extracting embeddings|x|x|x|x|x|x|x|x|
|LM from scratch|x||||||||
|Text Pair Classification|x|x|x|x|x|x|x|x|
|Passage Ranking|x|x|x|x|x|x|x|x|
|Document retrieval (DPR)|x|x||x|x|x|x|x|

---

# Compare to others

{% include image.html id="1TZoRpza8-o4wSTr0s16f8hHQRroLQg30" desc="[ê·¸ë¦¼12] ë‹¤ë¥¸ íŒ¨í‚¤ì§€ì™€ì˜ ë¹„êµ" width="100%" height="auto" %}

ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµí•´ë³´ë©´ FARMì€ ì¡°ê¸ˆ ë” huggingfaceì™€ pytorch-lightningì˜ í•©ë³¸ ì¶•ì•½ ë²„ì „ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ë§ˆì¹˜ Tensorflow v1ê³¼ kerasì˜ ì°¨ì´ ëŠë‚Œì´ë‹¤.

## FARM ì¥ë‹¨ì 

ì¥ì :

* ë°ì´í„° ì„¸íŠ¸ë§Œ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë©´, ë‹¤ë¥¸ íŒ¨í‚¤ì§€ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì„¤ì • í•  ê²ƒì´ ì ìŒ
* í›ˆë ¨ ì†ë„ê°€ ë¹ ë¥´ê³ , ì‹¤í—˜ ê¸°ë¡ ë° ê´€ë¦¬ì´ í¸ë¦¬í•´ì„œ ë¹ ë¥´ê²Œ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆìŒ(í…ì„œë³´ë“œ ëŒ€ì‹  mlflow ì‚¬ìš© ê°€ëŠ¥)
* ë©€í‹° GPU ì„¤ì •ì„ í•´ì¤„ í•„ìš”ê°€ ì—†ìŒ

ë‹¨ì : 

* customizationì´ ìƒëŒ€ì ìœ¼ë¡œ í˜ë“¦
* ì•„ì§ ë°œì „ ì¤‘ì´ë¼ ë¶ˆì•ˆì •í•˜ê³  documentatonì´ ì˜ ì•ˆë˜ì–´ ìˆìŒ