---
layout: post
title: "FARM tutorial"
date: "2021-03-31 01:30:01 +0900"
categories: nlp
author: "Soo"
comments: true
toc: true
---

{% include image.html id="1hbtUClFoXg45IbViZoFRLnnDGVlr9Dlb" desc="" width="50%" height="auto" %}

# FARM

> Framework for Adapting Representation Models

이 패키지를 한 마디로 요약하면 Fine-tuning에 최적화된 도구다.

최근의 자연어처리 분야는 Transformer와 그 변형의 등장으로 인해, 보통 2단계로 나눠서 학습이 진행된다. 
1. **Pretrained Language Modeling**
    
   대량의 텍스트 데이터를 이용해 비지도학습(unsupervised learning)으로 언어 모델링은 진행한다. 언어 모델링이란 인간의 언어를 컴퓨터로 모델링하는 과정이다. 쉽게 말하면, 모델에게 단어들을 입력했을 때, 제일 말이 되는 단어(토큰)을 뱉어내게 하는 것이다. 과거에는 단어(토큰)의 순서가 중요했었다. 즉, 일정 단어들의 시퀀스 $x_{1:t-1}$가 주어지면, $t$번째 단어인 $x_t$를 잘 학습시키는 것이었다. 이를 **Auto Regressive Modeling**이라고도 한다. 그러나, **Masked Language Modeling** 방법이 등장했는데, 이는 랜덤으로 맞춰야할 단어를 가린 다음에 가려진 단어 $x_{mask}$가 포함된 시퀀스 $x_{1:t}$ 를 모델에게 입력하여 맞추는 학습 방법이다. 이러한 방법이 좋은 성과를 거두면서, 최근에는 모든 언어모델링 기법들이 MLM을 기반으로 하고 있다. 
   
   다만, 얼만큼의 확률로 적절하게 가릴지, transformer가 가지고 있는 태생적인 단점인 처리할 수 있는 토큰의 개수 제약 등 해결하려는 시도가 많이 있고, 앞으로도 해결해야할 문제들이다.
   
2. **Fine-tuning**

    **PLM(Pretrained Language Model)**을 만들고 나면, 각기 다른 downstream task에 따라서 fine-tuning을 하게 된다. Downstream task은 구체적으로 풀고 싶은 문제를 말하며, 주로 다음과 같은 문제들이다.
    * **텍스트 분류 Text Classification** - 예시: 영화 댓글 긍정/부정 분류하기
    * **개체명인식 NER(Named Entity Recognition)** - 예시: 특정 기관명, 인명 및 시간 날짜 등 토큰에 알맞는 태그로 분류하기
    * **질의응답 Question and Answering** - 예시: 특정 지문과 질의(query)가 주어지면 대답하기

오늘 소개할 FARM 패키지는 2번째 단계인 Fine-tuning을 보다 손쉽게 만들어 놓은 패키지다. 

- **Tutorial github:** [https://github.com/simonjisu/FARM_tutorial](https://github.com/simonjisu/FARM_tutorial)
- **Colab Tutorial:** [링크](https://colab.research.google.com/github/simonjisu/FARM_tutorial/blob/main/notebooks/FARM_colab.ipynb)

## Core Features

- **Easy fine-tuning of language models** to your task and domain language
- **Speed**: AMP(Automatic Mixed Precision) optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)
- **Modular design** of language models and prediction heads
- Switch between heads or combine them for **multitask learning**
- **Full Compatibility** with HuggingFace Transformers' models and model hub
- **Smooth upgrading** to newer language models
- Integration of **custom datasets** via Processor class
- Powerful **experiment tracking** & execution
- **Checkpointing & Caching** to resume training and reduce costs with spot instances
- Simple **deployment** and **visualization** to showcase your model

[expand]summary:What is AMP? 👈 

**Reference**
- [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex)
- [https://forums.fast.ai/t/mixed-precision-training/20720](https://forums.fast.ai/t/mixed-precision-training/20720)

**mixed precision training이란**
- 처리 속도를 높이기 위한 FP16(16bit floating point)연산과 정확도 유지를 위한 FP32 연산을 섞어 학습하는 방법
- Tensor Core를 활용한 FP16연산을 이용하면 FP32연산 대비 절반의 메모리 사용량과 8배의 연산 처리량 & 2배의 메모리 처리량 효과가 있다.

[/expand]

---

# NSMC 데이터로 FARM 알아보기

## NSMC 데이터

**NSMC(Naver Sentiment Movie Corpus)**는 한국어로 된 영화 댓글 데이터 세트다. 해당 Task는 타겟 값이 긍정(1)/부정(0)이 되는 Binary Text Classification 문제로 볼 수 있다. [https://github.com/e9t/nsmc](https://github.com/e9t/nsmc)에서 받을 수 있다(아래 그림은 label을 bad와 good으로 처리해놓은 상태). 

{% include image.html id="1FIGIBtZxtuKD5Prps5vPOPldBb0xHwzH" desc="[그림1] NSMC Dataset" width="100%" height="auto" %}

## Fine-tuning Process

{% include image.html id="1j9pn8Lpg7sy6S8Ubvq3E7JLWf28KvRt4" desc="[그림2] Fine-tuning Process" width="50%" height="auto" %}

Fine-tuning Process는 위 그림과 같이 진행된다.

* Load Data: 데이터를 알맞는 형식(json, csv 등)으로 불러온다.
* Create Dataset: 데이터세트(Dataset) 만들기
    * Tokenization: 텍스트를 토큰으로 나누고, 단어장(vocab)을 생성한다.
    * ToTensor: vocab에 해당하는 단어를 수치화하는 과정 (transformers 패키지 `input_ids`에 해당)
    * Attention Mask: 패딩계산을 피하기 위해 Attention 해야할 토큰만 masking(transformers 패키지 `attention_mask` 에 해당)
* Create Dataloader: 훈련, 평가시 배치크기 단위로 데이터를 불러오는 객체
* Create Model:
    * Pretrained Language Model: 대량의 텍스트 데이터로 사전에 훈련된 모델 

        $$\underset{\theta}{\arg \max} P(x_{mask} \vert x_{1:t})$$

    * Fine-tuninig Layer: Downstream Task에 맞춰서 학습한다. 
      
        $$\underset{\theta}{\arg \max}P(y\vert x_{1:t})$$

        예를 들어, 영화 긍정/부정 분류 문제의 경우

        $$\underset{\theta}{\arg \max} P(y=\text{긍정/부정} \vert x_{1:t})$$

* Train Model: 모델 훈련
* Eval Model: 모델 평가
* Inference: 모델 서비스

## Processor & Data Silo

{% include image.html id="1XCc0AJpPBMFcC81NW0A6w0mpswZ2KU7h" desc="[그림3] Fine-tuning Process" width="90%" height="auto" %}

* **Processor**는 file 혹은 request를 PyTorch Datset로 만들어 주는 역할이다. 자세한 인자값은 다음 코드 블록에서 설명한다.
* **Data Silo**는 train, dev, test sets를 관리하고, Processor의 function들 이용해 각 set를 DataLoader로 변환한다.
* **Processor**는 각 데이터를 처리할 때, **Samples**, **SampleBasket**에 담게 되는데, 이들은 raw document를 관리하는 객체이며 tokenized, features등 데이터와 각 샘플을 관리하는 id를 저장하고 있다. 이렇게 하는 이유는 하나의 소스 텍스트(raw text)에서 여러개의 샘플을 생성할 수도 있기 때문이다
  여담이지만 huggingface의 SquadProcessor는 512개 토큰이 넘어가면, 뒤에서 부터 512토큰을 세서 하나의 데이터를 두 개의 샘플로 만든다.
    ```python
    def dataset_from_dicts(self, ...)
        # ...
        for dictionary, input_ids, segment_ids, padding_mask, tokens in zip(
                dicts, input_ids_batch, segment_ids_batch, padding_masks_batch, tokens_batch
        ):
            # ...
            # Add Basket to self.baskets
            curr_sample = Sample(
                id=None,
                clear_text=dictionary,
                tokenized=tokenized,
                features=[feat_dict]
            )
            curr_basket = SampleBasket(
                id_internal=None,
                raw=dictionary,
                id_external=None,
                samples=[curr_sample]
            )
            self.baskets.append(curr_basket)

        # ...
    ```

사용하는 방법은 다음과 같다.

[expand]summary: 코드보기 👈 

```python
# Reference: https://github.com/Beomi/KcBERT
PRETRAINED_MODEL_NAME_OR_PATH = "beomi/kcbert-base"  
MAX_LENGTH = 150
LABEL_LIST = ["bad", "good"]
TRAIN_FILE = "train.tsv"
TEST_FILE = "test.tsv"
TASK_TYPE = "text_classification"

# Tokenizer
tokenizer = Tokenizer.load(
    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,
    do_lower_case=False,
)
# Processor
processor = TextClassificationProcessor(
    tokenizer=tokenizer,  # tokenizer 
    train_filename=TRAIN_FILE,  # training data 파일명
    dev_filename=None,  # development data 파일명, 없으면, dev_split 비율만큼 training data에서 자른다 
    test_filename=TEST_FILE,  # test data 파일명
    dev_split=0.1,  # development data로 설정할 비율
    header=0,  # csv, tsv, excel 등 tabular형태 데이터에서 첫행(보통은 컬럼명)의 위치
    max_seq_len=MAX_LENGTH,  # 문장의 최대 길이
    data_dir=str(DATA_PATH),  # 데이터의 디렉토리
    label_list=LABEL_LIST,  # 레이블 리스트(string 필요)
    metric="acc",  # 평가지표
    label_column_name="label",  # tabular형태 데이터에서 레이블의 컬럼명
    text_column_name="document",  # tabular형태 데이터에서 텍스트의 컬럼명
    delimiter="\t"
)

data_silo = DataSilo(
    processor=processor,
    batch_size=8,
    eval_batch_size=8,
    caching=True
)
```

[/expand]

코드 실행 후, 다음과 같이 tokenization 되며, sample 객체에 저장된다. 

{% include image.html id="1DVPT_Rjv_SI4ggJZzqfPh0MgsMa1Q9El" desc="[그림4] 실행화면" width="100%" height="auto" %}

하나를 확대해서 살펴보면 Sample객체 안에 다양한 정보들이 들어 있다. 

```plaintext
03/28/2021 22:12:15 - INFO - farm.data_handler.processor -   

      .--.        _____                       _      
    .'_\/_'.     / ____|                     | |     
    '. /\ .'    | (___   __ _ _ __ ___  _ __ | | ___ 
      "||"       \___ \ / _` | '_ ` _ \| '_ \| |/ _ \ 
       || /\     ____) | (_| | | | | | | |_) | |  __/
    /\ ||//\)   |_____/ \__,_|_| |_| |_| .__/|_|\___|
   (/\||/                             |_|           
______\||/___________________________________________                     

ID: 437-0
Clear Text: 
 	text_classification_label: good
 	text: 이 영화를 보고 두통이 나았습니다. ㅠ ㅠ
Tokenized: 
 	tokens: ['이', '영화를', '보고', '두', '##통이', '나', '##았습니다', '.', '[UNK]', '[UNK]']
 	offsets: [0, 2, 6, 9, 10, 13, 14, 18, 20, 22]
 	start_of_word: [True, True, True, True, False, True, False, False, True, True]
Features: 
 	input_ids: [2, 2451, 25833, 8198, 917, 11765, 587, 21809, 17, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 	padding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 	segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 	text_classification_label_ids: [1]
```

## Modeling Layers: AdaptiveModel = LanguageModel + PredictionHead

{% include image.html id="1OLWdr8rh7ucpF9t55gzVeMawMBJbRiEC" desc="[그림5] Modeling Layers" width="90%" height="auto" %}

* **LanguageModel**은 pretrained language models(BERT, XLNet ...)의 표준 클래스 
* **PredictionHead**는 모든 down-stream tasks(NER, Text classification, QA ...)를 표준 클래스
* **AdaptiveModel**은 위 두 가지 모들의 결합, 하나의 LanguageModel과 여러 개의 PredictionHead를 결합할 수 있다.

[expand]summary: 코드보기 👈 

```python
# LanguageModel: Build pretrained language model
EMBEDS_DROPOUT_PROB = 0.1
TASK_NAME = "text_classification"

language_model = LanguageModel.load(PRETRAINED_MODEL_NAME_OR_PATH, language="korean")
# PredictionHead: Build predictor layer
prediction_head = TextClassificationHead(
    num_labels=len(LABEL_LIST), 
    class_weights=data_silo.calculate_class_weights(
        task_name=TASK_NAME
    )
)
model = AdaptiveModel(
    language_model=language_model,
    prediction_heads=[prediction_head],
    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,
    lm_output_types=["per_sequence"],
    device=device
)
```

실제 모델의 구성을 살펴보면 classification을 위한 bert와 유사하게 `PredictionHead`에서는 `pooler`에서 나온 `pooled_output`을 `dropout`층을 통과한 후에 `FeedForwardBlock`으로 보내서 최종 logits을 생성한다. `AdaptiveModel` class에서 `embeds_dropout_prob`를 바꾸면, dropout 확률을 조절할 수 있다.

[/expand]

## Train & Eval & Inference

{% include image.html id="1bD54igqAn7T96gDCFZ2uxzFHpZIL5GOh" desc="[그림6] Modeling Layers" width="90%" height="auto" %}

여타 다른 패키지와 마찬가지로 Trainer는 모델과 분리되어 있다. FARM에는 EarlyStopping callback을 지원한다. 훈련 진행도중 정해진 스텝마다 평가를 하는데, 이때 callback이 작동한다.

### Train & Eval

[expand]summary: 코드보기 👈 

```python
LEARNING_RATE = 2e-5
N_EPOCHS = 1
N_GPU = 1
checkpoint_path = "./ckpt/NSMC"

# Initialize Optimizer
model, optimizer, lr_schedule = initialize_optimizer(
    model=model,
    device=device,
    learning_rate=LEARNING_RATE,
    n_batches=len(data_silo.loaders["train"]),
    n_epochs=N_EPOCHS
)
# EarlyStopping
earlymetric = "f1" if args.task_name == "question_answering" else "acc" 
mode = "max" if args.task_name in ["text_classification", "question_answering"] else "min"
earlystop = EarlyStopping(
    save_dir=checkpoint_path,
    metric=earlymetric,
    mode=mode,
    patience=3,
)

# Trainer
trainer = Trainer(
    model=model,
    optimizer=optimizer,
    lr_schedule=lr_schedule,
    data_silo=data_silo,
    early_stopping=earlystop,
    evaluate_every=20,
    checkpoints_to_keep=3,
    checkpoint_root_dir=checkpoint_path,
    checkpoint_every=200,
    epochs=N_EPOCHS,
    n_gpu=N_GPU,
    device=device, 
)
# now train!
model = trainer.train()
```

[/expand]

훈련 과정에 계속 Log가 찍히고, Processor단계에서 입력해둔 `test_filename`로 평가도 해준다. 다음 그림은 430개의 배치 데이터(`batch_size=256`)를 돌렸을 때 earlystopping한 결과다.

{% include image.html id="1m1K9CjBNulC4dzSxC1vKjLb94p9BQu26" desc="[그림7] logging 내용" width="90%" height="auto" %}

### Inference

실제 네이버 영화 두 곳에서 각기 다른 평점을 가져와서 테스트 해보았다.

[expand]summary: 코드보기 👈 

```python
import termcolor

from farm.infer import Inferencer
from pprint import PrettyPrinter

# https://movie.naver.com/movie/bi/mi/basic.nhn?code=161967
# https://movie.naver.com/movie/bi/mi/point.nhn?code=196051

basic_texts = [
    {"text": "비에 젖지 않는 고급 장난감 텐트와, 비에 젖다 못해 잠겨버리는 반지하 가구"},  # 161967 / 평점 10
    {"text": """남들이 난해하단거 보고 혼자 이해했다며 심오한척 하고플때나 보면 딱인 영화. 통찰도 시사점도 재미도 의미도 감동도 없는... 
진정한 킬링타임. 가난한 사람들 다 기생충에 비유한거야? 그렇다면 감독 개똥철학 완전꽝이고..."""},  # 161967 / 평점 1
    {"text": "와 이거 안보면 인생 절반 후회한겁니다 여러분"},  # 196051 / 평점 10
    {"text": "절레절레 돈주고 보지마셈ㅋㅋㅋㅋ"}  # 196051 / 평점 1
]

infer_model = Inferencer.load(
    model_name_or_path="./ckpt/best_nsmc",
    task_type="text_classification"
)
result = infer_model.inference_from_dicts(dicts=basic_texts)

for p, mid, star in zip(
        result[0]["predictions"], [161967, 161967, 196051, 196051], [10, 1, 10, 1]
    ):
    context = p["context"]
    label = p["label"]
    probability = p["probability"]
    star = termcolor.colored(str(star), "blue", attrs=["bold"])
    if label == "bad":
        label = termcolor.colored(label, "red", attrs=["bold"])
    else:
        label = termcolor.colored(label, "green", attrs=["bold"])
    print(termcolor.colored(f"[Movie: {mid}] Context:", attrs=["bold"]))
    print(context)
    print(f"Probability {probability*100:.2f}% | Predict: {label} | Real Star: {star}")
    print()
```

[/expand]

{% include image.html id="1fI8ME4YexqN75CumIcCO32jUWl3BB86U" desc="[그림8] 테스트 결과" width="100%" height="auto" %}

두 영화는 봉준호 감독님의 '기생충(id=161967)', 최근 인기가도를 달리고 있는 '극장판 귀멸의 칼날: 무한열차편(id=196051)'를 선정했다. 하나를 제외하고 잘 맞춘 모습을 보여줬는데, 첫번째 샘플의 경우 사실 영화의 장면을 묘사한 것으로, 그만큼 인상깊었던 장면들을 달면서 평점은 10점으로 달았다. 사람으로써 이 영화을 본 관객이라면 이 평가가 10점에 알맞는 평점(혹은 긍정)이지만, 기계에게는 아직 어려운 점 중에 하나라고 생각한다. 

---

# MLflow

MLflow를 이용하 빠르고 쉽게 실험을 관리하고, 관련 평가지표도 함께 볼 수 있다. 다음 그림들은 TITAN RTX 4대에서 배치크기를 256으로 훈련 시킨 결과다(440 batches 에서 Early Stopping했다.).

* public mlflow([링크](https://public-mlflow.deepset.ai/#/experiments/313/runs/05e7e3d4945642f9ab3e296637d57c26))에서 확인하기

{% include image.html id="13Cg8eziHBgA3JLwZJ3Bo8YzeySWPRmiP" desc="[그림9] Parameters" width="50%" height="auto" %}

Train과 Dev 세트의 loss는 다음과 같다.

{% include image.html id="1cpFWVvjkSqshvN0hS_CuPk4RjyEyM0AV" desc="[그림10] Loss Graph" width="100%" height="auto" %}

Dev 세트의 정확도는 다음과 같다.

{% include image.html id="1VPso9Gx60V8_dgE4as054n7kymCoQ9w5" desc="[그림11] Dev Accuracy Graph" width="100%" height="auto" %}

---

# TASK Supported

현재 지원되는 모델과 SubTask는 다음과 같다.

|Task|BERT|RoBERTa*|XLNet|ALBERT|DistilBERT|XLMRoBERTa|ELECTRA|MiniLM|
|---|---|---|---|---|---|---|---|---|
|Text classification|x|x|x|x|x|x|x|x|
|NER|x|x|x|x|x|x|x|x|
|Question Answering|x|x|x|x|x|x|x|x|
|Language Model Fine-tuning|x||||||||
|Text Regression|x|x|x|x|x|x|x|x|
|Multilabel Text classif.|x|x|x|x|x|x|x|x|
|Extracting embeddings|x|x|x|x|x|x|x|x|
|LM from scratch|x||||||||
|Text Pair Classification|x|x|x|x|x|x|x|x|
|Passage Ranking|x|x|x|x|x|x|x|x|
|Document retrieval (DPR)|x|x||x|x|x|x|x|

---

# Compare to others

{% include image.html id="1TZoRpza8-o4wSTr0s16f8hHQRroLQg30" desc="[그림12] 다른 패키지와의 비교" width="100%" height="auto" %}

다른 모델과 비교해보면 FARM은 조금 더 huggingface와 pytorch-lightning의 합본 축약 버전이라고 생각할 수 있다. 마치 Tensorflow v1과 keras의 차이 느낌이다.

## FARM 장단점

장점:

* 데이터 세트만 준비되어 있으면, 다른 패키지에 비해 상대적으로 설정 할 것이 적음
* 훈련 속도가 빠르고, 실험 기록 및 관리이 편리해서 빠르게 실험해 볼 수 있음(텐서보드 대신 mlflow 사용 가능)
* 멀티 GPU 설정을 해줄 필요가 없음

단점: 

* customization이 상대적으로 힘듦
* 아직 발전 중이라 불안정하고 documentaton이 잘 안되어 있음