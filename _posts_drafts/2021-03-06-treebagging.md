---
layout: post
title: "Tree-based Ensemble: Gradient Boosting"
date: "2021-03-04 17:49:01 +0900"
categories: machinelearning
author: "Soo"
comments: true
toc: true
---

# References

- [ìœ„í‚¤ë°±ê³¼: ê²°ì • íŠ¸ë¦¬ í•™ìŠµë²•](https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95)
- [Xgboost ì‚¬ìš©í•˜ê¸°](https://brunch.co.kr/@snobberys/137)
- [Ensemble: bagging, boosting](https://brunch.co.kr/@chris-song/98)
- [Gradient Boosting Algorithmì˜ ì§ê´€ì  ì´í•´](https://3months.tistory.com/368)
- [rpmcruzì˜ gboostì½”ë“œ](https://github.com/rpmcruz/machine-learning/blob/master/ensemble/boosting/gboost.py)

# Ensemble

**ì•™ìƒë¸”(Ensemble)**ì˜ ì‚¬ì „ì  ì˜ë¯¸ëŠ” 2ì¸ ì´ìƒì˜ ë…¸ë˜ë‚˜ ì—°ì£¼ë¥¼ ëœ»í•˜ëŠ”ë°, ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì•™ìƒë¸” í•™ìŠµì´ë€ í•˜ë‚˜ì˜ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ ë‹¤ìˆ˜ì˜ ì•½í•œ ì„±ëŠ¥ì„ ê°€ì§„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ í•©ì³ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ë•Œ, ì•½í•œ ì„±ëŠ¥ì„ ê°€ì§„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ **ì•½í•œ í•™ìŠµì(Weak Learner)**ë¼ê³  ë¶€ë¥¸ë‹¤. ì•™ìƒë¸” í•™ìŠµì€ ì¼ë°˜ì ìœ¼ë¡œ **ë°°ê¹…(Bagging)** ê³¼ **ë¶€ìŠ¤íŒ…(Boosting)** ë‘ ê°€ì§€ì˜ ìœ í˜•ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

1. Bagging(Bootstrap Aggregation): ìƒ˜í”Œì„ ì—¬ëŸ¬ë²ˆ ë½‘ì•„(Bootstrap)ì„œ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ í•™ìŠµìë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ í•™ìŠµì‹œì¼œ ê²°ê³¼ë¬¼ì„ ì§‘ê³„(Aggregration)í•˜ëŠ” ë°©ë²•. ê²°ê³¼ë¬¼ì„ ì§‘ê³„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ íšŒê·€ ë¬¸ì œì˜ ê²½ìš° í‰ê· ì„ ë‚´ê±°ë‚˜, ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í´ë˜ìŠ¤ë¡œ íˆ¬í‘œ(Hard Voting) í˜¹ì€ í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ í‰ê· í™”í•´ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ë¡œ ë„ì¶œ(Soft Voting)í•œë‹¤. ëª¨ë¸ì˜ varianceë¥¼ ì¤„ì´ëŠ” ë°©í–¥ì„ ì›í•œë‹¤ë©´ ë°°ê¹…ë°©ë²•ì´ ì í•©í•˜ë‹¤.
2. Boosting: í›ˆë ¨ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ì•½í•œ í•™ìŠµìë¥¼ í•˜ë‚˜ì”© ì¶”ê°€ì‹œí‚¤ë©´ì„œ í•™ìŠµí•œë‹¤. ê° ì•½í•œ í•™ìŠµìë“¤ì€ ê°€ì¤‘ì¹˜ë¡œ ì—°ê²°ëœë‹¤ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ë‹¤ë§Œ ì´ì „ ì•½í•œ í•™ìŠµìê°€ í‹€ë¦° ë°ì´í„°ì˜ ìƒ˜í”Œë§ì´ ë” ì˜ ë˜ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ë‹¤ì‹œ í•™ìŠµí•œë‹¤. ëª¨ë¸ì˜ biasë¥¼ ì¤„ì´ëŠ” ë°©í–¥ì„ ìƒê°í•˜ê³  ìˆë‹¤ë©´, ë¶€ìŠ¤íŒ… ë°©ë²•ì´ ì í•©í•˜ë‹¤.

---
ì´ë²ˆ ê¸€ì—ì„œëŠ” Gradient Boostingì„ ë¨¼ì € ë‹¤ë¤„ë³¸ë‹¤.

## Gradient Boosting

Gradient Boostingì„ í•œë§ˆë””ë¡œ í•˜ë©´ Pseudo-Residual Fittingì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•½í•œ í•™ìŠµì $A$ë¡œ ë°ì´í„°($Y, X$)ë¥¼ í•™ìŠµì‹œí‚¨ ê²°ê³¼ë¥¼ ë‹¤ìŒ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•´ë³¸ë‹¤.

$$Y = A(X) + \epsilon_1$$

ì—¬ê¸°ì„œ $\epsilon_1$ì€ ì˜¤ì°¨(error) í˜¹ì€ ì”ì°¨(residual)ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ê·¸ëŸ¬ë©´ ë‚¨ì€ $\epsilon_1$ì— ëŒ€í•´ì„œ ë‹¤ë¥¸ ì•½í•œ í•™ìŠµì $B$ë¥¼ ì˜ˆì¸¡ì„ ì˜í•˜ê²Œ í•™ìŠµì‹œì¼œì„œ $Y$ë¥¼ ì˜ˆì¸¡í•œë‹¤.

$$\epsilon_1 = B(X) + \epsilon_2$$

ì´ë ‡ê²Œ ê³„ì† ì”ì°¨ë¥¼ ì¤„ì—¬ë‚˜ê°€ë©´ì„œ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ í•™ìŠµìë¥¼ ì—°ê²°ì‹œí‚¤ëŠ” ê²ƒì´ Gradient Boostingì´ë‹¤. 

### Negative Gradient

ê·¸ë ‡ë‹¤ë©´ ì™œ Gradientê°€ ë“¤ì–´ê°€ëŠ”ê°€? ê·¸ í•´ë‹µì€ ì†ì‹¤í•¨ìˆ˜(loss function)ì™€ ì—°ê²°ëœë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, Mean Squared Errorë¥¼ ì†ì‹¤í•¨ìˆ˜ë¡œ ì„¤ì •í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ ì˜ˆì¸¡ ê°’ì— ëŒ€í•´ ê²½ì‚¬(gradient)ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. 

$$\begin{aligned} 
\text{Loss} &= L \big(Y, f(X) \big) = \dfrac{1}{2} \big( Y - f(X) \big)^2 \\
\text{gradient} &= \dfrac{\partial L}{\partial f(X)} = \dfrac{1}{2} \times 2 \big( Y - f(X) \big) \times (-1) = -\big( Y - f(X) \big) \\
\text{residual} &= -\dfrac{\partial L}{\partial f(X)} = - \text{gradient}
\end{aligned}$$

ì´ë•Œì˜ ì”ì°¨ëŠ” ìŒì˜ ê²½ì‚¬ê°’ì´ ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ ë°ì´í„°ë¥¼ ë‚¨ì€ ì”ì°¨ì— ëŒ€í•´ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ ê³§ ì „ì²´ ì†ì‹¤ê°’ì„ ì¤„ì´ëŠ” ê²ƒê³¼ ê°™ë‹¤ëŠ” ì´ì•¼ê¸°ë‹¤. 

<details class="collaspe-article">
<summary>ë¶€ì—° ì„¤ëª…ë³´ê¸° ğŸ‘ˆ</summary>
<div markdown="1">

ë¶€ì—° ì„¤ëª…í•˜ìë©´, ë‘ ê°œì˜ ì•½í•œ í•™ìŠµìë¥¼ ì˜ˆë¡œ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$\begin{aligned}
\text{fitting: } Y &= f_1(X) + \epsilon_1 \\
L_1 &= L\big(Y, f_1(X) \big) = \dfrac{1}{2}\big( Y - f_1(X)\big)^2 \\
\dfrac{\partial L_1}{\partial f_1(X)} &= - \big( Y - f_1(X)\big) = - \epsilon_1 \\
\\
\text{fitting: } \epsilon_1 &= f_2(X) + \epsilon_2 \\
L_2 &= L\big(\epsilon_1, f_2(X) \big) = \dfrac{1}{2}\big( \epsilon_1 - f_2(X)\big)^2 \\
\dfrac{\partial L_2}{\partial f_2(X)} &= - \big( \epsilon_1 - f_2(X)\big) = - \epsilon_2 \\
\end{aligned}$$

ì—¬ê¸°ì„œ $\epsilon_1$ì„ $L_2$ ì— ë„£ì–´ë³´ë©´

$$\begin{aligned}
L_2 &= \dfrac{1}{2}\big( Y - f_1(X) - f_2(X)\big)^2 \\
\dfrac{\partial L_2}{\partial f_2(X)} &= - \big( Y - f_1(X) - f_2(X)\big) = - \epsilon_2
\end{aligned}$$

ê°€ ë˜ê³ , $F(X) = f_1(X) + f_2(X)$ ë¼ê³  í•˜ë©´, ì•½í•œ í•™ìŠµìë¥¼ ì—¬ëŸ¬ ê°œë¥¼ ë”í•œ ëª¨ë¸($f_1(X) + f_2(X)$)ì„ ìµœì í™” í•˜ëŠ” ê²ƒê³¼ í•˜ë‚˜ì˜ ì˜ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸($F(X)$)ë¥¼ ìµœì í™”í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤(ë§ì¥ë‚œ ê°™ì§€ë§Œ, ì´ìœ ëŠ” ê°™ì€ ì”ì°¨$\epsilon_2$ ê°€ ë‚¨ê¸° ë•Œë¬¸).

$$\begin{aligned}
L &= \dfrac{1}{2}\big( Y - F(X)\big)^2 \\
\dfrac{\partial L}{\partial F(X)} &= - \big( Y - F(X)\big) = - \epsilon_2
\end{aligned}$$

ë˜í•œ, ì›ë˜ ì”ì°¨ë¥¼ êµ¬í•˜ë ¤ë©´ í•™ìŠµëœ ëª¨ë¸ì— ì˜ˆì¸¡ì„ í•˜ê³ , íƒ€ê²Ÿê°’ì— ì˜ˆì¸¡ê°’ì„ ë¹¼ì¤˜ì•¼í•˜ëŠ”ë°, ë¯¸ë¶„ìœ¼ë¡œ ì”ì°¨ë¥¼ êµ¬í•  ìˆ˜ ìˆìœ¼ë‹ˆ ë” ë¹ ë¥´ê²Œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.

</div></details>

ê²°êµ­ì—ëŠ” negative gradientê°€ ì”ì°¨ì™€ ì¼ì¹˜í•˜ê¸° ë•Œë¬¸ì— Gradient ìš©ì–´ê°€ ë“¤ì–´ê°€ëŠ” ê²ƒì´ë‹¤.

ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰í•œë‹¤([rpmcruzì˜ gboostì½”ë“œ](https://github.com/rpmcruz/machine-learning/blob/master/ensemble/boosting/gboost.py)ë¥¼ ë¹Œë ¤ ì•½ê°„ì˜ ë³€í˜•í›„ í•´ì„í•´ë³¸ë‹¤).

```python
# self.first_estimator = ë”ë¯¸ëª¨ë¸
# self.base_estimator = ì•½í•œ í•™ìŠµì
# self.loss = residual í•¨ìˆ˜, ì—¬ê¸°ì„œëŠ” y - y_pred
# self.eta = Loss(y, y_pred)ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” etaê°’ì„ êµ¬í•˜ëŠ”ë°,
# ì—¬ê¸°ì„œëŠ” ìƒëµí•˜ê³  í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ í†µì¼

def fit(self, X, y):
    self.fs = [self.first_estimator]
    # step 0
    f = self.first_estimator.fit(X, y)
    # step 1
    for m in range(self.M):
        y_pred = self.predict(X)
        # step 2
        R = self.loss(y, y_pred)
        # step 3
        g = clone(self.base_estimator).fit(X, R)
        # step 4
        self.fs.append(g)

def predict(self, X):
    f0 = self.fs[0].predict(X)
    r = np.sum([self.eta * f.predict(X) for f in self.fs[1:]], 0)
    return f0 + r
```

- **Step 0.** dummy ëª¨ë¸(ì˜ˆë¥¼ ë“¤ì–´ ëª¨ë“  ì˜ˆì¸¡ê°’ì´ 0ì¸ ëª¨ë¸)ë¡œ ì´ˆê¸°í™” í•œë‹¤. 
- **Step 1.** ëª¨ë¸ì˜ ê°œìˆ˜ `M` ë§Œí¼ ì§„í–‰í•˜ë©° í•­ìƒ ì²« ìŠ¤í…ì— `X`ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•œë‹¤. ì˜ˆì¸¡ì€ í˜„ì¬ê¹Œì§€ ì €ì¥í•´ë‘” ì•½í•œ í•™ìŠµìì˜ ì˜ˆì¸¡ê°’ê³¼ ê°€ì¤‘ì¹˜(`self.eta`)ë¥¼ ê³±í•´ì„œ í•©í•œ ê°’ìœ¼ë¡œ í•œë‹¤.
- **Step 2.** ì˜ˆì¸¡ê°’(`y_pred`)ê³¼ íƒ€ê²Ÿê°’(`y`)ì„ ì´ìš©í•´ ì”ì°¨(`R`)ì„ êµ¬í•œë‹¤.
- **Step 3.** ì•½í•œ í•™ìŠµìë¥¼ íƒ€ê²Ÿê°’ì´ ì•„ë‹Œ ì”ì°¨ê°’ì— ëŒ€í•´ì„œ í•™ìŠµí•œë‹¤.


# Tree-based Ensemble

ì˜ì‚¬ê²°ì • ë‚˜ë¬´ ê¸°ë°˜ì˜ ì•™ìƒë¸” ë°©ë²•ë¡ ìœ¼ë¡œ ë¶€ìŠ¤íŒ… ë°©ë²•ì¸ **ê²½ì‚¬ ë¶€ìŠ¤íŒ…(Gradient Boosting)**ì´ ìˆë‹¤. 

## XGBoost

Gradient Boosting ì•Œê³ ë¦¬ì¦˜ì—ì„œ ê°€ì¥ ìœ ëª…í•œ íŒ¨í‚¤ì§€ëŠ” XGBoostì´ë‹¤.

Advantages of using Gradient Boosting technique:
Supports different loss function.
Works well with interactions.
Disadvantages of using Gradient Boosting technique:
Prone to over-fitting.
Requires careful tuning of different hyper-parameters


## Random Forest

Advantages of using Random Forest technique:
Handles higher dimensionality data very well.
Handles missing values and maintains accuracy for missing data.
Disadvantages of using Random Forest technique:
Since final prediction is based on the mean predictions from subset trees, it wonâ€™t give precise values for the regression model.


