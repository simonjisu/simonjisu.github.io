<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>All about Word Vectors: Negative Sampling</title>
  <meta name="description" content="All about Word Vectors: Negative Sampling">
  
  <meta name="author" content="Soo">
  <meta name="copyright" content="&copy; Soo 2018">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="All about Word Vectors: Negative Sampling" />
  <meta property="og:url" content="http://simonjisu.github.io" />
  <meta property="og:site_name" content="Soo" />
  <meta property="og:title" content="All about Word Vectors: Negative Sampling" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://simonjisu.github.io/assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="All about Word Vectors: Negative Sampling">
  <meta name="twitter:description" content="All about Word Vectors: Negative Sampling">
  <meta name="twitter:image" content="http://simonjisu.github.io/assets/logo.png">
  <meta name="twitter:url" content="http://simonjisu.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html">
  <link rel="alternate" type="application/rss+xml" title="Soo" href="http://simonjisu.github.io/feed.xml" />


  <!-- <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" ></script> -->

  <!-- Latex -->
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    MathJax.Hub.Config({
        TeX: {
            extensions: ["mhchem.js", "cancel.js"]
        },
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true,
            ignoreClass: "no-mathjax",
        },
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            // linebreaks: { automatic: true }
        },
        menuSettings: {
            zoom: "Click",
            zscale: "200%"
        }
    });
  </script>

  <!--lightSlider-->
  <link rel="stylesheet" href="/css/lightslider.css" />

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5GSH93K');</script>
  <!-- End Google Tag Manager -->

</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="Soo">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        
          
          <li class="nav-link"><a href="/about/">About</a>
          
        
          
        
          
        
          
        
          
          <li class="nav-link"><a href="/posts/">Posts</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          <li class="nav-link"><a href="https://github.com/simonjisu"> GitHub </a>
      </ul>
    </nav>
  </div>
</header>


    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5GSH93K"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div class="page-content">
        <div class="post">
<!-- <h1></h1>
 -->
<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">All about Word Vectors: Negative Sampling</h1>
      <p class="info">by <strong>Soo</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">April 24, 2018</div>
  <div class="post-categories">
   in 
    
    <a href="/posts/#NLP">Nlp</a>
    
  
  </div>
</section>

<article class="post-content">
  <h1 id="all-about-word-vectors-negative-sampling">All about Word Vectors: Negative Sampling</h1>

<hr />
<p>본 포스팅은 <a href="http://web.stanford.edu/class/cs224n/">CS224n</a> Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.</p>

<p><img src="/assets/ML/nlp/L2_model_train.png" /></p>

<h2 id="navie-softmax--">Navie Softmax 의 단점</h2>

<p>Navie Softmax 를 최종단에 출력으로 두고 Backpropagation 할때는 큰 단점이 있다.</p>

<p>사실 Softmax가 그리 값싼 연산은 아니다. 우리가 학습하고 싶은 단어 벡터 1000개가 있다고 가정해보자. 그렇다면 매 window size=2 마다, 다시 말해 총 업데이트 할 5개의 단어 (중심단어 1 + 주변 단어 2 x 2) 를 위해서, $W, W’$ 안에 파라미터를 업데이트 해야하는데, 그 갯수가 최소 $(2 \times d \times 1000)$ 만큼된다.</p>

<script type="math/tex; mode=display">\triangledown_\theta J_t(\theta) \in \Bbb{R}^{2dV}</script>

<p>많은 양의 단어에 비해 업데이트 하는 파라미터수는 적기 때문에 gradient matrix $\triangledown_\theta J_t(\theta)$ 가 굉장히 sparse 해질 수 있다 (0이 많다는 소리). Adam 같은 알고리즘은 sparse 한 matrix 에 취약하다.</p>

<p><a href="https://simonjisu.github.io/deeplearning/2018/01/13/numpywithnn_5.html">Numpy with NN: Optimizer 편 참고</a></p>

<p>그래서 <strong>“window에 실제로 등장하는 단어들만 업데이트 하면 좋지 않을까?”</strong> 라는 생각을 하게 된다.</p>

<h2 id="negative-sampling">Negative Sampling</h2>

<blockquote>
  <p>paper 1: <a href="https://arxiv.org/abs/1310.4546">Distributed representaions of Words and Phrases and their Compositionality (Mikolov et al. 2013)</a></p>

  <p>paper 2: <a href="https://arxiv.org/abs/1402.3722">word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method</a></p>
</blockquote>

<p>요약하면 아래와 같은 목적함수를 최대화 하는 것이다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
J(\theta) &= \dfrac{1}{T}\sum_{t=1}^{T} J_t(\theta)\\
J_t(\theta) &= \underbrace{\log \sigma(u_o^T v_c)}_{(1)} + \underbrace{\sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_c)]}_{(2)}
\end{aligned} %]]></script>

<ul>
  <li>$T$: total num of words</li>
  <li>$\sigma$: sigmoid function</li>
  <li>$P(w) = {U(w)^{3/4}} / {Z}$: unigram distribution U(w) raised to the 3/4 power
    <ul>
      <li>The power makes less frequent words be sampled more often</li>
    </ul>
  </li>
</ul>

<p>말로 풀어보자면, 모든 단어 $T$ 에 대해서 중심단어 $c$ 와 그 주변단어 $o$ 가 같이 나올 확률 <strong>[수식 (1)]</strong> 을 최대화 하고, 그 주변단어가 아닌 집합에서 sampling 하여 나온 $k$ 개의 단어의 확률 <strong>[수식 (2)]</strong> 을 최소화 시키는 것이다. (음수가 붙기 때문에 최소하하게 되면 최대화가 된다.)</p>

<hr />

<h3 id="section">상세 논문 설명</h3>

<p>논문 기준으로 위에 <strong><span style="color: #e87d7d">표기법</span></strong> 이 조금 다르다.</p>
<ul>
  <li>여기서 <strong>$w$ = center word, $c$ = context</strong> 다.</li>
</ul>

<p>출발점은 아래와 같다.</p>

<blockquote>
  <p>$(w, c)$ 세트가 정말로 corpus data로 부터 왔는가?</p>
</blockquote>

<p>라고 생각하고 아래와 같은 <strong>정의</strong> 를 하게 된다.</p>

<ul>
  <li>$P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로 부터 왔을 확률</li>
  <li>$P(D = 0 \vert w, c) = 1 - P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로부터 오지 않았을 확률</li>
</ul>

<p>따라서, 우리의 목적은 확률 $P(D = 1\vert\ w, c)$ 를 최대화하는 parameter $\theta$를 찾는 것이기 때문에 아래와 같은 목적함수를 세울 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} &\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &\arg \underset{\theta}{\max} \log \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta)
\end{aligned} %]]></script>

<p>파라미터 $\theta$ 는 단어들의 벡터라고 생각할 수 있다. 즉, 위의 식을 만족하는 어떤 최적의 단어 벡터를 찾는것이다.</p>

<p>또한, 확률 $P(D=1\vert\ w,c;\theta)$ 은 sigmoid로 아래와 같이 정의 할 수 있다.</p>

<script type="math/tex; mode=display">P(D=1\vert\ w,c;\theta) = \dfrac{1}{1+e^{-v_c v_w}}</script>

<p>따라서 우리의 목적함수는 아래와 같이 다시 고쳐 쓸수 있다.</p>

<script type="math/tex; mode=display">\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} }</script>

<p>그러나 우리의 목적 함수는 매 $(w, c)$ 세트마다 $P(D=1\vert\ w,c;\theta)=1$ 를 만족하는 trivial solution이 존재한다. $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 $\theta$ (보통 $K$ 가 40이 넘어가면 위 방정식의 값이 0에 가까워짐) 는 모든 값을 똑같이 0으로 만들어 버리기 때문에, 같은 값을 갖지 못하게 하는 매커니즘이 필요하다. ($\theta$ 에 뭘 넣어도 0이 되면 최대값을 찾는 의미가 없어진다, 자세한건 밑에 <span style="color: #e87d7d">참고 1</span> 를 참조) 여기서 “같은 값을 같는다” 라는 말은 단어 벡터가 같은 값을 갖는 것이다.</p>

<p>따라서, 하나의 방법으로 랜덤 $(w, c)$ 조합을 생성하는 집합 $D’$를 만들어 corpus data 로부터 올 확률 $P(D=1\vert \ w,c;\theta)$ 를 낮게 강제하는 것이다. 즉, $D’$ 에서 생성된 $(w, c)$ 조합은 <strong>corpus data 로부터 오지 않게</strong> 하는 확률 $P(D=0\vert\ w,c;\theta)$ 을 최대화 하는 것.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
& \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} P(D=0\vert\ w,c;\theta) \\
&= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta) + \underset{(w,c) \in D'}{\sum} \log \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \big(1- \dfrac{1}{1+e^{-v_c v_w} } \big) \\
&= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} }
\end{aligned} %]]></script>

<p>$\sigma(x) = \dfrac{1}{1+e^{-x} }$ 시그모이드 함수로 정의 하면, 아래와 같이 정리 할 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
& \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} } \\
&= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \sigma(v_c v_w) + \underset{(w,c) \in D'}{\sum} \log \sigma(- v_c v_w) \quad \cdots (3)
\end{aligned} %]]></script>

<p>이는 <span style="color: #e87d7d">paper 1</span> 의 (4) 번 식과 같아지는다.</p>

<script type="math/tex; mode=display">\log \sigma(u_c^T v_w) + \sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_w)]</script>

<p>다른 점이라면, 우리가 만든 (3)식에서는 전체 corpus ($D \cup D’$) 을 포함하지만, Mikolov 논문의 식은 $D$ 에 속하는 $(w, c)$ 조합 하나와 $k$ 개의 다른 $(w, c_j)$ 의 조합을 들었다는 것이다. 구체적으로, $k$ 번의 negative sampling 에서 Mikolov 는 $D’$ 를 $k \times D$ 보다 크게 설정했고, k개의 샘플 $(w, c_1), (w, c_2), \cdots, (w, c_k)$ 에 대해서 $c_j$ 는 <strong>unigram distribution</strong> 에 <strong>3/4</strong> 승으로 부터 도출된다. 이는 아래의 분포에서 $(w, c)$ 조합을 추출 하는 것과 같다.</p>

<script type="math/tex; mode=display">p_{words}(w) = \dfrac{p_{contexts} (c)^{3/4} }{Z}</script>

<ul>
  <li>$p_{words}(w)$, $p_{contexts} (c)$ 는 각각 words and contexts 의 unigram distribution 이다.</li>
  <li>$Z$ 는 normalization constant</li>
</ul>

<p>Unigram distribution 은 단어가 등장하는 비율에 비례하게 확률을 설정하는 분포다. 예를 들어 “I have a pen. I have an apple. I have a pineapple.” 라는 문장이 있다면, 아래와 같은 분포를 만들 수 있다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">I</th>
      <th style="text-align: center">have</th>
      <th style="text-align: center">a</th>
      <th style="text-align: center">pen</th>
      <th style="text-align: center">an</th>
      <th style="text-align: center">apple</th>
      <th style="text-align: center">pineapple</th>
      <th style="text-align: center">.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">3/15</td>
      <td style="text-align: center">3/15</td>
      <td style="text-align: center">2/15</td>
      <td style="text-align: center">1/15</td>
      <td style="text-align: center">1/15</td>
      <td style="text-align: center">1/15</td>
      <td style="text-align: center">1/15</td>
      <td style="text-align: center">3/15</td>
    </tr>
  </tbody>
</table>

<p>여기서 3/4 승을 해주면, 가끔 등장하는 단어는 확률을 높혀주는 효과가 있다. 물론 자주 나오는 단어의 확률도 올라가지만 가끔 등장하는 단어의 상승폭 보다 적다.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>a</th>
      <th>$a^{\frac{3}{4} }$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>apple</td>
      <td>$\frac{1}{15}=0.067$</td>
      <td>${\frac{1}{15} }^{\frac{3}{4} }=0.131$</td>
    </tr>
    <tr>
      <td>have</td>
      <td>$\frac{3}{15}=0.020$</td>
      <td>${\frac{3}{15} }^{\frac{3}{4} }=0.299$</td>
    </tr>
  </tbody>
</table>

<p>Mikolov 논문에서는 context는 하나의 단어이기 때문에 $p_{words}(w)$ 는 아래와 동일하다.</p>

<script type="math/tex; mode=display">p_{words}(w) = p_{contexts} (c) = \dfrac{count(x)}{ \vert text \vert }</script>

<hr />

<h3 id="trivial-solution">참고 1. Trivial Solution</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} L(\theta;w,c) &= \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } \\
&= \underset{(w,c) \in D}{\sum} \log(1) - \log(1+e^{-v_c v_w}) \\
&= \underset{(w,c) \in D}{\sum} - \log(1+e^{-v_c v_w})
\end{aligned} %]]></script>

<p>같은 두 벡터의 내적을 하게 되면 값은 최대가 된다. $\cos$ 값이 1이 되기 때문이다. (여기서는 최대 값이 중요한건 아니지만 값이 커진다는데 의의가 있다.)
<script type="math/tex">a\cdot a=\vert a \vert \vert a \vert \cos \theta</script></p>

<div class="highlighter-rouge"><pre class="highlight"><code>a = np.array([1,2,3,4,5,6,7])
b = np.array([.1,.2,.3,.4,.5,.6,.7])
print(np.dot(a, a))
print(np.dot(a, b))
</code></pre>
</div>
<blockquote>
  <p>140</p>

  <p>14.0</p>
</blockquote>

<p>즉, $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 모든 값들이 $e^{-v_c v_w}$ 를 0으로 만든다면, $L(\theta; w, c)$ 값은 0이 될것이다. 보통 $K$ 가 40 이 상이면, $L(\theta;w,c)$ 의 해는 모두 0 일 것이며 이것을 <strong>trivial solution</strong> 이라고 한다. 우리의 목적은 단어 벡터 $v_c$ 와 $v_w$ 의 구별이기 때문에, $v_c \not = v_w$ 으로 만들어야한다.</p>

<hr />

<p><br /></p>

<p>다음 시간에는 말뭉치의 공기정보(co-occurance)를 고려해 단어를 벡터화 시킨 <strong>GloVe</strong> 에 대해 알아보자.</p>

</article>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
      <a href="//www.facebook.com/sharer.php?t=All+about+Word+Vectors%3A+Negative+Sampling&u=http%3A%2F%2Fsimonjisu.github.io%2Fnlp%2F2018%2F04%2F24%2Fallaboutwv3.html"
        onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
</section>



  

  
    
    <!-- <h2 id="NLP">0</h2> -->
    <!-- <h2 id="NLP">/nlp/2018/05/02/allaboutwv4.html</h2> -->
    
    <!-- <h2 id="NLP">2</h2> -->
    <!-- <h2 id="NLP">/nlp/2018/04/20/allaboutwv2.html</h2> -->
    



  
	<section class="post-navigation">
		<span class="prev-post">
			
				<a href="/nlp/2018/04/20/allaboutwv2.html">
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-left fa-stack-1x fa-inverse"></i>
					</span>
					<span class="page-number">All about Word Vectors: Word2Vec</span>
				</a>
			
		</span>
		<span class="next-post">
			
				<a href="/nlp/2018/05/02/allaboutwv4.html">
					<span class="page-number">All about Word Vectors: GloVe</span>
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-right fa-stack-1x fa-inverse"></i>
					</span>
				</a>
			
		</span>
	</section>




  
  <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'soopace';

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </section>
  


</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Soo</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">
        
        
          <li class="nav-link"><a href="/about/">About</a>
        
        
        
        
        
        
        
        
        
          <li class="nav-link"><a href="/posts/">Posts</a>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:simonjisu@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">simonjisu@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://www.facebook.com/simonjisu" title="Friend me on Facebook">
              <i class="fa fa-facebook"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://github.com/simonjisu" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
        
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">My Blog
</p>
    </div>

  </div>
</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>

<!-- Mathjax -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();
  $("code").addClass("python")
  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});
$("code").addClass("python")
</script>

<!--lightSlider-->
<script src="/js/lightslider.js"></script>
<script src='/js/multi_slider.js' type="text/javascript"></script>





    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-102691608-1', 'auto');
  ga('send', 'pageview');
</script>


  </body>

</html>
