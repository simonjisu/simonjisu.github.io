<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>DeepMindNLP 강의 정리 1</title>
  <meta name="description" content="Word Vectors and Lexical Semantics">
  
  <meta name="author" content="Soo">
  <meta name="copyright" content="&copy; Soo 2018">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="Word Vectors and Lexical Semantics" />
  <meta property="og:url" content="http://simonjisu.github.io" />
  <meta property="og:site_name" content="Soo" />
  <meta property="og:title" content="DeepMindNLP 강의 정리 1" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://simonjisu.github.io/assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="DeepMindNLP 강의 정리 1">
  <meta name="twitter:description" content="Word Vectors and Lexical Semantics">
  <meta name="twitter:image" content="http://simonjisu.github.io/assets/logo.png">
  <meta name="twitter:url" content="http://simonjisu.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://simonjisu.github.io/deepnlp/2018/01/02/deepmindnlp1.html">
  <link rel="alternate" type="application/rss+xml" title="Soo" href="http://simonjisu.github.io/feed.xml" />


  <!-- <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" ></script> -->

  <!-- Latex -->
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    MathJax.Hub.Config({
        TeX: {
            extensions: ["mhchem.js", "cancel.js"]
        },
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true,
            ignoreClass: "no-mathjax",
        },
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            // linebreaks: { automatic: true }
        },
        menuSettings: {
            zoom: "Click",
            zscale: "200%"
        }
    });
  </script>

  <!--lightSlider-->
  <link rel="stylesheet" href="/css/lightslider.css" />

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5GSH93K');</script>
  <!-- End Google Tag Manager -->

</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="Soo">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        
          
          <li class="nav-link"><a href="/about/">About</a>
          
        
          
        
          
        
          
        
          
          <li class="nav-link"><a href="/posts/">Posts</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          <li class="nav-link"><a href="https://github.com/simonjisu"> GitHub </a>
      </ul>
    </nav>
  </div>
</header>


    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5GSH93K"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div class="page-content">
        <div class="post">
<!-- <h1></h1>
 -->
<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">DeepMindNLP 강의 정리 1</h1>
      <p class="info">by <strong>Soo</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">January 2, 2018</div>
  <div class="post-categories">
   in 
    
    <a href="/posts/#Deepnlp">Deepnlp</a>
    
  
  </div>
</section>

<article class="post-content">
  <h1 id="word-vectors-and-lexical-semantics">Word Vectors and Lexical Semantics</h1>

<h2 id="how-to-represent-words">How to represent Words</h2>
<ul>
  <li>Natural language text = sequences of discrete symbols 이산 기호들의 배열(시퀀스)</li>
  <li>
    <p>Navie representaion: one hot vectors $\in$ $R^{vocabulary}$, one hot 인코딩된 벡터들로 표현 아주큼</p>

    <p>words = [‘딥마인드’, ‘워드’, ‘벡터’]
  df = pd.DataFrame(np.eye(len(words)), index=words, dtype=np.int)
  df</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">word</th>
      <th style="text-align: center">0</th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">딥마인드</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">워드</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">벡터</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>Classical IR: document and query vectors are superpositions of word vectors
<script type="math/tex">\hat{d_q}=\underset{d}{\arg \max} \sim(d,q)</script></p>
  </li>
  <li>
    <p>Similarly for word classification problems(e.g. Navie Bayes topic models)</p>
  </li>
  <li>
    <p>Issues: sparse, orthogonal representations, semantically weak</p>
  </li>
</ul>

<h2 id="semantic-similarity--">Semantic similarity 의미론적 유사성</h2>
<ul>
  <li>더 풍부하게 단어를 표현하고 싶다!!</li>
  <li>Distributional semantics: 분산 의미론
    <ul>
      <li>Idea: produce dense vector representations based on the contex/use of words</li>
      <li>Approaches:
        <ul>
          <li>count-based</li>
          <li>predictive</li>
          <li>task-based</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="count-based-methods">Count-based methods</h3>
<p>Define a basis vocabulary C of context words</p>
<ul>
  <li>고를 때는 linguistic intutition(언어적 직관, 주관적인) / statistics of the corpus 에 의해 고름</li>
  <li>이것을 하는 이유는 a, the 같은 의미와 무관한 function word를 포함시키지 않기 위함</li>
</ul>

<p>Define a word window size $w$.</p>

<p>Count the basis vocabulary words occurring $w$ words to the left or right of each instance of a target word in the corpus</p>

<p>From a vector representation of the target word based on these counts</p>

<p>Example:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>from collections import Counter, defaultdict
from operator import itemgetter

def get_vocabulary_dict(contexts, stopwords):
vocabulary = Counter()
for sentence in contexts:
    words = [word for word in sentence.split() if word not in stopwords]
    vocabulary.update(words)
return vocabulary

def represent_vector(contexts_words, vocabulary):
vocab_len = len(vocabulary)
word2idx = {w: i for i, w in enumerate(vocabulary)}
count_based_vector = defaultdict()

for key_word, context_w in contexts_words.items():
    temp = np.zeros(vocab_len, dtype=np.int)
    for w in context_w:
        temp[word2idx[w]] += 1
    count_based_vector[key_word] = temp
return count_based_vector, word2idx

contexts = ['and the cute kitten purred and then',
        'the cute furry cat purred and miaowed',
        'that the small kitten miaowed and she',
        'the loud furry dog ran and bit']
stopwords=['and', 'then', 'she', 'that', 'the', 'cat', 'dog', 'kitten']
contexts_words = {'kitten': {'cute', 'purred', 'small', 'miaowed'},
              'cat': {'cute', 'furry', 'miaowed'},
              'dog': {'loud', 'furry', 'ran', 'bit'}}

vocabulary = get_vocabulary_dict(contexts, stopwords)
count_based_vector, word2idx = represent_vector(contexts_words, vocabulary)

word_idx_list = [w for i, w in sorted([(i, w) for w, i in word2idx.items()], key=itemgetter(0))]
df = pd.DataFrame(count_based_vector, index=word_idx_list)
df.T
</code></pre>
</div>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">cute</th>
      <th style="text-align: center">purred</th>
      <th style="text-align: center">furry</th>
      <th style="text-align: center">miaowed</th>
      <th style="text-align: center">small</th>
      <th style="text-align: center">loud</th>
      <th style="text-align: center">ran</th>
      <th style="text-align: center">bit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">cat</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">dog</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">kitten</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<p>Compare as similarity kernel:
$cosine(u, v) = \dfrac{u\cdot v}{|u|\times|v|}$</p>

<div class="highlighter-rouge"><pre class="highlight"><code>def cosine(u, v):
    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))

print('kitten-cat:', cosine(df['kitten'], df['cat']))
print('kitten-dog:',cosine(df['kitten'], df['dog']))
print('cat-dog:',cosine(df['cat'], df['dog']))
</code></pre>
</div>

<blockquote>
  <p>kitten-cat: 0.57735026919</p>

  <p>kitten-dog: 0.0</p>

  <p>cat-dog: 0.288675134595</p>
</blockquote>

<p>Count-based method는 Navie Approach으로 접근</p>

<ul>
  <li>
    <p>Not all features are equal: we must distinguish counts that are high, because they are informative from those that are just independently frequent contexts.</p>
  </li>
  <li>
    <p>Many Normalisation methods: TF-IDF, PMI, etc</p>
  </li>
  <li>
    <p>Some remove the need for norm-invariant similarity metrics</p>
  </li>
</ul>

<p>But… perhaps there are easier ways to address this problem of count-based mothods(and others, e.g. choice of basis context)</p>

<h3 id="neural-embedding-models">Neural Embedding Models</h3>
<p>Learning count based vecotrs produces an embedding matrix in $R^{|vocab|\times|context|}$</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">cute</th>
      <th style="text-align: center">purred</th>
      <th style="text-align: center">furry</th>
      <th style="text-align: center">miaowed</th>
      <th style="text-align: center">small</th>
      <th style="text-align: center">loud</th>
      <th style="text-align: center">ran</th>
      <th style="text-align: center">bit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">cat</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">dog</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">kitten</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Rows are word vectors, so we can retrieve them with one hot vectors in ${0,1}^{</td>
      <td>vocab</td>
      <td>}$</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">onehot_{cat} = \begin{bmatrix} 0 \newline 1 \newline 0 \end{bmatrix}, cat=onehot_{cat}^TE</script>

<p>Symbols = unique vectors. Representation = embedding symbols with $E$</p>

<h4 id="generic-idea-behind-embedding-learning">Generic(포괄적인) idea behind embedding learning:</h4>
<ol>
  <li>Collect instances $t_i \in inst(t)$ of a word $t$ of vocab $V$</li>
  <li>For each instance, collect its context words $c(t_i)$ (e.g. k-word window)</li>
  <li>Define some score function $score(t_i, c(t_i); \theta, E)$ with upper bound on output</li>
  <li>Define a loss:
<script type="math/tex">L=-\sum_{t\in V}\sum_{t_i \in inst(t)}score(t_i, c(t_i);\theta,E)</script></li>
  <li>Estimate:
<script type="math/tex">\hat{\theta}, \hat{E}=\underset{\theta, E}{\arg \min}\ L</script></li>
  <li>Use the estimated $E$ as your embedding matrix</li>
</ol>

<h4 id="problems-scoring-function">Problems: Scoring function</h4>

<p>Easy to design a useless scorer(e.g. ignore input, output upper bound)</p>

<p>Implicitly define is useful</p>

<p>Ideally, scorer:</p>
<ul>
  <li>Embeds $t_i$ with $E$</li>
  <li>Produces a score which is a function of how well $t_i$ is accounted for by $c(t_i)$, and/or vice versa</li>
  <li>Requires the word to account for the context(or the reverse) more than another word in the same place.</li>
  <li>Produces a loss which is differentiable w.r.t. $\theta$ and $E$</li>
</ul>

<h4 id="cwcollobert-et-al-2011">C&amp;W(Collobert et al. 2011)</h4>
<p><a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">paper</a></p>

<p>Interpretation: representations carry information about what neighbouring representations should look like</p>

<p>where it belongs? 같은 정보를 포함</p>

<h4 id="cbow-mikolov-et-al-2013">CBoW (Mikolov et al. 2013)</h4>
<p><a href="https://arxiv.org/abs/1301.3781">paper</a></p>

<p>Embed context words. Add them.</p>

<p>Project back to vocabulary size. Softmax.</p>

<script type="math/tex; mode=display">softmax(l)_i=\dfrac{e^{l_i}}{\sum_{j}e^{l_i}}</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray} P(t_i|context(t_i) & = & softmax(\sum_{t_j\in context(t_i)} onehot_{t_j}^{t}\cdot E\cdot W_v) \newline
& = & softmax((\sum_{t_j\in context(t_i)} onehot_{t_j}^{t}\cdot E)\cdot W_v) \end{eqnarray} %]]></script>

<p>Minimize Negative Log Likelihood:</p>

<script type="math/tex; mode=display">L_{data} = -\sum_{t_i \in data}\log P(t_i|context(t_i))</script>

<p>장점:</p>
<ul>
  <li>All linear, so very fast. Basically a cheap way of applying one matrix to all inputs.</li>
  <li>Historically, negative sampling used instead of expensive softmax.</li>
  <li>NLL(negative log-likelihood) minimisation is more stable and is fast enough today</li>
  <li>Variants: postion specific matrix per input(Ling et al. 2015)</li>
</ul>

<h4 id="skip-gram-mikolov-et-al-2013">Skip-gram (Mikolov et al. 2013)</h4>
<p><a href="https://arxiv.org/abs/1301.3781">paper</a></p>

<p>Target word predicts context words.</p>

<p>Embed target word.</p>

<p>Project into vocabulary. Softmax.</p>

<script type="math/tex; mode=display">P(t_j|t_i) = softmax(onehot_{t_i}^T\cdot E \cdot W_v)</script>

<p>Learn to estimate Likelihood of context words.</p>

<script type="math/tex; mode=display">-\log P(context(t_i)|t_i) = -\log \prod_{t_j\in context(t_i)}P(t_j|t_i) - \sum_{t_j\in context(t_i)}\log P(t_j|t_i)</script>

<p>장점:</p>

<ul>
  <li>Fast: One embedding versus $C$ (size of contexts) embeddings</li>
  <li>Just read off probabilities from softmax</li>
  <li>Similiar variants to CBoW possible: position specific projections</li>
  <li>Trade off between efficiency and more structured notion of context</li>
</ul>

<h4 id="section">기타</h4>
<p>Word Embedding 하는 목적이 뭐냐? dense 한 vector 를 얻는 거다</p>

<p>Word2Vec은 딥러닝이 아니라 shallow model(얕은 모델: 층이 하나밖에 없는)이다.</p>

<p>Word2Vec == PMI Matrix factorization of count based models(Levy and Goldberg, 2014)</p>

<h3 id="specific-benefits-of-neural-approaches">Specific Benefits of Neural Approaches</h3>
<ul>
  <li>Easy to learn, especially with good linear algebra libraries.</li>
  <li>Highly parallel problem: minibatching, GPUs, distributed models.</li>
  <li>Can predict other discrete aspects of context(dependencies, POS tags, etc). Can estimate these probabilities with counts, but sparsity quickly becomes a problems.</li>
  <li>Can predict/condition on continuous contexts: e.g. images.</li>
</ul>

<h3 id="evaluating-word-representations">Evaluating Word Representations</h3>
<p>Intrinsic Evaluation:</p>
<ul>
  <li>WordSim-353 (Finkelstein et al. 2003)</li>
  <li>SimLex-999 (Hill et al 2016, but has been around since 2014)</li>
  <li>Word analogy task (Mikolov et al. 2013)</li>
  <li>Embedding visualisation (nearest neighbours, T-SNE projection)</li>
</ul>

<p>t-SNE visualize, word 2 dimension cluster: <span style="color: #e87d7d"> http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/</span></p>

<p>Extrinsic Evaluation:</p>
<ul>
  <li>Simply: do your embeddings improve performance on other task(s).</li>
  <li>More …</li>
</ul>

<h3 id="task-based-embedding-learning">Task-based Embedding Learning</h3>
<p>Just saw methods for learning $E$ through minimising a loss.</p>

<p>One use for $E$ is to get input features to a neural network from words.</p>

<p>Neural network parameters are updated using gradients on loss $L(x, y, \theta)$:
<script type="math/tex">\theta_{t+1} = update(\theta_t, \triangledown_{\theta}L(x, y, \theta_t))</script></p>

<p>If $E \subseteq \theta$ then this update can modify $E$ (if we let it):
<script type="math/tex">E_{t+1} = update(E_t, \triangledown_E L(x, y, \theta_t))</script></p>

<h4 id="task-based-features-bow-classifiers">Task-based Features: Bow Classifiers</h4>
<p>Classify sentences/documents based on a variable number of word representations</p>

<p>Simplest options: bag of vectors
<script type="math/tex">P(C|D)=softmax(W_C \sum_{t_i \in D} embed_E(t_i))</script></p>

<p>Projection into logits (input to softmax) canbe arbitrarily complex. E.g.:
<script type="math/tex">P(C|D)=softmax(W_C \cdot \sigma (\sum_{t_i \in D} embed_E(t_i)))</script></p>

<ul>
  <li>$C$: class</li>
  <li>$D$: document</li>
</ul>

<p>Example tasks:</p>
<ul>
  <li>Sentiment analysis: tweets, movie reviews</li>
  <li>Document classification: 20 Newsgroups</li>
  <li>Author identification</li>
</ul>

<h4 id="task-based-features-bilingual-features">Task-based Features: Bilingual Features</h4>
<p>linguistic general approach: translations</p>

<p>데이터가 많으면 그냥 pre-trained할 필요 없이 Embedding을 만든(random initialize) 담에 같이 train하면 됨, 만약에 데이터가 충분치 않다면, 미리 training하는 것이 좋아 보임</p>

<h2 id="torch-word2vec-">Torch로 word2vec 짜보기</h2>

<div class="highlighter-rouge"><pre class="highlight"><code>import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torch.utils.data as data_utils
from torch.utils.data import DataLoader
from scipy.spatial.distance import cosine
import matplotlib.pylab as plt
from collections import Counter, defaultdict, deque
from nltk.tokenize import word_tokenize
from operator import itemgetter

class WORD2VEC(nn.Module):
    def __init__(self, N, half_window_size, lr, mode='cbow'):
        """
        V: vocab_size
        N: hidden layer size(word vector size)
        window_size: how many words that you want to see near target word
        mode: cbow / skipgram
        """
        super(WORD2VEC, self).__init__()

        self.V = None
        self.N = N
        # vocab and data setting
        self.half_window_size = half_window_size
        self.vocab_count = Counter()
        self.vocab2idx = defaultdict()
        self.vocab2idx['NULL'] = 0
        self.lr = lr

    def build_network(self):
        # network setting
        self.i2h = nn.Embedding(self.V, self.N, padding_idx=0)  # Embedding
        self.h2o = nn.Linear(self.N, self.V)
        self.softmax = nn.Softmax(dim=1)

    def get_vocabulary(self, corpus_list):
        for sentence in corpus_list:
            self.vocab_count.update(sentence)
        for i, w in enumerate(self.vocab_count.keys()):
            self.vocab2idx[w] = i + 1
        self.idx2vocab = {i: w for w, i in self.vocab2idx.items()}

    def generate_batch(self, sentence):
        # sentence size와 window size 결정 조건 확인(추가할것)
        target_words = []
        batch_windows = []

        # add padding data
        batch_sentence = ['NULL'] * self.half_window_size + sentence + ['NULL'] * self.half_window_size
        for i, target_word in enumerate(sentence):
            target_words.append(target_word)
            center_idx = i + self.half_window_size
            window = deque(maxlen=self.half_window_size * 2)
            window.extendleft(reversed(batch_sentence[i:center_idx]))
            window.extend(batch_sentence[center_idx + 1:center_idx + 1 + self.half_window_size])
            batch_windows.append(window)

        return batch_windows, target_words

    def data_transfer(self, corpus_list):
        """batch_data = [windows(list), target(list)]"""
        batch_data = []
        for sentence in corpus_list:
            batch_windows, target_words = self.generate_batch(sentence)
            for window, target in zip(batch_windows, target_words):
                idxed_window = [self.vocab2idx[word] for word in window]
                idxed_target = [self.vocab2idx[target]]
                batch_data.append([idxed_window, idxed_target])
        return batch_data

    def tokenize_corpus(self, corpus):
        """문장에 부호를 제거하고 단어 단위로 tokenize 한다"""
        check = ['.', '!', ':', ',', '(', ')', '?', '@', '#', '[', ']', '-', '+', '=', '_']
        corpus_list = []
        for sentence in corpus:
            temp = word_tokenize(sentence)
            temp = [word.lower() for word in temp if word not in check]
            corpus_list.append(temp)
        return corpus_list

    def fit(self, corpus):
        """
        corpus를 학습시킬 데이터로 전환시켜준다. 모든 데이터는 단어의 vocab2idx를 근거해서 바뀐다.
        Vocab이 설정되면 네트워크도 같이 설정된다.
        batch_data = [window, target]
        """
        corpus_list = self.tokenize_corpus(corpus)
        self.get_vocabulary(corpus_list)
        self.V = len(self.vocab2idx)
        batch_data = self.data_transfer(corpus_list)
        self.build_network()
        print('fit done!')
        return batch_data

    def forward(self, X):
        embed = self.i2h(X)  # batch x V x N
        h = Variable(embed.data.mean(dim=1))  # batch x N
        output = self.h2o(h)  # batch x V
        probs = self.softmax(output)  # batch x V
        return output, probs

#################################################
# Sample Data
#################################################

def create_sample_data():
    corpus = ['the king loves the queen',
              'the queen loves the king',
              'the dwarf hates the king',
              'the queen hates the dwarf',
              'the dwarf poisons the king',
              'the dwarf poisons the queen',]

    return corpus


def get_data_loader(batch_data, batch_size, num_workers, shuffle=False):
    features = torch.LongTensor([batch_data[i][0] for i in range(len(batch_data))])
    targets = torch.LongTensor([batch_data[i][1] for i in range(len(batch_data))])
    data = data_utils.TensorDataset(features, targets)

    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)

    return loader

#################################################
# Train
#################################################

def word2vec_train(corpus, N, half_window_size=2, lr=0.01, n_epoch=1000, batch_size=10, print_epoch=100, num_workers=2, shuffle=False):
    """본격적으로 데이터를 학습한다"""
    word2vec = WORD2VEC(N=N, half_window_size=half_window_size, lr=lr)
    batch_data = word2vec.fit(corpus)
    loader = get_data_loader(batch_data, batch_size, num_workers, shuffle)

    F = nn.CrossEntropyLoss()
    optimizer = optim.SGD(word2vec.parameters(), lr=word2vec.lr)

    loss_list = []
    for epoch in range(n_epoch):

        for batch_X, batch_y in loader:
            optimizer.zero_grad()
            batch_X = Variable(batch_X)
            batch_y = Variable(batch_y)

            output, probs = word2vec.forward(batch_X)
            loss = F(output, batch_y.squeeze(-1))  # must be 1-d tensor in labels

            loss.backward()
            optimizer.step()
        loss_list.append(loss.data[0])

        if epoch % print_epoch == 0:
            print('#{}| loss:{}'.format(epoch, loss.data[0]))

    return word2vec, loss_list
</code></pre>
</div>

<p>Training은 아래와 같다.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>corpus = create_sample_data()
N = 2
half_window_size = 2
lr = 0.01
n_epoch = 3000
print_epoch = 200
batch_size = 4
num_workers=2
shuffle=False
word2vec, loss_list = word2vec_train(corpus, N=N, half_window_size=half_window_size,
  lr=lr, n_epoch=n_epoch, batch_size=batch_size, print_epoch=print_epoch,
  num_workers=num_workers, shuffle=shuffle)
</code></pre>
</div>

<p><img src="/assets/ML/Deepnlp/lec1/Loss.png" alt="Drawing" style="width: 300px;" /></p>

<p>2차원으로 embedding 했으니 평면에 그려보았다.</p>

<p><img src="/assets/ML/Deepnlp/lec1/vector.png" alt="Drawing" style="width: 300px;" /></p>

<p>조금더 큰 데이터를 그냥 CBOW 혹은 Skip-gram으로 학습 시킬 경우 속도가 아주 느린 것을 발견 할 수 가 있다. 이는 말뭉치가 많아질 수록 단어의 수도 많아 지기 때문에, 말단에 Hierarchical Softmax와 Negative Sampling 방법을 쓴다고 한다. 김범수님의 블로그 <a href="https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/">[링크]</a> 참조</p>

</article>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
      <a href="//www.facebook.com/sharer.php?t=DeepMindNLP+%EA%B0%95%EC%9D%98+%EC%A0%95%EB%A6%AC+1&u=http%3A%2F%2Fsimonjisu.github.io%2Fdeepnlp%2F2018%2F01%2F02%2Fdeepmindnlp1.html"
        onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
</section>



  
    
    <!-- <h2 id="Deepnlp">-1</h2> -->
    <!-- <h2 id="Deepnlp">/deepnlp/2018/01/02/deepmindnlp1.html</h2> -->
    
    <!-- <h2 id="Deepnlp">1</h2> -->
    <!-- <h2 id="Deepnlp"></h2> -->
    



  
	<section class="post-navigation">
		<span class="prev-post">
			
		</span>
		<span class="next-post">
			
		</span>
	</section>




  
  <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'soopace';

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </section>
  


</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Soo</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">
        
        
          <li class="nav-link"><a href="/about/">About</a>
        
        
        
        
        
        
        
        
        
          <li class="nav-link"><a href="/posts/">Posts</a>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:simonjisu@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">simonjisu@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://www.facebook.com/simonjisu" title="Friend me on Facebook">
              <i class="fa fa-facebook"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://github.com/simonjisu" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
        
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">My Blog
</p>
    </div>

  </div>
</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>

<!-- Mathjax -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();
  $("code").addClass("python")
  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});
$("code").addClass("python")
</script>

<!--lightSlider-->
<script src="/js/lightslider.js"></script>
<script src='/js/multi_slider.js' type="text/javascript"></script>





    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-102691608-1', 'auto');
  ga('send', 'pageview');
</script>


  </body>

</html>
