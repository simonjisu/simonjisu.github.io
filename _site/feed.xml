<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soopace</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>https://simonjisu.github.io/</link>
    <atom:link href="https://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 14 Jan 2021 20:18:32 +0900</pubDate>
    <lastBuildDate>Thu, 14 Jan 2021 20:18:32 +0900</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>[XAI] Explainable Artificial Intelligence (XAI) - 2 </title>
        <description>&lt;h1 id=&quot;explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai&quot;&gt;Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI&lt;/h1&gt;

&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1910.10045&quot;&gt;https://arxiv.org/abs/1910.10045&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;XAI에 대한 전반적인 소개를 정리한 논문이 나와서 차근 차근 요약 정리해보려고 한다(무려 115페이지, reference만 6페이지). 약간의 번역 어투와 생략된 것도 있으니 영어 원문을 참고하길 바란다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/paper/2020/12/31/xaitutorial1.html&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/paper/2021/01/07/xaitutorial2.html&quot;&gt;&lt;span style=&quot;color:#e25252&quot;&gt;Explainability: What, why, what for and how?(이번편)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Transparent machine learning models&lt;/li&gt;
  &lt;li&gt;Post-hoc explainability techniques for machile learning models: Taxonomy, shallow models and deep learning&lt;/li&gt;
  &lt;li&gt;XAI: Opportunities, challenges and future research needs&lt;/li&gt;
  &lt;li&gt;Toward responsible AI: Principles of artificial intelligence, fairness, privacy and data fusion&lt;/li&gt;
  &lt;li&gt;Conclusions and outlook&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;2-explainability-what-why-what-for-and-how&quot;&gt;2. Explainability: What, Why, What For and How?&lt;/h1&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Before proceeding with our literature study, it is convenient to first establish a common point of understanding on what the term explainability stands for in the context of AI and, more specifically, ML. This is indeed the purpose of this section, namely, to pause at the numerous definitions that have been done in regards to this concept (what?), to argue why explainability is an important issue in AI and ML (why? what for?) and to introduce the general classification of XAI approaches that will drive the literature study thereafter (how?).&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 시작하기 전에 &lt;strong&gt;설명가능성(explainability)&lt;/strong&gt;이라는 용어가 AI 혹은 ML의 맥락에서 무엇을 뜻하는지, 공통의 이해점을 확립해야한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What? 이 개념에 대한 정립된 수 많은 정의들을 정리&lt;/li&gt;
  &lt;li&gt;Why? What for? 왜 설명가능성이 AI와 ML에서 중요한 이슈인지&lt;/li&gt;
  &lt;li&gt;How? 이후 연구할 XAI 접근 방식의 일반적인 분류방식들 소개&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;21-terminology-clarification&quot;&gt;2.1 Terminology Clarification&lt;/h2&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;One of the issues that hinders the establishment of common grounds is the interchangeable misuse of interpretability and explainability in the literature. There are notable differences among these concepts. To begin with, interpretability refers to a passive characteristic of a model referring to the level at which a given model makes sense for a human observer. This feature is also expressed as transparency. By contrast, explainability can be viewed as an active characteristic of a model, denoting any action or procedure taken by a model with the intent of clarifying or detailing its internal functions.
&lt;br /&gt;
To summarize the most commonly used nomenclature, in this section we clarify the distinction and similarities among terms often used in the ethical AI and XAI communities.
&lt;br /&gt;
- Understandability (or equivalently, intelligibility) denotes the characteristic of a model to make a human understand its function – how the model works – without any need for explaining its internal structure or the algorithmic means by which the model processes data internally [18].
&lt;br /&gt;
- Comprehensibility: When conceived for ML models, comprehensibility refers to the ability of a learning algorithm to represent its learned knowledge in a human understandable fashion [19], [20], [21]. This notion of model comprehensibility stems from the postulates of Michalski [22], which stated that *“the results of computer induction should be symbolic descriptions of given entities, semantically and structurally similar to those a human expert might produce observing the same entities. Components of these descriptions should be comprehensible as single ‘chunks’ of information, directly interpretable in natural language, and should relate quantitative and qualitative concepts in an integrated fashion”*. Given its difficult quantification, comprehensibility is normally tied to the evaluation of the model complexity [17].
&lt;br /&gt;
- Interpretability: It is defined as the ability to explain or to provide the meaning in understandable terms to a human.
&lt;br /&gt;
- Explainability: Explainability is associated with the notion of explanation as an interface between humans and a decision maker that is, at the same time, both an accurate proxy of the decision maker and comprehensible to humans [17].
&lt;br /&gt;
- Transparency: A model is considered to be transparent if by itself it is understandable. Since a model can feature different degrees of understandability, transparent models in Section 3 are divided into three categories: simulatable models, decomposable models and algorithmically transparent models [5].&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#aaa&quot;&gt; 참고: 용어가 한국어로 거의다 비슷해서 최대한 의미를 붙여서 추가함 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 공통의 이해점 확립을 방해하는 요소중 하나는 &lt;strong&gt;설명가능성(explainability)&lt;/strong&gt;과 &lt;strong&gt;해석가능성(interpretability)&lt;/strong&gt;용어의 혼용이다. 이 둘의 개념읜 차이점이 있다. 결론부터 말하자면&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;해석가능성(Interpretability):&lt;/strong&gt; 모델이 인간에게 맞춰서 설명하는 &lt;u&gt;모델의 수동적 특성&lt;/u&gt;, 투명성(transparency)과 같은 말&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;설명가능성(Explainability):&lt;/strong&gt; 모델의 내부 기능을 명확히 하거나 자세히 설명할 목적으로, 수행된 모든 행동 또는 절차를 나타내는 &lt;u&gt;모델의 능동적 특성&lt;/u&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;가장 일반적으로 사용되는 명명법을 이야기하고자 ethical AI 및 XAI 커뮤니티에서 자주 사용되는 용어 간의 구별과 그 유사성을 명확히한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;이해가능성(Understandability)&lt;/strong&gt; 혹은 &lt;strong&gt;명료성(Intelligibility):&lt;/strong&gt; 모델 구조 혹은 내부의 알고리즘 기능의 부가 설명 없이도 인간이 바로 이해할 수 있는 모델의 특성 &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1051200417302385&quot;&gt;[18]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;포괄적 이해가능성(Comprehensibility):&lt;/strong&gt; 모델이 학습한 지식을 인간의 이해방식으로 나타내는 능력 &lt;a href=&quot;https://scholar.google.com/scholar?q=Evolutionary%20fuzzy%20systems%20for%20explainable%20artificial%20intelligence:%20Why,%20when,%20what%20for,%20and%20where%20to&quot;&gt;[19]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=A%20framework%20for%20considering%20comprehensibility%20in%20modeling&amp;amp;publication_year=2016&amp;amp;author=M.%20Gleicher&quot;&gt;[20]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Extracting%20comprehensible%20models%20from%20trained%20neural%20networks&amp;amp;publication_year=1996&amp;amp;author=M.W.%20Craven&quot;&gt;[21]&lt;/a&gt;. 이 개념은 Michalski&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=A%20theory%20and%20methodology%20of%20inductive%20learning&amp;amp;publication_year=1983&amp;amp;author=R.S.%20Michalski&quot;&gt;[22]&lt;/a&gt;의 가정에서 비롯됐다.&lt;/p&gt;

    &lt;p&gt;“컴퓨터 유도 결과는 주어진 실체에 대한 상징적 설명이어야 하며, 인간 전문가가 동일한 실체를 관찰하는 것과 의미론적이고 구조적으로 유사해야 한다. 이러한 설명의 구성요소는 자연어로 직접 해석할 수 있는 정보의 단일 ‘청크’로 이해할 수 있어야 하며, 통합된 방식으로 양적 및 질적 개념을 연관시켜야 한다.”&lt;/p&gt;

    &lt;p&gt;정량화가 어렵다는 점을 고려했을 때, 포괄적 이해가능성은 일반적으로 모델 복잡도 평가와 연관된다&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20methods%20for%20explaining%20black%20box%20models&amp;amp;publication_year=2018&amp;amp;author=R.%20Guidotti&amp;amp;author=A.%20Monreale&amp;amp;author=S.%20Ruggieri&amp;amp;author=F.%20Turini&amp;amp;author=F.%20Giannotti&amp;amp;author=D.%20Pedreschi&quot;&gt;[17]&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;해석가능성(Interpretability)&lt;/strong&gt;: 인간이 이해할 수 있는 용어로 의미를 설명하거나 제공하는 능력&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;설명가능성(Explainability):&lt;/strong&gt; 사람과 모델간의 “인터페이스(interface)” 역할로서 설명(explanation)과 연관되어 있다. 설명은 모델이 내린 의사결정의 정확한 대리이자 인간이 이해할 수 있는 것이어야 한다&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20methods%20for%20explaining%20black%20box%20models&amp;amp;publication_year=2018&amp;amp;author=R.%20Guidotti&amp;amp;author=A.%20Monreale&amp;amp;author=S.%20Ruggieri&amp;amp;author=F.%20Turini&amp;amp;author=F.%20Giannotti&amp;amp;author=D.%20Pedreschi&quot;&gt;[17]&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;투명성(Transparency):&lt;/strong&gt; 모델은 자신이 스스로 이해가능하다면 투명한 것으로 간주된다. 모델은 정도에 따라 이해력를 다르게 제공할 수 있기 때문에, 투명성 있는 모델은 section 3에서 3가지 항목(시뮬레이션 가능한 모델, 분해가능한 모델 그리고 알고리즘 자체가 투명한 모델)으로 나눌 것이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;In all the above definitions, understandability emerges as the most essential concept in XAI. Both transparency and interpretability are strongly tied to this concept: while transparency refers to the characteristic of a model to be, on its own, understandable for a human, understandability measures the degree to which a human can understand a decision made by a model. Comprehensibility is also connected to understandability in that it relies on the capability of the audience to understand the knowledge contained in the model. All in all, understandability is a two-sided matter: model understandability and human understandability. This is the reason why the definition of XAI given in Section 2.2 refers to the concept of audience, as the cognitive skills and pursued goal of the users of the model have to be taken into account jointly with the intelligibility and comprehensibility of the model in use. This prominent role taken by understandability makes the concept of audience the cornerstone of XAI, as we next elaborate in further detail.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 위의 모든 정의에서 이해가능성(understandability)은 XAI에서 가장 필수적인 개념이다. 투명성(transparency)과 해석가능성(interpretability)은 모두 이 개념과 강하게 연관되어 있다. 투명성(transparency)은 인간에게는 이해가능해야 하고, 모델 스스로는 이해할 수 있는 특성이지만, 이해가능성(interpretability)은 모델의 결정을 인간이 얼만큼 이해가능한지 측정한다.&lt;/p&gt;

&lt;p&gt;또한 포괄적 이해가능성(comprehensibility)은 &lt;strong&gt;청중&lt;/strong&gt;(audience, &lt;span style=&quot;color:#aaa&quot;&gt;[주] 설명을 듣는 사람&lt;/span&gt;)이 얼만큼 모델에 포함된 지식을 이해하는 지를 측정한다는 점에서 이해가능성(understandability)과 연결된다.&lt;/p&gt;

&lt;p&gt;대체로, 이해가능성(understandability)은 “모델”과 “인간”의 이해가능성으로 나눌 수 있다. 모델 이용자의 인지능력 및 추구목표는 모델의 명료성과 포괄적 이해가능성이 함께 고려되어야 하기 때문에, Section 2.2에서는 XAI개념을 정의할 때, &lt;strong&gt;청중&lt;/strong&gt;&lt;span style=&quot;color:#aaa&quot;&gt;([주] 모델을 사용하는 인간)&lt;/span&gt;의 개념을 먼저 이야기하려고 한다. 이해가능성은 XAI의 초석이 되는 청중의 개념을 만드는 역할을 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;22-what&quot;&gt;2.2 What?&lt;/h2&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Although it might be considered to be beyond the scope of this paper, it is worth noting the discussion held around general theories of explanation in the realm of philosophy [23]. Many proposals have been done in this regard, suggesting the need for a general, unified theory that approximates the structure and intent of an explanation. However, nobody has stood the critique when presenting such a general theory. For the time being, the most agreed-upon thought blends together different approaches to explanation drawn from diverse knowledge disciplines. A similar problem is found when addressing interpretability in AI. It appears from the literature that there is not yet a common point of understanding on what interpretability or explainability are. However, many contributions claim the achievement of interpretable models and techniques that empower explainability.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 철학의 영역에서 설명(explanation)에 대해 토의에 주목할 필요가 있다&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=General%20theories%20of%20explanation%3A%20buyer%20beware&amp;amp;publication_year=2013&amp;amp;author=J.%20D%C3%ADez&amp;amp;author=K.%20Khalifa&amp;amp;author=B.%20Leuridan&quot;&gt;[23]&lt;/a&gt;. 왜냐면 일반적이고 통일된 설명 이론의 구조와 의도를 근사하게나마 제시했기 때문이다. 그렇지만 튼튼한 이론은 아니였다. 그래서 그동안 다양한 지식 분야에서 도출된 설명에 대한 접근 박싱을 혼합한 정의를 사용했다. AI에서 해석가능성(interpretability)을 다룰 때도 비슷한 문제가 발견됐다. 해석가능성(interpretability)이나 설명가능성(explainability)의 공통점을 찾지 못했으나, 많은 연구자들은 해석가능한 모델의 생성과 모델 설명력을 강화했다는 연구성과를 주장해왔다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;To shed some light on this lack of consensus, it might be interesting to place the reference starting point at the definition of the term Explainable Artificial Intelligence (XAI) given by D. Gunning in [7]:
&lt;br /&gt;
'XAI will create a suite of machine learning techniques that enables human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.'
&lt;br /&gt;
This definition brings together two concepts (understanding and trust) that need to be addressed in advance. However, it misses to consider other purposes motivating the need for interpretable AI models, such as causality, transferability, informativeness, fairness and confidence [5], [24], [25], [26]. We will later delve into these topics, mentioning them here as a supporting example of the incompleteness of the above definition.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 부족한 공감대를 어느 정도 형성하기 위해서 D. Gunning이 제시한 Explainable Artificial Intelligence(XAI) 용어의 정의를 기준점으로 시작할 수 있을 것 같다. &lt;a href=&quot;https://scholar.google.com/scholar?q=Explainable%20artificial%20intelligence&quot;&gt;[7]&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“인간이 이해할 수 있고, 적절하게 신뢰할 수 있으며, 효과적으로 세로운 세대의 인공지능 파트너를 관리할 수 있는 머신러닝 기법 제품군을 XAI는 만들어 낼 것이다”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 정의는 다루어야 할 두 가지 개념(이해와 신뢰)을 담았다. 그러나 해석가능한 AI 모델에 필요한 인과성(causality), 전이성(transferability), 정보성(informativeness), 공정성(fairness)과 확실성(confidence) 등을 담지 않았다&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=The%20mythos%20of%20model%20interpretability&amp;amp;publication_year=2018&amp;amp;author=Z.C.%20Lipton&quot;&gt;[5]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar?q=D.%20Doran,%20S.%20Schulz,%20T.R.%20Besold,%20What%20does%20explainable%20AI%20really%20mean%20a%20new%20conceptualization%20of%20perspectives,%202017.&quot;&gt;[24]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar?q=F.%20Doshi-Velez,%20B.%20Kim,%20Towards%20a%20rigorous%20science%20of%20interpretable%20machine%20learning,%202017.&quot;&gt;[25]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Making%20machine%20learning%20models%20interpretable.&amp;amp;publication_year=2012&amp;amp;author=A.%20Vellido&amp;amp;author=J.D.%20Mart%C3%ADn-Guerrero&amp;amp;author=P.J.%20Lisboa&quot;&gt;[26]&lt;/a&gt;. 언급한 주제들은 D. Gunning의 정의에 대한 불완전성을 뒷받침하는 사례로 여기 언급하면서, 나중에 이 주제들을 파헤칠 것이다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;As exemplified by the definition above, a thorough, complete definition of explainability in AI still slips from our fingers. A broader reformulation of this definition (e.g. 'An explainable Artificial Intelligence is one that produces explanations about its functioning') would fail to fully characterize the term in question, leaving aside important aspects such as its purpose. To build upon the completeness, a definition of explanation is first required.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 위 예시와 같이, 설명가능성에 대한 완벽한 정의를 내리기 어렵다. 예를 들어, “설명 가능한 인공지능은 그에 대한 기능을 설명하는 것이다” 경우, 설명가능성의 목적성 측면만 이야기한다. 따라서, 먼저 설명(explanation)에 대한 정의가 필요하다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;As extracted from the Cambridge Dictionary of English Language, an explanation is 'the details or reasons that someone gives to make something clear or easy to understand' [27]. In the context of an ML model, this can be rephrased as: 'the details or reasons a model gives to make its functioning clear or easy to understand'. It is at this point where opinions start to diverge. Inherently stemming from the previous definitions, two ambiguities can be pointed out. First, the details or the reasons used to explain, are completely dependent of the audience to which they are presented. Second, whether the explanation has left the concept clear or easy to understand also depends completely on the audience. Therefore, the definition must be rephrased to reflect explicitly the dependence of the explainability of the model on the audience. To this end, a reworked definition could read as: 'Given a certain audience, explainability refers to the details and reasons a model gives to make its functioning clear or easy to understand.' &lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 케임브릿지 영어사전을 인용하면, 설명(explanation)은 “어떤 것을 명백하게 혹은 이해하기 쉽게 만들어주기 위해 누군가가 밝혀주는 세부 사항이나 이유”다&lt;a href=&quot;https://scholar.google.com/scholar?q=Cambridge%20advanced%20learners%20dictionary&quot;&gt;[27]&lt;/a&gt;. &lt;span style=&quot;color:#aaa&quot;&gt;([주] 네이버 국어사전의 경우, “어떤 일이나 대상의 내용을 상대편이 잘 알 수 있도록 밝혀 말함. 또는 그런 말.”) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Machine learning의 맥락 상, “모델이 자신의 기능을 명백히 하거나 이해하기 쉽게 세부사항 혹은 이유를 밝히는 것”으로 바꿀수 있다. 여기에서 의견이 갈리기 시작한다. 본질적으로 이전 정의에서 비롯 된것으로 두 가지 애매모호한 점이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;설명에 관한 세부 사항이나 이유는 이를 듣는 청중(audience)과 가장 연관이 있다.&lt;/li&gt;
  &lt;li&gt;설명이 명료하게 혹은 알기 쉽게 되었는지의 여부도 완전히 청중에게 달려있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서, 두 가지를 반영해 다시 정의하면 다음과 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;“특정 청중에게 모델이 자신의 기능을 명백하게 혹은 이해하기 쉽게 밝히는 세부사항/이유를 설명가능성이라고 말한다.”&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Since explaining, as argumenting, may involve weighting, comparing or convincing an audience with logic-based formalizations of (counter) arguments [28], explainability might convey us into the realm of cognitive psychology and the psychology of explanations [7], since measuring whether something has been understood or put clearly is a hard task to be gauged objectively. However, measuring to which extent the internals of a model can be explained could be tackled objectively. Any means to reduce the complexity of the model or to simplify its outputs should be considered as an XAI approach. How big this leap is in terms of complexity or simplicity will correspond to how explainable the resulting model is. An underlying problem that remains unsolved is that the interpretability gain provided by such XAI approaches may not be straightforward to quantify: for instance, a model simplification can be evaluated based on the reduction of the number of architectural elements or number of parameters of the model itself (as often made, for instance, for DNNs). On the contrary, the use of visualization methods or natural language for the same purpose does not favor a clear quantification of the improvements gained in terms of interpretability. The derivation of general metrics to assess the quality of XAI approaches remain as an open challenge that should be under the spotlight of the field in forthcoming years. We will further discuss on this research direction in Section 5.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 어떤 것이 분명하게 이해되었는지는 객관적으로 측정하기 어렵기 때문에, 설명가능성은 인지 심리학영역을 끌어들일 수도 있다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:#aaa&quot;&gt;([주] 주관적인 해석이 섞여 있습니다.)&lt;/span&gt; 그러나 모델의 내부가 어느 정도까지 설명될 수 있는지는 객관적으로 측정가능하다. 모델의 복잡성/결과의 단순화 수단들을 XAI 접근법으로 생각할 수 있다. 단순화 정도를 측정하하여, 그 성과를 설명가능성으로 계량하는 것이다. &lt;span style=&quot;color:#aaa&quot;&gt;(왜? … 적절한 예시가 떠오르지 않는다…)&lt;/span&gt; 하지만 해결되지 않은 근본적인 문제는 이러한 접근 방식으로 제공하는 해석가능성이 직관적으로 정량화하기가 쉽지 않을 수 있다.&lt;/p&gt;

&lt;p&gt;예를들어, 모델의 단순화(simplification)는 아키텍쳐를 간소화 하거나 매개변수(parameters) 수를 줄임으로서 달성할 수 있다. 반면, 시각화 벙법들이나 자연어 설명은 해석가능성을 계량하기에 좋은 방법은 아니다.
XAI 방법들의 퀄리티를 평가하기 위한 일반적인 측정지표의 도출은 향후 몇 년간 과제로 남아있다. 이를 Section 5에서 논의 할 것이다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Explainability is linked to post-hoc explainability since it covers the techniques used to convert a non-interpretable model into a explainable one. In the remaining of this manuscript, explainability will be considered as the main design objective, since it represents a broader concept. A model can be explained, but the interpretability of the model is something that comes from the design of the model itself. Bearing these observations in mind, explainable AI can be defined as follows:
&lt;br /&gt;
'Given an audience, an explainable Artificial Intelligence is one that produces details or reasons to make its functioning clear or easy to understand.'
&lt;br /&gt;
This definition is posed here as a first contribution of the present overview, implicitly assumes that the ease of understanding and clarity targeted by XAI techniques for the model at hand reverts on different application purposes, such as a better trustworthiness of the model’s output by the audience.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 설명가능성(explainability)는 해석이 불가능한 모델을 가능케한다는 점에서 사후(post-hoc) 설명성과 연관이 있다. 이 후의 논문에서는 더 넓은 의미인 설명가능성을 주요 목표로 생각할 것이다. 그러나 모델의 해석가능성(interpretability)은 모델 자체 설계에서 비롯된다. 이러한 생각을 염두해두고, explainable AI는 다음과 같이 정의할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;설명 가능한 인공지능(explainable Artificial Intelligence)은 청중에게 자신의 기능을 명백하게 혹은 이해하기 쉬운 세부사항/이유를 생산하는 인공지능을 말한다.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;23-why&quot;&gt;2.3 Why?&lt;/h2&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;As stated in the introduction, explainability is one of the main barriers AI is facing nowadays in regards to its practical implementation. The inability to explain or to fully understand the reasons by which state-of-the-art ML algorithms perform as well as they do, is a problem that find its roots in two different causes, which are conceptually illustrated in Fig. 2.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 도입부에 기술한 바와 같이, 설명가능성은 AI의 실질적 활용에 직면하고 있는 주요 장벽 중 하나이다. 최첨단 ML 알고리즘이 잘 작동하는 이유를 설명하지 못하거나 완전히 이해할 수 없는 것은 두 가지 다른 원인에 그 뿌리를 찾는 문제이며, 이는 개념적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fig 2&lt;/code&gt;에 나타나 있다. &lt;span style=&quot;color:#aaa&quot;&gt;([주] 그 원인은 청중이 누구인가에 따라서 ML 알고리즘의 필요한 설명이 다르기 때문이다)&lt;/span&gt;&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1KcJ-gbJw7a8xSghhYs5uM3eefZ9h2vo7&quot; alt=&quot;Fig 2. 각기 다른 청중에 따라 달라지는 설명가능성의 목적&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Fig 2. 각기 다른 청중에 따라 달라지는 설명가능성의 목적&lt;/figcaption&gt;&lt;/figure&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Without a doubt, the first cause is the gap between the research community and business sectors, impeding the full penetration of the newest ML models in sectors that have traditionally lagged behind in the digital transformation of their processes, such as banking, finances, security and health, among many others. In general this issue occurs in strictly regulated sectors with some reluctance to implement techniques that may put at risk their assets.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 의심할 여지 없이, 첫 번째 원인은 연구 커뮤니티와 사업 부문 사이의 격차로 인해 은행, 금융, 보안, 건강 등 전통적으로 프로세스의 디지털 전환에서 뒤처진 분야에서 최신 ML 모델의 완전한 보급에 장애가 되고 있다. 일반적으로 이 문제는 엄격하게 규제되는 부문에서 발생하며 자산의 위험을 초래할 수 있는 기법의 시행을 일부 꺼린다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;The second axis is that of knowledge. AI has helped research across the world with the task of inferring relations that were far beyond the human cognitive reach. Every field dealing with huge amounts of reliable data has largely benefited from the adoption of AI and ML techniques. However, we are entering an era in which results and performance metrics are the only interest shown up in research studies. Although for certain disciplines this might be the fair case, science and society are far from being concerned just by performance. The search for understanding is what opens the door for further model improvement and its practical utility.
&lt;br /&gt;
The following section develops these ideas further by analyzing the goals motivating the search for explainable AI models.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 두번째는 지식을 추구하는 측면이다. AI는 인간의 인지 범위를 벗어난 관계를 추론하는 연구를 도왔다. 막대한 양의 데이터를 다루는 모든 분야는 AI와 ML 기술을 도입함으로서 큰 혜택을 입었다. 그러나 연구 성과 지표에만 관심을 가지는 시대가 접어들 면서 이는 문제가 된다. 성과 지표로만 과학과 사회를 이야기 하기에는 올바르지 않기 때문이다. 이해를 연구한다는 것은 모델을 개선시키고 그 유용성을 증진시키는 일이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;24-what-for&quot;&gt;2.4 What for?&lt;/h2&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;The research activity around XAI has so far exposed different goals to draw from the achievement of an explainable model. Almost none of the papers reviewed completely agrees in the goals required to describe what an explainable model should compel. However, all these different goals might help discriminate the purpose for which a given exercise of ML explainability is performed. Unfortunately, scarce contributions have attempted to define such goals from a conceptual perspective [5], [13], [24], [30]. We now synthesize and enumerate definitions for these XAI goals, so as to settle a first classification criteria for the full suit of papers covered in this review:&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; “설명 가능한 모델이 무엇을 강조해야하는가?”라는 목적(혹은 이에대한 합의제시)을 가진 논문은 거의 없었다. 이제부터 분류기준을 정하고, XAI의 목표에 대한 정의를 종합적으로 열거하려고 한다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Trustworthiness: Several authors agree upon the search for trustworthiness as the primary aim of an explainable AI model [31], [32]. However, declaring a model as explainable as per its capabilities of inducing trust might not be fully compliant with the requirement of model explainability. Trustworthiness might be considered as the confidence of whether a model will act as intended when facing a given problem. Although it should most certainly be a property of any explainable model, it does not imply that every trustworthy model can be considered explainable on its own, nor is trustworthiness a property easy to quantify. Trust might be far from being the only purpose of an explainable model since the relation among the two, if agreed upon, is not reciprocal. Part of the reviewed papers mention the concept of trust when stating their purpose for achieving explainability. However, as seen in Table 1, they do not amount to a large share of the recent contributions related to XAI.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;신뢰도(Trustworthiness):&lt;/strong&gt; 몇몇 연구자들은 신뢰도를 설명 가능한 모델의 우선적 목표를 둬야한다고 주장한다(&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=iBCM%3A%20Interactive%20Bayesian%20case%20model%20empowering%20humans%20via%20intuitive%20interaction&amp;amp;publication_year=2015&amp;amp;author=B.%20Kim&amp;amp;author=E.%20Glassman&amp;amp;author=B.%20Johnson&amp;amp;author=J.%20Shah&quot;&gt;[31]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar?q=Why%20should%20I%20trust%20you:%20Explaining%20the%20predictions%20of%20any%20classifier&quot;&gt;[32]&lt;/a&gt;). 그러나 이러한 주장은 모델 설명성의 요구조건을 완전히 충족하지 못한다. 신뢰도는 모델이 직면한 어떤 문제에서 설계 의도된 바로 행동하는 것으로 간주 할 수 있다. 신뢰도는 설명 가능한 모델의 속성이 되어야 하지만, 모든 신뢰성있는 모델이 설명 가능하지는 않으며, 이 특성을 계량하기 쉽지도 않다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;표1&lt;/code&gt;에서도 확인 할 수 있지만 최근의 연구기여들 중에서 큰 비중을 차지 하지 않는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Causality: Another common goal for explainability is that of finding causality among data variables. Several authors argue that explainable models might ease the task of finding relationships that, should they occur, could be tested further for a stronger causal link between the involved variables [159], [160]. The inference of causal relationships from observational data is a field that has been broadly studied over time [161]. As widely acknowledged by the community working on this topic, causality requires a wide frame of prior knowledge to prove that observed effects are causal. A ML model only discovers correlations among the data it learns from, and therefore might not suffice for unveiling a cause-effect relationship. However, causation involves correlation, so an explainable ML model could validate the results provided by causality inference techniques, or provide a first intuition of possible causal relationships within the available data. Again, Table 1 reveals that causality is not among the most important goals if we attend to the amount of papers that state it explicitly as their goal.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;인과성(Causality):&lt;/strong&gt; 설명가능성의 다른 목표로는 변수들 간의 인과성을 찾는 것이다. 몇몇 저자들은 이 과정을 용이하게 할 수 있다고 주장한다(&lt;a href=&quot;https://scholar.google.com/scholar?q=Smoking%20and%20the%20occurence%20of%20alzheimers%20disease:%20Cross-sectional%20and%20longitudinal%20data%20in%20a%20population-based%20study&quot;&gt;[159]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar?q=An%20empirical%20study%20of%20machine%20learning%20techniques%20for%20affect%20recognition%20in%20humanrobot%20interaction&quot;&gt;[160]&lt;/a&gt;). 인과 관계의 추론은 상당히 오랜시간 연구되었다(&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Causality&amp;amp;publication_year=2009&amp;amp;author=J.%20Pearl&quot;&gt;[161]&lt;/a&gt;). 우리가 관찰한 영향(effects)이 인과성이 있다는 것을 증명하기 위해서, 인과 관계 분야는 광범위한 사전 지식의 프레임을 필요로 한다. 머신러닝 모델은 데이터의 상관 관계를 찾기만 하지, 인과 관계를 충분하게 밝히지는 않는다. 그러나, 인과성은 상관성을 포함하기 때문에, 다양한 기법을 이용해 설명 가능한 머신러닝 모델이 결과에 대해서 검증하거나, 인과관계를 찾아볼 수는 있다. 하지만 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;표1&lt;/code&gt;에서 볼 수 있듯이, 논문의 양을 기준으로 한다면, 메인 목표는 아직 아니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Transferability: Models are always bounded by constraints that should allow for their seamless transferability. This is the main reason why a training-testing approach is used when dealing with ML problems [162], [163]. Explainability is also an advocate for transferability, since it may ease the task of elucidating the boundaries that might affect a model, allowing for a better understanding and implementation. Similarly, the mere understanding of the inner relations taking place within a model facilitates the ability of a user to reuse this knowledge in another problem. There are cases in which the lack of a proper understanding of the model might drive the user toward incorrect assumptions and fatal consequences [44], [164]. Transferability should also fall between the resulting properties of an explainable model, but again, not every transferable model should be considered as explainable. As observed in Table 1, the amount of papers stating that the ability of rendering a model explainable is to better understand the concepts needed to reuse it or to improve its performance is the second most used reason for pursuing model explainability.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;전이가능성(Transferability):&lt;/strong&gt; 모델은 원활한 전이가능성을 가지기 위해서 일반화가 잘 되어야 한다. 그래서 training-testing 방법을 사용해서 훈련하는 것이다(&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Applied%20predictive%20modeling&amp;amp;publication_year=2013&amp;amp;author=M.%20Kuhn&amp;amp;author=K.%20Johnson&quot;&gt;[162]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=An%20introduction%20to%20statistical%20learning&amp;amp;publication_year=2013&amp;amp;author=G.%20James&amp;amp;author=D.%20Witten&amp;amp;author=T.%20Hastie&amp;amp;author=R.%20Tibshirani&quot;&gt;[163]&lt;/a&gt;). 설명가능성도 전이가능성이 필요한데, 모델에 영향을 미칠 수 있는 경계를 확장 하면서 더 나은 이해와 구현을 가능하게 하기 때문이고, 내부관계의 이해를 바탕으로 사용자가 다른 문제에서 이 지식을 재사용 할 수 있기 때문이다. 다만, 모델에 대한 이해가 부족하여 잘못된 가정과 치명적인 결과를 초래할 경우도 있다(&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Intelligible%20models%20for%20healthcare%3A%20Predicting%20pneumonia%20risk%20and%20hospital%2030-day%20readmission&amp;amp;publication_year=2015&amp;amp;author=R.%20Caruana&amp;amp;author=Y.%20Lou&amp;amp;author=J.%20Gehrke&amp;amp;author=P.%20Koch&amp;amp;author=M.%20Sturm&amp;amp;author=N.%20Elhadad&quot;&gt;[44]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar?q=C.%20Szegedy,%20W.%20Zaremba,%20I.%20Sutskever,%20J.%20Bruna,%20D.%20Erhan,%20I.%20Goodfellow,%20R.%20Fergus,%20Intriguing%20properties%20of%20neural%20networks,%202013.&quot;&gt;[164]&lt;/a&gt;). 또한, 전이가능성은 설명 가능한 모델의 특성이지만, 모든 전이가능한 모델이 설명가능하지는 않다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;표1&lt;/code&gt;에서 볼수 있듯이, 모델의 전이가능성은 (연구기여의 양적으로 따졌을 때) 모델의 설명성을 추구하는 2번째 이유가 되며, 설명가능성은 모델을 재사용 하기 위해 필요한 개념을 더 잘 이해하고, 모델 성능을 향상시키기 위해 사용될 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Informativeness: ML models are used with the ultimate intention of supporting decision making [92]. However, it should not be forgotten that the problem being solved by the model is not equal to that being faced by its human counterpart. Hence, a great deal of information is needed in order to be able to relate the user's decision to the solution given by the model, and to avoid falling in misconception pitfalls. For this purpose, explainable ML models should give information about the problem being tackled. Most of the reasons found among the papers reviewed is that of extracting information about the inner relations of a model. Almost all rule extraction techniques substantiate their approach on the search for a simpler understanding of what the model internally does, stating that the knowledge (information) can be expressed in these simpler proxies that they consider explaining the antecedent. This is the most used argument found among the reviewed papers to back up what they expect from reaching explainable models.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;정보성(Informativeness):&lt;/strong&gt; 머신러닝 모델의 궁극적인 목적은 의사 결정의 지원이다(&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=An%20empirical%20evaluation%20of%20the%20comprehensibility%20of%20decision%20table%2C%20tree%20and%20rule%20based%20predictive%20models&amp;amp;publication_year=2011&amp;amp;author=J.%20Huysmans&amp;amp;author=K.%20Dejaeger&amp;amp;author=C.%20Mues&amp;amp;author=J.%20Vanthienen&amp;amp;author=B.%20Baesens&quot;&gt;[92]&lt;/a&gt;). 그러나 모델이 풀고 있는 문제는 인간이 직면하고 있는 문제와 항상 같은 것은 아니다. ([주] 아직까지 모델은 더 단순한 문제를 해결하고 있기 때문, 아직 복합적인 정보를 결합하여 문제를 해결하지는 못한다.) 따라서 사용자의 의사결정에 모델이 내놓은 솔루션을 오해하지 않게 잘 연관 시키려면 더 많은 양의 정보가 필요하다. 이를 위해, 설명 가능한 모델은 문제에 태클이 될 만한 정보를 더 많이 제공해야한다. 대부분 논문을 살펴본 결과, 그 이유는 모델의 내부 관계에서 정보를 추출하기 위함이었다. 대부분의 규칙기반 기술을 사용하려는 사람들은 모델 행동의 이해를 더 간단하게 만들기 위해 방법론을 연구하고 있었다. 그들은 모델의 지식이나 정보를 더 간단하게 대체할 수 있을 것이라 주장했다. 이는 설명 가능한 모델에서 가장 많은 기대를 하는 특성이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Confidence: As a generalization of robustness and stability, confidence should always be assessed on a model in which reliability is expected. The methods to maintain confidence under control are different depending on the model. As stated in [165], [166], [167], stability is a must-have when drawing interpretations from a certain model. Trustworthy interpretations should not be produced by models that are not stable. Hence, an explainable model should contain information about the confidence of its working regime.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;확신(Confidence):&lt;/strong&gt; 건전성(robustness)와 안전성(stability)의 일반화로서, 신뢰성이 기대되는 모델은 그 확신의 정도를 가늠할 수 있어야 한다. 확신을 유지하는 방법은 모델에 따라서 다르다. &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Robust%20statistics%3A%20The%20approach%20based%20on%20influence%20functions&amp;amp;publication_year=1987&amp;amp;author=D.%20Ruppert&quot;&gt;[165]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Iterative%20random%20forests%20to%20discover%20predictive%20and%20stable%20high-order%20interactions&amp;amp;publication_year=2018&amp;amp;author=S.%20Basu&amp;amp;author=K.%20Kumbier&amp;amp;author=J.B.%20Brown&amp;amp;author=B.%20Yu&quot;&gt;[166]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Stability&amp;amp;publication_year=2013&amp;amp;author=B.%20Yu&quot;&gt;[167]&lt;/a&gt;에서 기술한 바와 같이, 안정성은 어떤 모델에서 해석을 도출하기 위해서 꼭 필요한 특성이다. ([주] 일단 모델이 일반화가 잘 되어 있어야 그 해석 또한 안정적으로 도출 할 수 있다.)&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Fairness: From a social standpoint, explainability can be considered as the capacity to reach and guarantee fairness in ML models. In a certain literature strand, an explainable ML model suggests a clear visualization of the relations affecting a result, allowing for a fairness or ethical analysis of the model at hand [3], [100]. Likewise, a related objective of XAI is highlighting bias in the data a model was exposed to [168], [169]. The support of algorithms and models is growing fast in fields that involve human lives, hence explainability should be considered as a bridge to avoid the unfair or unethical use of algorithm’s outputs.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;공정성(Fairness):&lt;/strong&gt; 사회적 관점에서 설명가능성은 머신러닝의 공정성을 보장하는 능력으로 볼 수 있다. 특정 문헌에서 설명 가능한 모델은 명백한 결과의 관계 시각화를 통해 공정성 혹은 윤리적 분석을 가능하게 한다(&lt;a href=&quot;https://scholar.google.com/scholar?q=European%20union%20regulations%20on%20algorithmic%20decision-making%20and%20a%20right%20to%20explanation&quot;&gt;[3]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments&amp;amp;publication_year=2017&amp;amp;author=A.%20Chouldechova&quot;&gt;[100]&lt;/a&gt;). 이와 관련된 목표로는 &lt;a href=&quot;https://scholar.google.com/scholar?q=K.%20Burns,%20L.A.%20Hendricks,%20K.%20Saenko,%20T.%20Darrell,%20A.%20Rohrbach,%20Women%20also%20Snowboard:%20Overcoming%20Bias%20in%20Captioning%20Models,%202018.&quot;&gt;[168]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Towards%20explainable%20neural-symbolic%20visual%20reasoning&amp;amp;publication_year=2019&amp;amp;author=A.%20Bennetot&amp;amp;author=J.-L.%20Laurent&amp;amp;author=R.%20Chatila&amp;amp;author=N.%20D%C3%ADaz-Rodr%C3%ADguez&quot;&gt;[169]&lt;/a&gt;에서 보여준 바와 같이, 모델이 학습한 데이터의 편향을 강조하는 것이다. 사람들의 삶에서 머신러닝 모델(알고리즘)은 앞으로 계속 빠르게 노출될 수 밖에 없기에, 설명가능성은 이러한 불공평과 비윤리적인 알고리즘의 산출물을 피할 수 있게 하는 가교역할이 되어야 한다. ([주] 최근 &lt;a href=&quot;https://blog.pingpong.us/luda-issue-faq/?fbclid=IwAR15-eiWeIPSnv8lT0WXlO07HBpP0aJaoN36vThaGDmIaBeZpU6jiIy_oJw&quot;&gt;이루다&lt;/a&gt; 일은 신뢰도에도 속하지만, 이 범주에도 속한다고 할 수 있겠다.)&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Accessibility: A minor subset of the reviewed contributions argues for explainability as the property that allows end users to get more involved in the process of improving and developing a certain ML model [37], [86]. It seems clear that explainable models will ease the burden felt by non-technical or non-expert users when having to deal with algorithms that seem incomprehensible at first sight. This concept is expressed as the third most considered goal among the surveyed literature.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;접근성(Accessibility):&lt;/strong&gt; 일부 연구자들은 설명가능성은 최종 사용자가 특정 모델을 개발하고 개선하는 프로세스에 더 많이 관여할 수 있는 속성이라고 주장한다(&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Working%20with%20beliefs%3A%20AI%20transparency%20in%20the%20enterprise.&amp;amp;publication_year=2018&amp;amp;author=A.%20Chander&amp;amp;author=R.%20Srinivasan&amp;amp;author=S.%20Chelian&amp;amp;author=J.%20Wang&amp;amp;author=K.%20Uchino&quot;&gt;[38]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Explainable%20AI%3A%20Beware%20of%20inmates%20running%20the%20asylum&amp;amp;publication_year=2017&amp;amp;author=T.%20Miller&amp;amp;author=P.%20Howe&amp;amp;author=L.%20Sonenberg&quot;&gt;[86]&lt;/a&gt;). 설명 가능한 모델은 비전문가 사용자에게 처음 보는 알 수 없는 알고리즘에 대한 부담을 덜어 줄 수 있어보인다. 이는 이번 조사에서 3번째로 많이 고려된 목표다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Interactivity: Some contributions [50], [59] include the ability of a model to be interactive with the user as one of the goals targeted by an explainable ML model. Once again, this goal is related to fields in which the end users are of great importance, and their ability to tweak and interact with the models is what ensures success.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;상호작용(Interactivity):&lt;/strong&gt; 특정 논문([50], [59])에서는 사용자와 모델이 상호작용하는 능력을 설명 가능한 모델의 목표로 잡았다. 이는 최종 사용자와 더 관련이 있으며, 이들의 모델을 수정하고 상호작용하는 능력이 목표달성의 성공을 보장한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Privacy awareness: Almost forgotten in the reviewed literature, one of the byproducts enabled by explainability in ML models is its ability to assess privacy. ML models may have complex representations of their learned patterns. Not being able to understand what has been captured by the model [4] and stored in its internal representation may entail a privacy breach. Contrarily, the ability to explain the inner relations of a trained model by non-authorized third parties may also compromise the differential privacy of the data origin. Due to its criticality in sectors where XAI is foreseen to play a crucial role, confidentiality and privacy issues will be covered further in Sections 5.4 and 6.3, respectively.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt;&lt;strong&gt;프라이버시 인식(Privacy awareness):&lt;/strong&gt; 대부분의 논문에서 언급되지 않았지만 설명 가능한 모델의 부산물 중에 하나는 프라이버시를 평가하는 능력이다. 머신러닝에서 학습한 패턴은 복잡한 표현(representations)([주] 여기서 “표현”이란 데이터 혹은 패턴을 압축하여 나타내는 어떤 상태다.)으로 나타낼 수 있다. 모델 내부에 어떤 것이 포착되어 있는지 알 수 없다면, 개인정보의 침해가 일어날 수 있다. 반대로, 비인가 제3자에 의해 훈련된 모델의 내부를 설명할 수 있다면, 그것 또한 다른 의미로써 데이터 출처에 대한 프라이버시 침해라고 할 수 있다. 사안의 중요성 때문에, 이 기밀성(confidentiality)와 개인정보 문제는 section 5.4와 6.3에서 더 다룰 예정이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;XAI Goal&lt;/th&gt;
      &lt;th&gt;Main target audience (Fig. 2)&lt;/th&gt;
      &lt;th&gt;References&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Trustworthiness&lt;/td&gt;
      &lt;td&gt;Domain experts, users of the model affected by decisions&lt;/td&gt;
      &lt;td&gt;[5], [10], [24], [32], [33], [34], [35], [36], [37]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Causality&lt;/td&gt;
      &lt;td&gt;Domain experts, managers and executive board members, regulatory entities/agencies&lt;/td&gt;
      &lt;td&gt;[35], [38], [39], [40], [41], [42], [43]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Transferability&lt;/td&gt;
      &lt;td&gt;Domain experts, data scientists&lt;/td&gt;
      &lt;td&gt;[5], [21], [26], [30], [32], [37], [38], [39], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Informativeness&lt;/td&gt;
      &lt;td&gt;All&lt;/td&gt;
      &lt;td&gt;[5], [21], [25], [26], [30], [32], [34], [35], [37], [38], [41], [44], [45], [46], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [59], [60], [63], [64], [65], [66], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Confidence&lt;/td&gt;
      &lt;td&gt;Domain experts, developers, managers, regulatory entities/agencies&lt;/td&gt;
      &lt;td&gt;[5], [35], [45], [46], [48], [54], [61], [72], [88], [89], [96], [108], [117], [119], [155]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Fairness&lt;/td&gt;
      &lt;td&gt;Users affected by model decisions, regulatory entities/agencies&lt;/td&gt;
      &lt;td&gt;[5], [24], [35], [45], [47], [99], [100], [101], [120], [121], [128], [156], [157], [158]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Accessibility&lt;/td&gt;
      &lt;td&gt;Product owners, managers, users affected by model decisions&lt;/td&gt;
      &lt;td&gt;[21], [26], [30], [32], [37], [50], [53], [55], [62], [67], [68], [69], [70], [71], [74], [75], [76], [86], [93], [94], [103], [105], [107], [108], [111], [112], [113], [114], [115], [124], [129]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Interactivity&lt;/td&gt;
      &lt;td&gt;Domain experts, users affected by model decisions&lt;/td&gt;
      &lt;td&gt;[37], [50], [59], [65], [67], [74], [86], [124]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Privacy awareness&lt;/td&gt;
      &lt;td&gt;Users affected by model decisions, regulatory entities/agencies&lt;/td&gt;
      &lt;td&gt;[89]&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;표 1&lt;/code&gt; 설명가능성에 도달하기 위해 검토된 문헌에서 추구된 목표들과 그들의 주요 목표 청중들.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;25-how&quot;&gt;2.5 How?&lt;/h2&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;The literature makes a clear distinction among models that are interpretable by design, and those that can be explained by means of external XAI techniques. This duality could also be regarded as the difference between interpretable models and model interpretability techniques; a more widely accepted classification is that of transparent models and post-hoc explainability. This same duality also appears in the paper presented in [17] in which the distinction its authors make refers to the methods to solve the transparent box design problem against the problem of explaining the black-box problem. This work, further extends the distinction made among transparent models including the different levels of transparency considered.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 본 논문은 해석 가능한 모델을 &lt;strong&gt;해석 가능한 모델&lt;/strong&gt;(구조적으로 설명 가능한 부분)과 &lt;strong&gt;해석 가능한 기법&lt;/strong&gt;(외부 XAI 기법에 의해 설명될 수 있는 부분)으로 나눈다. 현재 보다 널리 받아들여지고 있는 분류법 용어는 투명한 모델(transparent models)과 사후 설명가능성(post-hoc explainability)이다. [17]에서도 언급하는데, 블랙박스 문제를 설명하려면 투명한 모델의 디자인 문제를 참고해야한다. 본 논문에서는 투명한 모델을 다양한 단계로 세분화해서 그 차이를 알아 볼 것이다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Within transparency, three levels are contemplated: algorithmic transparency, decomposability and simulatability Among post-hoc techniques we may distinguish among text explanations, visualizations, local explanations, explanations by example, explanations by simplification and feature relevance. In this context, there is a broader distinction proposed by [24] discerning between 1) opaque systems, where the mappings from input to output are invisible to the user; 2) interpretable systems, in which users can mathematically analyze the mappings; and 3) comprehensible systems, in which the models should output symbols or rules along with their specific output to aid in the understanding process of the rationale behind the mappings being made. This last classification criterion could be considered included within the one proposed earlier, hence this paper will attempt at following the more specific one.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 투명성(transparency)은 알고리즘 투명성(algorithmic transparency), 분해가능성(decomposability) 그리고 시뮬레이션성(simulatability) &lt;span style=&quot;color:#aaa&quot;&gt;([주] 시스템 혹은 프로세스의 시뮬레이션 능력, the capacity of a system or process to be simulated, 아직 어떤 느낌이 안온다.)&lt;/span&gt;순으로 3 가지 단계를 고려해야한다. 사후 분석(post-hoc) 기법은 텍스트 설명, 시각화, 국지적 설명, 예시 설명, 단순화 및 피처중요도등 방법과 구별되어야 한다. 이러한 맥락에서 더 광범위한 구별법이 &lt;a href=&quot;https://scholar.google.com/scholar?q=D.%20Doran,%20S.%20Schulz,%20T.R.%20Besold,%20What%20does%20explainable%20AI%20really%20mean%20a%20new%20conceptualization%20of%20perspectives,%202017.&quot;&gt;[24]&lt;/a&gt;에서 제시 되었다. 1) 입력에서 출력까지의 매핑이 사용자에게 보이지 않는 불투명한(opaque) 시스템 2) 사용자가 수학적으로 매핑을 분석할 수 있는 해석 가능한(interpretable) 시스템 3) 모델이 결정한 매핑의 이유를 사람이 이해 가능하게 출력하는 포괄적 이해 가능한(comprehensible) 시스템&lt;/p&gt;

&lt;h3 id=&quot;251-levels-of-transparency-in-machine-learning-models&quot;&gt;2.5.1. Levels of transparency in machine learning models&lt;/h3&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Transparent models convey some degree of interpretability by themselves. Models belonging to this category can be also approached in terms of the domain in which they are interpretable, namely, algorithmic transparency, decomposability and simulatability. As we elaborate next in connection to Fig. 3, each of these classes contains its predecessors, e.g. a simulatable model is at the same time a model that is decomposable and algorithmically transparent:&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 투명한 모델은 모델 그 자체러 어느 정도의 해석가능성을 가지고 있다. 위에 언급한 대로 알고리즘 투명성, 분해가능성 그리고 시뮬레이션성 순으로 접근 할 수 있다. Fig 3에서 설명하겠지만, 각 분류는 이전 단계의 구성을 포함한다.. 예를 들어, 시뮬레이션 가능한 모델은 분해 가능하며, 투명한 알고리즘을 포함한다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Simulatability denotes the ability of a model of being simulated or thought about strictly by a human, hence complexity takes a dominant place in this class. This being said, simple but extensive (i.e., with too large amount of rules) rule based systems fall out of this characteristic, whereas a single perceptron neural network falls within. This aspect aligns with the claim that sparse linear models are more interpretable than dense ones [170], and that an interpretable model is one that can be easily presented to a human by means of text and visualizations [32]. Again, endowing a decomposable model with simulatability requires that the model has to be self-contained enough for a human to think and reason about it as a whole.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;시뮬레이션성(Simulatability):&lt;/strong&gt; 사람에 의해 엄격하게 시뮬레이션된 모델의 능력이다. 따라서 복잡성이 가장 중요하다. 단순하지만 광범위한 규칙기반 시스템 보다는 퍼셉트론 신경망이 기준에 더 부합한다. 이러한 관점에서 sparse한 선형모델이 dense 한 것보다 더 해석가능성이 높으며 &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Regression%20shrinkage%20and%20selection%20via%20the%20lasso&amp;amp;publication_year=1996&amp;amp;author=R.%20Tibshirani&quot;&gt;[170]&lt;/a&gt;, 설명 가능한 모델의 텍스트와 시각화를 통해 인간이 더 쉽게 설명 할 수 있다라는 주장&lt;a href=&quot;https://scholar.google.com/scholar?q=Why%20should%20I%20trust%20you:%20Explaining%20the%20predictions%20of%20any%20classifier&quot;&gt;[32]&lt;/a&gt;과 일치한다. 다시 말하지만 시뮬레이션성을 가지는 분해 가능한 모델은 인간에게 생각과 이유를 혼자 설명할 수 있어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Decomposability stands for the ability to explain each of the parts of a model (input, parameter and calculation). It can be considered as intelligibility as stated in [171]. This characteristic might empower the ability to understand, interpret or explain the behavior of a model. However, as occurs with algorithmic transparency, not every model can fulfill this property. Decomposability requires every input to be readily interpretable (e.g. cumbersome features will not fit the premise). The added constraint for an algorithmically transparent model to become decomposable is that every part of the model must be understandable by a human without the need for additional tools.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;분해가능성(Decomposability):&lt;/strong&gt; 모델의 각 부분을 설명하는 능력인데, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Intelligible%20models%20for%20classification%20and%20regression&amp;amp;publication_year=2012&amp;amp;author=Y.%20Lou&amp;amp;author=R.%20Caruana&amp;amp;author=J.%20Gehrke&quot;&gt;[171]&lt;/a&gt;에서 언급된 명료성(intelligibility)으로 볼 수 있다. 이 특성은 모델의 행동을 이해하고, 해석하거나 설명하는 능력을 강조한다. 그러나 모든 모델이 이 특성이 있는 것은 아니다. 분해가능성은 모든 입력의 쉽게 해석할 수 있어야 하는데, 알고리즘적 투명성까지 만족하기 위해서는 인간이 모델의 모든 부분에서 추가해석 없이 이해할 수 있어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Algorithmic transparency can be seen in different ways. It deals with the ability of the user to understand the process followed by the model to produce any given output from its input data. Put it differently, a linear model is deemed transparent because its error surface can be understood and reasoned about, allowing the user to understand how the model will act in every situation it may face [163]. Contrarily, it is not possible to understand it in deep architectures as the loss landscape might be opaque [172], [173] since it cannot be fully observed and the solution has to be approximated through heuristic optimization (e.g. through stochastic gradient descent). The main constraint for algorithmically transparent models is that the model has to be fully explorable by means of mathematical analysis and methods.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;알고리즘적 투명성(Algorithmic transparency):&lt;/strong&gt; 사용자가 모델에 대한 프로세스(모델이 도출한 입력 데이터에 대한 출력) 이해 능력을 나타낸다. 선형모델은 사용자가 모델이 어떻게 행동할 지 예측할 수 있고, error surface를 이해하고 설명할 수 있기 때문에 투명하다고 볼 수 있다&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=An%20introduction%20to%20statistical%20learning&amp;amp;publication_year=2013&amp;amp;author=G.%20James&amp;amp;author=D.%20Witten&amp;amp;author=T.%20Hastie&amp;amp;author=R.%20Tibshirani&quot;&gt;[163]&lt;/a&gt;. 반대로, 깊은 모델 구조를 가지는 모델은 손실값이 불투명하여(&lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Deep%20learning%20without%20poor%20local%20minima&amp;amp;publication_year=2016&amp;amp;author=K.%20Kawaguchi&quot;&gt;[172]&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/scholar_lookup?title=Algorithmic%20transparency%20via%20quantitative%20input%20influence%3A%20Theory%20and%20experiments%20with%20learning%20systems&amp;amp;publication_year=2016&amp;amp;author=A.%20Datta&amp;amp;author=S.%20Sen&amp;amp;author=Y.%20Zick&quot;&gt;[173]&lt;/a&gt;), 특정 휴리스틱한 최적화(예, stochastic gradient descent)를 통해서 근사치를 구해야한다. &lt;span style=&quot;color:#aaa&quot;&gt;([주] 수학적으로 명쾌한 solution이 안보이면 불투명하다고 보는 것 같다. 특히 비선형함수의 손실값)&lt;/span&gt; 알고리즘적 투명성의 주된 제약 조건은 모델이 수학적 분석과 방법을 통해 완전이 탐구 가능해야 한다는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1iZXqf9hnwcu-N9If2_R4WWF5RinPIhPX&quot; alt=&quot;Fig 3. 다양한 단계의 투명성&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Fig 3. 다양한 단계의 투명성&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 다양한 단계의 투명성을 설명해두었다. 머신러닝 모델은 $M_{\varphi}$, 그에 해당하는 파라미터는 $\varphi$ 로 표기했다. (a) 시뮬레이션성 (b) 분해가능성 (c) 알고리즘적 투명성. 각 예제는 일반적인 손실값 가정을 고려하지 않은 선에서, 모델이 설명 대상에 따라서 얼마나 달라지는지를 보여준다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:#aaa&quot;&gt; [주] 이 파트가 제일 이해하기 어려웠는데, (a) 같은 경우 입력 데이터는 잘 몰라도, 내부의 구조를 그대로 재현 가능하면 만족하는 것 같고, (b) 의 경우 사람이 각 입력 피처까지 이해할 수 있어야 한다. (c)의 경우 사람이 전체 데이터의 특성까지 파악하고 이에 대한 규칙을 이해해야 한다라고 이해했다. &lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;252-post-hoc-explainability-techniques-for-machine-learning-models&quot;&gt;2.5.2. Post-hoc explainability techniques for machine learning models&lt;/h3&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Post-hoc explainability targets models that are not readily interpretable by design by resorting to diverse means to enhance their interpretability, such as text explanations, visual explanations, local explanations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves.
&lt;br /&gt;
Further along this river, actual techniques, or better put, actual group of techniques are specified to ease the future work of any researcher that intends to look up for an specific technique that suits its knowledge. Not ending there, the classification also includes the type of data in which the techniques has been applied. Note that many techniques might be suitable for many different types of data, although the categorization only considers the type used by the authors that proposed such technique. Overall, post-hoc explainability techniques are divided first by the intention of the author (explanation technique e.g. Explanation by simplification), then, by the method utilized (actual technique e.g. sensitivity analysis) and finally by the type of data in which it was applied (e.g. images).&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;사후 설명가능성(post-hoc explainability)&lt;/strong&gt;은 쉽게 해석할 수 없는 모델을 위해 고안된 방법이다. 텍스트, 시각화, 부분, 예시, 단순화 그리고 피처 연관 설명 등 방법들이 있다.&lt;/p&gt;

&lt;p&gt;각 방법들에 대해 특정 기술 뿐만 아니라 적용되는 데이터 유형까지 소개한다. 여기서는 인용한 저자들이 적용한 데이터에 의거해 분류를 했지만, 이 중에 어떤 방법들은 다른 분야(데이터)에도 충분히 적용할 수 있다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Text explanations deal with the problem of bringing explainability for a model by means of learning to generate text explanations that help explaining the results from the model [169]. Text explanations also include every method generating symbols that represent the functioning of the model. These symbols may portrait the rationale of the algorithm by means of a semantic mapping from model to symbols.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;텍스트 설명(Text explanation)&lt;/strong&gt;은 모델이 자신의 결과를 설명하는 “텍스트 생성 학습” 문제다. 이 방법은 모델의 기능을 나타내는 심볼을 생성([주] 인간의 언어가 될 수도 있고, 수식일 수도)하는 방식인데, 이 심볼들은 의미론적(semantic)으로 알고리즘의 작동 방식을 매핑한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Visual explanation techniques for post-hoc explainability aim at visualizing the model's behavior. Many of the visualization methods existing in the literature come along with dimensionality reduction techniques that allow for a human interpretable simple visualization. Visualizations may be coupled with other techniques to improve their understanding, and are considered as the most suitable way to introduce complex interactions within the variables involved in the model to users not acquainted to ML modeling.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;시각적 설명(Visual explanation)&lt;/strong&gt;의 목표는 모델의 행동을 시각적으로 설명한는 것이다. 많은 방법들 중에서 대부분 인간이 해석하기 쉽게 차원감소(dimension reduction) 기법과 함께 사용된다. 이 방법은 다른 방법들과 함께 사용되서 이해를 향상시킬 수 있다. 머신러닝 모델링에 익숙하지 않은 사용자에게 모델과 관련된 변수의 복잡한 상호작용을 알리는데 있어 가장 적합한 도구다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Local explanations tackle explainability by segmenting the solution space and giving explanations to less complex solution subspaces that are relevant for the whole model. These explanations can be formed by means of techniques with the differentiating property that these only explain part of the whole system’s functioning.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;부분 설명(Local explanation)&lt;/strong&gt;은 전체모델의 일부분을 쉽게 설명하는데 집중한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Explanations by example consider the extraction of data examples that relate to the result generated by a certain model, enabling to get a better understanding of the model itself. Similarly to how humans behave when attempting to explain a given process, explanations by example are mainly centered in extracting representative examples that grasp the inner relationships and correlations found by the model being analyzed.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;예시 설명(Example explanation)&lt;/strong&gt;은 데이터 샘플에서 결과와 관련된 예제를 추출하는 방법이다. 주로 모델 결과의 내적 관계 혹은 상관관계에 관련된 예제를 추출하게 된다. 이 방법은 사람이 어떤 프로세스를 설명하려고 할 때랑 비슷하게 행동한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Explanations by simplification collectively denote those techniques in which a whole new system is rebuilt based on the trained model to be explained. This new, simplified model usually attempts at optimizing its resemblance to its antecedent functioning, while reducing its complexity, and keeping a similar performance score. An interesting byproduct of this family of post-hoc techniques is that the simplified model is, in general, easier to be implemented due to its reduced complexity with respect to the model it represents.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;단순화 설명(Simplification explanation)&lt;/strong&gt;은 훈련된 모델에 기초하여 설명을 위한 새로운 시스템을 만들어내는 방법이다. 이 새로운 시스템은 복잡성을 최대한 줄이고, 유사한 기능 및 성능을 유지하는 것이 중요하다. 이 방법은 기존에 복잡한 모델에 반해, 더 쉽게 구현할 수 있다는 것이 장점이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Finally, feature relevance explanation methods for post-hoc explainability clarify the inner functioning of a model by computing a relevance score for its managed variables. These scores quantify the affection (sensitivity) a feature has upon the output of the model. A comparison of the scores among different variables unveils the importance granted by the model to each of such variables when producing its output. Feature relevance methods can be thought to be an indirect method to explain a model.&lt;/p&gt;&lt;/details&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;strong&gt;피처 연관 설명(Feature relevance explanation)&lt;/strong&gt;은 모델의 변수와 관련된 점수를 계산하는 방식으로 구현된다. 이 점수는 피처의 영향력(affection 혹은 sensitivity)를 계량화한다. 출력에 대한 점수가 각기 다르기 때문에 각 피처의 중요도를 확인할 수 있다. 이는 모델을 설명하는 간접적인 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;The above classification (portrayed graphically in Fig. 4) will be used when reviewing specific/agnostic XAI techniques for ML models in the following sections (Table 2). For each ML model, a distinction of the propositions to each of these categories is presented in order to pose an overall image of the field’s trends.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 위 분류를 Fig 4로 표현했으며, 표 2에서도 정리했다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1CDEwm6jcM8SKvE3YT21n9HlTTPq2Orol&quot; alt=&quot;Fig 4. 사후 설명가능성 방법에 대한 컨셉 다이어그램&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Fig 4. 사후 설명가능성 방법에 대한 컨셉 다이어그램&lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Transparent ML Models&lt;/th&gt;
      &lt;th&gt;Transparent ML Models&lt;/th&gt;
      &lt;th&gt;Transparent ML Models&lt;/th&gt;
      &lt;th&gt;Post-hoc analysis&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Simulatability&lt;/th&gt;
      &lt;th&gt;Decomposability&lt;/th&gt;
      &lt;th&gt;Algorithmic Transparency&lt;/th&gt;
      &lt;th&gt;Post-hoc analysis&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Linear/Logistic Regression&lt;/td&gt;
      &lt;td&gt;예측 변수는 사람이 판독할 수 있으며 예측 변수 간의 상호 작용은 최소한으로 유지됨&lt;/td&gt;
      &lt;td&gt;변수는 여전히 읽을 수 있지만, 변수와 관련된 상호작용과 예측 변수의 수는 분해를 강요하는 수준으로 증가했다.&lt;/td&gt;
      &lt;td&gt;변수와 교호작용이 너무 복잡하여 수학적 도구가 없으면 분석할 수 없음&lt;/td&gt;
      &lt;td&gt;필요 없음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Decision Trees&lt;/td&gt;
      &lt;td&gt;사람은 어떤 수학적 배경도 요구하지 않고 스스로 의사결정 나무의 예측을 시뮬레이션하고 얻을 수 있다.&lt;/td&gt;
      &lt;td&gt;데이터가 어떻든 모델은 규칙을 전혀 변경하지 않고 가독성을 유지함&lt;/td&gt;
      &lt;td&gt;데이터로부터 학습한 지식을 사람이 이해 할 만한 규칙으로 설명하고, 예측 프로세스를 직관적으로 알 수 있음&lt;/td&gt;
      &lt;td&gt;필요 없음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;K-Nearest Neighbors&lt;/td&gt;
      &lt;td&gt;인간의 나이브한 능력에 따라서 모델 복잡도가 결정된다(변수의 개수, 변수간 유사성의 이해도)&lt;/td&gt;
      &lt;td&gt;변수의 양이 너무 많거나 유사성 측도가 너무 복잡하여 모형을 완전히 시뮬레이션할 수 없지만 유사성 측도와 변수 집합을 별도로 분해하여 분석할 수 있다.&lt;/td&gt;
      &lt;td&gt;유사성 측정은 분해될 수 없으며/또는 변수 수가 너무 많아서 사용자는 모델을 분석하기 위해 수학적 및 통계적 도구에 의존해야함&lt;/td&gt;
      &lt;td&gt;필요 없음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Rule Based Learners&lt;/td&gt;
      &lt;td&gt;규칙에 포함된 변수는 읽을 수 있으며 규칙 집합의 크기는 외부 도움 없이 사용자가 관리할 수 있음&lt;/td&gt;
      &lt;td&gt;규칙 집합의 크기가 너무 커서 작은 규칙 청크로 분해하지 않고 분석할 수 없음&lt;/td&gt;
      &lt;td&gt;규칙이 너무 복잡해져서(그리고 규칙 집합 크기가 너무 커져서) 모델 동작을 검사하는 데 수학적 도구가 필요함&lt;/td&gt;
      &lt;td&gt;필요 없음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;General Additive Models&lt;/td&gt;
      &lt;td&gt;모델에 포함된 원활한 기능에 따라 변수 및 변수 간의 상호 작용은 인간이 이해할 수 있는 범위에 제한되어야 함&lt;/td&gt;
      &lt;td&gt;교호작용이 너무 복잡해져서 시뮬레이션할 수 없으므로 모델을 분석하려면 분해 기법이 필요함&lt;/td&gt;
      &lt;td&gt;복잡성 때문에, 변수와 상호작용은 수학적, 통계적 도구를 적용하지 않고는 분석할 수 없음&lt;/td&gt;
      &lt;td&gt;필요 없음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bayesian Models&lt;/td&gt;
      &lt;td&gt;변수 및 변수 자체의 통계적 관계는 청중들이 직접 이해할 수 있어야 함&lt;/td&gt;
      &lt;td&gt;통계적 관계가 너무 많이 포함되어 있어서, 분해를 해야 분석이 용이함&lt;/td&gt;
      &lt;td&gt;통계적 관계는 이미 분해되어도 해석할 수 없는 수준이고 복잡한 수학적 도구를 사용해야 모델을 분석할 수 있음&lt;/td&gt;
      &lt;td&gt;필요 없음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Tree Ensembles&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;필요: 모델 단순화 혹은 피처 연관 설명&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Support Vector Machines&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;필요: 모델 단순화 혹은 부분 설명&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Multi–layer Neural Network&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;필요: 모델 단순화, 피처 연관 설명 혹은 시각화 설명&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Convolutional Neural Network&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;필요: 모델 단순화, 피처 연관 설명 혹은 시각화 설명&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Recurrent Neural Network&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;✗&lt;/td&gt;
      &lt;td&gt;필요: 피처 연관 설명&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;표 2&lt;/code&gt; 설명가능성 수준에 따른 ML 모델의 분류에 대한 전체적인 그림.&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Jan 2021 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2021/01/07/xaitutorial2.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2021/01/07/xaitutorial2.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>[XAI] Explainable Artificial Intelligence (XAI) - 1 </title>
        <description>&lt;h1 id=&quot;explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai&quot;&gt;Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI&lt;/h1&gt;

&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1910.10045&quot;&gt;https://arxiv.org/abs/1910.10045&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;XAI에 대한 전반적인 소개를 정리한 논문이 나와서 차근 차근 요약 정리해보려고 한다(무려 115페이지, reference만 6페이지). 약간의 번역 어투와 생략된 것도 있으니 영어 원문을 참고하길 바란다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/paper/2020/12/31/xaitutorial1.html&quot;&gt;&lt;span style=&quot;color:#e25252&quot;&gt;Introduction(이번편)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Explainability: What, why, what for and how?&lt;/li&gt;
  &lt;li&gt;Transparent machine learning models&lt;/li&gt;
  &lt;li&gt;Post-hoc explainability techniques for machile learning models: Taxonomy, shallow models and deep learning&lt;/li&gt;
  &lt;li&gt;XAI: Opportunities, challenges and future research needs&lt;/li&gt;
  &lt;li&gt;Toward responsible AI: Principles of artificial intelligence, fairness, privacy and data fusion&lt;/li&gt;
  &lt;li&gt;Conclusions and outlook&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Artificial Intelligence (AI) lies at the core of many activity sectors that have embraced new information technologies [1]. While the roots of AI trace back to several decades ago, there is a clear consensus on the paramount importance featured nowadays by intelligent machines endowed with learning, reasoning and adaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedented levels of performance when learning to solve increasingly complex computational tasks, making them pivotal for the future development of the human society [2]. The sophistication of AI-powered systems has lately increased to such an extent that almost no human intervention is required for their design and deployment. When decisions derived from such systems ultimately affect humans’ lives (as in e.g. medicine, law or defense), there is an emerging need for understanding how such decisions are furnished by AI methods [3].&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 인공지능이 정교해지면서 계산이 점점 복잡해지는 반면, 궁극적으로 인간의 삶에 영향을 미치는(의학, 법률, 국방) 시스템(기계)의 결정이 어떻게 내려졌는지, 우리는 이해할 필요가 있다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;While the very first AI systems were easily interpretable, the last years have witnessed the rise of opaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning (DL) models such as DNNs stems from a combination of efficient learning algorithms and their huge parametric space. The latter space comprises hundreds of layers and millions of parameters, which makes DNNs be considered as complex black-box models [4]. The opposite of black-box-ness is transparency, i.e., the search for a direct understanding of the mechanism by which a model works [5].&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 딥러닝 모델은 효율적인 학습 알고리즘과 거대한 파라미터 공간의 결합에서 비롯된다. 그리고 black-box 모델로 간주 된다. 이의 반대는 &lt;strong&gt;투명성(transparency)&lt;/strong&gt;이다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;As black-box Machine Learning (ML) models are increasingly being employed to make important predictions in critical contexts, the demand for transparency is increasing from the various stakeholders in AI [6]. The danger is on creating and using decisions that are not justifiable, legitimate, or that simply do not allow obtaining detailed explanations of their behaviour [7]. Explanations supporting the output of a model are crucial, e.g., in precision medicine, where experts require far more information from the model than a simple binary prediction for supporting their diagnosis [8]. Other examples include autonomous vehicles in transportation, security, and finance, among others.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; Machine Learning 모델이 점점 많이 활용되면서, 이해관계자들로부터 투명성의 요구가 높아지고 있다. 예를 들어, 의료(진단), 교통(자율주행), 보안, 금융등 이 있다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;In general, humans are reticent to adopt techniques that are not directly interpretable, tractable and trustworthy [9], given the increasing demand for ethical AI [3]. It is customary to think that by focusing solely on performance, the systems will be increasingly opaque. This is true in the sense that there is a trade-off between the performance of a model and its transparency [10]. However, an improvement in the understanding of a system can lead to the correction of its deficiencies. When developing a ML model, the consideration of interpretability as an additional design driver can improve its implementability for 3 reasons:
&lt;br /&gt;
* Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correct from bias in the training dataset.
&lt;br /&gt;
* Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbations that could change the prediction.
&lt;br /&gt;
* Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeing that an underlying truthful causality exists in the model reasoning.
&lt;br /&gt;
All these means that the interpretation of the system should, in order to be considered practical, provide either an understanding of the model mechanisms and predictions, a visualization of the model’s discrimination rules, or hints on what could perturb the model [11].&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 통상적으로 성과에만 치중할 수록 시스템은 점점 불투명해질 것이라 생각한다. 모델의 성능과 투명성 사이에 trade-off가 있다는 점은 사실이나, 모델에 대한 이해는 모델의 성능 향상을 이끌어 낼 수도 있다. 추가로 ML모델을 개발할 때, 해석 가능성을 모듈로 넣으면 세 가지 이유로 구현 가능성을 향상 시킬 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;해석가능성은 의사결정에서 공정성을 보장하는데 도움이 된다. 즉, 교육 데이터 집합의 편향성을 탐지하고 결과적으로 수정한다.&lt;/li&gt;
  &lt;li&gt;해석가능성은 예측을 바꿀 수 있는 잠재적 적대적 섭동을 강조함으로써 건전성의 제공을 촉진한다.&lt;/li&gt;
  &lt;li&gt;해석가능성은 유의미한 변수만으로 산출물을 유추하는 보험으로서, 즉 모형 추론에서 근본적인 진실적 인과관계가 존재함을 보증하는 보험으로 작용할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 해석가능한 시스템은 모델 매커니즘과 예측에 대한 이해, 모델의 판결 규칙 시각화, 또는 모델을 방해하는 것에 대한 힌트 등을 제공해야한다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI (XAI) [7] proposes creating a suite of ML techniques that 1) produce more explainable models while maintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners. XAI draws as well insights from the Social Sciences [12] and considers the psychology of explanation.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 현재의 효과적인 AI 시스템을 제한시키지 않는 선에서, eXplainable AI(XAI)은 1) 학습 퍼포먼스는 최대한으로 유지하면서 설명가능한 모델을 만들것을 제안 2) 사람이 이해하고, 적절하고 효과적으로 신뢰할 수 있도록 한다.&lt;/p&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;Fig. 1 displays the rising trend of contributions on XAI and related concepts. This literature outbreak shares its rationale with the research agendas of national governments and agencies. Although some recent surveys [8], [10], [13], [14], [15], [16], [17] summarize the upsurge of activity in XAI across sectors and disciplines, this overview aims to cover the creation of a complete unified framework of categories and concepts that allow for scrutiny and understanding of the field of XAI methods. Furthermore, we pose intriguing thoughts around the explainability of AI models in data fusion contexts with regards to data privacy and model confidentiality. This, along with other research opportunities and challenges identified throughout our study, serve as the pull factor toward Responsible Artificial Intelligence, term by which we refer to a series of AI principles to be necessarily met when deploying AI in real applications. As we will later show in detail, model explainability is among the most crucial aspects to be ensured within this methodological framework. All in all, the novel contributions of this overview can be summarized as follows:
&lt;br /&gt;
1. Grounded on a first elaboration of concepts and terms used in XAI-related research, we propose a novel definition of explainability that places audience (Fig. 2) as a key aspect to be considered when explaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques, from trustworthiness to privacy awareness, which round up the claimed importance of purpose and targeted audience in model explainability.
&lt;br /&gt;
2. We define and examine the different levels of transparency that a ML model can feature by itself, as well as the diverse approaches to post-hoc explainability, namely, the explanation of ML models that are not transparent by design.
&lt;br /&gt;
3. We thoroughly analyze the literature on XAI and related concepts published to date, covering approximately 400 contributions arranged into two different taxonomies. The first taxonomy addresses the explainability of ML models using the previously made distinction between transparency and post-hoc explainability, including models that are transparent by themselves, Deep and non-Deep (i.e., shallow) learning models. The second taxonomy deals with XAI methods suited for the explanation of Deep Learning models, using classification criteria closely linked to this family of ML methods (e.g. layerwise explanations, representation vectors, attention).
&lt;br /&gt;
4. We enumerate a series of challenges of XAI that still remain insufficiently addressed to date. Specifically, we identify research needs around the concepts and metrics to evaluate the explainability of ML models, and outline research directions toward making Deep Learning models more understandable. We further augment the scope of our prospects toward the implications of XAI techniques in regards to confidentiality, robustness in adversarial settings, data diversity, and other areas intersecting with explainability.
&lt;br /&gt;
5. After the previous prospective discussion, we arrive at the concept of Responsible Artificial Intelligence, a manifold concept that imposes the systematic adoption of several AI principles for AI models to be of practical use. In addition to explainability, the guidelines behind Responsible AI establish that fairness, accountability and privacy should also be considered when implementing AI models in real environments.
&lt;br /&gt;
6. Since Responsible AI blends together model explainability and privacy/security by design, we call for a profound reflection around the benefits and risks of XAI techniques in scenarios dealing with sensitive information and/or confidential ML models. As we will later show, the regulatory push toward data privacy, quality, integrity and governance demands more efforts to assess the role of XAI in this arena. In this regard, we provide an insight on the implications of XAI in terms of privacy and security under different data fusion paradigms.&lt;/p&gt;&lt;/details&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=119QnRBvYV4gHiuKz7kpaOVo_2b2tlhz5&quot; alt=&quot;Fig 1. 학계에서 XAI 및 연관된 개념의 기여도 추세&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Fig 1. 학계에서 XAI 및 연관된 개념의 기여도 추세&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fig 1&lt;/code&gt;에서 볼 수 있듯이 국가 정부 및 기관의 연구의제의 키워드 추세를 살펴보면 XAI관련 활동이 최근 급증했지만, 통일된 프레임워크가 없다. 이번 논문에서는 통일된 프레임워크의 작성하고, 개인정보 보호 및 모델 기밀성에 대해서 의견을 제시할 것이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;지금까지 XAI 관련 연구에서 사용된 개념과 용어의 기초하여, ML 모델을 설명할 때 청중(audience)을 핵심으로 고려할 것이다(그림 2). 또한 XAI 기법을 사용할 때 추구하는 다양한 목적에 따라 세분화할 것이다. 그리고 설명가능성에서 목적과 타겟 청중의 중요함을 이야기 한다.&lt;/li&gt;
  &lt;li&gt;다양한 레벨의 투명성을 정의하고 검토한다. 대상에는 사후(post-hoc) 설명이 가능한, 자체 설명가능한 혹은 설계에 의해 설명이 불가능한 모델들 등이 있다.&lt;/li&gt;
  &lt;li&gt;XAI에 관한 문헌과 지금까지 출판된 관련 개념들을 철저하게 분석하여, 대략 400개의 기여를 두 개의 다른 분류법으로 배열하였다. 첫 번째 분류법은 이전에 만든 투명성(transparency)과 사후 설명성(post-hoc explainability) 사이의 구별을 사용하여 ML 모델의 설명가능성을 다루고 있으며, 여기에는 스스로 투명하고 깊지 않은(즉, shallow 얉은) 학습 모델이 포함된다. 두 번째 분류법은 딥러닝 모델의 설명에 적합한 XAI 방법을 다루며, 이 ML 방법 계열과 밀접하게 연계된 분류 기준(예: 계층적 설명 layer-wise explanations, 표현 벡터 representation vectors, 어텐션 attention)을 사용한다.&lt;/li&gt;
  &lt;li&gt;지금까지도 불충분하게 다루어지지 않고 있는 XAI의 일련의 과제를 열거한다. 구체적으로는 ML 모델의 설명 가능성을 평가하기 위해 개념 및 메트릭스를 중심으로 연구 요구를 파악하고, 딥러닝 모델을 보다 이해할 수 있도록 연구 방향을 정리한다. 기밀성, 적대적 설정의 견고성, 데이터 다양성 및 설명 가능성과 교차하는 기타 영역에 관한 XAI 기법의 함축성을 향한 전망의 범위를 더욱 확대합니다.&lt;/li&gt;
  &lt;li&gt;앞서의 장래의 논의를 거쳐, AI 모델이 실용화하기 위해 여러 가지 AI 원리를 체계적으로 채택하는 매니폴드 개념인 책임감 있는 인공지능의 개념에 도달한다. 책임 AI를 뒷받침하는 가이드라인은 설명가능성 외에도 실제 환경에서 AI 모델을 구현할 때 공정성, 책임성, 프라이버시 등도 고려해야 한다고 규정하고 있다.&lt;/li&gt;
  &lt;li&gt;책임 있는 AI는 모델 설명 가능성과 개인 정보 보호/보안성을 설계별로 혼합하므로, 민감한 정보 및/또는 기밀 ML 모델을 다루는 시나리오에서 XAI 기법의 유익성과 위해성에 대해 심오한 반성을 요구한다. 나중에 보여드리겠지만, 데이터 개인 정보 보호, 품질, 무결성 및 거버넌스를 향한 규제는 이 분야에서 XAI의 역할을 평가하기 위한 더 많은 노력을 요구합니다. 이와 관련하여, 우리는 서로 다른 데이터 융합 패러다임 하에서의 프라이버시 및 보안 측면에서 XAI의 의미에 대한 통찰력을 제공한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;details class=&quot;collaspe-article&quot;&gt;&lt;summary&gt;영어원문&lt;/summary&gt;&lt;p&gt;The remainder of this overview is structured as follows: first, Section 2 and subsections therein open a discussion on the terminology and concepts revolving around explainability and interpretability in AI, ending up with the aforementioned novel definition of interpretability (Section 2.1 and 2.2), and a general criterion to categorize and analyze ML models from the XAI perspective. Sections 3 and 4 proceed by reviewing recent findings on XAI for ML models (on transparent models and post-hoc techniques respectively) that comprise the main division in the aforementioned taxonomy. We also include a review on hybrid approaches among the two, to attain XAI. Benefits and caveats of the synergies among the families of methods are discussed in Section 5, where we present a prospect of general challenges and some consequences to be cautious about. Finally, Section 6 elaborates on the concept of Responsible Artificial Intelligence. Section 7 concludes the survey with an outlook aimed at engaging the community around this vibrant research area, which has the potential to impact society, in particular those sectors that have progressively embraced ML as a core technology of their activity.&lt;/p&gt;&lt;/details&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;요약:&lt;/span&gt; 나머지 부분은 다음과 같이 구성되어 있다:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Section 2: 설명가능성(explainability)와 해석가능성(interpretability)의 새로운 정의, XAI 관점에서 ML 모델 분류 및 분석을 위한 용어 및 개념에 대한 이야기&lt;/li&gt;
  &lt;li&gt;Section 3, 4: 최근 연구 결과와 하이브리드 방법&lt;/li&gt;
  &lt;li&gt;Section 5: 해당 방법들에 대한 장단점 및 주의해야할 몇 가지 결과들 제시&lt;/li&gt;
  &lt;li&gt;Section 6: “책임감 있는 인공지능” 개념에 대한 설명&lt;/li&gt;
  &lt;li&gt;Section 7: 사회에 영향을 미칠 가능성이 있는 연구 영역인 만큼, ML 기술을 채택한 사람들을 커뮤니티를 참여시키는 목표로 결론을 내리고자 한다.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 31 Dec 2020 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/12/31/xaitutorial1.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/12/31/xaitutorial1.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>Topics to think about</title>
        <description>&lt;p&gt;공부를 하면서 앞으로 어떤것을 연구할지 생각할 시간을 가져봤다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1EFBMDgoDpcGadISMTbWMquvP0cJ80MUN&quot; alt=&quot;Img Source: https://pixabay.com/images/id-1868728/&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Img Source: https://pixabay.com/images/id-1868728/&lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;1-misleading&quot;&gt;1. Misleading&lt;/h1&gt;

&lt;p&gt;최근에 트럼프가 코로나 바이러스 대처에 대해서 “미국이 다른 나라들 보다 더 잘하고 있다”라고 주장했다. 6월 부터 이스라엘, 일본, 호주 등 국가들의 14배 심지어 35배 만큼 증가했다는 것을 근거로 들었다(아마 이전의 몇배를 측정).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Since the beginning of June, daily new cases have increased by a factor of 14 times in Israel; 35 times — that’s 35 times — in Japan; and nearly 30 times in Australia, just to name a few. These were countries that were doing incredibly well; leadership was being praised.&lt;/p&gt;

  &lt;p&gt;Source: whitehouse briefings - &lt;a href=&quot;https://www.whitehouse.gov/briefings-statements/remarks-president-trump-press-briefing-july-30-2020/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그러나 이는 함정이다. 기준을 6월부터 잡은 것은 기저효과가 깔려있는 문구다. 트럼프가 비교한 나라들의 6월 이전 코로나 수치를 살펴보자.&lt;/p&gt;

&lt;iframe src=&quot;https://ourworldindata.org/coronavirus-data-explorer?zoomToSelection=true&amp;amp;country=ISR~JPN~AUS~USA&amp;amp;casesMetric=true&amp;amp;interval=smoothed&amp;amp;hideControls=true&amp;amp;perCapita=true&amp;amp;smoothing=7&amp;amp;pickerMetric=location&amp;amp;pickerSort=asc&quot; loading=&quot;lazy&quot; style=&quot;width: 100%; height: 600px; border: 0px none;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;5월에서 이 나라들은 미국보다 훨씬 더 적은 코로나 cases를 기록했다는 사실이다. 따라서 6월 들어서 코로나 수치가 급증했으면, 그 배수도 그만큼 훨씬 더 커질 것이다. 5월달에 미국 정부는 코로나 관리를 잘한게 아닌데, 이 나라들 보다 더 잘하고 있다고 이야기한 것은 명백하게 오도하는 것이다(일본도 통계를 조작하는 낌세가 있긴 하지만 언급은 안하기로함).&lt;/p&gt;

&lt;p&gt;이처럼 사실인 정보를 자신의 입맛대로 바꿔서 전달하여 사람들을 호도하는 것은 분명 잘못된 것이다. 그래서 생각해본 연구주제로 도표에서 얻을 수 있는 정보를 제대로 서술하는 인공지능을 만들 수 있을까? 도표에 대해 잘못 서술하면 그 잘못된 원인과 고쳐야할 문구를 이야기할 수 있을까?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;관련 기사: &lt;a href=&quot;https://www.factcheck.org/2020/08/trumps-misleading-covid-19-comparisons-to-other-countries/&quot;&gt;Trump’s Misleading COVID-19 Comparisons to Other Countries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;찾아보다 재밌게본 지금까지 트럼프가 한 거짓말 DataBase: &lt;a href=&quot;https://www.washingtonpost.com/graphics/politics/trump-claims-database/&quot;&gt;WashingtonPost-Trump-Claims-Database&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2-trustability&quot;&gt;2. Trustability&lt;/h1&gt;

&lt;p&gt;최근 친구들과 술자리에서 유튜브에 관련해 이야기를 하다가 재밌는 것을 들었다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;의사에게 진료하러가면, 최근에 어떤 환자들은 유튜브 이야기를 한다.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;의사:&lt;/strong&gt; “이쪽 부위는 수술이 좀 필요해보입니다”&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;환자:&lt;/strong&gt; “그런데요, 요즘 xxx 유튜브 보니까, 이거 꼭 수술하지 않아도 된다던대요?”&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;의사:&lt;/strong&gt; “유튜브를 너무 신뢰하지 마시고, 제 의견상 이 부분은 ~~ 때문에 수술을 하지 않으면, 향후에 문제가 있을 것 같습니다.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;환자&lt;/strong&gt;: “그런데, 의사 yyy 유튜브에서는 …”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이런 대화를 듣다가, 생각한 것은 “전문가”인 의사가 “비전문가”인 환자에게 바로 신뢰를 얻지 못하는 이유가 무엇일까?&lt;/p&gt;

&lt;p&gt;“비전문가”인 우리가 접하는 정보가 과거보다 훨씬 더 많아진 것이 원인 중 하나라고 생각한다. 위와 같은 현상의 원인은 “비전문가”인 내가 의사결정을 하기 전에 자신이 가지고 있는 지식과, “전문가”가 가지고 있는 지식의 gap을 메꾸려고하기 때문이다. 즉, 비전문가는 다양해진 정보소스로 인해 자신이 모르는 분야에서 해당정보를 듣고 어떤 불안감이 형성됐고, 이를 해소하려고 전문가에게 묻는 과정이고, 그 매개체가 유튜브가 된것이다.&lt;/p&gt;

&lt;p&gt;이렇게 생각하다보니 두 가지 질문이 떠올라 생각들을 적어봤다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;특정 분야에서 뛰어난 AI 전문가(컴퓨터)가 이 불안감을 해소시킬 수 있을까?&lt;/p&gt;

    &lt;p&gt;AI 전문가(“모델”이라고 생각해도 될것 같다)가 사람에게 신뢰를 줄 수 있을 만큼이 되면, 불안감을 해소시킬수 있을 것이다. 어떻게 신뢰를 얻을 수 있을까? 어느 정도의 설명을 해야지 우리는 모델을 신뢰한다고 이야기 할 수 있을까? 일단 생각나는 것은 설명의 눈높이를 인간에게 맞추면, 조금더 신뢰가능한 모델이 될것이다. 단순히 모델의 내부 원리를 설명하는 것 보다, 인간의 눈높이에 맞춰서 이야기하는게 더 믿음이 가기 마련이다.&lt;/p&gt;

    &lt;p&gt;한편으로 이런 생각도 든다, 이러한 정보의 gap이 존재하는 구도에서 한번 의심하기 시작하면, 본인이 경험을 하기 전까지는 믿기가 쉽지 않아지는게 사실이다. 마치 중고차를 사기 전에는 계속 의심하고, 어디 하자가 있는지 계속 체크하게 되지만, 사기로 결정하고 잘 타고 다니면, 이러한 의심은 싹 사라지는 법이다.&lt;/p&gt;

    &lt;p&gt;문제는 본인이 경험을 하기 전에 의사결정을 꼭 해야되는 상황인 것이다. 중고차를 구매할지 말지, 수술을 할지 말지 등 결정을 하고나면, 그 후의 경험으로 그 때 이야기했던 말이 믿을 만했던것인지 알 수 있다는 점이다. 만약에 의사결정 후 상황이 돌이킬수 없는 것이라면? 전문가의 잘못된 설명이 해가 되면 그만큼 더 치명적이다. 따라서 AI 전문가가 사람 전문가 보다 뛰어난다고 해도, 사람 전문가와 같이 협업하는 그림을 그려야하지 않을까?라는 생각이 든다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;왜 유튜브가 사람들의 정보를 얻는(혹은 searching)의 최대 창구가 되었을까?&lt;/p&gt;

    &lt;p&gt;유튜브로 정보를 취득하는 이유에 대해서는 다니엘 카너먼이 쓴 “생각에 관한 생각” 책에서 찾을 수 있을것 같다. 카너먼은 생각의 과정을 두 시스템을 비유로 이야기한다.&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;시스템 1과 시스템 2는 모두 우리가 깨어 있을 때 활성화 된다. 시스템1은 자동으로 작동하고, 시스템 2는 편안한 보통 상태에서는 별 노력을 요하지 않고 역량의 일부만 가동한다. 시스템 1은 시스템 2를 위해 인상, 직관, 의도, 느낌을 지속적으로 제안한다. 시스템 2의 승인을 받으면 인상과 직관은 믿음으로 바뀌고, 충동은 자발적 행위로 변한다. 실제 대부분의 경우가 그렇지만 이 모든 과정이 자연스럽게 진행될 때, 시스템 2는 거의 혹은 전혀 수정없이 시스템 1의 제안을 그대로 수용한다. 우리는 일반적으로 느낀 인상을 믿고, 자신의 바람에 따라 행동하는데 이는 일반적으로 우리에게 유익하고 양호하다.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;유튜브에서 정보를 시청하는 과정은 “시스템 1”에 해당한다고 할 수 있다. 궁금한 것에 대해서 별다른 생각없이 정보를 그대로 수용하고, “시스템 2” 또한 이를 그대로 수용하면 편하다. 반면 정보를 글로써 읽어야한다면, “시스템 1”이 이해하지 못한 내용에 대해서, “시스템 2”는 이를 해석해야하고, 이야말로 귀찮은 작업잉 아닐수가 없다. 따라서, 인간은 귀찮은 것을 피하는 존재로, 당연히 정보를 읽는 것보다 시청하는 쪽으로 기울어 질 수 밖에 없다고 생각한다(편리한 정도를 생각하면).&lt;/p&gt;

    &lt;p&gt;헤비 유튜브 소비자로서, 사람들이 유튜브 내에서 사람들이 검색하는 정보들이 궁금하다. 한국에서 “탐색”란에서 등장하는 인기 급상승 동영상은 주로 논란이 되는 사회이슈(“죄송합니다”관련 내용들, 태극기부대 관련 채널), 아이돌 뮤직비디오, 예능 모음집 등등이 있다. 이런 영상들은 검색을 통한 정보다 아니라고 생각된다. 반대로 검색창에 검색하는 내용들이 진짜 사람들이 필요하고, 알고싶어하는 마음을 대변할 수 있는 데이터들이라고 생각한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-bias-in-ai&quot;&gt;3. Bias in AI&lt;/h1&gt;

&lt;p&gt;글또에서 손영신님께서 쓰신 글(&lt;a href=&quot;https://pizza-thief.github.io/nlp-bias-against-person-with-disabilities/&quot;&gt;NLP 모델은 배리어 프리일까?&lt;/a&gt;)이  AI를 연구하는 사람으로서 생각해볼 만한 주제인것 같다. 완벽한 AI는 존재하지 않고, 데이터가 존재하는 사회의 Bias를 반영하기 마련이다. 실제 세상에 기술이 적용되기 전에, 이런 문제점을 꼭 고려하고 공론화를 통해 토론을 해야한다고 생각한다. 이 내용과 비슷하지만 관련된 조경현 교수님의 글(&lt;a href=&quot;https://kyunghyuncho.me/social-impacts-bias-of-ai/&quot;&gt;Social impacts &amp;amp; bias of AI&lt;/a&gt;)도 읽는 것을 추천한다.&lt;/p&gt;

</description>
        <pubDate>Sun, 16 Aug 2020 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/others/2020/08/16/aitopics.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/others/2020/08/16/aitopics.html</guid>
        
        
        <category>others</category>
        
      </item>
    
      <item>
        <title>[XAI] Classifier-agnostic saliency map extraction</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1805.08249&quot;&gt;Classifier-agnostic saliency map extraction&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;기존의 몇몇 논문에서 특정 클래스 점수에 대한 gradient가 네트워크의 내부 작동을 밝히는 수단으로 사용할 수 있다는 것을 증명했다.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1312.6034&quot;&gt;Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps&lt;/a&gt;(Simonyan et al., 2013): vanilla gradient를 사용한 saliency map 생성, 관련 논문 리뷰 &lt;a href=&quot;https://simonjisu.github.io/paper/2020/03/12/deepinsidecnn.html&quot;&gt;링크&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.6806&quot;&gt;Striving for Simplicity: The All Convolutional Net&lt;/a&gt;(Springenberg et al., 2014): guided backpropagation을 사용하여 정교한 saliency map 생성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이러한 추세에 따라서 saliency map을 정교하게 만들기 위한 몇몇 테크닉이 적용되었다.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;&gt;Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization&lt;/a&gt;(Selvaraju et al., 2017): GradCAM 논문, 여러개의 saliency map을 평균내서 조금더 smooth 한 맵을 형성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;논문의 저자들은 이러한 트릭들은 유용한 증거를 가지고 있는 saliency map을 찾는데 원칙적인 방법이 아니라고 주장한다.&lt;/li&gt;
  &lt;li&gt;이 논문에서는 저자들의 목표는 분류에 도움이 되는 픽셀을 알려주는 saliency map을 찾는 것이다. 문제는 기존의 방법들이 분류기(훈련된 모델)에 너무 의존한다는 것이다. 이러한 문제점을 저자들은 해결하려고 했고, &lt;strong&gt;“class-agnostic saliency map extraction”&lt;/strong&gt;이라는 것을 제시한다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 방법은 모델에 의존하지 않고 오직 입력데이터에만 더 집중할 수 있도록 했다. 결과는 Figure2 처럼 질적으로 더 좋은 saliency map을 생성함. 각 행이 어떤 그림을 그린건지는 파트5에서 설명한다.&lt;/p&gt;

    &lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1SJcqwn25JiuD4LHO-yPILhwvaxOi7Qp6&quot; alt=&quot;Paper Figure 2&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Paper Figure 2&lt;/figcaption&gt;&lt;/figure&gt;
  &lt;/li&gt;
  &lt;li&gt;ImageNet 데이터로 weakly-supervised 방법중에서 SOTA를 달성, strongly supervised 모델과 비슷한 성과를 냈다고 주장한다. 심지어, 훈련하지 않은 class에 대해서 잘 작동하는 모습도 보여줬다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2-related-work&quot;&gt;2. Related work&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;생략&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-classifier-agnostic-saliency-map-extraction&quot;&gt;3. Classifier-Agnostic Saliency Map Extraction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이 논문에서 다루는 문제는 다음과 같이, 주어진 이미지에 해당하는 salient region을 추출하는 매핑(mapping)을 찾는 것이다. 이 매핑은 분류기(모델)에 도움이 되는 픽셀은 1을 유지하고 그렇지 않은 픽셀은 0으로 masking되어야 한다.&lt;/p&gt;

\[m: \Bbb{R}^{W\times H\times3} \rightarrow [0, 1]^{W\times H} \text{ over } x \in \Bbb{R}^{W\times H\times3}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;31-classifier-dependent-saliency-map-extraction&quot;&gt;3.1 Classifier-Dependent Saliency Map Extraction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;기존의 연구(&lt;a href=&quot;https://arxiv.org/abs/1704.03296&quot;&gt;Fong &amp;amp; Vedaldi, 2017&lt;/a&gt;과 &lt;a href=&quot;https://arxiv.org/abs/1705.07857&quot;&gt;Dabkowski &amp;amp; Gal, 2017&lt;/a&gt;)들은 주로 분류기 $f$ 가 주어진 상태에서 최적의 masking을 찾는 형태가 많았다.&lt;/p&gt;

\[m = \arg \underset{m'}{\max} S(m', f) \qquad \cdots (1)\]

    &lt;p&gt;이러한 방법을 &lt;strong&gt;Classifier-Dependent Saliency Map Extraction&lt;/strong&gt; 이라고 하며 자세한 방법은 다음과 같다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$S$는 일종의 score function인데, 분류 오차와 연관이 있다.&lt;/p&gt;

\[S(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} \bigg[ l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big) + R\big(m(x_n)\big) \bigg] \qquad \cdots (2)\]
  &lt;/li&gt;
  &lt;li&gt;수식을 하나씩 보자. $m(x_n)$는 마스크로 매핑된 값들이고, $1-m(x_n)$은 자연스럽게 마스크로 지워지지 않은 지역이다. 여기어 $\odot x_n$은 element-wise product로 입력 이미지에 덧씌움으로써 마스크 되지 않은 픽셀들을 가르킨다. 따라서 분류기 $f$ 의 입력으로 지워지지 않은 픽셀을 넣고, 그 예측값과 타겟을 비교한 Loss가 수식의 앞부분 $l(f( * ), y_n)$이다(분류의 경우 보통 Cross-Entropy Loss 다). 뒤에 $R( * )$ 항목은 정규화 항목이다.&lt;/li&gt;
  &lt;li&gt;같은 입력에서 특정 분류기 $f$로부터 생성된 매핑 $m$은, 다른 분류기 $f’$ 로부터 생성된 $m’$과 다를 수도 있다(심지어 같은 성능을 지녀도).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예를 들어, 다음 수식 $L$을 성능으로 측정한다면, 마스크를 씌우지 않았을 경우 $L(0, f) = L(0, f’)$와 마스크가 씌워진 경우 $L(m, f) = L(m’, f’)$의 성능이 같다고 해도, $m$과 $m’$은 다른 형태를 보여줄 수가 있다는 말이다.&lt;/p&gt;

\[L(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big) \qquad \cdots (3)\]
  &lt;/li&gt;
  &lt;li&gt;이런 현상의 원인은 두 개의 성능은 동일하나 가중치가 완전히 다른 분류기들이, 같은 입력에 대해서 각자 이미지의 다른 부분집합(픽셀들)을 사용하여 분류할 가능성이 있기 때문이다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;극단적인 예시로 이미지의 너비를 반으로 줄이고 옆으로 복사해서 각기 다른 구조의 분류기에 넣는다면, 하나는 이미지의 왼쪽에 saliency map을 생성하고, 다른 분류기는 이미지의 오른쪽에 saliency map을 생성할 수도 있다는 것이다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;코멘트:&lt;/strong&gt; 이 부분은 실험결과가 없어서 실제로 해봐야 할것 같다. 두 개다 탐지할 수도 있는것 아닌가?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;32-classifier-agnostic-saliency-map-extraction&quot;&gt;3.2 Classifier-Agnostic Saliency Map Extraction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2.1&lt;/code&gt;에서 제기한 문제를 해결하려고 모든 분류기에 대한 사후확률의 평균을 최적화는 방식으로 전환하여, 수식(1)을 다음과 같이 변형했다.&lt;/p&gt;

\[m = \arg \underset{m'}{\max} \Bbb{E}_f \big[ S(m', f) \big] \qquad \cdots (4)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;코멘트:&lt;/strong&gt; 아래는 공부한 것을 토대로 풀어써봤는데, 틀릴 수도 있으니 주의..&lt;/p&gt;

    &lt;p&gt;사후확률 $p(f \vert D, m’)$은 masking된 이미지가 주어졌을 때, 해당하는 분류기의 확률이라고 생각할 수 있겠다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$D, m’$ 부분은 $(1-m’(x_n))\odot x_n \text{ where } x_n \in D$ 부분이라고 생각할 수 있다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;따라서 $p(f \vert D, m’) \propto p(f) p(D, m’ \vert f) = p(f) \exp(-L(m’, f))$ 처럼 쓸 수 있다(아마 수식(3) 형태로 가져가려고 $L$에서 입력 이미지 $x_n\in D$는 생략한듯 하다).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;$p(D, m’ \vert f)$의 뜻은 특정 분류기 $f$가 주어졌을 때, masking된 이미지를 생성할 확률이다. 이는 Classifier-Dependent Saliency Map Extraction의 목적함수를 확률로 표현한 것이다.&lt;/li&gt;
      &lt;li&gt;$p(D, m’ \vert f) = \exp(-L(m’, f))$로 쓸수 있는 이유는 해당 항이 Likelihood 인데,  $\log$를 취하고 마이너스를 곱해줌으로써 Negative Log Likelihood로 바뀐다. $-\log p(D, m’ \vert f)$는 곧 수식(3)인 $L(m’,f)$와 일치한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;수식(4)는 모든 가능한 분류기의 공간(the space of all possible classifiers)을 탐색하고, 그 중에서 잘 작동하는 매핑 $m$을 찾는 과정이다. 모든 가능한 분류기의 공간은 모든 분류기의 파라미터($\theta_f$)의 공간과 동일하다.&lt;/li&gt;
  &lt;li&gt;이러한 과정을 저자들은 &lt;strong&gt;Classifier-Agnostic Saliency Map Extraction&lt;/strong&gt;라고 부르기로 했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;33-algorithm&quot;&gt;3.3 Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;수식(4)의 최적화 문제는 불행하게도 풀수가 없다(intractable). 특히 기댓값안에 있는 매핑 $m$을 loop를 통해 최적화 해야한다는 것이 이 문제를 더 난해하게 만든다. 따라서 논문에서는 이 문제를 매핑 $m$과 기댓값 목적함수를 동시에 추정함으로써 해결하려고 한다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;구체적인 알고리즘은 다음과 같다.&lt;/p&gt;

    &lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1xCajsDh2yozXxhMwYy82TzJ702BMv6BA&quot; alt=&quot;Paper Algorithm 1&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Paper Algorithm 1&lt;/figcaption&gt;&lt;/figure&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;먼저  $\theta_f$ 대해 classification loss $L$의 미분을 구함으로써, 사후확률 $p(f \vert D, m^{(k-1)})$를 가지는 $f^{(k)}$를 샘플링한다.&lt;/p&gt;

\[\theta_{f^{(k)}} \leftarrow \theta_{f^{(k-1)}} - \eta_f \triangledown_{\theta_f} L(m^{(k-1)}, f^{(k-1)})\]

        &lt;p&gt;이러한 방법은 &lt;a href=&quot;https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf&quot;&gt;Welling &amp;amp; Teh, 2011&lt;/a&gt;와 &lt;a href=&quot;https://arxiv.org/abs/1704.04289&quot;&gt;Mandt et al., 2017&lt;/a&gt; 연구에서 SGD에 noise를 일부 주면  Bayesian Posterior Inference를 수행할 수 있다는 점에서 착안했다. &lt;strong&gt;공부가 더 필요한 부분..&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;EM 알고리즘이랑 비슷한데, SGD로 한다는 점이 다른듯하다. 참고자료: &lt;a href=&quot;http://norman3.github.io/prml/docs/chapter09/4.html&quot;&gt;4. The EM Algorithm in General&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;업데이트된 파라미터 공간을 $F^{(k-1)}$와 합친뒤에 $F^{(k)}$에서 새로운 파라미터 $f’$를 샘플링한다.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;새로운 모델을 score function $S$에 넣어서 다시 마스크 네트워크 $\theta_{m^{(k-1)}}$값을 $\theta_{m^{(k)}}$로 업데이트 한다.&lt;/p&gt;

\[\theta_{m^{(k)}} \leftarrow \theta_{m^{(k-1)}} + \eta_m \triangledown_{\theta_m} S(m^{(k-1)}, f')\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;score-function&quot;&gt;Score Function&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Score Function은 saliency map의 퀄리티를 측정하는 도구다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;또한, Precision 과 Recall의 조건을 동시에 만족하게 디자인 되어야한다.&lt;/p&gt;

\[\text{precision} = \dfrac{TP}{TP + FP} \quad \text{recall}=\dfrac{TP}{TP+FN}\]

    &lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1OEpXqK1p1lwdTLJHzHbliw_EScvBBjO2&quot; alt=&quot;Confusion Matrix&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Confusion Matrix&lt;/figcaption&gt;&lt;/figure&gt;

    &lt;ul&gt;
      &lt;li&gt;Precision: 마킹된 픽셀들 중에서 연관된 픽셀이 얼마나 있는지&lt;/li&gt;
      &lt;li&gt;Recall: 실제 연관된 픽셀들중에서 얼마나 정확하게 마킹되었는지&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기존의 score 함수를 살펴보면, $A$파트는 연관된 픽셀이 더 많이 마킹되게 만들어주는 항이다(high recall). 마스킹된 이미지를 넣어서 classification loss가 높아지면 분류에 도움되는 픽셀들을 잡아주고 있다는 뜻이고, 낮아지면 마스킹이 잘 안되고 있다는 뜻으로 해석할 수 있다.&lt;/p&gt;

\[S(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} \bigg[ \underbrace{ l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big)}_{A} + \underbrace{ R\big(m(x_n)\big) }_{B}\bigg] \qquad \cdots (2)\]
  &lt;/li&gt;
  &lt;li&gt;하지만 단순히 $A$파트를 쓰기에는 문제가 있는데, sampling할때 마스킹된 입력을 넣어서 파라미터 $\theta_f$를 업데이트하기 때문에, 모델의 성능을 저하시킬 가능성이 있다. 변형된 파라미터에 같은 classification loss를 그대로 사용하는 것은 이치에 안맞을 수도 있다.&lt;/li&gt;
  &lt;li&gt;추가로 연관된 픽셀이 마스킹된 이미지를 분류기에게 넣었을 때, 정답 클래스가 아니라는 것만 판단해야지, 다른 클래스로 예측하면 안되기 때문에, classification loss를 그대로 사용하는 것은 문제가 있어 보인다.&lt;/li&gt;
  &lt;li&gt;따라서 저자들은 $A$파트를 $\mathcal{H}\big( f( (1-m(x)) \odot x_n) \big)$ Entropy로 바꿨다. 즉, 마스킹을 찾아내는 작업은 정확도를 최소화하는 것이 아닌 불확실성(uncertainty)을 최대화하는 방향으로 진행해야한다.&lt;/li&gt;
  &lt;li&gt;또한, Entropy로 바꾸면서 ground-truth label의 필요성을 제거했다.&lt;/li&gt;
  &lt;li&gt;$B$파트는 정규화 항목인데, trivial solution을 배제하고 있다. 만약에 마스크 $m$이 전부 1인 경우, 최대 recall 및 아주 낮은 precision을 달성할 수가 있다. 그래서 total variation(&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/016727899290242F&quot;&gt;Rudin et al., 1992&lt;/a&gt;)과 L1 Norm을 사용하기로 한다.
    &lt;ul&gt;
      &lt;li&gt;임성빈님의 자료 참고: &lt;a href=&quot;https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i&quot;&gt;링크&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;따라서 수식(2)는 다음과 같이 변한다&lt;/p&gt;

\[S(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} \bigg[ \mathcal{H}\Big( f\big( (1-m(x_n))\odot x_n \big) \Big) + \lambda_R \Vert m(x_n) \Vert_1 \bigg] \qquad \cdots (7)\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;thining&quot;&gt;Thining&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;알고리즘이 사후확률 분포에서 분류기 집합 $f^{(k)}$을 저장해서 많은 양의 데이터를 수집하기 때문에, 그중 작은 부분집합만 보존하는 전략을 취하기로 했다.&lt;/li&gt;
  &lt;li&gt;고정된 크기 $F^{(k)}$를 취하는 방식으로 다음과 같이 정한다.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;F&lt;/strong&gt;: 첫번째 분류기만 저장 $F^{(k)} = { f^{(0)}}$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;L&lt;/strong&gt;: 마지막 분류기만 저장 $F^{(k)} = { f^{(k)}}$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;FL&lt;/strong&gt;: 첫번째와 마지막 분류기만 저장 $F^{(k)} = { f^{(0)}, f^{(k)}}$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;L1000&lt;/strong&gt;: 1000번째 iteration 마다 저장하고, $\vert F^{(k)} \vert =30$ 을 넘어갈때, 랜덤하게 하나를 제거한다.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;L100&lt;/strong&gt;: 100번째 iteration 마다 저장하고, $\vert F^{(k)} \vert =30$ 을 넘어갈때, 랜덤하게 하나를 제거한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;classification-loss&quot;&gt;Classification loss&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;수식(3)처럼 classification loss를 정의해도 되지만 꼭 그럴 필요도 없다.&lt;/p&gt;

\[L(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big) \qquad \cdots (3)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;대신 &lt;a href=&quot;https://arxiv.org/abs/1312.6199&quot;&gt;Szegedy et al., 2013&lt;/a&gt; 방식이 더 잘 됐다.&lt;/p&gt;

\[L(m, f) = \dfrac{1}{2N} \sum_{n=1}^{N} \bigg[ l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big) + l\big( f(x_n), y_n \big) \bigg] \qquad \cdots (8)\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;4-training-and-evaluation-details&quot;&gt;4. Training and evaluation details&lt;/h1&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Official ImageNet Training Set&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;classifier-f-and-mapping-m&quot;&gt;Classifier $f$ and mapping $m$&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1FHVgUhAdmJqAlJ0UE1l55lbnna070zJq&quot; alt=&quot;Paper Figure 1&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Paper Figure 1&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;모델 $f$는 ResNet-50을 사용했다. 그리고 Encoder-Decoder 구조를 취해서 마스크 $m$를 생성한다.&lt;/li&gt;
  &lt;li&gt;Encoder는 ResNet-50 구조를 사용하고, 가중치는 $f$와 공유할 수도 있고 아닐수도 있다(실험결과 공유하는게 더 유리하다).&lt;/li&gt;
  &lt;li&gt;Decoder는 Deconvolutional Network를 사용하여 마스크를 생성한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;regularization-coeffieient-lambda_r&quot;&gt;Regularization coeffieient $\lambda_R$&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lld-workshop.github.io/2017/papers/LLD_2017_paper_64.pdf&quot;&gt;Fan et al. 2017&lt;/a&gt; 에서 최적의 $\lambda_R$는 쉽게 찾을 수 없다고(not trivial)하다고 이야기하면서, adaptive 전략을 취해서 인위적인 $\lambda_R$을 고르는 것을 배제했다.&lt;/li&gt;
  &lt;li&gt;저자들을 같은 방법을 사용하면 saliency map의 평균 크기를 제어하기가 어려워 $f(x)$와 $f((1-m(x)) \odot x)$간에 차이가 있을 때만 $\lambda_R$를 적용했다.&lt;/li&gt;
  &lt;li&gt;각 실험에서 대략 50%의 픽셀이 연관되게 하도록 마스크 $m$를 생성했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baseline-and-casm&quot;&gt;Baseline and CASM&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Baseline 모델은 CASM과 같은 모델 구조를 가지지만 CDSM(classifier-dependent saliency mapping) 방법으로 훈련킨 모델이(Thinning은 &lt;strong&gt;F&lt;/strong&gt;를 사용).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mask-discretization&quot;&gt;Mask discretization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;마스크는 다음과 같이 생성한다. $\bar{m}(x)$은 마스크의 평균값이고, $\alpha$는 하이퍼파라미터다.&lt;/p&gt;

\[b_{ij}(x) = \begin{cases} 1, \quad \text{if } m_{ij}(x) \geq \alpha \bar{m}(x) \\ 0, \quad \text{otherwise} \end{cases}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\alpha$를 1로 설성하면 마스크 평균값이 그대로 binary mask를 생성한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;5-qualitative-comparisons&quot;&gt;5. Qualitative comparisons&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;논문의 그림에 대해서 설명한다. 각 행에 대하여 다음과 같은 visualization을 했다.
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;original image&lt;/strong&gt;: 원본 이미지&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;masked-in image&lt;/strong&gt;( $b(x) \odot x$ ): 마스크안의 이미지&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;masked-out image&lt;/strong&gt;( $(1-b(x)) \odot x$ ): 마스킹된 이미지&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;inpainted masked-out image&lt;/strong&gt;: inpainting 알고리즘 &lt;a href=&quot;https://www.researchgate.net/publication/238183352_An_Image_Inpainting_Technique_Based_on_the_Fast_Marching_Method&quot;&gt;Telea, 2004&lt;/a&gt;을 사용해서 마스크 내부 이미지를 채웠다.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Random 하게 7개의 연속된 그림을 골라서 visualization 했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;6-evaluation&quot;&gt;6. Evaluation&lt;/h1&gt;

&lt;h2 id=&quot;61-basic-statistics&quot;&gt;6.1 Basic statistics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Validation Set로 saliency map을 만들었다.&lt;/li&gt;
  &lt;li&gt;CASM 모델로 마스크를 추출시 total variation이 더 낮았다( $2.5 \times 10^3$ vs $7.0 \times 10 ^3$ ). 즉, total variation 정규화를 적게 줘도 CASM이 더 많은 mask를 생성한다. &amp;gt; 무슨말이냐&lt;/li&gt;
  &lt;li&gt;Entropy는 Baseline 보다 많이 작았는데( $0.05$ vs $0.21$ ), 이는 mask intensities가 거의 0과 1 사이의 값을 평균적으로 가진다는 것을 뜻한다.&lt;/li&gt;
  &lt;li&gt;masked out volume의 표준편차가 더 컸는데( $0.19$ vs $0.14$ ), 이를 통해 CASM이 입력 이미지에 따라서 더 다양한 크기의 saliency map을 생성할 수 있다는 것을 알 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;62-classification-by-multiple-classifiers&quot;&gt;6.2 Classification by multiple classifiers&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=11zPpcGsV3JDEPhvOBVGsIm9uO-mtIcGz&quot; alt=&quot;Paper Figure 3&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Paper Figure 3&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;CASM이 정말로 classifier-agnostic인지 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.models&lt;/code&gt;에 있는 모델들로 주장을 확인해봤다. 자신들이 기대한 것은 CASM을 통해 만든 inpainted masked-out 이미지로 해당 모델들의 정확도를 깎아 내리고, masked-in 이미지들은 좋은 성능을 내는거다. 그리고 그 기대는 맞아 떨어졌다.&lt;/li&gt;
  &lt;li&gt;Masked-out 이미지에서 Baseline 모델이 성능이 굉장히 낮게 나왔는데, 저자들이 어림짐작으로 보았을때 Baseline 모델이 생성한 saliency map의 적대적인 성질인것 겉다. 그 이유는 masked-out 이미지를 채운 inpainted masked-out 이미지와 비교했을 때, Baseline 모델은 성능이 드라마틱하게 향상하는데, CASM 모델은 그 혜택을 많이 못보기 때문이다.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;코멘트&lt;/strong&gt;: 기존에 완전한 이미지로 모델을 학습 할 때, 주변 사물등 다른 정보를 사용하여 물체를 분류했을 가능성이 있는데, Baseline 모델은 일부 연관된 물체도 지워버리니까 오히려 더 CASM 보다 성능이 하락하는 것 같음. 예를 들어, 아래 그림 처럼 나무를 아예 지워버리니까, 판별을 더 못하는 듯. 어떻게 생각하면 훈련 데이터가 다양하지 못했다는 것이, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f(나무 + 새) = 새&lt;/code&gt; 라는 것이 되니까, 주변 사물이 분류기에 많은 영향을 끼치는 것을 알 수 있음.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1UAR_h0c0RE4uLM5LK3SBu21qj3rcuQLL&quot; alt=&quot;Comparing CASM vs Baseline&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Comparing CASM vs Baseline&lt;/figcaption&gt;&lt;/figure&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;코멘트&lt;/strong&gt;: 자신들이 만든 baseline과 비교한것도 좋지만, 다른 방법(CAM방식) 등 하고 비교해보았어도 괜찮을 것 같음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;63-object-localization&quot;&gt;6.3 Object localization&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1WZ6W1xGCCfgnpA-8irC-_l-3Fljbe3aY&quot; alt=&quot;Paper Table 1&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Paper Table 1&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;saliency map으로 weakly supervised localization도 같이 수행했다.&lt;/li&gt;
  &lt;li&gt;3가지 metrics으로 localization을 계량했다.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;OM&lt;/strong&gt;: ImageNet Localization 챌린지에서 사용하는 official metric, 예측 bounding box와 정답 bounding box의 IOU가 0.5 이상이여야하고, 클래스를 맞춰야한다. 맞췄을 경우 0, 그렇지 않을 경우 1이 되서, OM이 낮을 수록 좋다.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;LE&lt;/strong&gt;: OM방식은 분류기에 따라 다르다. 저자들은 분류기와 상관없이 훈련했기에 bounding box만 예측하는 “Localization Error”라는 다른 방법을 사용했다. (Cao et al. 2015, Fong &amp;amp; Vedaldi 2017) 이 방법도 낮을 수록 좋다.&lt;/li&gt;
      &lt;li&gt;마지막으로 원본 saliency map을 conintious F1 score로 평가했다.
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Precision과 Recall은 다음과 같이 정해진다.&lt;/p&gt;

\[P=\dfrac{\sum_{(i,j) \in B^*(x)} m_{ij}(x)}{\sum_{ij} m_{ij}(x)} \quad \text{and} \quad R=\dfrac{\sum_{(i,j) \in B^*(x)} m_{ij}(x)}{\vert B^*(x) \vert}\]
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;supervised 보다 성능이 대부분 뛰어났다고 주장, 다만 전제 자체가 조금 다르기에 적절한 비교가 힘들다고 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;thinning-strategies--score-function--sharing-the-encoder-and-classifier&quot;&gt;Thinning strategies &amp;amp; Score function &amp;amp; Sharing the encoder and classifier&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1YPJ_KmdfPrHgIeZOSAB1m0ZDi97zsamg&quot; alt=&quot;Paper Table 2&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Paper Table 2&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;$S$: score function 의 선택 (E: entropy loss, C: classification loss)&lt;/li&gt;
  &lt;li&gt;$SHR$: Encoder와 Classifier를 공유 했는지 여부&lt;/li&gt;
  &lt;li&gt;$THIN$: Thinning 전략&lt;/li&gt;
  &lt;li&gt;Entropy 쓰고, parameter sharing하고, L100 thinning 전략인 E가 제일 좋음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;64-unseen-classes&quot;&gt;6.4 Unseen classes&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1pyIiM1Np5OWJB85TAczggvXwdFl69pdX&quot; alt=&quot;Paper Table 3&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;Paper Table 3&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;제안한 방법이 클래스의 정답을 필요로 하지 않기 때문에, 학습하지 않았던 데이터의 localization을 수행할 수 있다.&lt;/li&gt;
  &lt;li&gt;이를 테스트하기 위해서 1000개의 클래스를 5개(6개아닌가..?)의 서로소 부분집합(disjoint subset)으로 쪼갠다. 즉, 각각의 집합에 서로 다른 클래스가 들어가 있다. 그리고 각 집합에는 다음과 같은 크기의 데이터를 담고, 각각 해당하는 % 만큼 CASM 모델을 훈련시킨다(Thinning은 &lt;strong&gt;L&lt;/strong&gt; 전략 사용).
    &lt;ul&gt;
      &lt;li&gt;A: 50, B: 50, C: 100, D: 300, E: 300, F: 200&lt;/li&gt;
      &lt;li&gt;95%: B, C, D, E, F&lt;/li&gt;
      &lt;li&gt;90%: C, D, E, F&lt;/li&gt;
      &lt;li&gt;80%: D, E, F&lt;/li&gt;
      &lt;li&gt;50%: E, F&lt;/li&gt;
      &lt;li&gt;20%: F&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;모든 모델들의 일반화가 좋은 편이었고, 정확도는 무시할정도 수준으로 작았다(20% 훈련한 모델을 제외).&lt;/li&gt;
  &lt;li&gt;seen과 unseen의 &lt;strong&gt;LE&lt;/strong&gt; 차이는 훈련 데이터가 적어질 수록 높아졌다. 그러나 적당한 크기의 traning set 이라면 차이는 크게 나지 않는다.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 23 Jul 2020 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/07/23/casm.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/07/23/casm.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>[NLP] Mask-Predict: Parallel Decoding of Conditional Masked Language Models</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1904.09324&quot;&gt;Mask-Predict: Parallel Decoding of Conditional Masked Language Models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;모두의 연구소에서 진행하는 “beyondBERT” 프로그램에서 참여하다가 본 논문을 정리해보려고 한다. 흥미롭게 생각했던 논문이라 중요 부분만 일단 정리했다.&lt;/p&gt;

&lt;p&gt;기존의 기계번역등 작업을 진행할때 Seq2Seq 모델(with Attention)을 사용할 경우 보통 autoregressive하게 토큰을 하나씩 디코딩했다. 예를 들어, “내가 아이언맨이다.”라는 문장을 번역하려면, Encoder에 다음과 같이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source&lt;/code&gt; 토큰들을 넣어주고, Decoder는 문장의 시작을 알리는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;SOS&amp;gt;&lt;/code&gt; 토큰으로 시작하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;I&quot;&lt;/code&gt;를 예측하고, 예측한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;I&quot;&lt;/code&gt;로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;am&quot;&lt;/code&gt;을 예측하고, 마지막에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;&amp;lt;EOS&amp;gt;&quot;&lt;/code&gt; 토큰이 등장하면 끝나는 구조다. sudo-code로 다음과 같이 작성 할 수 있겠다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;나는&quot;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;아이언맨&quot;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;이다&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;SOS&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;I&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;am&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;IronMan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;EOS&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;hiddens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;EOS&amp;gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;EOS&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hiddens&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이 논문에서는 non-autoregressive하게 decoding하는 방법을 제시했는데, 구체적인 방법은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림1&lt;/code&gt;을 보면 단번에 이해가 되리라고 믿는다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;일부 스터디에서 나온 의견 및 개인 의견이 섞여서 들어가 있음을 밝힌다.&lt;/li&gt;
    &lt;li&gt;Model Distillation 부분은 비교군이 적절하지 않다고 스터디에서 나온 의견이 있어서 다루지 않았다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;생략&lt;/p&gt;

&lt;h1 id=&quot;2-cmlmconditional-masked-language-models&quot;&gt;2. CMLM(Conditional Masked Language Models)&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;CMLM은 입력 토큰 $X$와 일부 타겟 토큰 $Y_{obs}$가 주어지면 마스크가 된 타겟 토큰 $Y_{mask}$를 맞추는 문제다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;강한 가정: 마스크된 타겟 토큰들 $Y_{mask}$은 입력데이터에 대해서 조건부 독립이다.&lt;/p&gt;

\[\text{Predict: } P(y \vert X, Y_{obs}) \ \forall y \in Y_{mask}\]

    &lt;p&gt;이를 분해해보면 다음과 같다.&lt;/p&gt;

\[\begin{aligned} P(Y_{mask} \vert X, Y_{obs}) &amp;amp;= P(Y_{mask}^{K};Y_{mask}^{1:(K-1)} \vert X, Y_{obs}) P(Y_{mask}^{1:(K-1)} \vert X, Y_{obs}) \\ &amp;amp;= P(Y_{mask}^{K};Y_{mask}^{1:(K-1)} \vert X, Y_{obs}) \cdots P(Y_{mask}^{2};Y_{mask}^{1} \vert X, Y_{obs}) P(Y_{mask}^{1} \vert X, Y_{obs}) \end{aligned} \\ \text{if } Y_{mask} \text{ is conditionally independent by each other } \rightarrow \\ \begin{aligned} P(Y_{mask} \vert X, Y_{obs}) &amp;amp;\approx P(Y_{mask}^{K} \vert X, Y_{obs}) P(Y_{mask}^{K-1} \vert X, Y_{obs}) \cdots P(Y_{mask}^{2} \vert X, Y_{obs}) P(Y_{mask}^{1} \vert X, Y_{obs}) \end{aligned}\]

    &lt;p&gt;(beyondBERT 에서 나온 리뷰: 이러한 최종 예측파트에서는 가정이 맞지만, 훈련시킬때는 아닐 것이다)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;추가로 마스크 개수는 정해져 있기 때문에 토큰 길이에 대한 제약도 명시적으로 달려있는 셈이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;21-architecture&quot;&gt;2.1 Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;클래식한 Transformer에 Decoder만 Masked-self attention을 제거하기로함&lt;/li&gt;
  &lt;li&gt;fair-style Transformer&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;22-training-objective&quot;&gt;2.2 Training Objective&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;$1$~$N$(토큰길이) 만큼의 uniform distribution에서 랜덤하게 숫자를 고른다음에 그 개수만큼 $Y_{mask}$를 선택&lt;/li&gt;
  &lt;li&gt;Cross-entropy Loss로 최적화, parallel하게 할 수 있는 이유는 이전의 $Y_{mask}$에 취한 conditionally independent 가정 때문이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;23-predicting-target-sequence-length&quot;&gt;2.3 Predicting Target Sequence Length&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;전통적인 left-to-right 기계번역의 경우, 이전 예측 토큰이 다음 예측 토큰으로 들어가게 된다. 그리고 최종적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EOS&lt;/code&gt;이 나오면 종료가 되는 형태라서 자동적으로 문장의 길이를 알 수 있었지만 , CMLMs에서는 전체 시퀀스를 parallel하게 예측하기 때문에 타겟 문장 전체의 길이를 예측해야한다.&lt;/li&gt;
  &lt;li&gt;논문에서는 BERT 의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLS&lt;/code&gt; 토큰처럼, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LENGTH&lt;/code&gt; 토큰을 Encoder에 집어넣기로 한다. 해당 토큰의 loss도 마지막에 추가한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-decoding-with-mask-predict&quot;&gt;3. Decoding with Mask-Predict&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;요약하면 각 iteration마다 알고리즘은 토큰의 부분집합을 선택하여 masking하고, CMLM으로 예측한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;31-formal-description&quot;&gt;3.1 Formal Description&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;타겟 시퀀스 $(y_1, \cdots, y_N)$ 와 각 토큰의 확률 $(p_1, \cdots, p_N)$이라는 두 변수가 있고, 미러 정의된 $T$번 동안 알고리즘을 돌린다(이는 상수거나 $N$에 관련된 간단한 함수로 결정된다).&lt;/li&gt;
  &lt;li&gt;각 iteration마다 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt; 작업을 수행하고, 예측(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict&lt;/code&gt;)한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mask&quot;&gt;&lt;strong&gt;Mask&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;첫 iteration에는 모든 토큰을 마스킹한다. 그 이후부터는 가장 낮은 확률을 가진 $n$개의 토큰을 masking한다.&lt;/p&gt;

\[\begin{aligned} Y_{mask}^{(t)} &amp;amp;= \arg \underset{i}{\min} (p_i, n) \\ Y_{obs}^{(t)} &amp;amp;= Y \setminus Y_{mask}^{(t)}\end{aligned}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n$은 $t$의 함수이며 논문에서는 $n=N \cdot \dfrac{T-t}{T}$를 사용했다($T$는 iteration 횟수).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;predict&quot;&gt;&lt;strong&gt;Predict&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Masking후, CMLM은 주어진 입력$X$와 masking 안된 $Y_{obs}^{(t)}$를 기반으로 $Y_{mask}^{(t)}$를 예측하는데, 각 마스킹된 토큰 $y_i \in Y_{mask}^{(t)}$에 대해서 확률이 가장 높은 것을 예측값으로 선택한다.&lt;/p&gt;

\[\begin{aligned} y_i^{(t)} &amp;amp;= \arg \underset{w}{\max} P(y_i = w \vert X, Y_{obs}^{(t)} ) \\ p_i^{(t)} &amp;amp;= \underset{w}{\max} P(y_i = w \vert X, Y_{obs}^{(t)} ) \end{aligned}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;마스크가 안된 token들은 이전 스텝의 값을 그대로 따라간다.&lt;/p&gt;

\[\begin{aligned} y_i^{(t)} &amp;amp;=y_i^{(t-1)} \\ p_i^{(t)} &amp;amp;= p_i^{(t-1)} \end{aligned}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;특정 토큰의 확률이 계속 희박하여 이러한 휴리스틱한 작업에도 불구하고 잘 작동했다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;32-example&quot;&gt;3.2 Example&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;그림으로 보면 조금더 이해가 쉬운데, 차후 3.3에서 이야기하는 Length predict 이후의 예시를 들은 것이다.&lt;/li&gt;
  &lt;li&gt;그림에서 나오는 용어들이 있다.
    &lt;ul&gt;
      &lt;li&gt;각 $t$ 스텝 마다 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Mask &amp;gt; Predict&lt;/code&gt; 의 과정을 반복한다.&lt;/li&gt;
      &lt;li&gt;$t$: 현재 스텝&lt;/li&gt;
      &lt;li&gt;$n$: masking 해야할 토큰의 수&lt;/li&gt;
      &lt;li&gt;$probability$(보라색): 각 예측의 확률을 담는 container&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=12HUzuQzCWwkaO4B6H07EOkEIJlKpjMnv&quot; alt=&quot;[그림1] Example of parallel decoding&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림1] Example of parallel decoding&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;장점&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;마스킹되지 않았던 것들도 차후에 확률이 다른 토큰에 비해 상대적으로 낮아지면 다시 마스킹될 수도 있다. 즉, 초기에 잘못 예측했더라도, iteration을 통해 점차 바른 예측으로 고쳐질 수도 있다는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;문제점: Multi-modality Problem&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1VSrcpclqKZ5KxqJF7yKAw-AP8u-hIuFQ&quot; alt=&quot;[그림2] Paper Figure 1&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림2] Paper Figure 1&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;논문의 Figure 1 처럼, t=0 인 상황에서 중복된 단어가 생성 될 수가 있음(“completed”) 이는 non-autoregressive 모델에서 자주 등장하는 문제다. 이는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;5.1&lt;/code&gt;에서 자세히 다룬다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;33-deciding-target-sequence-length&quot;&gt;3.3 Deciding Target Sequence Length&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;타켓 문장의 길이인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LENGTH&lt;/code&gt; 토큰을 예측하기 때문에 배치 연산을 할 수 있다.&lt;/li&gt;
  &lt;li&gt;확률이 가장 높은 길이를 여러개 뽑아서 배치 연산으로 3.2의 과정을 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1WPZ4xsitujEWF9yfeacc6V587DmjGZ2x&quot; alt=&quot;[그림3] Length Predict&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림3] Length Predict&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;차후에 제일 높은 평균 로그 확률로 길이를 선택하게 된다(beam search 와 연관)&lt;/li&gt;
&lt;/ul&gt;

\[\dfrac{1}{N} \sum_i \log p_i^{(T)}\]

&lt;hr /&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;41-experimental-setup&quot;&gt;4.1 Experimental Setup&lt;/h2&gt;

&lt;h3 id=&quot;translation-benchmarks&quot;&gt;Translation Benchmarks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;총 3개의 데이터 세트를 사용: WMT’14 EN-DE (4.5M sentence pairs), WMT’16 EN-RO (610k pairs), WMT’17 EN-ZH (20M pairs)&lt;/li&gt;
  &lt;li&gt;모든 데이터는 BPE로 인코딩했으며, 퍼포먼스는 BELU score를 계산했다.&lt;/li&gt;
  &lt;li&gt;EN-ZH 만 ScareBLEU를 사용했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hyperparameters&quot;&gt;Hyperparameters&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Attention is All you Need 논문과 똑같이 각 stack마다 6개의 layer, 각 layer마다 8개의 attention heads, 모델 $h_{model}, h_{ffn}$ hidden size는 각 512, 2048로 진행했다.&lt;/li&gt;
  &lt;li&gt;가중치 초기화는 BERT 논문에서 진행한 $\mathcal{N}(0, 0.02)$, bias는 0으로 초기화 했다.&lt;/li&gt;
  &lt;li&gt;LayerNorm은 $\beta=0, \gamma=1$&lt;/li&gt;
  &lt;li&gt;Regularization은 $\text{dropout}=0.3, \text{weight decay}=0.01$ 로 실험했다.&lt;/li&gt;
  &lt;li&gt;Smoothed CV Loss $\varepsilon=0.1$&lt;/li&gt;
  &lt;li&gt;훈련은 Adam에 $\beta=(0.9, 0.999), \varepsilon=10^{-6}$으로 진행, warm up 은 $10000$ 스텝에 $5\cdot 10^{-4}$까지 피크로 가다가 역제곱근의 형태로 내려간다.&lt;/li&gt;
  &lt;li&gt;훈련 스텝은 300k 각 epoch 마다 validation 진행하고, 가장 좋은 5개의 checkpoint를 평균내서 최종모델을 만든다.&lt;/li&gt;
  &lt;li&gt;Decoding을 비교하기 위해서 autoregressive 모델에서 beam search($b=5$), 논문의 모델은 $l=5$개의 후보를 사용해서 decoding했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;42-translation-quality&quot;&gt;4.2 Translation Quality&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1sdffQlPXdG9Fgs_O_xgE1xfn-kxIPKmL&quot; alt=&quot;[그림4] Paper Table 1 &amp;amp; 2&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림4] Paper Table 1 &amp;amp; 2&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;같은 non-autoregressive 방법들 중에서 논문의 모델이 가장 높은 BLEU score를 달성했다고 주장하고 있다.&lt;/li&gt;
  &lt;li&gt;다른 non-autoregressive 방법들을 확인 해봐야 더 자세히 알것 같다.
    &lt;ul&gt;
      &lt;li&gt;NAT w/ Fertility (&lt;a href=&quot;https://arxiv.org/abs/1802.06901&quot;&gt;Gu et al., 2018&lt;/a&gt;)&lt;/li&gt;
      &lt;li&gt;CTC Loss (&lt;a href=&quot;https://arxiv.org/abs/1811.04719&quot;&gt;Libovicky et al., 2018&lt;/a&gt;)&lt;/li&gt;
      &lt;li&gt;Iterative Refinement(&lt;a href=&quot;https://www.aclweb.org/anthology/D18-1149/&quot;&gt;Lee et al., 2018&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;43-decoding-speed&quot;&gt;4.3 Decoding Speed&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1pqAa4SEq-SNleVeUp0IUkcTrzQFqpjLr&quot; alt=&quot;[그림5] Paper Figure 2&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림5] Paper Figure 2&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;파란점은 논문저자들의 실험 결과며, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L2R b=1&lt;/code&gt;는 beam search(b=1)를 사용한 Left-to-Right(autoregressive) 모델이다.&lt;/li&gt;
  &lt;li&gt;Decoding 스피드와 퍼포먼스간의 trade-off 를 이야기하면, $T=4, l=2$인 경우 2 point의 퍼포먼스를 대가로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L2R b=5&lt;/code&gt;모델 보다 3배의 스피드를 끌어 올릴 수 있다고 주장한다.&lt;/li&gt;
  &lt;li&gt;beyondBERT에서 나온 리뷰중에 하나가 2 point BELU score 면 엄청나게 큰 점수라고 한다(quality 가 상당히 떨어질 수도?!).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;5-analysis&quot;&gt;5. Analysis&lt;/h1&gt;

&lt;h2 id=&quot;51-why-are-multiple-iterataions-necessary&quot;&gt;5.1 Why Are Multiple Iterataions Necessary?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Various non-autoregressive 모델에서는 각 예측 토큰들이 서로 조건부 독립이라는 큰 가정이 들어간다. 때문에 예측할때 서로 다른 토큰에 영향을 받지 않아서 다른 위치라도 높은 확률로 같은 토큰을 반복적으로 예측하는 문제가 생긴다.&lt;/li&gt;
  &lt;li&gt;이러한 문제를 Multi-modality 문제라고 &lt;a href=&quot;https://arxiv.org/abs/1711.02281&quot;&gt;Gu et al., 2018&lt;/a&gt;의 논문에서 이야기 한적이 있다.&lt;/li&gt;
  &lt;li&gt;저자들은 예측한 토큰을 모델의 입력으로 사용하여, 반복적인 masking-predict 수행을 통해(multi-modal distribution을 uni-modal distribution으로 전환) 이 문제를 완화시키려고 했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1G5DV3t-J1dp6-nOdx9Nh1Vwo-XXy6xpx&quot; alt=&quot;[그림6] Paper Table 3&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림6] Paper Table 3&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;가설을 검증하기 위해서, Proxy Metric으로 중복 된 예측 토큰의 개수가 몇 퍼센트를 차지하는지 살펴보았다. 확실히 $T$가 높아질 수록 해당 비율은 현저하게 줄어든다.&lt;/li&gt;
  &lt;li&gt;$T$가 작을 수록(중복된 토큰 예측이 많아질 수록) BLEU score가 현저하게 낮아지는 것도 이해가 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;52-do-longer-sequence-need-more-iterations&quot;&gt;5.2 Do Longer Sequence Need More Iterations?&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1qf97x-hNgWXtl8NbHJ-z7iUG0KZ-uNg2&quot; alt=&quot;[그림7] Paper Table 4&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림7] Paper Table 4&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;긴 문장일 수록 더 많은 iteration이 도움이 되긴했다. 그러나 $T$가 많아질 수록 연산비용이 많아지는 것을 고려해야 할 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;53-do-more-length-candidates-help&quot;&gt;5.3 Do More Length Candidates Help?&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=127K3YP4LlTrPHEiu9XO5v7U_rlrByMlA&quot; alt=&quot;[그림8] Paper Table 5&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림8] Paper Table 5&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;적당한 길이 후보($\mathcal{l}$)는 번역에 도움이 되지만 너무 많은 후보를 두면 도움이 안된다.&lt;/li&gt;
  &lt;li&gt;상식적으로 후보들이 비슷한 길이를 가진다면 예측에 도움이 되겠지만, 많은 후보들 중에 비슷하지 않은 길이들이 있다면, 번역의 품질이 떨어질 수밖에 없을 것 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;개인적-리뷰-및-결론&quot;&gt;개인적 리뷰 및 결론&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;non-autoregressive 모델의 decoding은 실무에서 빠르게 decoding 할 수있기 때문에 앞으로 연구할 가치가 있는 분야인것 같다.&lt;/li&gt;
  &lt;li&gt;이런 분야도 있다는 것을 처음 접해서 신선한 decoding 방법이라고 생각했다. 다른 decoding 방법들(&lt;a href=&quot;https://www.facebook.com/groups/ChatbotDevKR/permalink/1000241780393951/&quot;&gt;챗봇 코리아 게시물&lt;/a&gt;)도 참고하면 좋을 것 같다.&lt;/li&gt;
  &lt;li&gt;그러나 해결되지 않은 몇 가지 문제(multi-modality)를 해결할 필요가 있어 보인다.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 19 Jul 2020 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/07/19/maskpredict.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/07/19/maskpredict.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>대학원 1학기 후기</title>
        <description>&lt;p&gt;2020년이 시작한지 얼마 안된것 같은데, 벌써 데이터 사이언스 대학원에 들어온 지 거의 4개월이 지났다. 올해 첫 개설하는 대학원으로 기대반 걱정반으로 시작했는데, 우선 첫 학기는 잘 마무리한것 같다. 숨가쁘게 달려온 첫학기에 어떻게 지냈는지 이야기를 해보려고 한다.&lt;/p&gt;

&lt;h1 id=&quot;한-학기동안-배운것&quot;&gt;한 학기동안 배운것&lt;/h1&gt;

&lt;p&gt;이번 학기에 총 4개의 수업들 들었다. 사실상 수업이 많지 않아 보이지만, 올해 코로나로 인해, 수업 방식이 100% 온라인으로 전환되면서, 사실상 온라인 수업-과제 및 프로젝트의 무한 루프였다. 여유로울것 같아 보이는데, 생각보다 빠듯했다. 앞으로 연구까지하면 시간을 잘 배분해야 할 것 같다.&lt;/p&gt;

&lt;p&gt;1학기 후기를 요약하자면&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;프로젝트에 사용되는 데이터를 전처리 하거나 탐색하는 시간이 제일 많았던 것 같았다.&lt;/li&gt;
  &lt;li&gt;머리속에 있는 로직을 코드로 구현하는데 시간이 생각보다 오래걸린 다는 것을 다시 한번 확인하게 됐다.&lt;/li&gt;
  &lt;li&gt;다양한 프로그램을 사용해보았는데(Neo4j, SAP HANA, Spark 등등), 튜토리얼과 Documentation의 중요성을 다시 한번 깨달은 수업들이 있었다(왠만하면 사람들이 많이 쓰는 것을 쓰자).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1학기에 들었던 수업을 간략하게 정리하면 다음과 같다.&lt;/p&gt;

&lt;h2 id=&quot;빅데이터-및-지식-관리-시스템&quot;&gt;빅데이터 및 지식 관리 시스템&lt;/h2&gt;

&lt;p&gt;이 수업은 “데이터가 어떻게 저장되고 관리되어야하는가?” 라고 한마디로 표현할 수 있겠다. 밑단에서 데이터 저장을 위해, 필요한 정보를 효율적으로 저장하는 방법을 배웠다. 주요 개념들을 나열하면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;relational model: (In-Memory) DBMS, SQL&lt;/li&gt;
  &lt;li&gt;storage: 데이터가 저장되는 방식 contiguous vs paged, row store vs column store&lt;/li&gt;
  &lt;li&gt;concurrency control: 데이터베이스의 버전 관리 시스템&lt;/li&gt;
  &lt;li&gt;logging: 컴퓨터가 예기치 못한 상황에서 종료 됐을 때 복구하는 방법, undo logging vs redo logging&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;데이터-사이언스를-위한-소프트웨어-플랫폼&quot;&gt;데이터 사이언스를 위한 소프트웨어 플랫폼&lt;/h2&gt;

&lt;p&gt;이 수업은 초중반에는 알고리즘 및 자료구조를 배우고, 중후반에는 데이터 분석 및 딥러닝에 사용되는 NumPy, pandas, PyTorch, Tensorflow 등 패키지를 다룬다.&lt;/p&gt;

&lt;p&gt;개인적으로 학부전공이 경제학이라 자료구조 및 알고리즘등을 몰랐는데, 이번 수업을 통해서 일부이지만 조금 체계적으로 배운것 같다. 매주 과제를 내줘서 컴공에 근접한 수업(실제로 들어보진 않았지만 소문으로는 ㅎㅎ)인것 같다.&lt;/p&gt;

&lt;h2 id=&quot;데이터-사이언스를-위한-머신러닝-및-딥러닝&quot;&gt;데이터 사이언스를 위한 머신러닝 및 딥러닝&lt;/h2&gt;

&lt;p&gt;사실상 머신러닝과 딥러닝 알고리즘을 전체적으로 훑어보는 과정이다. 너무 많은 것을 다루다보니 초심자에게는 조금 버거웠던 수업인것 같다. 다만 다양한 생각해보지 못했던 프로젝트들을 경험해서 좋았다. 예를 들어, 코로나와 관련된 CNN 뉴스 등을 보고 분석글을 쓴다던지, “오늘 저녁에 뭐먹지?”라는 주제로 수도코드를 작성하기 등이 있었다. 이런 프로젝트를 통해서 데이터를 다루는 사람은 어떤 생각을 가져야하는지 배운것 같다. 이미 주어진 문제에 대해서 모델링 하는것도 중요하지만, 실생활과 연관된 다양한 프로젝트를 통해, 문제를 어떻게 정의할 것인지, 데이터 탐색을 하고 어떻게 해결할 것인지를 더 많이 배운 수업이다.&lt;/p&gt;

&lt;h2 id=&quot;데이터-사이언스-세미나-특강&quot;&gt;데이터 사이언스 세미나 특강&lt;/h2&gt;

&lt;p&gt;세미나 특강은 Google, IBM, NYU 등 유명한 기업이나 대학교에서 다양한 분야의 전문가 분들께서 오셔서 강의를 해주시는 수업이다. 개인적으로 조경현 교수님의 “learning to finding evidence” 강의가 제일 좋았었는데, 이전 회사에서 했던 일도 있고, 앞으로 하고싶은 분야의 이야기를 해주셔서 더 관심을 가졌던것 같다.&lt;/p&gt;

&lt;p&gt;일부 세미나 영상은 아래 사이트에서 공개하고 있으니 관심가지는 주제가 있다면 시청을 추천한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://gsds.snu.ac.kr/ko/board/seminar_info/view/122&quot;&gt;데이터 사이언스 세미나 특강 링크&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;지도교수-시스템&quot;&gt;지도교수 시스템&lt;/h1&gt;

&lt;p&gt;이 대학원은 일반 대학원과 약간 다른 지도교수 시스템을 가지고 있다. 기존에는 입학전에 다른 교수님에게 컨택해서 이야기를 나누는 반면, 여기는 입학 후 1학기가 지난 시점에서 지도교수를 선택하는 시스템이다. 2학기가 끝나기 전에 한번더 선택할 수 있는 기간이 있어서, 1학기에 선택하지 못했다면 미뤄도 된다. 다만 이번이 첫 개설이고 첫학기라 더 다양한 수업과 다른 교수님들을 만나지 못해서 조금 아쉬운 점이 있다. 차후에 다양한 분야의 교수님께서 오신다면 선택의 폭이 조금 넓어질 것 같다.&lt;/p&gt;

&lt;h1 id=&quot;향후-계획과-다짐&quot;&gt;향후 계획과 다짐&lt;/h1&gt;

&lt;p&gt;앞으로 올해 초에 공부 했던 XAI 쪽으로 더 공부와 연구를 해볼 생각이다(물론 NLP 도 놓고 싶지 않아서 현재 논문스터디도 하고 있다만… 욕심가득…). 이 분야의 발전이 워낙 빨라서 끈기있게 더 달려야할 시기인것 같다. 나 자신을 뒤돌아 봤을 때 후회가 남지 않도록, 남은 일년 반 동안 더 열심히 성장 해야겠다.&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Jul 2020 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/others/2020/07/05/gsds1.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/others/2020/07/05/gsds1.html</guid>
        
        
        <category>others</category>
        
      </item>
    
      <item>
        <title>[Algorithms] Comparison Sort</title>
        <description>&lt;h1 id=&quot;comparison-sort&quot;&gt;Comparison Sort&lt;/h1&gt;

&lt;p&gt;지금까지 봐온 정렬 알고리즘들은 원소들간의 어떤 추상적인 비교연산을 통해 순서를 정하기 때문에 &lt;a href=&quot;https://en.wikipedia.org/wiki/Comparison_sort&quot;&gt;비교 정렬(comparison sort)&lt;/a&gt;이라고 한다. 이번 포스팅에서는 각 정렬방법들의 비교를 해보려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;각-정렬-방법들간-비교&quot;&gt;각 정렬 방법들간 비교&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Sort&lt;/th&gt;
      &lt;th&gt;Worst $T(N)$&lt;/th&gt;
      &lt;th&gt;Best $T(N)$&lt;/th&gt;
      &lt;th&gt;Stability&lt;/th&gt;
      &lt;th&gt;Inplace&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Bubble&lt;/td&gt;
      &lt;td&gt;$O(N^2)$&lt;/td&gt;
      &lt;td&gt;$O(N^2)$&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Insertion&lt;/td&gt;
      &lt;td&gt;$O(N^2)$&lt;/td&gt;
      &lt;td&gt;$O(N^2)$&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Selection&lt;/td&gt;
      &lt;td&gt;$O(N^2)$&lt;/td&gt;
      &lt;td&gt;$O(N^2)$&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Merge&lt;/td&gt;
      &lt;td&gt;$O(N\log N)$&lt;/td&gt;
      &lt;td&gt;$O(N\log N)$&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quick&lt;/td&gt;
      &lt;td&gt;$O(N^2)$&lt;/td&gt;
      &lt;td&gt;$O(N\log N)$&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;각-정렬-방법들의-시간-복잡도-비교&quot;&gt;각 정렬 방법들의 시간 복잡도 비교&lt;/h2&gt;

&lt;p&gt;각 정렬 방법들의 시간 복잡도를 비교하기 위해서 다음과 같은 실험을 하였다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;입력 리스트의 크기 n 은 2000 부터 시작하여 17000 까지 1000 개씩 추가하여, 총 16 개 크기로 진행한다.&lt;/li&gt;
  &lt;li&gt;각 입력 크기마다 10회 실험을 진행하고, 리스트는 0 ~ 2000 사이의 숫자로 랜덤 샘플링을 하여 구성한다(2000 포함). 최종 수치는 10회 실험의 평균 값으로 결정한다.&lt;/li&gt;
  &lt;li&gt;각 알고리즘이 실행되는 실제 시간(t로 표기)은 time 패키지로 시작시간과 끝나는 시간의 차이로 측정한다.&lt;/li&gt;
  &lt;li&gt;입력크기에 따라 차이나 너무 크게 나서 기존 수치과 log로 변환한 수치를 같이 본다. 각 복잡도 수치를 log로 치환하면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림1&lt;/code&gt;과 같다.&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=129gdSPce6z6nLokn6uJPZszfBt9Qa4G-&quot; alt=&quot;[그림1] Sorting Experiment&quot; width=&quot;110%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림1] Sorting Experiment&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;실험진행의 결과는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림2&lt;/code&gt;와 같다. selection sort와 insertion sort는 비슷한 시간을 가진다는 것을 알 수있다. 반면 bubble sort는 두 번의 for문을 모두 꼭 돌아야하기 때문에 이들 보다 실행히간이 많이 걸린다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1dPN8TOjaW3wzEEz62i80fVOj0WOjmJQc&quot; alt=&quot;[그림2] 모든 알고리즘의 Time Complexity&quot; width=&quot;110%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림2] 모든 알고리즘의 Time Complexity&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;merge sort와 quick sort는 비슷한 실행시간을 가진다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1HQsKgKLFKRSrK8hyRzIKtlyAK9gFP3fk&quot; alt=&quot;[그림3] merge sort와 quick sort의 Time Complexity&quot; width=&quot;110%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림3] merge sort와 quick sort의 Time Complexity&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;stability&quot;&gt;Stability&lt;/h2&gt;

&lt;p&gt;Stability는 Stable 과 Unstable 두 가지로 나뉘는데, 정렬 후에도 기존 입력 시퀀스의 특성 또한 그 순서을 유지하는 것이 stable sort, 그렇지 않은 것을 unstable sort라고 한다. 다음 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림4&lt;/code&gt;처럼 카드를 숫자의 순서대로 정렬하려고 한다. 각 카드는 고유의 문양이 같이 있다. 기존의 문양 순서대로 정렬되면 stable sort라고 한다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1fPY3iG4szY1UxiB6D6c_yn4x7aL_yFfr&quot; alt=&quot;[그림4] 카드 정렬하기&quot; width=&quot;110%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림4] 카드 정렬하기&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;각 정렬 알고리즘의 stability를 확인해보면 Bubble, Insertion, Merge는 stable sort고, Selection과 Quick은 unstable sort다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;colver&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;diamond&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;diamond&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;heart&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;spade&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;spade&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bubble_sort_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'heart'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colver'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;insertion_sort_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'heart'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colver'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;selection_sort_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'heart'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colver'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge_sort_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'heart'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colver'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quick_sort_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'diamond'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'heart'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colver'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'spade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;references&quot;&gt;References:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;본 글은 기본적으로 서울대학교 이재진 교수님의 강의를 듣고 제가 공부한 것을 정리한 글입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;관련-포스팅&quot;&gt;관련 포스팅:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/02/bubbleinsertion.html&quot;&gt;Bubble Sort &amp;amp; Insertion Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/02/selection.html&quot;&gt;Selection Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/03/merge.html&quot;&gt;Merge Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/04/quick.html&quot;&gt;Quick Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;(현재글)&lt;a href=&quot;https://simonjisu.github.io/python/2020/06/06/comparisonsort.html&quot;&gt;Comparsion Sort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 06 Jun 2020 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/python/2020/06/06/comparisonsort.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/python/2020/06/06/comparisonsort.html</guid>
        
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>Binary Relation</title>
        <description>&lt;p&gt;이번 포스팅에서는 이항관계(Binary Relation)의 수학적 정의에 대해서 알아보려고 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Binary_relation&quot;&gt;이항관계(Binary Relation)&lt;/a&gt;&lt;/strong&gt; 란 다음과 같은 두 집합의 곱집합의 부분집합으로 정의된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;순서쌍(Ordered pairs): 두 집합($A, B$)이 있고, 두 집합의 곱집합(Cartesian product, $A\times B$)에 속한 원소 $\lbrace (a, b) \vert a \in A, b\in B \rbrace$ 를 순서쌍이라고 한다.&lt;/li&gt;
  &lt;li&gt;관계 집합 $R$의 원소가 순서쌍이라면, $R$을 이항관계라고 한다. $(a, b) \in R$를 $aRb$로 표기한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;어떤 집합 $A$에 이항관계 $R$이 있다고 말하는 것은 $R$은 $A\times A$의 부분 집합이라고 말하는 것과 같다.&lt;/p&gt;

&lt;p&gt;예를 들어, 자연수 집합($\Bbb{N}$)에 대하여 “…보다 작다”라는 관계 $\lt$는 다음과 같이 정의된다.&lt;/p&gt;

\[\lt =\{ (a, b): a, b \in \Bbb{N} \text{  and  } a \lt b \}\]

&lt;p&gt;예를 들어, 순서쌍 $(1, 3)$은 $aRb = 1 \lt 3$ 이라는 관계를 만족하기 때문에(당연히 자연수 집합의 원소 조건도 만족), 따라서 $(1, 3)$은 이항관계 $\lt$에 속한다고 말할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;properties-of-relations&quot;&gt;Properties of Relations&lt;/h1&gt;

&lt;h2 id=&quot;reflexive-binary-relations&quot;&gt;Reflexive Binary Relations&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/반사관계&quot;&gt;반사(Reflexive) 이항관계&lt;/a&gt;&lt;/strong&gt;란 모든 집합 $A$에 속하는 원소 $a$에 대하여 $aRa$ 관계를 만족하는 이항관계를 말한다. 자기 자신과의 relation 성립여부로 생각하면 편하다.&lt;/p&gt;

&lt;p&gt;예를 들어 이항관계 $\leq$은 다음과 같이 정의되는데, 순서쌍 $a=4$일 경우 $aRa = 4 \leq 4$이기 때문에 reflexive하다고 말할 수 있다.&lt;/p&gt;

\[\leq = \lbrace (a, b): a, b \in \Bbb{N} \text{  and  } a \leq b \rbrace\]

&lt;p&gt;반면, 이전에 본 이항관계 $\lt$는 $aRa$를 만족할 수 없기 때문에 reflexive 하지 않다고 말할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;symmetric--antisymmetric-binary-relations&quot;&gt;Symmetric &amp;amp; Antisymmetric Binary Relations&lt;/h2&gt;

&lt;p&gt;$a, b \in A$에서 $aRb$이면 $bRa$일 경우, 이러한 관계 $R$을 &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Symmetric_relation&quot;&gt;대칭(Symmetric)관계&lt;/a&gt;&lt;/strong&gt;라고 한다.&lt;/p&gt;

\[\forall a, b \in A, aRb \Rightarrow bRa\]

&lt;p&gt;예를 들어, 순서쌍 $(1, 1)$과 “같다”($=$)라는 관계에 대해서, $1^{ left } = 1^{ right }$ 는 $1^{ right } = 1^{ left }$를 의미하기 때문에, “같다”관계는 Symmetric Relation 이다.&lt;/p&gt;

&lt;p&gt;$a, b \in A$에서 $aRb$와 $bRa$가 $a=b$를 도출한다면, $R$은 &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Antisymmetric_relation&quot;&gt;반대칭(Antisymmetric)관계&lt;/a&gt;&lt;/strong&gt;라고 한다. 반대칭관계는 대칭관계의 반대가 아니라는 점을 명심하자(한글보다는 영어로 쓰는게 더 이해가 쉽다).&lt;/p&gt;

\[\forall a, b \in A, aRb \land bRa \Rightarrow a=b\]

&lt;p&gt;예를 들어, 순서쌍 $(1, 1)$과 “…보다 작거나 같다”라는 관계 $\leq$는 $1^{ left } \leq 1^{ right }$ 와 $1^{ right } \leq  1^{ left }$ 로부터 $1^{ left } = 1^{ right }$ 를 도출할 수 있기 때문에, Antisymmetric Relation 이다.&lt;/p&gt;

&lt;h2 id=&quot;transitive-binary-relations&quot;&gt;Transitive Binary Relations&lt;/h2&gt;

&lt;p&gt;모든 $a, b, c \in A$에 대해서 $aRb$와 $bRc$ 관계로부터 $aRc$를 도출할 경우 &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Transitive_relation&quot;&gt;추이적(Transitive) 관계&lt;/a&gt;&lt;/strong&gt;라고 한다(영어가 더 직관적이다).&lt;/p&gt;

\[\forall a, b, c \in A, aRb \land bRc \Rightarrow aRc\]

&lt;p&gt;예를 들어, $a=1, b=2, c=3$일때, “…보다 작거나 같다”라는 관계 $\leq$는 Transitive Relation이다. 왜냐면 $1 \leq 2$와 $2 \leq 3$로부터 $1 \leq 3$을 추론할 수 있기 때문이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;equivalence-relations&quot;&gt;Equivalence Relations&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Equivalence_relation&quot;&gt;동치(Equivalence) 관계&lt;/a&gt;&lt;/strong&gt;는 Reflexive, Symmetric, Transitive 관계를 모두 만족하는 관계를 말한다.&lt;/p&gt;

&lt;p&gt;예를 들어, 자연수 $a, b, c$에 대하여($\lbrace a, b, c \rbrace \in \Bbb{N}$), “같다”($=$)는 동치 관계라고 말할 수 있다. 그 이유는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reflexive: $a = a$&lt;/li&gt;
  &lt;li&gt;Symmetric: $a=b$는 곧 $b=a$이다.&lt;/li&gt;
  &lt;li&gt;Transitive: $a=b$와 $b=c$로부터 $a=c$를 알 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;equivalence-classes&quot;&gt;Equivalence Classes&lt;/h2&gt;

&lt;p&gt;관계 $R$이 집합 $A$($a\in A$)에 대하여 동치 관계를 만족하는 경우, $a$의 &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Equivalence_class&quot;&gt;동치류(Equivalence Class)&lt;/a&gt;&lt;/strong&gt;는 집합 $[a]={b\in A : aRb}$으로 정의할 수 있는데, 이는 해당 집합의 모든 원소가 $a$와 동등(equivalent)하다고 할 수 있다.&lt;/p&gt;

&lt;p&gt;자연수 $a, b$와 $a+b$가 짝수인 관계 $R$을 예로 들어본다.&lt;/p&gt;

\[R = \lbrace (a, b): a, b \in \Bbb{N} \text{  and  } a+b \ \text{ is even number} \rbrace\]

&lt;p&gt;해당 관계에 만족하는  equivalence relation&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Reflexive: $aRa$가 짝수 인지, 예)&lt;/p&gt;

\[3 + 3 = 6(\text{even})\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Symmetric: $aRb$가 짝수 일때 $bRa$도 짝수인지, 예)&lt;/p&gt;

\[3 + 5 = 8(\text{even}) \Rightarrow 5 + 3 = 8(\text{even})\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transitive: $aRb$, $bRc$가 짝수 일때, $aRc$도 짝수인지, 예)&lt;/p&gt;

\[[3+5=8(\text{even})] \land [5+7=12(\text{even})] \Rightarrow 3+7 = 10(\text{even})\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transitive의 증명 예시: &lt;a href=&quot;https://math.stackexchange.com/questions/311151/equivalence-relation-even-number&quot;&gt;링크&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이때 자연수 4의 equivalence class는 $[4] = \lbrace 0, 2, 4, 6, \cdots \rbrace$라고 할 수 있다. 왜냐면 $b = 0, 2, 4, \cdots$등 짝수와 $4$를 더하면($4Rb$) 짝수가 나오기 때문이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;partial-order--total-order&quot;&gt;Partial Order &amp;amp; Total Order&lt;/h1&gt;

&lt;h2 id=&quot;partial-order&quot;&gt;Partial Order&lt;/h2&gt;

&lt;p&gt;집합 $X$에서 Reflexive, Antisymmetric, Transitive를 만족하는 관계 $R$을 &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Partially_ordered_set&quot;&gt;부분 순서(Partial Order)&lt;/a&gt;&lt;/strong&gt;라고 한다. 또한, $(X, R)$을 부분 순서 집합(Partial Ordered Set)이라고 한다.&lt;/p&gt;

&lt;p&gt;예를 들어, 집합 $X = \lbrace a, b, c \rbrace$의 멱집합(power set) $P$와 포함 관계$\subseteq$는 부분 순서 집합이라고 할 수 있다.&lt;/p&gt;

\[P = \lbrace 
 \emptyset , \lbrace a \rbrace, \lbrace b \rbrace, \lbrace c \rbrace, \lbrace a, b \rbrace, \lbrace b, c \rbrace, \lbrace c, a \rbrace, \lbrace a, b, c \rbrace
\rbrace\]

&lt;p&gt;그 이유는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Reflexive:&lt;/p&gt;

\[\forall \lbrace x \rbrace \in P,\quad \lbrace x \rbrace \subseteq \lbrace x \rbrace\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Antisymmetric:&lt;/p&gt;

\[\forall \lbrace x \rbrace, \lbrace y \rbrace \in P,\quad (\lbrace x \rbrace \subseteq \lbrace y \rbrace) \land (\lbrace y \rbrace \subseteq \lbrace x \rbrace) \Rightarrow (\lbrace x \rbrace = \lbrace y \rbrace)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transitive:&lt;/p&gt;

\[\forall \lbrace x \rbrace, \lbrace y \rbrace, \lbrace z \rbrace \in P,\quad (\lbrace x \rbrace \subseteq \lbrace y \rbrace) \land (\lbrace y \rbrace \subseteq \lbrace z \rbrace) \Rightarrow (\lbrace x \rbrace \subseteq \lbrace z \rbrace)\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;total-order&quot;&gt;Total Order&lt;/h2&gt;

&lt;p&gt;모든 $a, b \in A$에 대해서 $aRb$ 혹은 $bRa$을 만족할 때(혹은 둘다 만족), 집합 $A$에 대한 관계 $R$을 &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Total_order&quot;&gt;전순서(Total Order)&lt;/a&gt;&lt;/strong&gt;라고 한다. 또한, $(A, R)$을 전순서 집합(Total Ordered Set)이라고 한다.&lt;/p&gt;

&lt;p&gt;이전에 이야기한 $X = \lbrace a, b, c \rbrace$의 부분 순서 집합 $(P, \subseteq)$의 예시 경우, $\lbrace a \rbrace \not\subseteq \lbrace b \rbrace$ 이기 때문에 전순서 집합이 아니라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;전순서 집합의 예시를 들자면 다음과 같다. 만약 $Y = \emptyset$ 라면, $Y$의 멱집합 $P = \lbrace \emptyset \rbrace$ 인데, 이때 $\emptyset \subseteq \emptyset$ 이기 때문에(반대도 마찬가지), 집합 $Y$에 대한 관계 $\subseteq$는 전순서(Total Order)이라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;증명 예시: &lt;a href=&quot;https://math.stackexchange.com/questions/2454424/showing-that-px-subseteq-is-a-partial-order-total-order-or-lattice&quot;&gt;링크&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;References:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;본 글은 기본적으로 서울대학교 이재진 교수님의 강의를 듣고 제가 공부한 것을 정리한 글입니다.&lt;/li&gt;
  &lt;li&gt;Introduction to Algorithms, by Thomas H. Cormen (In Appendix)&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 14 May 2020 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/math/2020/05/14/relation.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/math/2020/05/14/relation.html</guid>
        
        
        <category>math</category>
        
      </item>
    
      <item>
        <title>[Algorithms] Quick Sort</title>
        <description>&lt;h1 id=&quot;quick-sort&quot;&gt;Quick Sort&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;퀵 정렬(Quick Sort)&lt;/strong&gt;도 분할정복(devide &amp;amp; conquer) 알고리즘 중에 하나다. 전체 프로세스는 합병 정렬과 같은데 프로세스는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Devide
 피벗(pivot)원소를 골라서, 입력을 피벗원소보다 작은 쪽은 왼쪽, 피벗보다 큰쪽은 오른쪽으로 나눈다. 따라서 무조건 반으로 나뉘는 합병 정렬과 달리 좌우 입력의 길이가 달라질 수 도 있다.&lt;/li&gt;
  &lt;li&gt;Conquer
 재귀적으로 두 파트를 다시 분할하고 정렬 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림1&lt;/code&gt;과 같이 알고리즘의 각 스텝을 표현할 수 있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1EhKY4ujGLYJk7J5sX-IjTpuPSKI_1w1Q&quot; alt=&quot;[그림1] Quick Sort-1&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림1] Quick Sort-1&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;각 스텝에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition&lt;/code&gt;이라는 함수를 재귀적으로 호출하게 된다. 예를 들어 첫번째 스텝의 결과는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림2&lt;/code&gt;의 과정을 통해 얻는다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=17ikyWsTHY_CgYu28Y2ciahXoi_Id5sCe&quot; alt=&quot;[그림2] Quick Sort-2&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림2] Quick Sort-2&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;보통 입력의 제일 첫번째 원소를 피벗(pivot)으로 두고 정렬을 시작한다. 그리고 비교대상의 인덱스 &lt;strong&gt;j&lt;/strong&gt;의 순환을 통해 피벗의 위치 인덱스 &lt;strong&gt;m&lt;/strong&gt;을 찾는 과정을 거친다. &lt;strong&gt;j&lt;/strong&gt;가 순환하면서 피벗보다 작으면, &lt;strong&gt;j-1&lt;/strong&gt;번째 원소와 교환(swap)을 하고, &lt;strong&gt;m&lt;/strong&gt;을 하나씩 증가 시킨다. 마지막으로 &lt;strong&gt;j&lt;/strong&gt;의 순환이 끝나면 &lt;strong&gt;m&lt;/strong&gt;의 위치해 있는 원소와 피벗을 교환한다&lt;/p&gt;

&lt;p&gt;재귀함수가 호출 될때마다 하나의 원소가 피벗으로 위치가 정해지기 때문에, 원소가 하나가 남게되면 재귀가 끝나게 된다. 조금더 큰 예시를 들면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림3&lt;/code&gt; 과 같다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1gvg6cfbe2loulMnOcQwO8BIursi03Uf3&quot; alt=&quot;[그림3] Quick Sort-3&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림3] Quick Sort-3&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;퀵 정렬은 새로운 메모리가 필요하지 않은 inplace 알고리즘이다. 코드로 구현하면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;quick_sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;r&quot;&quot;&quot;
    Quick Sort
    Args: 
        l: input list
    Return:
        sorted list by ascending
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;r&quot;&quot;&quot;
        Partitioin the list into small and large part by pivot
        Args:
            l: input list
            low: lowest index of the partitioned list
            high: highest index of the partitioned list
        Return:
            pivot index in the list
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;r&quot;&quot;&quot;swap p-th element and q-th element&quot;&quot;&quot;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;r&quot;&quot;&quot;
        stop sorting when the `low` index is higher then `high` index 
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pivot_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot_idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot_idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;알고리즘-복잡도&quot;&gt;알고리즘 복잡도&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# k-1
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 비교: k-1
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# k-1
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 교환: k-1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 교환: 1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;먼저 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition&lt;/code&gt; 함수의 복잡도를 분석하면 다음과 같다. 만약 정렬해야할 입력 원소의 개수가  $k$ 라면, 비교와 교환을 하는데 각 $k-1$ 번이 필요하기에 총 $c \times k$ 번이 필요하다. 따라서, $T(k) = O(k)$라고 할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pivot_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot_idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot_idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort&lt;/code&gt; 함수가 몇번 재귀적으로 호출 되었는지만 계산하면 모든 계산이 끝난다. $N$ 개의 입력 원소가 있다면, 각 i 단계(level)에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition&lt;/code&gt; 함수가 얼마나 호출 되는지 생각해보자.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1Q9xmO9uGQ8791Nd_RGuzfCt7HdwL7s-r&quot; alt=&quot;[그림4] Quick Sort-4&quot; width=&quot;60%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림4] Quick Sort-4&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림4&lt;/code&gt;와 같이 최악의 경우, 가령 이미 정렬되어 있을 때, 피벗원소와 피벗원소보다 큰 파트로 나뉠 것이며, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort&lt;/code&gt;함수는 총 $N$ 번이 호출된다. 즉, 총 단계의 깊이는 $N$ 이라고 생각할 수 있다. 또한, 각 단계에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition&lt;/code&gt;은 입력 길이 $k$에 의존하기 때문에, 이를 고려하면 $T(N) = O(N + (N-1) + (N-2) + \cdots + 1)) = O(N^2)$ 만큼의 복잡도를 가진다고 할 수 있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1FZn63RSPIkIgJKMc958mpD8jdsUxPck6&quot; alt=&quot;[그림5] Quick Sort-5&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림5] Quick Sort-5&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;최상의 경우는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림5&lt;/code&gt;와 같이 모든 원소가 균등하게 나눠졌을 경우, 더이상 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition&lt;/code&gt;함수를 호출하지 않는 1개의 피벗을 제외하면, 총 $\log_2 N$ 단계가 있다. 각 i 단계(level)에서 $O(2(N-(2\log_2 N -1))) = O(N)$ 만큼의 시간이 걸려서, 최종적인 시간 복잡도는 $T(N) = O(N \times \log_2 N)$ 이다.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;본 글은 기본적으로 서울대학교 이재진 교수님의 강의를 듣고 제가 공부한 것을 정리한 글입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;관련-포스팅&quot;&gt;관련 포스팅:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/02/bubbleinsertion.html&quot;&gt;Bubble Sort &amp;amp; Insertion Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/02/selection.html&quot;&gt;Selection Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/03/merge.html&quot;&gt;Merge Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;(현재글)&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/04/quick.html&quot;&gt;Quick Sort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 04 May 2020 11:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/python/2020/05/04/quick.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/python/2020/05/04/quick.html</guid>
        
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>[Algorithms] Merge Sort</title>
        <description>&lt;h1 id=&quot;merge-sort&quot;&gt;Merge Sort&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;합병 정렬(Merge Sort)&lt;/strong&gt;의 아이디어는 분할정복(devide &amp;amp; conquer)이다. 말 그대로 두 스텝으로 나뉜다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Devide
 들어온 입력을 반으로 쪼갠 다음에 반으로 나눠진 입력에 대해서 재귀적으로 합병 정렬을 실행 한다.&lt;/li&gt;
  &lt;li&gt;Conquer
 두 개의 정렬된 데이터가 입력으로 들어온다. 그러면 새로운 공간에 두 입력을 하나로 합치(merge)면서 정렬한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림1&lt;/code&gt;과 같이 알고리즘의 각 스텝을 표현할 수 있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1rrgsIUkDOKgSC3cDj8X5vT3bicah_aaz&quot; alt=&quot;[그림1] Merge Sort-1&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림1] Merge Sort-1&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;먼저 순차적으로 입력 시퀀스의 중간에 있는 값을 찾아서 원소가 1개가 될때까지 좌/우로 나눈다(devide 과정). 그 후 하나씩 합치면서 정렬을 하게되는데, 그 예시로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[2]&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1, 3]&lt;/code&gt;의 합치는 과정은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;그림2&lt;/code&gt;와 같다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1NCdPs94V6ooeidUhCX-jzIbsWDg-M8Cq&quot; alt=&quot;[그림2] Merge Sort-2&quot; width=&quot;80%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림2] Merge Sort-2&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;좌측(left)과 우측(right)의 원소를 하나씩 비교한 뒤에 새로운 리스트에 담는다. 더이상 비교할 원소가 없을때 합치는 과정은 끝난다. 이부분에서 알수 있듯이, 합병 정렬은 이전에 소개했던 알고리즘과 달리 inplace 알고리즘이 아니다. 코드로 구현하면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;merge_sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;r&quot;&quot;&quot;
    Merge Sort
    Args: 
        l: input list
    Return:
        sorted list by ascending
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;r&quot;&quot;&quot;
        Merge two sorted list into one
        Args: 
            left, right: sorted list
        Return:
            sorted list
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;len_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;idx_mid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge_sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_mid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge_sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_mid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;알고리즘-복잡도&quot;&gt;알고리즘 복잡도&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;li&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# k-1
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 비교: k-1
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (k-1/2)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (k-1/2)
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (k-1/2)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (k-1/2)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 마지막 남은 원소가 왼쪽인 경우: 1
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;idx_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 마지막 남은 원소가 오른쪽인 경우: 1
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;idx_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;먼저 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merge&lt;/code&gt; 함수의 복잡도를 분석하면 다음과 같다. 만약 입력 좌/우 리스트를 합쳐서 총 $k$ 개의 원소가 있다면, 비교를 하는데 $k-1$ 번이 필요하고, 새로운 리스트에 담는데 총 $k-1=(k-1/2)+(k-1/2)$이 걸려서 전부 합치면 대략 $c \times k$ 정도 걸린다. 따라서, $T(k) = O(k)$라고 할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;idx_mid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge_sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_mid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge_sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_mid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;li&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1yWlItt2_BSJLBd_DwH7BOdDqDAPpO0CX&quot; alt=&quot;[그림3] Merge Sort-3&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림3] Merge Sort-3&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;이제 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merge&lt;/code&gt; 함수가 몇번 호출 되었는지만 계산하면 모든 계산이 끝난다. $N$ 개의 입력 원소가 있다면, 각 i 단계(level)에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merge&lt;/code&gt; 함수가 $2^{i-1}$ 번만큼 호출 된다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Level-1 에서는 $2^0=1$번 호출되고, 각 분리된 2 파트에서 걸리는 시간 복잡도는 $O(\dfrac{N}{2})$이기 때문에, 총 시간 복잡도는 $O(1 \times 2\times \dfrac{N}{2}) = O(N)$가 된다.&lt;/li&gt;
  &lt;li&gt;Level-2 에서는 $2^1=2$번 호출되고 시간 복잡도는 $O(2 \times 2\times \dfrac{N}{2^2}) = O(N)$가 된다.&lt;/li&gt;
  &lt;li&gt;Level-3 에서는 $2^2=4$번 호출되고 시간 복잡도는 $O(2^2 \times 2\times \dfrac{N}{2^3}) = O(N)$가 된다.&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
  &lt;li&gt;Level-$(\log_2N)$ 에서는 $2^{(\log_2 N)-1}=\dfrac{N}{2}$번 호출되고 시간 복잡도는 $O(\dfrac{N}{2} \times 2\times \dfrac{N}{2^{\log_2 N}}) = O(N)$가 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 총 $\log_2 N$ 번의 단계를 거치기 때문에, 최종 시간 복잡도는 $T(N) = O(N \log_2 N)$으로 계산된다.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;본 글은 기본적으로 서울대학교 이재진 교수님의 강의를 듣고 제가 공부한 것을 정리한 글입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;관련-포스팅&quot;&gt;관련 포스팅:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/02/bubbleinsertion.html&quot;&gt;Bubble Sort &amp;amp; Insertion Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/02/selection.html&quot;&gt;Selection Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;(현재글)&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/03/merge.html&quot;&gt;Merge Sort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/python/2020/05/04/quick.html&quot;&gt;Quick Sort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 03 May 2020 14:38:38 +0900</pubDate>
        <link>https://simonjisu.github.io/python/2020/05/03/merge.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/python/2020/05/03/merge.html</guid>
        
        
        <category>python</category>
        
      </item>
    
  </channel>
</rss>
