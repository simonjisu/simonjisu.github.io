<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soo</title>
    <description>My Blog
</description>
    <link>http://simonjisu.github.io/</link>
    <atom:link href="http://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 20 Jul 2018 01:17:11 +0900</pubDate>
    <lastBuildDate>Fri, 20 Jul 2018 01:17:11 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Torchtext Tutorial</title>
        <description>&lt;h1 id=&quot;pytorch-torchtext-tutorial&quot;&gt;Pytorch TorchText Tutorial&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;튜토리얼 Notebook: &lt;a href=&quot;https://github.com/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb&quot;&gt;github&lt;/a&gt;, &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb&quot;&gt;nbviewer&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;자연어 처리에서 전처리시 자주 사용하는 패키지 하나를 소개하려고 한다.&lt;/p&gt;

&lt;p&gt;Pytorch 는 데이터를 불러오는 강력한 &lt;a href=&quot;https://pytorch.org/docs/stable/data.html&quot;&gt;Data Loader&lt;/a&gt; 라는 유틸이 있는데, TorchText 는 NLP 분야만을 위한 Data Loader 이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;documentation:&lt;/strong&gt; &lt;a href=&quot;http://torchtext.readthedocs.io/en/latest/index.html&quot;&gt;http://torchtext.readthedocs.io/en/latest/index.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;설치:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install torchtext
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;TorchText 는 자연어 처리에서 아래의 과정을 한번에 쉽게 해준다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;토크나이징(Tokenization)&lt;/li&gt;
  &lt;li&gt;단어장 생성(Build Vocabulary)&lt;/li&gt;
  &lt;li&gt;토큰의 수치화(Numericalize all tokens)&lt;/li&gt;
  &lt;li&gt;데이터 로더 생성(Create Data Loader)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;사용방법&lt;/h2&gt;

&lt;h3 id=&quot;create-field&quot;&gt;1. 필드지정(Create Field)&lt;/h3&gt;

&lt;p&gt;필드란 텐서로 표현 될 수 있는 텍스트 데이터 타입을 처리한다. 각 토큰을 숫자 인덱으로 맵핑시켜주는 단어장(Vocabulary) 객체가 있다. 또한 토큰화 하는 함수, 전처리 등을 지정할 수 있다.&lt;/p&gt;

&lt;p&gt;아래와 같은 문장과 이에 대한 긍정/부정 정도를 분류하는 데이터셋이 있다면,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[&quot;The Importance of Being Earnest , so thick with wit it plays like a reading from Bartlett 's Familiar Quotations&quot;, 
'3']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;텍스트를 뜻하는 &lt;code class=&quot;highlighter-rouge&quot;&gt;TEXT&lt;/code&gt;, 해당 문장의 sentiment 를 뜻하는 &lt;code class=&quot;highlighter-rouge&quot;&gt;LABEL&lt;/code&gt; 필드객체 두 개를 만든다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchtext.data import Field

TEXT = Field(sequential=True,
             use_vocab=True,
             tokenize=str.split,
             lower=True, 
             batch_first=True)  
LABEL = Field(sequential=False,  
              use_vocab=False,   
              preprocessing = lambda x: int(x),  
              batch_first=True)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;sequential:&lt;/strong&gt; TEXT 는 순서가 있는 (sequential) 데이터기 때문에 인자를 True 로 두고, LABEL 데이터는 순서가 필요없기 때문에 False 로 둔다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;use_vocab:&lt;/strong&gt; 단어장(Vocab) 객체를 사용할지의 여부. 텍스트 데이터있는 TEXT 에만 True 로 인자를 전달한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;tokenize:&lt;/strong&gt; 단어의 토크나이징을 맡아줄 함수다. 여기선 “공백”을 기준으로 나누는 함수를 사용했습니다. 한국어의 경우 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;konlpy&lt;/code&gt; 의 토크나이징 함수들을 사용한다. 혹은 개인이 만든 함수도 사용할 수 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;lower:&lt;/strong&gt; 소문자 전환 여부 입니다. 보통 True 로 두며, 단어가 많아질수록 나중에 더 많은 차원에 임베딩해야하기 때문에, 왠만하면 영어는 소문자로 만들어준다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;batch_first:&lt;/strong&gt; 배치를 우선시 하게 되면, tensor 의 크기는 (B, 문장의 최대 길이) 로 만들어진다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;preprocessing:&lt;/strong&gt; 전처리는 토큰화 후, 수치화하기 전 사이에서 작동한다. 여기서는 Label 데이터가 string 타입이기 때문에 int 타입으로 만들어준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;더 자세한 것은 문서를 참조하시길 바란다.&lt;/p&gt;

&lt;h3 id=&quot;create-datasets&quot;&gt;2. 데이터 세트 만들기(Create Datasets)&lt;/h3&gt;

&lt;p&gt;데이터 세트는 위에 지정한 필드에 기반하여 데이터를 불러오는 작업을 한다. 보통 Train, Valid, Test 세트가 있으면 &lt;code class=&quot;highlighter-rouge&quot;&gt;splits&lt;/code&gt; 메서드를 사용해서 아래와 같이 만들어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchtext.data import TabularDataset

train_data = TabularDataset.splits(path='./data/',
					train='train_path',
					valid='valid_path',
					test='test_path',
					format='tsv', 
					fields=[('text', TEXT), ('label', LABEL)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;만약에 없다면? 아래와 같이 객체에 그냥 넣어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_data = TabularDataset(path='./data/examples.tsv', 
				format='tsv', 
				fields=[('text', TEXT), ('label', LABEL)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;fields:&lt;/strong&gt; 아까 만들어준 필드는 리스트 형태로 &lt;code class=&quot;highlighter-rouge&quot;&gt;[('필드이름(임의지정)', 필드객체), ('필드이름(임의지정)', 필드객체)]&lt;/code&gt; 로 넣어준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;build-vocabulary&quot;&gt;3. 단어장 생성(Build vocabulary)&lt;/h3&gt;

&lt;p&gt;토큰과 Interger index 를 매칭시켜주는 단어장을 생성한다. 단, 기본적으로 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;unk&amp;gt;&lt;/code&gt; 토큰을 0, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰을 1 로 만들어준다. 단, 필드지정시, 문장의 시작 토큰(init_token)과, 끝의 토큰(eos_token)을 넣으면 3, 4 번으로 할당된다. 메서드 안에는 생성한 데이터 세트를 넣어준다.&lt;/p&gt;

&lt;p&gt;훈련 데이터를 기반으로 단어장을 생성하려면 아래의 명령어를 입력한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TEXT.build_vocab(train_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;create-data-loader&quot;&gt;4. 데이터 로더 만들기(Create Data Loader)&lt;/h3&gt;

&lt;p&gt;마지막으로 배치 사이즈 만큼 데이터를 불러올 데이터 로더를 만든다. 데이터 세트 때와 마찬가지로 데이터 세트가 분리되어 있다면, &lt;code class=&quot;highlighter-rouge&quot;&gt;splits&lt;/code&gt; 메서드를 사용한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchtext.data import Iterator

train_loader, valid_loader, test_loader = \
	TabularDataset.splits((train_data, valid_data, test_data), 
				batch_size=3, 
				device=None,  # gpu 사용시 &quot;cuda&quot; 입력
				repeat=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;만약에 없다면? 아래와 같이 &lt;code class=&quot;highlighter-rouge&quot;&gt;Iterator&lt;/code&gt; 객체에 그냥 넣어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_loader = Iterator(train_data, 
			batch_size=3, 
			device=None,  # gpu 사용시 &quot;cuda&quot; 입력
			repeat=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이렇게 하면 매 배치 때마다 최대 길이에 따라 알아서 패딩(padding) 작업도 같이 해준다. 패딩이란 문장의 길이를 같게 만들기 위해서 의미없는 토큰 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 를 나머지 길이가 부족한 문장에게 붙여주는 토큰이다. 잘 생각해보면 input 길이를 같게 만들어 주는 과정이다.&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;테스트&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for batch in train_loader:
    break
print(batch.text)
print(batch.label)
=======================================================
tensor([[   643,    191,      4,     43,   1447,      3,   4384,    485,
              7,    207,    892,    107,     43,     85,    408,      3,
            376,     17,      5,   6447,  11035,     37,     98,     43,
            199,   5859,      2,      1,      1,      1,      1,      1,
              1,      1,      1],
        [     3,   4515,     51,    444,      4,   3738,     30,     94,
            957,   3498,     59,    700,  13967,      6,   2287,   4435,
              4,    431,     40,      3,   1201,      7,    486,   1134,
           4120,     59,      5,    166,   1749,    547,      6,   1339,
            144,  14759,      2],
        [    29,      7,    195,    568,    192,     63,    229,     60,
             17,     21,    202,    334,     18,      5,    535,     20,
              4,     15,    628,    231,     52,      9,    303,    195,
           6910,      8,  10136,      8,      3,   2204,   4340,      2,
              1,      1,      1]])
tensor([ 0,  3,  1])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3개의 배치에, 각 토큰에 해당하는 단어의 숫자가 들어가게 되고, 패딩 또한 잘 되었다.&lt;/p&gt;

&lt;p&gt;이처럼 거의 5 줄이면 이 모든 과정을 처리해주는 강력크한 도구다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-2&quot;&gt;만약에 사용하지 않겠다면?&lt;/h2&gt;

&lt;p&gt;노트북: &lt;a href=&quot;https://github.com/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb&quot;&gt;github&lt;/a&gt;, &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb&quot;&gt;nbviewer&lt;/a&gt; 에 상세하게 나만의 데이터 로더를 커스터마이징 하는 방법 또한 적어 두었다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;순수 파이썬 만 사용한 코드&lt;/li&gt;
  &lt;li&gt;파이토치의 Custom Dataset 를 활용한 Data Loader 만들기&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;하지만, 여기서는 소개하지 않겠다. 한 번 TorchText를 사용하게 되면 위 두 가지 방법은 왠만하면 생각도 안날 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-3&quot;&gt;다양한 데이터 세트&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torchtext.datasets&lt;/code&gt; 안에는 자연어 처리에 많이 사용되는 데이터 세트들이 이미 포함돼있다. 여기서는 소개하지 않겠다.&lt;/p&gt;

&lt;p&gt;Documentation 참고: &lt;a href=&quot;http://torchtext.readthedocs.io/en/latest/datasets.html#&quot;&gt;http://torchtext.readthedocs.io/en/latest/datasets.html#&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sentiment Analysis&lt;/li&gt;
  &lt;li&gt;Question Classification&lt;/li&gt;
  &lt;li&gt;Entailment&lt;/li&gt;
  &lt;li&gt;Language Modeling&lt;/li&gt;
  &lt;li&gt;Machine Translation&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 19 Jul 2018 00:18:29 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/07/19/torchtext.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/07/19/torchtext.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Pytorch 의 PackedSequence object 알아보기</title>
        <description>&lt;h1 id=&quot;pytorch--packedsequence-object-&quot;&gt;Pytorch 의 PackedSequence object 알아보기&lt;/h1&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;packedsequence-&quot;&gt;PackedSequence 란?&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;아래의 일련의 과정을 PackedSequence 라고 할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;NLP 에서 매 배치(batch)마다 고정된 문장의 길이로 만들어주기 위해서 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰을 넣어야 한다. 아래 그림의 파란색 영역은 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/ctd209m9zlzs0cw/0705img1.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;사진 출처: &lt;a href=&quot;https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983&quot;&gt;Understanding emotions — from Keras to pyTorch&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그림과 같은 내용을 연산을 하게 되면, 쓸모없는 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰까지 연산을 하게 된다.
따라서 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 를 계산 안하고 효율적으로 진행하기 위해 병렬처리를 하려고한다. 그렇다면 아래의 조건을 만족해야한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN의 히든 스테이트가 이전 타임스텝에 의존해서 최대한 많은 토큰을 병렬적으로 처리해야한다.&lt;/li&gt;
  &lt;li&gt;각 문장의 마지막 토큰이 마지막 타임스텝에서 계산을 멈춰야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아직 어떤 느낌인지 잘 모르겠다면 아래의 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/3ze3svhdz05aakk/0705img3.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, 컴퓨터로 하여금 각 &lt;strong&gt;타임스텝&lt;/strong&gt;(T=배치내에서 문장의 최대 길이) 마다 일련의 단어를 처리해야한다는 뜻이다.&lt;/p&gt;

&lt;p&gt;하지만 $T=2, 3$ 인 부분은 중간에 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt;이 끼어 있어서 어쩔수 없이 연산을 하게 되는데, 이를 방지하기 위해서, 아래의 그림같이 각 배치내에 문장의 길이를 기준으로 &lt;span style=&quot;color: #e87d7d&quot;&gt;정렬(sorting)&lt;/span&gt; 후, 하나의 통합된 배치로 만들어준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/op87oonnoqegn5c/0705img2.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;사진 출처: &lt;a href=&quot;https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983&quot;&gt;Understanding emotions — from Keras to pyTorch&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;data:&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰이 제거후 합병된 데이터&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;batch_sizes:&lt;/strong&gt; 각 타임스텝 마다 배치를 몇개를 넣는지 기록해 둠&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이처럼 PackedSequence 의 &lt;strong&gt;장점&lt;/strong&gt;은 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰을 계산 안하기 때문에 더 빠른 연산을 처리 할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;pytorch---packedsequence&quot;&gt;Pytorch - PackedSequence&lt;/h2&gt;

&lt;p&gt;Pytorch 에서 사용하는 방법은 의외로 간단하다. 실습 코드는 &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/00_Basic/02_PackedSequence.ipynb&quot;&gt;nbviewer&lt;/a&gt; 혹은 &lt;a href=&quot;https://github.com/simonjisu/pytorch_tutorials/blob/master/00_Basic/02_PackedSequence.ipynb&quot;&gt;github&lt;/a&gt;에 있다.&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;과정&lt;/h3&gt;

&lt;p&gt;전처리를 통해 위 배치의 문장들을 숫자로 바꿔주었다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input_seq2idx
============================================
tensor([[  1,  16,   7,  11,  13,   2],
        [  1,  16,   6,  15,   8,   0],
        [ 12,   9,   0,   0,   0,   0],
        [  5,  14,   3,  17,   0,   0],
        [ 10,   0,   0,   0,   0,   0]])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;하단의 코드를 통해서 정렬을 해주고, 각 문장의 길이를 담은 list를 만들어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input_lengths = torch.LongTensor([torch.max(input_seq2idx[i, :].data.nonzero())+1 for i in range(input_seq2idx.size(0))])
input_lengths, sorted_idx = input_lengths.sort(0, descending=True)
input_seq2idx = input_seq2idx[sorted_idx]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;모든 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰의 인덱스인 0 이 밑으로 내려간 것을 알 수 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input_seq2idx, input_lengths
============================================
tensor([[  1,  16,   7,  11,  13,   2],
        [  1,  16,   6,  15,   8,   0],
        [  5,  14,   3,  17,   0,   0],
        [ 12,   9,   0,   0,   0,   0],
        [ 10,   0,   0,   0,   0,   0]])

tensor([ 6,  5,  4,  2,  1])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;torch.nn.utils.rnn&lt;/strong&gt; 에서 &lt;strong&gt;pack_padded_sequence&lt;/strong&gt; 를 사용하면 PackedSequence object를 얻을 수 있다. packed_input 에는 위에서 말한 합병된 데이터와 각 타임스텝의 배치사이즈들이 담겨있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;packed_input = torch.nn.utils.rnn.pack_padded_sequence(input_seq2idx, input_lengths.tolist(), batch_first=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rnn---&quot;&gt;RNN 에서의 사용 방법&lt;/h3&gt;

&lt;p&gt;실수 벡터공간에 임베딩된 문장들을 pack 한 다음에 RNN 에 input을 넣기만 하면 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;embed = nn.Embedding(vocab_size, embedding_size, padding_idx=0)
gru = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=False, batch_first=True)

embeded = embed(input_seq2idx)
packed_input = pack_padded_sequence(embeded, input_lengths.tolist(), batch_first=True)
packed_output, hidden = gru(packed_input)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;packed_output 에는 합병된 output 과 batch_sizes 가 포함되어 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;packed_output[0].size(), packed_output[1]
=========================================================
(torch.Size([18, 2]), tensor([ 5,  4,  3,  3,  2,  1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이를 다시 원래 형태의 &lt;strong&gt;(배치크기, 문장의 최대 길이, 히든크기)&lt;/strong&gt; 로 바꾸려면 &lt;strong&gt;pad_packed_sequence&lt;/strong&gt; 를 사용하면 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)
output.size(), output_lengths
=========================================================
(torch.Size([5, 6, 2]), tensor([ 6,  5,  4,  2,  1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;실습코드에서 출력 결과를 살펴보면 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰과 연관된 행은 모드 0으로 채워져 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;rnn-backend--&quot;&gt;RNN Backend 작동 방식&lt;/h2&gt;

&lt;h3 id=&quot;rnn-----&quot;&gt;RNN 안에서 어떤 방법으로 실행되는 것일까?&lt;/h3&gt;

&lt;p&gt;아래의 그림을 살펴보자&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/jl1iymxj6fdtvoe/0705img4.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;은닉층에서는 매 타임스텝마다 batch_sizes 를 참고해서 배치수 만큼 은닉층을 골라서 뒤로 전파한다.&lt;/p&gt;

&lt;p&gt;기존의 RNN 이라면, &lt;strong&gt;(배치크기 $\times$ 문장의 최대 길이 $\times$ 층의 갯수)&lt;/strong&gt; 만큼 연산을 해야하지만, &lt;strong&gt;(실제 토큰의 갯수 $\times$ 층의 갯수)&lt;/strong&gt; 만큼 계산하면 된다. 이 예제로 말하면 $(5 \times 6 \times 1)=30 \rightarrow (18 \times 1)=18$ 로 크게 줄었다.&lt;/p&gt;

&lt;h3 id=&quot;hidden---&quot;&gt;그렇다면 Hidden 어떻게 출력 되는가?&lt;/h3&gt;

&lt;p&gt;기존의 RNN 이라면 마지막 타임스텝 때 hidden vector 만 출력하지만, packed sequence 는 아래의 그림 처럼 골라서 출력하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/e1kjq4jsehbixiq/0705img5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;참고자료: &lt;a href=&quot;https://discuss.pytorch.org/t/lstm-hidden-cell-outputs-and-packed-sequence-for-variable-length-sequence-inputs/1183&quot;&gt;https://discuss.pytorch.org/t/lstm-hidden-cell-outputs-and-packed-sequence-for-variable-length-sequence-inputs/1183&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Jul 2018 09:45:37 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/07/05/packedsequence.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/07/05/packedsequence.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>PyCharm SSH 연결하기</title>
        <description>&lt;h1 id=&quot;pycharm-ssh-&quot;&gt;PyCharm SSH 연결하기&lt;/h1&gt;

&lt;p&gt;최근 딥러닝서버를 만들고 나서 Jupyter Notebook 만 사용했다. 그런데 오늘 파이참에서는 학생들을 위해 매 1년 마다 프로 버젼을 제공해주고 있다는 것을 들었다. 이를 사용해 보기로 했다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;PyCharm For Student: &lt;a href=&quot;https://www.jetbrains.com/student/&quot;&gt;https://www.jetbrains.com/student/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PyCharm Pro를 쓰면 &lt;strong&gt;Run On Remote Server&lt;/strong&gt; 기능을 쓸 수 가 있는데, 현재 운영체제인 Mac OS 에서 코드를 실행하면, 서버 Ubuntu 18.04 LTS 에서 실행 된다는 것이다. 게다가 PyCharm의 디버깅 툴도 사용할수 있다.&lt;/p&gt;

&lt;p&gt;그러나 이를 실행하는 과정이 쉬운 것은 아니었다. 아래 3개의 과정으로 설명하려고 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Interpreter 설정&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deployment 설정&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;프로젝트 연결 설정&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;interpreter&quot;&gt;Interpreter&lt;/h2&gt;

&lt;p&gt;많은 블로그에서 우선 Interpreter 설정을 진행하라고 해서 나도 따라했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/4kqy5xwpdz7qe26/0625_interpreter.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파이참 시작화면에서 오른쪽 아래 &lt;code class=&quot;highlighter-rouge&quot;&gt;Configure &amp;gt; Preferences&lt;/code&gt; 혹은 새로운 프로젝트를 만든 뒤, &lt;code class=&quot;highlighter-rouge&quot;&gt;⌘,&lt;/code&gt;를 누르자. 그러면 위와 같은 화면이 나오는데, &lt;code class=&quot;highlighter-rouge&quot;&gt;Project Interpreter&lt;/code&gt; 를 선택하자.&lt;/p&gt;

&lt;p&gt;처음에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;No interpreter&amp;gt;&lt;/code&gt; 라고 나올텐데, 옆에 &lt;code class=&quot;highlighter-rouge&quot;&gt;톱니바퀴 &amp;gt; add&lt;/code&gt; 를 눌러주자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/mewttyzbf7btzqs/0625_add_interpreter.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Host&lt;/code&gt; : IP&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Port&lt;/code&gt; : 포트번호&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Username&lt;/code&gt; : 사용자 이름&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 세가지 사항을 차례대로 입력한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/r29aktz21vy5e9g/0625_sshkey.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Private Key file&lt;/code&gt; : SSH 에서 Private key 의 위치를 써준다. 보통은 &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.ssh&lt;/code&gt; 폴더 안에 있다.&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Passphrase&lt;/code&gt; : Private Key 의 비밀번호를 넣는다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이렇게 쉽게 성공했으면 얼마나 좋았을까?&lt;/p&gt;

&lt;p&gt;그런데 여기서 아무리 연결을 하려고해도 &lt;span style=&quot;color: #e87d7d&quot;&gt;Authentication Fail&lt;/span&gt; 이라는 빨간 글씨만 보였다.&lt;/p&gt;

&lt;p&gt;구글링 결과 나와 같은 오류를 가진 사람들이 올린글이 하나 있었는데(&lt;a href=&quot;https://bit.ly/2Im44VD&quot;&gt;링크&lt;/a&gt;), 요약하면, &lt;strong&gt;“아무리 패스워드를 정확하게 입력해도 연결이 안된다. 내가 뭘 놓치고 있는거냐?”&lt;/strong&gt; 라는 글이었다.&lt;/p&gt;

&lt;p&gt;나는 아래 댓글의 방안으로 해결 했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/hu4h1mlmsuaxezh/0625_solution.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;요약하면, &lt;strong&gt;deployment server&lt;/strong&gt; 를 먼저 설정한 후에 하라는 말이었다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;deployment&quot;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Deployment&lt;/code&gt; 는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Build, Execution, Deployment&lt;/code&gt; 속성에 있었다. 아무것도 없다면 &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt; 눌러서 새로 만들자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/u59u4f4qcte59dv/0625_deployment.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SFTP&lt;/code&gt; 를 선택한 후&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SFTP host&lt;/code&gt; : IP&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Port&lt;/code&gt; : 포트번호&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Root path&lt;/code&gt; : 루트 패스인데 $HOME 위치를 설정한다.&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;User name&lt;/code&gt; : 사용자 이름&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Auth type&lt;/code&gt; : 패스워드 혹은 Key Pair(나는 ssh 키를 쓰기 때문에 이것을 선택)&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Private key&lt;/code&gt; : 키위치&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Key passphrase&lt;/code&gt; : 키 비밀번호&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 정보를 다 입력하면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Test SFTP connection&lt;/code&gt; 을 눌러봐서 테스트 해본다. 만약 통과가 되면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Apply&lt;/code&gt; 를 누르자!&lt;/p&gt;

&lt;p&gt;그리고 &lt;code class=&quot;highlighter-rouge&quot;&gt;Interpreter&lt;/code&gt; 에 돌아가서 다시 설정해준다.&lt;/p&gt;

&lt;p&gt;접속후 파이썬을 연결 해야하는데, 따로 python을 설치한게 없다면, 기본적으로 2.7 버전인 &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/python&lt;/code&gt; 패스가 설정 될 것이다.&lt;/p&gt;

&lt;p&gt;만약에 자신의 서버에서 Python3 을 따로 설치했다면 서버 terminal 에서 아래와 같이 쳐준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ which python3
/usr/local/bin/python3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;이 위치를 복사해서 쓰자.&lt;/p&gt;

&lt;p&gt;모든게 정상적으로 작동하면, &lt;strong&gt;Connection Sucessfully&lt;/strong&gt; 를 확인 할 수 가 있다.&lt;/p&gt;

&lt;p&gt;아까 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;No interpreter&amp;gt;&lt;/code&gt; 칸에 &lt;code class=&quot;highlighter-rouge&quot;&gt;remote Python 3.6.5 (sftp://[유저이름]@[IP]:[포트]/[파이썬위치])&lt;/code&gt; 가 뜨면 성공한 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section&quot;&gt;프로젝트 연결 설정 확인&lt;/h2&gt;

&lt;p&gt;자신의 프로젝트와 잘 연결 되었는지 확인 해보는 작업을 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/k04w9sa9qplxr1m/0625_openproject.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pycharm에서 새로운 프로젝트를 시작하거나 이미 존재하는 프로젝트를 오픈한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/h8ac2hnryxs9t23/0625_mappings.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫째, &lt;code class=&quot;highlighter-rouge&quot;&gt;⌘,&lt;/code&gt; 를 눌러서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Deployment&lt;/code&gt; 설정에 들어가서 아래의 설정을 해준다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Local Path&lt;/code&gt; : 로컬 PC 의 프로젝트 위치&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Deployment path on Server ***&lt;/code&gt; : 서버의 프로젝트 위치, 이때 앞단에 설정했던 홈 디렉토리 &lt;code class=&quot;highlighter-rouge&quot;&gt;Root Path&lt;/code&gt; 를 빼고 설정해줘야 한다. $HOME/프로젝트위치&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;둘째, 다시 &lt;code class=&quot;highlighter-rouge&quot;&gt;Project Interpreter&lt;/code&gt; 에 접속해서 서버에 있는 파이썬과 연결 됐는지 확인한다.&lt;/p&gt;

&lt;p&gt;셋째, 파일 실행을위해 주 실행파일 &lt;code class=&quot;highlighter-rouge&quot;&gt;main.py&lt;/code&gt;을 선택 후 (없다면 실행할 파일을 선택), 메뉴에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Run &amp;gt; Edit Configuration&lt;/code&gt; 을 선택한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/gl4kjtjep5qjgjg/0625_remotepython.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt; 버튼을 눌러서 새로운 파이썬 실행파일을 연결하자.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Script path&lt;/code&gt;: 스크립트 실행 파일 루트&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Python Interpreter&lt;/code&gt; : Remote Python 으로 설정 됐는지 확인&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Environment Variables&lt;/code&gt; : 딥러닝에서 GPU를 쓰려면 환경을 인식해줘야한다. ㅠㅠ&lt;/p&gt;

  &lt;p&gt;아래 그림과 같이 설정해주자. (그전에 cuda 설치를 못했다면? &lt;a href=&quot;https://simonjisu.github.io/datascience/2018/06/03/gpuserver3.html&quot;&gt;링크&lt;/a&gt;)&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/ere9ckvmt23x343/0625_env.png&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-1&quot;&gt;꿀팁&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;SSH 터미널을 사용하려면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Tools &amp;gt; Start SSH session...&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;로컬에서 변경사항을 자동 업로드 하려면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Tools &amp;gt; Deployment &amp;gt; Automatic uploads(always)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 24 Jun 2018 18:41:07 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/24/pycharmssh.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/24/pycharmssh.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 3 PYTHON &amp; CUDA</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---3&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 3&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;install-python&quot;&gt;Install PYTHON&lt;/h2&gt;

&lt;p&gt;아나콘다를 통하지 않고 소스를 통해 파이썬을 설치하기로 했다. 일단 용량이 작고, 다른 부가 spyder 등 프로그램을 설치하기 싫어서 소스에서 직접 설치하기로 했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;https://www.python.org/&lt;/a&gt; 에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Download &amp;gt; Soruce code &amp;gt; Python 3.6.5 - 2018-03-28&lt;/code&gt; 로 들어가서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Gzipped source tarball&lt;/code&gt; 의 링크를 복사한 후 아래와 같이 쳐주자.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;압축을 헤제시켜주자
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo -zxvf Python-3.6.5.tgz
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;설치해보자
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
make
make test
sudo make install
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;기본적으로 이렇게 진행하면 설치가 완료 된다. (중간에 실패하면 어떤 패키지가 없는지 확인하고 apt-get으로 설치해준다.)&lt;/p&gt;

&lt;p&gt;하지만 &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip3 install numpy&lt;/code&gt; 하게 되면 아래와 같이 “TLS/SSL ~” 이라며 에러가 뜰 수도 있다. 자세히 뭔지는 모르겠지만, 구글링을 통해서 해결했다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.
Collecting
  Could not fetch URL https://pypi.python.org/simple//: There was a problem confirming the ssl certificate: Can't connect to HTTPS URL because the SSL module is not available. - skipping
  Could not find a version that satisfies the requirement (from versions: )
No matching distribution found for
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section&quot;&gt;해결책&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;필수 패키지를 설치한다
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;소스폴더로 돌아가서 다시 설치해준다.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo make
sudo make install
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;마지막으로, pip 를 업그레이드 해준다.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo pip3 install --upgrade pip
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이제 설치가 잘 될 것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;install-cuda-90&quot;&gt;Install CUDA 9.0&lt;/h2&gt;

&lt;p&gt;CUDA-Toolkit 를 설치하면 드라이버를 설치할 필요는 없다고하나 만약에 먼저 설치해야하면 아래와 같이 설치(업데이트) 해주자&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
sudo apt install nvidia-390
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cuda-toolkit&quot;&gt;1. CUDA-Toolkit&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-90-download-archive&quot;&gt;https://developer.nvidia.com/cuda-90-download-archive&lt;/a&gt; 에서 자신에 버젼에 맞는 CUDA-Toolkit 을 받자.&lt;/p&gt;

&lt;p&gt;나는 우분투이기에 &lt;code class=&quot;highlighter-rouge&quot;&gt;Linux &amp;gt; x86_64 &amp;gt; Ubuntu &amp;gt; 17.04 &amp;gt; del[local]&lt;/code&gt; 를 골랐다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Base Installer&lt;/p&gt;

  &lt;p&gt;Patch 1 (Released Jan 25, 2018)&lt;/p&gt;

  &lt;p&gt;Patch 2 (Released Mar 5, 2018)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;세개의 다운로드 링크를 복사한 뒤 &lt;code class=&quot;highlighter-rouge&quot;&gt;wget&lt;/code&gt; 메서드로 받아준다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Base Installer&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb
sudo apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub
sudo apt-get update
sudo apt-get install cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Patch 1&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1604-9-0-local-cublas-performance-update_1.0-1_amd64.deb
sudo apt-get update
sudo apt-get upgrade cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Patch 2&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-2_1.0-1_amd64.deb
sudo apt-get update
sudo apt-get upgrade cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;만약에 쿠다 드라이버 명령어인 &lt;code class=&quot;highlighter-rouge&quot;&gt;nvcc&lt;/code&gt; 를 쓰고 싶다면 &lt;code class=&quot;highlighter-rouge&quot;&gt;./profile&lt;/code&gt; 파일 밑에다 아래 항목을 추가해주면 된다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export PATH=/usr/local/cuda-9.0/bin${PATH:+:$PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;아래 둘중에 하나를 한번 시도해보면 된다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvcc -V
nvidia-smi
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cudnn-&quot;&gt;2. CuDNN 설치&lt;/h3&gt;

&lt;p&gt;CuDNN 을 설치하려면 NVIDIA 회원 가입을 해야한다. 그리고 아래 싸이트에서 받아서 &lt;code class=&quot;highlighter-rouge&quot;&gt;scp&lt;/code&gt; 명령어로 서버로 옮기자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot;&gt;https://developer.nvidia.com/rdp/cudnn-download&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;압축을 해제하고, 파일들을 옮겨주면 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar -xzvf cudnn-9.0-linux-x64-v7.tgz
sudo cp cuda/include/cudnn.h /usr/local/cuda/include
sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;pytorch---&quot;&gt;Pytorch 설치한 후 테스트&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/torch.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이로써 설치 과정을 마치겠다. 컴퓨터 설치는 처음이라 3일 걸렸지만 앞으로는 더 짧아 지겠지…&lt;/p&gt;

&lt;p&gt;만약 오류가 나면 또 업데이트 하겠다.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jun 2018 21:08:24 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/03/gpuserver3.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/03/gpuserver3.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 2 원격 부팅 접속</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---2&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 2&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;after-install&quot;&gt;After Install&lt;/h2&gt;

&lt;p&gt;설치후에 아래 명령어들을 쳐준다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;apt-get(Advanced Packaging Tool) 패키지 명령어 업데이트 및 설치되어 있는 패키지 업그레이드. 설치된 파일은 &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/cache/apt/archive/&lt;/code&gt; 에 저장됨&lt;/li&gt;
  &lt;li&gt;gcc(GNU Compiler Collection) 패키지 설치. 파이썬 설치에 필요&lt;/li&gt;
  &lt;li&gt;make(GNU Make) 패키지 설치. 파이썬 설치시 필요&lt;/li&gt;
  &lt;li&gt;zlib1g-dev 설치. 파이썬 설치시 필요&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt update &amp;amp;&amp;amp; sudo apt upgrade
sudo apt install gcc
sudo apt install make
sudo apt install zlib1g-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;sshsecure-shell-&quot;&gt;SSH(SECURE SHELL) 접속&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;서버&lt;/code&gt; 컴퓨터에서 아래의 사항을 수정:&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo vi /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;ClientAliveInterval 60 : 클라이언트 살아있는지 확인하는 간격&lt;/li&gt;
  &lt;li&gt;ClientAliveCountMax 10 : 클라이언트 응답 없어도 접속 유지하는 횟수&lt;/li&gt;
  &lt;li&gt;PubkeyAuthentication yes : 활성화 시켜야 ssh를 통해서 접속 가능&lt;/li&gt;
  &lt;li&gt;PasswordAuthentication yes : 원격 서버 비밀번호로 로그인 가능하게 것, 나중에 ssh 로만 접속 가능케 하려면 바꿔줘야한다.&lt;/li&gt;
  &lt;li&gt;PermitEmptyPasswords no : 로그인시 빈 비밀번호를 가능케하는 옵션 기본으로 no로 되어 있다. 비밀번호 없이 로그인하게 하려면 yes로 바꿔줄 것, 권장은 안함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ssh-key----&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;로컬&lt;/code&gt; 컴퓨터에서 SSH KEY 생성하고 &lt;code class=&quot;highlighter-rouge&quot;&gt;서버&lt;/code&gt; 컴퓨터로 보내기:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;ssh-keygen:&lt;/strong&gt; SSH 키를 생성한다. 경로 지정을 안할 때 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.ssh&lt;/code&gt; 폴더 안에 &lt;code class=&quot;highlighter-rouge&quot;&gt;id_rsa&lt;/code&gt; 라는 이름으로 생성한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ssh-copy-id:&lt;/strong&gt; SSH 키를 서버로 보낸다. 옵션으로 포트번, 키 디렉토리 등등 설정 가능하다&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen -f [filepath]
ssh-copy-id -i [key_directory] -p [port] [user]@[ip_address]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;scp-&quot;&gt;파일전송 명령어 SCP 사용하기&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;서버 &amp;gt; 로컬
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scp [옵션] [계정명]@[원격지IP주소]:[원본 경로 및 파일] [전송받을 위치]
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;로컬 &amp;gt; 서버
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scp [옵션] [원본 경로 및 파일] [계정명]@[원격지IP주소]:[전송받을 위치]
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다만 주의 해야할 것은 &lt;code class=&quot;highlighter-rouge&quot;&gt;[옵션]&lt;/code&gt; 에다가 로그인 옵션 다 넣어줘야 보내진다는 점을 잊지 말자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;옵션:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;P: 포트&lt;/li&gt;
  &lt;li&gt;i: key&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;wolwake-on-lan&quot;&gt;WOL(Wake-On-Lan)&lt;/h2&gt;

&lt;p&gt;전기세 때문에 원격으로 컴퓨터를 껐다 켯다 하고 싶었다. 찾아보니 WOL 라는 방법이 있었다. 우선 자신의 컴퓨터의 메인보드가 이 기능을 지원해주고, 공유기도 이 기능을 지원해줘야 사용할 수 있다.
컴퓨터 부팅전 &lt;code class=&quot;highlighter-rouge&quot;&gt;BIOS&lt;/code&gt; (내경우는 DEL 키를 눌렀음) 에 들어가서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Wake-On-Lan&lt;/code&gt; 이라는 글귀가 있는지 찾아보고, 있다면 enable 로 바꿔주자. 그리고 아래 명령어를 통해 내컴퓨터의 &lt;code class=&quot;highlighter-rouge&quot;&gt;이더넷 포트(Ethernet port)&lt;/code&gt; 알아보자&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이더넷 포트는 첫째줄 제일 왼쪽에 있을 것이다. 보통 en~~ 로 시작하는 번호다&lt;/p&gt;

&lt;p&gt;그 후, 컴퓨터 내 컴퓨터가 &lt;code class=&quot;highlighter-rouge&quot;&gt;WOL&lt;/code&gt; 기능이 켜졌나 확인 하기 위해, 부팅후 커맨드 라인에 아래와 같이 쳐주자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install ethtool
sudo ethtool [Ethernet port]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Wake-on&lt;/code&gt; 이라는 곳에 &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; 라고 적혀져 있으면 켜진 것이다. 안되있다면 아래의 명령어를 통해 켜주자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ethtool -s [Ethernet port] wol g
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그 다음에 자신의 집의 공유기에 들어가서, WOL 설정을 해주자. NETIS 기준으로 설명 하겠다. IPTIME 은 다른 글들이 많으니 한번 찾아 보길 바란다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;원격 부팅(WOL)&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;사용 IP 목록에서 등록&lt;/code&gt; 누른후, 자신의 컴퓨터 IP 를 선택하고 이름을 지어준 다음 &lt;code class=&quot;highlighter-rouge&quot;&gt;등록&lt;/code&gt; 하게 되면 밑에 하나 등록 될 것이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;외부 연결 포트&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;포트 번호&lt;/code&gt;는 내 컴퓨터의 접속 포트로 했다. (이건 꼭 TCP 통신으로 하는 포트로 해야하는지 모르겠다. 다른 번호를 따로 지정해줄 수 있는지를 확인 못해봄)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/WOL.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 DDNS 서비스 신청한다. 그러면 집 밖에서도 집 공유기에 접속해서 컴퓨터를 킬 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;참고자료: &lt;a href=&quot;http://blog.daum.net/peace20/16779844&quot;&gt;http://blog.daum.net/peace20/16779844&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음 시간에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Python&lt;/code&gt; 설치와 &lt;code class=&quot;highlighter-rouge&quot;&gt;CUDA&lt;/code&gt; 설치를 다뤄보겠다.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jun 2018 10:09:54 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/03/gpuserver2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/03/gpuserver2.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 1 사양 및 우분투 서버 설치</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---1&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 1&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section&quot;&gt;컴퓨터 사양 상세&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;항목&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;상품코드&lt;/th&gt;
      &lt;th&gt;제품명&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;금액&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;수량&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;최종금액&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CPU&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;399920&lt;/td&gt;
      &lt;td&gt;[INTEL] 코어7세대 i5-7600 정품박스 (카비레이크/3.5GHz/6MB/쿨러포함)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;258,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;258,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MAIN&lt;br /&gt;BOARD&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;408703&lt;/td&gt;
      &lt;td&gt;[GIGABYTE] GA-H110M-M.2 듀러블에디션 피씨디렉트 (인텔H110/M-ATX)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71,100원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71,100원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;메모리&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;390790&lt;/td&gt;
      &lt;td&gt;[삼성전자] 삼성 DDR4 16GB PC4-19200&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;183,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;366,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HDD&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;347917&lt;/td&gt;
      &lt;td&gt;[WD] BLUE 2TB WD20EZRZ (3.5HDD/SATA3/5400rpm/64M)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67,730원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67,730원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GPU&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;373864&lt;/td&gt;
      &lt;td&gt;[MSI] GeForce GTX1060 OC D5 6GB 윈드스톰&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;389,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;389,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POWER&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;420859&lt;/td&gt;
      &lt;td&gt;[CORSAIR] CX750 NEW 80PLUS BRONZE (ATX/750W)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;94,370원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;94,370원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;BOX&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;365393&lt;/td&gt;
      &lt;td&gt;[COX] RC 170T USB3.0 (미들타워)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13,500원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13,500원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;OTHER&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3877&lt;/td&gt;
      &lt;td&gt;[컴퓨존] 일반조립비 (하드웨어조립/OS는 설치되지않습니다)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20,000원&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/1.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/2.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/3.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/4.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/5.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/6.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/7.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/8.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/9.jpeg&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2018년 5월 28일 컴퓨존에서 주문해서, 5월 30일 수요일 도착했다. 총비용은 대략 130만원 정도 ㅎㅎ 언른 GPU를 쓰고 싶은 생각에 그날밤 바로 설치를 진행하였다.&lt;/p&gt;

&lt;h2 id=&quot;ubuntu-server-&quot;&gt;Ubuntu Server 설치&lt;/h2&gt;

&lt;p&gt;5월 30일 저녁, 우분투를 설치하려고 하니 버전이 마음에 걸렸다. NVIDIA CUDA TOOLKIT을 보니 리눅스 17.04 버전 까지 지원하는듯 했기 때문이다. 16.04를 설치해야하나? 싶은 찰나에 그냥 최신으로 한번 도전해보기로 했다. 안되면 다시 갈지머..&lt;/p&gt;

&lt;h3 id=&quot;making-a-bootable-ubuntu-usb-disk-tutorial-at-mac-os&quot;&gt;Making a bootable Ubuntu USB disk Tutorial at Mac OS&lt;/h3&gt;

&lt;p&gt;맥에서 부팅 디스크 만들기, 정말 간단하다. &lt;a href=&quot;https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-macos#0&quot;&gt;tutorials.ubuntu.com&lt;/a&gt; 튜토리얼을 따라가면 된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;준비물&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;2GB 이상의 USB Driver&lt;/li&gt;
    &lt;li&gt;Mac OS 컴퓨터&lt;/li&gt;
    &lt;li&gt;우분투 서버 ISO 다운로드 &lt;a href=&quot;https://www.ubuntu.com/download/server&quot;&gt;우분투 서버 다운로드&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;format&quot;&gt;구동 디스크 FORMAT&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;응용프로그램 &amp;gt; 유틸리티 &amp;gt; 디스크 유틸리티 선택&lt;/li&gt;
  &lt;li&gt;MAC OS에 꼽은 USB 를 선택한 다음에 &lt;code class=&quot;highlighter-rouge&quot;&gt;지우기&lt;/code&gt; 를 선택한다.&lt;/li&gt;
  &lt;li&gt;이름을 짓고, &lt;code class=&quot;highlighter-rouge&quot;&gt;MS-DOS(FAT)&lt;/code&gt; 선택한다. (그림에는 Scheme가 있지만 Serria 이후에는 없다는 말이 있음)&lt;/li&gt;
  &lt;li&gt;포맷한다. 지운다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/format_disk.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;etcher-----&quot;&gt;Etcher 를 사용한 시동 디스크 생성&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://etcher.io/&quot;&gt;Etcher&lt;/a&gt; 먼저 받는다. 그후에는 정말 간단하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Select image&lt;/code&gt; 에 다운 받은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ubuntu Server 18.04 ISO&lt;/code&gt; 를 고른다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Select drive&lt;/code&gt; 에 포맷한 디스크를 선택&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Flash!&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/etcher.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 설치 준비 완료되었다.&lt;/p&gt;

&lt;h3 id=&quot;install-ubuntu-server-1804&quot;&gt;Install Ubuntu Server 18.04&lt;/h3&gt;

&lt;p&gt;이제 설치릃 해보자. 설치를 하려면, 최소 한번은 모니터에 연결해서 설치해야한다. 나는 정말 서버만을 생각해서 모니터를 않샀기에… HDMI 케이블로 티비화면으로 연결했다… 덕분에 고생이 두배!&lt;/p&gt;

&lt;p&gt;사실 간단하다. 아까 구운 &lt;code class=&quot;highlighter-rouge&quot;&gt;시동 디스크&lt;/code&gt;를 꼽아주고 부팅을 하면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u4.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u5.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u6.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u7.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u8.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u9.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u10.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u11.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u12.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사진출처: &lt;a href=&quot;https://websiteforstudents.com/install-ubuntu-18-04-lts-server-screenshots/&quot;&gt;https://websiteforstudents.com/install-ubuntu-18-04-lts-server-screenshots/&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;언어선택: 왠만하면 영어로 하자&lt;/li&gt;
  &lt;li&gt;키보드선택: 왠만하면 영어로 가자&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Install Ubuntu&lt;/code&gt; 선택&lt;/li&gt;
  &lt;li&gt;다음&lt;/li&gt;
  &lt;li&gt;특별한 주소가 있으면 작성 아니면, 다음&lt;/li&gt;
  &lt;li&gt;디스크 포맷: 디스크 통째로 포맷한 후에 설치할 것이니 1번&lt;/li&gt;
  &lt;li&gt;디스크 선택&lt;/li&gt;
  &lt;li&gt;마지막 확인&lt;/li&gt;
  &lt;li&gt;정말루?&lt;/li&gt;
  &lt;li&gt;당신의 이름 / 서버 이름 / 유저이름(로그인용) / 패스워드(로그인용) 등&lt;/li&gt;
  &lt;li&gt;설치중… 리붓!&lt;/li&gt;
  &lt;li&gt;완료&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;다음 장에는 설치후에 내가 했던 작업들: &lt;code class=&quot;highlighter-rouge&quot;&gt;원격 부팅과 접속&lt;/code&gt; 을 주로 이야기 하겠다.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jun 2018 22:26:58 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/02/gpuserver.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/02/gpuserver.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: GloVe</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-glove&quot;&gt;All about Word Vectors: GloVe&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;co-occurrence&quot;&gt;Co-occurrence&lt;/h2&gt;

&lt;p&gt;공기(Co-occurrence) 란 무엇인가? 두 개 이상의 어휘가 일정한 범위(range) 혹은 거리(distance) 내에서 함께 출현하는 현상을 말한다. 여기서 어휘는 단어 뿐만 아니라 형태소, 합성어 등의 단위로 의미를 부여할 수 있는 언어 단위다. 그렇다면 왜 &lt;strong&gt;공기 관계&lt;/strong&gt; 를 살피는 것일까?&lt;/p&gt;

&lt;p&gt;공기 관계를 통해 문서나 문장으로 부터 &lt;strong&gt;추상화된 정보&lt;/strong&gt; 를 얻기 위해서다. 이는 자연어처리의 가정을 생각해보면 이해할 수 있을 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;비슷한 맥락에 등장하는 단어들은 유사한 의미를 지니는 경향이 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;때문에, 두 단어가 같이 등장한 횟수가 많아지면 &lt;strong&gt;유사한 의미&lt;/strong&gt; 를 가졌다고 볼 수도 있다는 것이다. 이런 유사한 의미를 추상화된 정보로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;공기(Co-occurrence) 정보를 수집하는 방법은 두 가지다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;window 기반: 품사와 의미(semantic) 정보를 캡쳐할 수 있다.&lt;/li&gt;
  &lt;li&gt;word-document co-occurrence matrix 기반: 조금 더 일반적인 토픽을 추출 할 수 있고, 이는 Latent Semantic Analysis 와 연결 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;example-window-based-co-occurrence-matrix&quot;&gt;Example: Window based co-occurrence matrix&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;I like deep learning.&lt;/p&gt;

  &lt;p&gt;I like NLP.&lt;/p&gt;

  &lt;p&gt;I enjoy flying.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 3 문장을 사용해서, window size = 1 로 지정하는 co-occurrence matrix 를 만들어보자. 무슨 뜻인지는 아래 코드를 실행한 표를 살펴보자.&lt;/p&gt;

&lt;p&gt;단, 한 단어에 대해서 좌측에서 등장했는지 우측에서 등장했는 지는 상관없다(이는 co-occurrence matrix 가 대각을 기준으로 대칭하는 결과를 불러옴)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
import numpy as np
from collections import deque, Counter
from itertools import islice
from scipy.sparse import coo_matrix

flatten = lambda t: [tuple(j) for i in t for j in i]
window = 1

def get_cooccur_list(sentence, window):
    s_len = len(sentence)
    ngram_list = [deque(islice(sentence, i), window+1) for i in range(s_len+1)][2:]
    return ngram_list

sentences = ['I like deep learning .', 'I like NLP .', 'I enjoy flying .']
tokens = [s.split() for s in sentences]
vocab = list(set([w for s in tokens for w in s]))
# print(vocab)
vocab = ['I', 'like', 'enjoy', 'deep', 'learning', 'NLP', 'flying', '.'] # 표와 같은 모양을 만들어주기 위해 다시 지정
vocab2idx = {w: i for i, w in enumerate(vocab)}
tokens_idx = [[vocab2idx.get(w) for w in s] for s in tokens]
co_occurs = [get_cooccur_list(s, window) for s in tokens_idx]

d = Counter()
d.update(flatten(co_occurs))
row, col, data = list(zip(*[[r, c, v] for (r, c), v in d.items()]))
temp = coo_matrix((data, (row, col)), shape=(len(vocab), len(vocab))).toarray()
co_mat = temp.T + temp

pd.DataFrame(co_mat, index=vocab, columns=vocab)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;코드를 실행하면 아래와 같은 표가 나온다. window size 가 1이니까 “I” 주변 한칸에 동시 등장 단어는 “like” 가 2번 “enjoy” 가 1번이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;counts&lt;/th&gt;
      &lt;th&gt;I&lt;/th&gt;
      &lt;th&gt;like&lt;/th&gt;
      &lt;th&gt;enjoy&lt;/th&gt;
      &lt;th&gt;deep&lt;/th&gt;
      &lt;th&gt;learning&lt;/th&gt;
      &lt;th&gt;NLP&lt;/th&gt;
      &lt;th&gt;flying&lt;/th&gt;
      &lt;th&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;I&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;like&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;enjoy&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;deep&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;learning&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NLP&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;flying&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;co-occurrence matrix와 같은 단어 벡터는 어떤 문제점이 있을까?&lt;/p&gt;

&lt;p&gt;첫째로, 단어가 많아지면 벡터가 엄청 길어진다(데이터 차원이 커진)는 것이다. 이에 따른 많은 저장 비용이 들어갈 것이다. 둘째로, sparsity issues가 있을 수 있다(models are less robust).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그렇다면 꼭 하나의 단어로 해야만 하는가? 문서 전체의 단어의 공기 정보를 추출 하는 것은 안되는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이와 같은 생각이 GloVe 를 탄생시켰다.&lt;/p&gt;

&lt;h2 id=&quot;glove&quot;&gt;GloVe&lt;/h2&gt;

&lt;p&gt;Paper: &lt;a href=&quot;https://www.aclweb.org/anthology/D14-1162&quot;&gt;GloVe: Global Vectors for Word Representation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GloVe 의 학습방법은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \dfrac{1}{2} \sum_{i,j=1}^{W} f(P_{ij})(u_i^T v_j - \log P_{ij})^2&lt;/script&gt;

&lt;p&gt;논문해설을 통해서 자세히 보자.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;section&quot;&gt;논문 해설&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;GloVe:&lt;/strong&gt; Global Vectors라고 명칭을 지은 이유는 모델에서 직접적으로 문서 전체의 코퍼스 통계량을 포착할 수있기 때문이다. (the global corpus statistics are captured directly by the model)&lt;/p&gt;

&lt;p&gt;그전에 notation 을 정의해보자.&lt;/p&gt;

&lt;p&gt;Define notation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$X$: 단어간의 공기 매트릭스 (matrix of word-word co-occurrence counts)&lt;/li&gt;
  &lt;li&gt;$X_{ij}$: 단어 $j$ 와 문맥 단어 $i$ 가 같이 등장한 횟수 (the number of times that word $j$ occurs in the context word $i$)&lt;/li&gt;
  &lt;li&gt;$X_i = \sum_k X_{ik}$: 어떤 단어든 문맥 단어 $i$ 와 등장한 횟수 (the number of times any word appears in the context of word $i$)&lt;/li&gt;
  &lt;li&gt;$P_{ij} = P(j \vert i) = X_{ij} / X_i$: 단어 $j$ 와 문맥 단어 $i$ 동시 등장할 확률, 문맥 단어 $i$ 가 주어졌을 때 $j$ 가 등장할 확률 (probability that word $j$ appear in the context word $i$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래의 예시를 보자.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Probability and Ratio&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=solid$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=gas$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=water$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=fashion$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert ice)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.00019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000066&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.003&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert steam)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000022&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.00078&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0022&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert ice)/P(k\vert steam)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8.9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.085&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.96&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;위의 표에 따르면 $i=ice, j=steam$ 일때 $solid$ 와 동시 등장 확률이 높은 단어는 $ice$ 다. 직관적으로 생각해도, 단단한 $ice$ 가 $solid$ 와 연관될 확률이 더 높다. 따러서, 우리는 $P(k\vert ice)/P(k\vert steam)$ 를 구해서, 연관이 있는 단어일 경우 이 비율이 크게 높으며, 아니면 그 반대다. &lt;strong&gt;(엄청 크거나 혹은 엄청 작거나)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이처럼 직접적으로 단어간의 동시등장 확률을 비교하는 것보다. 확률간의 비율을 구하는 것이 &lt;strong&gt;연관성이 없는&lt;/strong&gt; 단어(water &amp;amp; fashion)들로 부터 관련된 단어(solid &amp;amp; gas)를 구별하기 좋으며, &lt;strong&gt;관련성 있는&lt;/strong&gt; 단어(solid &amp;amp; gas)들을 차별화 하기에도 좋다.&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;동시 등장 확률의 비율(ratios of co-occurrence probabilities)&lt;/span&gt;&lt;/strong&gt; 을 모델이 학습하게 하는 것이 바람직 해보인다.&lt;/p&gt;

&lt;p&gt;중요한 것은 이 비율은 3개의 단어 $i, j, k$ 와 연관이 있다. 따라서 아래의 함수를 구성할 수가 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i, w_j, \tilde{w}_k) = \dfrac{P_{ik} }{P_{jk} } \cdots (1)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$w \in \Bbb{R}^d$: word vectors&lt;/li&gt;
  &lt;li&gt;$\tilde{w} \in \Bbb{R}^d$: separate context word vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$(1)$ 식과 같이, 단어 벡터 공간에서 $w_i, w_j, \tilde{w}_k$ 를 input으로 넣었을 때,&lt;/p&gt;

&lt;p&gt;$\dfrac{ P_{ik} }{ P_{jk} }$ 비율을 나타내는 하는 선형구조인 함수를 구하는 것이 목적이다. 그리고 $F$ 를 아래와 같이 변형시켜 본다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F((w_i - w_j)^T \tilde{w}_k) = \dfrac{P_{ik} }{P_{jk} } \cdots (2)&lt;/script&gt;

&lt;p&gt;이로써 선형적인 관계를 포착하고, 양변 모두 스칼라 값으로 정해진 함수가 만들어 졌다. 그러나 단어 $i, j$ 와 $k$ 동시 등장 비율의 &lt;strong&gt;임의적인 차별화&lt;/strong&gt; 를 위해서 어떤 조건들을 만족해야한다. 그 조건들이란 단어 벡터 $w$ 와 문맥 단어 벡터 $\tilde{w}$ 간 서로 자유롭게 교환 될 수가 있어야 한다. 즉, 단어간 공기 매트릭스 $X$의 대칭(symmetric) 특성을 보존해야 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;임의적인 차별화&lt;/strong&gt; 가 무슨 말이냐면, 단어 $k$ 와 $i, j$ 단어 간의 비율을 확인 할때, $i$ 와 $k, j$ (혹은 $j$ 와 $i, k$) 의 관계도 확인 할 수 있어야 된다는 말이다.&lt;/p&gt;

&lt;p&gt;대칭(symmetric) 을 만족하려면 2 단계로 진행 된다. 우선, 두 그룹 $(\Bbb{R}, +)$ 과 $(\Bbb{R}_{&amp;gt;0}, \times)$ 에 대해서 함수 $F$ 가 &lt;strong&gt;homomorphism&lt;/strong&gt; 이어야 한다. (homomorphism 해설: 밑에 &lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt;를 보라), 예를 들어 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i, w_j, \tilde{w}_k) = \dfrac{F(w_i^T \tilde{w}_k) }{F(w_j^T \tilde{w}_k) } \cdots (3)&lt;/script&gt;

&lt;p&gt;$(2)$ 식에 의해서, 아래와 같이 풀 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i^T \tilde{w}_k) = P_{ik} = \dfrac{X_{ik} }{X_i} \cdots (4)&lt;/script&gt;

&lt;p&gt;$(3)$ 식에 만족하는 해답은 $F = \exp$ 임으로, 아래의 식을 도출 해낼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^T \tilde{w}_k = \log(P_{ik}) = \log(X_{ik}) - \log(X_i) \cdots (5)&lt;/script&gt;

&lt;p&gt;다음으로, $(5)$ 식은 $\log(X_i)$ 만 아니였다면 대칭이었을 것이다. $\log(P_{ik})=\log(P_{ki})$ 를 만족하는지 한번 보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\log(P_{ik}) &amp;= \log(X_{ik}) - \log(X_i) \\
\log(P_{ki}) &amp;= \log(X_{ki}) - \log(X_k)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;당연하게도, $\log(X_i) \neq \log(X_k)$ 이기 때문에 $\log(P_{ik}) \neq \log(P_{ki})$ 이다.&lt;/p&gt;

&lt;p&gt;하지만 $\log(X_i)$ 부분은 $k$ 에 대해서 독립적(independent) 이기 때문에, $w_i$ 의 bias $b_i$ 항으로 들어갈 수 있다. 그리고 대칭성을 유지하기 위해서 $\tilde{w}_k$ 의 bias $\tilde{b}_k$ 항도 더해준다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^T \tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik}) \cdots (6)&lt;/script&gt;

&lt;p&gt;$(6)$ 식이 우리의 제일 간단한 선형관계인 solution 이라고 해도 되지만 이는 문제가 좀 있다. $X_{ik} = 0$ 에서 명확하게 정의 되지 않는다. 이를 해결하기 위해서 $X_{ik}+1$ 하는 방법도 있지만, sparsity issue 를 벗어나기 힘들다. 그리고 하나의 큰 약점이 있다면 거의 등장하지 않는 단어들에게 동시 등장 비율이 모두 같을 수 있다는 점이다. 이게 왜 문제가 되냐면, co-occurrence 가 적을 수록 많이 등장하는 단어들 보다 정보 함량이 적고 데이터도 noisy 하기 때문이다.&lt;/p&gt;

&lt;p&gt;연구팀은 새로운 weighted least squares regression model 을 제시하여 문제를 풀고자 했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \dfrac{1}{2} \sum_{i,j=1}^{W} f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2 \cdots (7)&lt;/script&gt;

&lt;p&gt;가중치 함수 (Weighting function) $f(X_{ij})$ 는 아래의 특성을 따라야 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$f(0) = 0$. 만약 $f$ 가 연속함수(continuous function) 라면, $x \rightarrow 0$ 으로 갈때 $\lim_{x\rightarrow 0} f(x) log^2x$ 도 빠르게 수렴한다. 단, 유한한 값이여야 한다.&lt;/li&gt;
  &lt;li&gt;$f(x)$ 는 감소함수가 되면 안된다. (non-decreasing) 이유는 동시 등장이 희박한 단어들의 가중치가 많아져서는 안되기 때문이다.&lt;/li&gt;
  &lt;li&gt;$f(x)$ 는 큰 $x$ 값에 대해서 상대적으로 작은 값이어야 한다. 그 이유는 공기 횟수가 큰 단어들의 가중치가 너무 높게 설정하지 않기 위해서다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;연구팀은 이에 적합한 함수를 찾았다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x) =
\begin{cases} (x/x_{max})^{\alpha} \quad if\ x &lt; x_{max} \\
1 \quad otherwise
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L3_weight_f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림: $f(x)$ with $\alpha = 3/4$ 일때 좋은 성과를 얻었다. 재밌는 것은 Mikolov 논문에서 나온 unigram distribution 에 3/4 승을 해주는 것과 같다는 것을 발견했다.&lt;/p&gt;

&lt;p&gt;조금더 general 한 weighting function 은 아래와 같다. (자세한건 논문 3.1 참고)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{J} = \sum_{i,j} f(X_{ij})(w_i^T \tilde{w}_j - \log X_{ij})^2&lt;/script&gt;

&lt;p&gt;이는 연구팀이 도출한 $(7)$ 식과 같은 식이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;homomorphism&quot;&gt;참고 1: Homomorphism&lt;/h3&gt;

&lt;p&gt;혹시나 틀렸으면 댓글로 이야기 해주세요.&lt;/p&gt;

&lt;p&gt;우선 Group &lt;a href=&quot;https://en.wikipedia.org/wiki/Group_(mathematics)&quot;&gt;(위키 링크)&lt;/a&gt; 이란 것을 알아야한다. 내가 이해한 바로는 &lt;strong&gt;Group $(G, * )$&lt;/strong&gt; 이란, 집합 $G$ 와 연산 $* $ 로 구성되어 있다. 이 연산을 “the Group Law of $G$” 라고 부른다. 집합 $G$ 에 속한 원소 $a, b$ 의 연산을 $a * b$ 라고 표현한다. 또한 아래의 조건들을 만족해야한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Closure: $* $ 연산은 $G$ 에 대해 닫혀 있어야한다. 즉, $a * b$ 연산도 집합 $G$ 에 속해야한다.&lt;/li&gt;
  &lt;li&gt;Associativity: 교환 법칙이 성립해야한다. $(a * b) * c = a * (b * c)$&lt;/li&gt;
  &lt;li&gt;Identity element: 항등원이 존재해야 한다. $a * e = a = e * a$&lt;/li&gt;
  &lt;li&gt;Inverse element: 역원이 존재해야 한다. $a * x = e = x * a$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Group 를 이해 했으면 이제 Homomorphism 을 이해해보자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;정의:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;두 그룹 &lt;span style=&quot;color: #15b23c&quot;&gt;$(G, * )$&lt;/span&gt; 과 &lt;span style=&quot;color: #9013b2&quot;&gt;$(H, @)$&lt;/span&gt; 가 있으면, 모든 &lt;span style=&quot;color: #15b23c&quot;&gt;$x, y \in G$&lt;/span&gt; 에 대해서 $f:$ &lt;span style=&quot;color: #15b23c&quot;&gt;$G$&lt;/span&gt; $\rightarrow$ &lt;span style=&quot;color: #9013b2&quot;&gt;$H$&lt;/span&gt;, &lt;span style=&quot;color: #15b23c&quot;&gt;$f(x * y)$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$f(x) @ f(y)$&lt;/span&gt; 를 만족하는 map 을 말한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예시)&lt;/strong&gt;
두 그룹 &lt;span style=&quot;color: #15b23c&quot;&gt;$(\Bbb{R}, + )$&lt;/span&gt; 와 &lt;span style=&quot;color: #9013b2&quot;&gt;$(\Bbb{R}_{&amp;gt;0}, \times )$&lt;/span&gt; 사이에 어떤 map $f:$ &lt;span style=&quot;color: #15b23c&quot;&gt;$\Bbb{R}$&lt;/span&gt; $\rightarrow$ &lt;span style=&quot;color: #9013b2&quot;&gt;$\Bbb{R}_{&amp;gt;0}$&lt;/span&gt;, $f(x)=e^x$ 가 있다면, $f$ 가 Homomorphism 인지를 밝혀라.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;for any &lt;span style=&quot;color: #15b23c&quot;&gt;$x, y \in \Bbb{R}$&lt;/span&gt;,&lt;/p&gt;

  &lt;p&gt;&lt;span style=&quot;color: #15b23c&quot;&gt;$f(x + y)$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$e^{x+y}$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$e^x \times e^y$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$f(x) \times f(y)$&lt;/span&gt; 임으로&lt;/p&gt;

  &lt;p&gt;Homomorphism 을 만족한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;word2vec 과 glove 관련 포스팅은 &lt;strong&gt;“All about word vectors”&lt;/strong&gt; 시리즈로 마치겠다. 기회가 되면 gensim 의 사용법과, 데이터 차원 축소와 시각화 방법인 t-SNE 을 포스팅 하도록 하겠다.&lt;/p&gt;
</description>
        <pubDate>Wed, 02 May 2018 23:22:05 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/05/02/allaboutwv4.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/05/02/allaboutwv4.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Negative Sampling</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-negative-sampling&quot;&gt;All about Word Vectors: Negative Sampling&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_model_train.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;navie-softmax--&quot;&gt;Navie Softmax 의 단점&lt;/h2&gt;

&lt;p&gt;Navie Softmax 를 최종단에 출력으로 두고 Backpropagation 할때는 큰 단점이 있다.&lt;/p&gt;

&lt;p&gt;사실 Softmax가 그리 값싼 연산은 아니다. 우리가 학습하고 싶은 단어 벡터 1000개가 있다고 가정해보자. 그렇다면 매 window size=2 마다, 다시 말해 총 업데이트 할 5개의 단어 (중심단어 1 + 주변 단어 2 x 2) 를 위해서, $W, W’$ 안에 파라미터를 업데이트 해야하는데, 그 갯수가 최소 $(2 \times d \times 1000)$ 만큼된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown_\theta J_t(\theta) \in \Bbb{R}^{2dV}&lt;/script&gt;

&lt;p&gt;많은 양의 단어에 비해 업데이트 하는 파라미터수는 적기 때문에 gradient matrix $\triangledown_\theta J_t(\theta)$ 가 굉장히 sparse 해질 수 있다 (0이 많다는 소리). Adam 같은 알고리즘은 sparse 한 matrix 에 취약하다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://simonjisu.github.io/deeplearning/2018/01/13/numpywithnn_5.html&quot;&gt;Numpy with NN: Optimizer 편 참고&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그래서 &lt;strong&gt;“window에 실제로 등장하는 단어들만 업데이트 하면 좋지 않을까?”&lt;/strong&gt; 라는 생각을 하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;negative-sampling&quot;&gt;Negative Sampling&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;paper 1: &lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;&gt;Distributed representaions of Words and Phrases and their Compositionality (Mikolov et al. 2013)&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;paper 2: &lt;a href=&quot;https://arxiv.org/abs/1402.3722&quot;&gt;word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;요약하면 아래와 같은 목적함수를 최대화 하는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
J(\theta) &amp;= \dfrac{1}{T}\sum_{t=1}^{T} J_t(\theta)\\
J_t(\theta) &amp;= \underbrace{\log \sigma(u_o^T v_c)}_{(1)} + \underbrace{\sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_c)]}_{(2)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$T$: total num of words&lt;/li&gt;
  &lt;li&gt;$\sigma$: sigmoid function&lt;/li&gt;
  &lt;li&gt;$P(w) = {U(w)^{3/4}} / {Z}$: unigram distribution U(w) raised to the 3/4 power
    &lt;ul&gt;
      &lt;li&gt;The power makes less frequent words be sampled more often&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;말로 풀어보자면, 모든 단어 $T$ 에 대해서 중심단어 $c$ 와 그 주변단어 $o$ 가 같이 나올 확률 &lt;strong&gt;[수식 (1)]&lt;/strong&gt; 을 최대화 하고, 그 주변단어가 아닌 집합에서 sampling 하여 나온 $k$ 개의 단어의 확률 &lt;strong&gt;[수식 (2)]&lt;/strong&gt; 을 최소화 시키는 것이다. (음수가 붙기 때문에 최소하하게 되면 최대화가 된다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section&quot;&gt;상세 논문 설명&lt;/h3&gt;

&lt;p&gt;논문 기준으로 위에 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;표기법&lt;/span&gt;&lt;/strong&gt; 이 조금 다르다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;여기서 &lt;strong&gt;$w$ = center word, $c$ = context&lt;/strong&gt; 다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;출발점은 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$(w, c)$ 세트가 정말로 corpus data로 부터 왔는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;라고 생각하고 아래와 같은 &lt;strong&gt;정의&lt;/strong&gt; 를 하게 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로 부터 왔을 확률&lt;/li&gt;
  &lt;li&gt;$P(D = 0 \vert w, c) = 1 - P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로부터 오지 않았을 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, 우리의 목적은 확률 $P(D = 1\vert\ w, c)$ 를 최대화하는 parameter $\theta$를 찾는 것이기 때문에 아래와 같은 목적함수를 세울 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} &amp;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &amp;\arg \underset{\theta}{\max} \log \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &amp;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;파라미터 $\theta$ 는 단어들의 벡터라고 생각할 수 있다. 즉, 위의 식을 만족하는 어떤 최적의 단어 벡터를 찾는것이다.&lt;/p&gt;

&lt;p&gt;또한, 확률 $P(D=1\vert\ w,c;\theta)$ 은 sigmoid로 아래와 같이 정의 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D=1\vert\ w,c;\theta) = \dfrac{1}{1+e^{-v_c v_w}}&lt;/script&gt;

&lt;p&gt;따라서 우리의 목적함수는 아래와 같이 다시 고쳐 쓸수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} }&lt;/script&gt;

&lt;p&gt;그러나 우리의 목적 함수는 매 $(w, c)$ 세트마다 $P(D=1\vert\ w,c;\theta)=1$ 를 만족하는 trivial solution이 존재한다. $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 $\theta$ (보통 $K$ 가 40이 넘어가면 위 방정식의 값이 0에 가까워짐) 는 모든 값을 똑같이 0으로 만들어 버리기 때문에, 같은 값을 갖지 못하게 하는 매커니즘이 필요하다. ($\theta$ 에 뭘 넣어도 0이 되면 최대값을 찾는 의미가 없어진다, 자세한건 밑에 &lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt; 를 참조) 여기서 “같은 값을 같는다” 라는 말은 단어 벡터가 같은 값을 갖는 것이다.&lt;/p&gt;

&lt;p&gt;따라서, 하나의 방법으로 랜덤 $(w, c)$ 조합을 생성하는 집합 $D’$를 만들어 corpus data 로부터 올 확률 $P(D=1\vert \ w,c;\theta)$ 를 낮게 강제하는 것이다. 즉, $D’$ 에서 생성된 $(w, c)$ 조합은 &lt;strong&gt;corpus data 로부터 오지 않게&lt;/strong&gt; 하는 확률 $P(D=0\vert\ w,c;\theta)$ 을 최대화 하는 것.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} P(D=0\vert\ w,c;\theta) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta) + \underset{(w,c) \in D'}{\sum} \log \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \big(1- \dfrac{1}{1+e^{-v_c v_w} } \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} }
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\sigma(x) = \dfrac{1}{1+e^{-x} }$ 시그모이드 함수로 정의 하면, 아래와 같이 정리 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} } \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \sigma(v_c v_w) + \underset{(w,c) \in D'}{\sum} \log \sigma(- v_c v_w) \quad \cdots (3)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;이는 &lt;span style=&quot;color: #e87d7d&quot;&gt;paper 1&lt;/span&gt; 의 (4) 번 식과 같아지는다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log \sigma(u_c^T v_w) + \sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_w)]&lt;/script&gt;

&lt;p&gt;다른 점이라면, 우리가 만든 (3)식에서는 전체 corpus ($D \cup D’$) 을 포함하지만, Mikolov 논문의 식은 $D$ 에 속하는 $(w, c)$ 조합 하나와 $k$ 개의 다른 $(w, c_j)$ 의 조합을 들었다는 것이다. 구체적으로, $k$ 번의 negative sampling 에서 Mikolov 는 $D’$ 를 $k \times D$ 보다 크게 설정했고, k개의 샘플 $(w, c_1), (w, c_2), \cdots, (w, c_k)$ 에 대해서 $c_j$ 는 &lt;strong&gt;unigram distribution&lt;/strong&gt; 에 &lt;strong&gt;3/4&lt;/strong&gt; 승으로 부터 도출된다. 이는 아래의 분포에서 $(w, c)$ 조합을 추출 하는 것과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{words}(w) = \dfrac{p_{contexts} (c)^{3/4} }{Z}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$p_{words}(w)$, $p_{contexts} (c)$ 는 각각 words and contexts 의 unigram distribution 이다.&lt;/li&gt;
  &lt;li&gt;$Z$ 는 normalization constant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unigram distribution 은 단어가 등장하는 비율에 비례하게 확률을 설정하는 분포다. 예를 들어 “I have a pen. I have an apple. I have a pineapple.” 라는 문장이 있다면, 아래와 같은 분포를 만들 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;I&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;have&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;a&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;pen&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;an&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;apple&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;pineapple&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;여기서 3/4 승을 해주면, 가끔 등장하는 단어는 확률을 높혀주는 효과가 있다. 물론 자주 나오는 단어의 확률도 올라가지만 가끔 등장하는 단어의 상승폭 보다 적다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;a&lt;/th&gt;
      &lt;th&gt;$a^{\frac{3}{4} }$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;apple&lt;/td&gt;
      &lt;td&gt;$\frac{1}{15}=0.067$&lt;/td&gt;
      &lt;td&gt;${\frac{1}{15} }^{\frac{3}{4} }=0.131$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;have&lt;/td&gt;
      &lt;td&gt;$\frac{3}{15}=0.020$&lt;/td&gt;
      &lt;td&gt;${\frac{3}{15} }^{\frac{3}{4} }=0.299$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Mikolov 논문에서는 context는 하나의 단어이기 때문에 $p_{words}(w)$ 는 아래와 동일하다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{words}(w) = p_{contexts} (c) = \dfrac{count(x)}{ \vert text \vert }&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;trivial-solution&quot;&gt;참고 1. Trivial Solution&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} L(\theta;w,c) &amp;= \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } \\
&amp;= \underset{(w,c) \in D}{\sum} \log(1) - \log(1+e^{-v_c v_w}) \\
&amp;= \underset{(w,c) \in D}{\sum} - \log(1+e^{-v_c v_w})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;같은 두 벡터의 내적을 하게 되면 값은 최대가 된다. $\cos$ 값이 1이 되기 때문이다. (여기서는 최대 값이 중요한건 아니지만 값이 커진다는데 의의가 있다.)
&lt;script type=&quot;math/tex&quot;&gt;a\cdot a=\vert a \vert \vert a \vert \cos \theta&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a = np.array([1,2,3,4,5,6,7])
b = np.array([.1,.2,.3,.4,.5,.6,.7])
print(np.dot(a, a))
print(np.dot(a, b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;140&lt;/p&gt;

  &lt;p&gt;14.0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 모든 값들이 $e^{-v_c v_w}$ 를 0으로 만든다면, $L(\theta; w, c)$ 값은 0이 될것이다. 보통 $K$ 가 40 이 상이면, $L(\theta;w,c)$ 의 해는 모두 0 일 것이며 이것을 &lt;strong&gt;trivial solution&lt;/strong&gt; 이라고 한다. 우리의 목적은 단어 벡터 $v_c$ 와 $v_w$ 의 구별이기 때문에, $v_c \not = v_w$ 으로 만들어야한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음 시간에는 말뭉치의 공기정보(co-occurance)를 고려해 단어를 벡터화 시킨 &lt;strong&gt;GloVe&lt;/strong&gt; 에 대해 알아보자.&lt;/p&gt;
</description>
        <pubDate>Tue, 24 Apr 2018 16:14:13 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Big Little Data 참석후기</title>
        <description>&lt;h1 id=&quot;little-big-data--&quot;&gt;Little Big Data 참석 후기&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/littlebigdata.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;주최: ZEPL
링크: &lt;a href=&quot;https://festa.io/events/21&quot;&gt;festa&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;발표자분들께서 자신들이 겪은 다양한 경험을 들었다. 모든 일이 다 그렇지만, 문제를 파악하고 정의를 어떻게 하며, 방법을 모색하고 해결 후 결과를 다시 한번 정리해보는 사고 프로세스를 배운 것 같다.&lt;/p&gt;

&lt;p&gt;발표 세션을 듣고 일이 있어서 나와야했지만 유익했던 자리.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-1&quot;&gt;상세&lt;/h2&gt;

&lt;p&gt;발표자의 내용이 완벽하게 일치하지 않으며, 제가 중간중간 생각나서 제 생각을 기록한 것도 있습니다. (거의 없긴 하지만)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-2&quot;&gt;극한직업: 한국어 채팅 데이터로 머신러닝 하기&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.scatterlab.co.kr/&quot;&gt;Scatter Lab&lt;/a&gt; 조한석 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;한글 데이터는 문제가 많음:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hell 조사&lt;/li&gt;
  &lt;li&gt;자유로운 언어 변형&lt;/li&gt;
  &lt;li&gt;혀꼬인 소리&lt;/li&gt;
  &lt;li&gt;맞춤법 및 띄어쓰기 오류&lt;/li&gt;
  &lt;li&gt;챗팅에서 쓰이는 단어&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;한국어 데이터는 전처리(Preprocessing)이 80%다.&lt;/p&gt;

&lt;p&gt;일반 오픈소스 형태소 분석기의 한게점: 학습에 사용된 corpus가 잘 정돈된 데이터를 학습했기 때문에, 잘 안되는 경향이 있음.&lt;/p&gt;

&lt;p&gt;그래서 데이터로부터 학습하자! 김현중 님의 &lt;a href=&quot;https://github.com/lovit/soynlp&quot;&gt;soynlp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;다양한 문제들과 문제 정의 및 해결:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Normalize:
    &lt;ul&gt;
      &lt;li&gt;아이디어: 오류가 적다고 생각하는 데이터를 선택 후, 전체 데이터에서 조금 등장한 패턴을 자주 등장하는 것으로 수정하는 방법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;띄어쓰기 교정:
    &lt;ul&gt;
      &lt;li&gt;다음 글자가 띄어쓸지 아닐지 binary Classification&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tokenizing:
    &lt;ul&gt;
      &lt;li&gt;단어 추출 process&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Word Embedding:
    &lt;ul&gt;
      &lt;li&gt;oov 문제 (학습되지 않는 단어는 inference 단계에서 문제)&lt;/li&gt;
      &lt;li&gt;Fasttext 사용: substring 정보 활용 &lt;a href=&quot;https://arxiv.org/abs/1607.01759&quot;&gt;paper&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;한글의 경우: ngam 단위를 글자 / 자음모음으로 하게됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sentence Similarity
    &lt;ul&gt;
      &lt;li&gt;BOW + Word Embedding 방식: 임베딩에 너무 의존하게 됨, 학습된 데이터에 따라서 원하는 결과가 안나올 수도 있음, 따라서 다른 방법을 추가해서 쓰게됨.&lt;/li&gt;
      &lt;li&gt;참고한 논문: &lt;a href=&quot;http://cogcomp.org/papers/SongRo15.pdf&quot;&gt;Unsupervised Sparse Vector Densification for Short Text Similarity&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;팁
    &lt;ul&gt;
      &lt;li&gt;전처리 단계에서 &lt;a href=&quot;https://en.wikipedia.org/wiki/Heaps%27_law&quot;&gt;힙의 법칙(Heap’s Law)&lt;/a&gt; 에 따라서 빈도수가 너무 적은 단어는 과감하게 쳐내기&lt;/li&gt;
      &lt;li&gt;문제 정의를 잘하기, Countbase 모델이 오히려 더 잘 될 수도 있다.&lt;/li&gt;
      &lt;li&gt;unlabel 데이터에 label 을 달아서 인사이트를 얻어보자!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-3&quot;&gt;딥러닝에 필요한 로그 기깔나게 잘 디자인하는 법&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;구글 클라우드 엔지니어 백정상 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;탐색적 데이터 분석(EDA)을 잘 하기위해 어떻게 로그를 쌓는 것이 좋았는가?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로그 디자인&lt;/li&gt;
  &lt;li&gt;명확한 데이터&lt;/li&gt;
  &lt;li&gt;원시 데이터에서 분석 쿼리&lt;/li&gt;
  &lt;li&gt;등등&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 딥러닝은 조금 달랐다!!&lt;/p&gt;

&lt;p&gt;통계적으로 풀지 못하는 문제 생겼는데, 비정상적인 데미지를 만드는 플레이어가 핵유저인지 아닌지? 이상 탐지 문제 (정확히 기억이 안남)&lt;/p&gt;

&lt;p&gt;고전적인 머신러닝 기법으로 풀려고 보니 바운더리 필요하고, 어느정도 데미지가 비정상적인 플레이고, 정상적인 플레이인지 알수 없었음.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;정상적인 데이터가 충분히 많다면, 학습후 비정상 데이터 잡아내기 &lt;strong&gt;(지도학습)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;정상인지 아닌지 확신할 데이터 충분치 않다면, 학습후 클러스터링 해서 아웃라이어 잡기 &lt;strong&gt;(비지도학습)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;데이터가 충분하지 않았기 때문에 2번으로 선택후 가설을 세움.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;가설: 유저들이 평균적으로 내는 데미지에 비해 엄청 크면? –&amp;gt; 비정상&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Feature engineering: feature selection&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;유저 인덱스 들어가면 분류 너무 세분화,&lt;/li&gt;
  &lt;li&gt;페이즈 별로 데미지 얼마 넣었는지 보다 스테이지 완료시 데미지만,&lt;/li&gt;
  &lt;li&gt;캐릭터 직업별로 데미지 넣는 양이 다르니, 직업은 넣자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기까지 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;행복회로! &lt;/span&gt;&lt;/strong&gt; 상상의 나래였던거임.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;-현실-&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;무슨 모델을 쓸 것인가? 꼭 딥러닝을 써야되나?&lt;/p&gt;

&lt;p&gt;오코인코더 써서 대다수 유저와 loss 차이가 많이 나는지 확인! (위에 가설을 검정확인함)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deep AutoEncoder - Compressed Feature Vector: 데이터 복원하는 속성 학습&lt;/li&gt;
  &lt;li&gt;train, 평가 쉬움&lt;/li&gt;
  &lt;li&gt;주의할 것은 &lt;strong&gt;가설&lt;/strong&gt; 이 참이여야만 모델이 정확하다는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;만들면서 생긴 문제들:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;input data 에 따라서 달라짐 &amp;gt; 어떤 feature 를 쓸 것인가?
    &lt;ul&gt;
      &lt;li&gt;꼭 필요함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;훈련방식: 온라인, 배치, 저장된 데이터 &amp;gt; 3개 다해야함
    &lt;ul&gt;
      &lt;li&gt;이유: 왜냐면 많은 사람들이 어뷰징을 쓰게되면 바이어스가 정상으로 학습 될 수도&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;피쳐선택:
    &lt;ul&gt;
      &lt;li&gt;꼭 필요함. 피쳐 늘어날 수록 더 많은 데이터가 필요함&lt;/li&gt;
      &lt;li&gt;EDA: 게임상에서 일어나는 특징적인 패턴을 찾는 데 주력 &amp;gt; 어떤 피쳐가 상관관계가 높지?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로그 디자인: json 으로 일단 저장, nested repeated 정보 어떻게 저장?&lt;/li&gt;
  &lt;li&gt;관리: 용량이 더 커질 수 밖에 없음, 트레이닝 데이터 사이즈를 줄여보는 것도 방법, 콜드데이터는 비용 절감에 주력, 머신러닝에 들어가는 피쳐는 최대한 줄이고 차원을 축소해서 트레이닝 비용을 줄여라.&lt;/li&gt;
  &lt;li&gt;데이터 검증: 관리해야할 로그의 종류와 데이터 타입이 너무 많아짐, 테스트 기반 검증을 진행해야함, 모든 로그 데이터는 검증 로직 테스트를 통과해야 게임 업데이트가 가능 하도록 해야함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론:&lt;/strong&gt; 피쳐가 생명임, 즉 구하려고하는 것과 상관관계가 높은 데이터를 한번 찾아보자&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;협업 방식:&lt;/strong&gt; 처음에는 각 로그별로 정의 문서를 만듬, 나중에는 QA 후 valid 함, 오류나면 커뮤니케이션&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-4&quot;&gt;바닥부터 시작하는 데이터 인프라&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;(전) 레트리카, 변성윤 님&lt;/strong&gt; &lt;a href=&quot;https://zzsza.github.io/&quot;&gt;(블로그)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 대시보드 만들기&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;목표: 팀원들이 조회하고 싶은 데이터를 볼 수 있는 대쉬보드 만들기&lt;/li&gt;
  &lt;li&gt;단순 반복적인 작업을 줄이는 것&lt;/li&gt;
  &lt;li&gt;직접 구현하기 힘들면 오픈소스 툴을 사용하자! (superset)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. 데이터 파이프라인 생성&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;목표 이벤트 레벨까지 데이터를 조회할 수 있는 대시보드&lt;/li&gt;
  &lt;li&gt;문제: 이벤트로그 정리되어 있는가? 테이블 형태가 아니라면 못씀&lt;/li&gt;
  &lt;li&gt;해결: 테이블 형태로 변환&lt;/li&gt;
  &lt;li&gt;문제2: 빅쿼리비용이 너무 크게 나옴&lt;/li&gt;
  &lt;li&gt;해결: 이재광 님(NBT), 데이터를 최대한 줄여라 &amp;gt; flatten table 만들고, 목적에 맞게 데이터 구성(중복하지 않게)&lt;/li&gt;
  &lt;li&gt;기타:
    &lt;ul&gt;
      &lt;li&gt;bigquery vs dataflow&lt;/li&gt;
      &lt;li&gt;task management tool - airflow 도입&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. 음란사진 올라오면?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;처리 프로세스: user &amp;gt; scheduler &amp;gt; docker (NSFW score) &amp;gt; block / unblock&lt;/li&gt;
  &lt;li&gt;사진 말고 비디오, 움짤의 경우? webp 전환후 비디오/움짤의 일부분 만 input으로 집어 넣기&lt;/li&gt;
  &lt;li&gt;콜라보사진: 사람 얼굴 갯수가 많아졌을 때, score가 높음 &amp;gt; 사람 얼굴 갯수로 threshold (정확히 기억이 안남 ㅠㅠ)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. 팁&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;모든것을 만들 필요는 없다.&lt;/li&gt;
  &lt;li&gt;선인의 지혜를 빌리자.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-5&quot;&gt;게임회사 주니어 웹 개발자가 바라본 데이터 분석 이야기&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;넥슨 이준범 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 대시보드&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DB에서 데이터 불러올 때, ORM 굉장히 느리다. (몰랐는데 처음알아따…) 그런데, Pure SQL 썼는데도 느리다 &amp;gt; &lt;strong&gt;“인덱스 타고 있니??”&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DB를 full-scan 하지 않게 만들어야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. 정량적 접근 vs 정성적 접근&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;case1 신규 게임: 데이터가 없는 상황&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;모든 게임에 다 남고 있는 데이터 (접속기록, 계졍명…)&lt;/li&gt;
  &lt;li&gt;게임 유형에 맞는 공통 형식의 로그 -&amp;gt; 쉬움 -&amp;gt; 전부다 잡아낼까?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;case2 게임 특성에 따른 어뷰징&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;어떤 어뷰징 존재한가? 매크로 커뮤니티, 유저들의 신고&lt;/li&gt;
  &lt;li&gt;로그 속에서 패턴 찾아내기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;등등…(나머지 10분 뒤에 내용은 들어도 잘 모르겠어서 정리를 못했다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;그 뒤에 패널 토크에는 어마어마하신 분들이 오신거 같았는데, 못들었다… 다음엔 끝까지 들을 수 있기를~&lt;/p&gt;
</description>
        <pubDate>Sat, 21 Apr 2018 12:47:28 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/04/21/biglittledata.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/04/21/biglittledata.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Word2Vec</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-word2vec&quot;&gt;All about Word Vectors: Word2Vec&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 2 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;p&gt;자연어 처리 공부를 해보신 분이라면 한번쯤 접한 그림이 있을 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_linear-relationships.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“king” - “man” + “woman” = ?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;느낌상 “왕”에서 “남자”라는 속성을 빼주고, “여자”의 속성을 더해주면?&lt;/p&gt;

&lt;p&gt;“queen” 이 나와야할 것 같다. Word Representation은 이런 것을 가능하게 했다.&lt;/p&gt;

&lt;p&gt;이번 시간에는 &lt;strong&gt;Word2vec&lt;/strong&gt; 에 대해서 알아보려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;word2vec&quot;&gt;Word2Vec&lt;/h2&gt;

&lt;p&gt;Word2Vec은 두 가지 알고리즘이 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Skip-grams(SG)
      &lt;ul&gt;
        &lt;li&gt;target 단어를 기반으로 context 단어들을 예측한다. (position independent)&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;Continuous Bag of Words (CBOW)
      &lt;ul&gt;
        &lt;li&gt;context 단어들 집합(bag-of-words context)으로부터 target 단어를 예측한다.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_skipgram1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_skipgram2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_cbow1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_cbow2.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 몇 가지 효율적인 훈련 방법들이 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Two (moderately efficient) training methods (vs Naive Softmax)&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Hierarchical softmax&lt;/li&gt;
    &lt;li&gt;Negative sampling&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://web.stanford.edu/class/cs224n/syllabus.html&quot;&gt;CS224n Lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이번 포스팅에서는 Skip-gram 과 Negative Sampling을 메인으로 소개하겠다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;skip-gram-model-with-naive-softmax&quot;&gt;Skip-gram model with Naive Softmax&lt;/h2&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/1310.4546.pdf&quot;&gt;Distributed Representations of Words and Phrases
and their Compositionality&lt;/a&gt; (Mikolov et al. 2013)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;embedding-look-up&quot;&gt;Embedding Look up&lt;/h3&gt;

&lt;p&gt;모델 설명에 들어가기 앞서 &lt;strong&gt;Embedding Look up&lt;/strong&gt; 이란 것을 알아보자. 이 용어는 이제 여기저기서 많이 나올텐데 알아두면 좋다.&lt;/p&gt;

&lt;p&gt;우리가 하고 싶은 것은 엄청나게 차원이 큰 one-hot vector 를 고정된 작은 차원으로 넣고 싶은 것이다. 어떻게 하면 단어들을 &lt;strong&gt;2-dimension matrix&lt;/strong&gt; 로 표현 할 수 있을까?&lt;/p&gt;

&lt;p&gt;아래 그림의 예를 보자. 8차원 one-hot vector를 3차원으로 만들고 싶다. 그렇다면 $3\times 8$ 행렬을 만들어서 각 column vector 가 하나의 3차원 단어를 표현하면 2-D Matrix 가 되지 않는가? 이 Matrix를 &lt;strong&gt;Embedding Matrix&lt;/strong&gt; 라고 부르기로 하자&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup2.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그렇다면 어떻게 각 단어와 이 Embedding Matrix 를 매칭 시킬수 있을까? 여기서 &lt;strong&gt;내적&lt;/strong&gt; 을 활용하게 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider3&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup4.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup5.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그런데 자세히 보니, one-hot vector의 숫자 $1$ 이 위치한 index 가 Embedding Matrix 의 column vector 의 index 와 같다. 따라서 중복되지 않는 단어사전을 만들고, 각 단어에 대해 index를 메긴 다음, 찾고 싶은 단어를 Embedding Matrix 에서 column vector index 만 &lt;strong&gt;조회(Look up)&lt;/strong&gt; 하면 되는 것이다.&lt;/p&gt;

&lt;ul id=&quot;light-slider4&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup6.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup7.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;코드 예시:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
sentence = &quot;I am going to watch Avengers Infinity War&quot;.split()
embedding_matrix = np.array([[1,2,5,1,9,10,3,4], [5,1,4,1,8,1,2,5], [7,8,1,4,1,6,2,1]])
vocab = {w: i for i, w in enumerate(sentence)}
word = &quot;I&quot;
print(embedding_matrix)
print(&quot;=&quot;*30)
print(&quot;Word:&quot;, word)
print(&quot;Index:&quot;, vocab[word])
print(&quot;Vector:&quot;, embedding_matrix[:, vocab.get(word)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;[[ 1  2  5  1  9 10  3  4]&lt;/p&gt;

  &lt;p&gt;[ 5  1  4  1  8  1  2  5]&lt;/p&gt;

  &lt;p&gt;[ 7  8  1  4  1  6  2  1]]&lt;/p&gt;

  &lt;p&gt;==============================&lt;/p&gt;

  &lt;p&gt;Word: I&lt;/p&gt;

  &lt;p&gt;Index: 0&lt;/p&gt;

  &lt;p&gt;Vector: [1 5 7]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이해가 됐으면 이제 모델로 들어가보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_model_train.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;요약&lt;/h3&gt;

&lt;p&gt;Skip-gram 모델을 한 마디로 설명하자면, 문장의 모든 단어가 한번 씩 중심단어 $c$ 가 되어, $c$ 주변 문맥 단어 $o$ 가 나올 확률을 최대화 하는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;목적&lt;/h3&gt;

&lt;p&gt;각 중심단어 $c$ 에 대해서 아래의 &lt;strong&gt;가능도/우도 (Likelihood)&lt;/strong&gt; 를 구해본다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} p(w_{t+j} | w_t; \theta) \quad \cdots\cdots \quad (1)&lt;/script&gt;

&lt;p&gt;수식을 말로 풀어보자. 각 포지션 $(\prod_{t=1}^{T})$ 의 중심단어 $c$ = $w_t$ 에 대해서, $w_t$ 가 주어졌을 때 다른 문맥단어 $o$ = $w_{t+j}$ 가 나오는 확률 $\big( p (w_{t+j} \vert w_t; \theta) \big)$ 을 가능하게 만드는 $\theta$ 를 구하는 것이다. 단 $j$ 는 윈도우 크기 $m$ 을 넘지 않으며, $0$ 이 될 수 없다.&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;Likelihood&lt;/strong&gt; 를 &lt;strong&gt;최대화&lt;/strong&gt; 하는 것이 우리의 목적이 되겠다.&lt;/p&gt;

&lt;p&gt;그러나 여기서는 우리가 좋아하는 Gradient Descent 를 사용하기 위해서 이 식을 &lt;strong&gt;Negative Log Likelihood&lt;/strong&gt; 로 변형해서 쓰기로한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min J(\theta) = -\dfrac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m,\ j \neq 0} \log p(w_{t+j} | w_t) \quad \cdots\cdots \quad (2)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$(1)$ 식과 $(2)$ 식이 왜 동등한지는 밑에 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt;&lt;/strong&gt; 을 확인하길 바란다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 단어가 등장할 확률 $p(w_{t+j} \vert w_t)$ 는 어떻게 구할 것인가?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Softmax&lt;/strong&gt; 라는 input 값을 0과 1 사이로 만들어 주는 친근한 함수가 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(o|c) = \dfrac{\exp(u_o^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)} \quad \cdots\cdots \quad (3)&lt;/script&gt;

&lt;p&gt;따라서 모델에 있는 모든 파라미터를 $\theta \in \Bbb{R}^{2dV}$ 로 두고, $(2)$ 식을 최적화 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;왜 $\theta \in \Bbb{R}^{2dV}$ 인가?
Center Word 의 Embedding Matrix $W$ Context Words 의 Embedding Matrix $W’$ 두개를 학습 시켜야하기 때문이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;주의 )&lt;/span&gt;&lt;/strong&gt; $W’$ 는 $W$ 의 전치 행렬이 아니라 완전히 새로운 Embedding Matrix 다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;update&quot;&gt;Update&lt;/h3&gt;

&lt;p&gt;Gradient를 통해서 각 파라미터들을 업데이트 하게 된다. $(3)$ 식의 $\log$ 를 취하게 되면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f = \log \dfrac{\exp(u_o^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)}&lt;/script&gt;

&lt;p&gt;이제 $f$ 의 Gradient 를 구해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \dfrac{\partial f}{\partial V_c}
&amp;= \dfrac{\partial }{\partial V_c} \big(\log(\exp(u_o^T V_c)) - \log(\sum_{w=1}^V \exp(u_w^T V_c))\big) \\
&amp;= u_o - \dfrac{1}{\sum_{w=1}^V \exp(u_w^T V_c)}(\sum_{x=1}^V \exp(u_x^T V_c) u_x ) \\
&amp;= u_o - \sum_{x=1}^V \dfrac{\exp(u_x^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)} u_x \\
&amp;= u_o - \sum_{x=1}^V P(x | c) u_x
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$u_o$ : observed word, output context word&lt;/li&gt;
  &lt;li&gt;$P(x\vert c)$: probs context word $x$ given center word $c$&lt;/li&gt;
  &lt;li&gt;$P(x\vert c)u_x$: Expectation of all the context words: likelihood occurance probs $\times$ context vector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;흥미로운 점: &lt;strong&gt;미분 값&lt;/strong&gt; 은 관측된 context word 벡터 $u_o$ 에서 center word $c$ 가 주어졌을 때 나올 수 있는 모든 단어의 기대치를 빼준 다는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;why-mle-is-equivalent-to-minimize-nll&quot;&gt;참고 1: Why MLE is equivalent to minimize NLL?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt; 의 정의:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta|x_1,\cdots,x_n) = f(x_1, \cdots, x_n|\theta) = \prod_{i=1}^n f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;log를 취하게 되면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log L(\theta|x_1,\cdots,x_n) =  \sum_{i=1}^n log f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;MLE(maximum likelihood estimator)&lt;/strong&gt; 의 정의:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MLE} = \underset{\theta}{\arg \max} \sum_{i=1}^n \log f(x_i|\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{x}{\arg \max} (x) = \underset{x}{\arg \min}(-x)&lt;/script&gt;

&lt;p&gt;때문에 우리는 아래의 식을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MLE} = \underset{\theta}{\arg \max} \sum_{i=1}^n \log f(x_i|\theta) = \underset{\theta}{\arg \min} -\sum_{i=1}^n \log f(x_i|\theta)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;왜 log 로 바꾸는 것인가?
    &lt;ol&gt;
      &lt;li&gt;컴퓨터 연산시 곱하기 보다 더하기를 쓰면 &lt;strong&gt;복잡도&lt;/strong&gt; 가 훨씬 줄어들어 계산이 빠르다. ($O(n) \rightarrow O(1)$)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;언더플로우&lt;/strong&gt; 를 방지할수 있다. 언더플로우란 1보다 작은 수를 계속곱하면 0에 가까워져 컴퓨터에서 0 으로 표시되는 현상을 말한다.&lt;/li&gt;
      &lt;li&gt;자연로그함수는 &lt;strong&gt;단조증가함수(monotonic increase function)&lt;/strong&gt; 라서 대소관계가 바뀌지 않는다. 예를 들자면, $5 &amp;lt; 10 \Longleftrightarrow log(5) &amp;lt; log(10)$ 의 관계가 바뀌지 않는 다는 것. 따라서 언제든지 지수를 취해서 다시 원래의 값으로 복귀 가능.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;참고
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/&quot;&gt;why minimize negative log likelihood&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ratsgo.github.io/deep%20learning/2017/09/24/loss/&quot;&gt;(ratsgo 님) 손실함수&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음 시간에는 &lt;strong&gt;Naive Softmax&lt;/strong&gt; 로 훈련 시켰을 때의 단점과 이를 보완 해준 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;Negative Sampling&lt;/span&gt;&lt;/strong&gt; 에 대해서 알아보자.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Apr 2018 10:19:06 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/20/allaboutwv2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/20/allaboutwv2.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
  </channel>
</rss>
