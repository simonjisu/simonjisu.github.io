<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soo</title>
    <description>My Blog
</description>
    <link>http://simonjisu.github.io/</link>
    <atom:link href="http://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 28 Oct 2018 22:37:08 +0900</pubDate>
    <lastBuildDate>Sun, 28 Oct 2018 22:37:08 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>[Flask-0] 설치하기</title>
        <description>&lt;h1 id=&quot;flask-0-&quot;&gt;[Flask-0] 설치하기&lt;/h1&gt;

&lt;p&gt;python 으로 딥러닝 공부도 열심히 하지만, 라즈베리파이 서버에 웹 데모 프로그램 만드려고 한다. 나중에 app 만들때 활용할 수 있을 것 같다.&lt;/p&gt;

&lt;p&gt;우선 Flask 가 무엇인지는 공식 튜토리얼 문서를 한 번 쓰윽 훑고 오자. 머리말을 읽는 것은 중요하다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://flask-docs-kr.readthedocs.io/ko/latest/foreword.html&quot;&gt;공식 튜토리얼 - 머리말&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;읽고 오면 이제 본격적으로 시작해보자.&lt;/p&gt;

&lt;p&gt;컴퓨터를 다룰때 제일 짜증나는 부분이 설치다. 내 마음대로 안되는 것도 컴퓨터다. 시행착오도 겪어야 하고 … 아주 오냐오냐 해줘야 말을 듣기 때문에, 같이 한번 잘 다뤄줘보자.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section&quot;&gt;가상환경 설치&lt;/h2&gt;

&lt;p&gt;가상환경으로 작업하는 이유는 구글한테 물어보면 잘 대답해준다. 개인적으로는 &lt;strong&gt;“지우기 편하다”&lt;/strong&gt; 가 제일 큰 장점인것 같다.&lt;/p&gt;

&lt;p&gt;virtualenv 패키지를 받는다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo pip install virtualenv
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;자신의 폴더로 들어가서 가상환경을 만든다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ mkdir NMT_demo
$ cd NMT_demo
$ virtualenv nmt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;가상환경으로 접속한다&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ . nmt/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;앞에 괄호에 만든 가상환경이 뜨면, 성공한것이다. 만약에 가상환경을 빠져나오려면 아래와 같이 실행한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(nmt) $ deactivate
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-1&quot;&gt;필요한 패키지 설치하기&lt;/h2&gt;

&lt;h3 id=&quot;flask&quot;&gt;Flask&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(nmt) $ pip install Flask
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;pytorch&quot;&gt;PyTorch&lt;/h3&gt;

&lt;p&gt;PyTorch 를 설치하는 이유는 단순히 훈련된 모델을 실행하여 결과값을 얻으려고 하는 것이다. 필요없다면 이 과정을 건너뛰어도 좋다.&lt;/p&gt;

&lt;p&gt;만약에 당신의 서버가 라즈베리파이가 아니라면 그냥 &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install torch torchvision torchtext&lt;/code&gt; 을 써준다.&lt;/p&gt;

&lt;p&gt;하지만 우리의 귀엽고 작은 라즈베리파이는 파이토치를 pip 로 바로 설치를 못한다. “&lt;a href=&quot;https://gist.github.com/fgolemo/b973a3fa1aaa67ac61c480ae8440e754&quot;&gt;라즈베리파이에 파이토치 설치하기&lt;/a&gt;” 를 확인하고 따라해보자. 우선 필수 패키지를 설치해준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(nmt) $ sudo apt-get install libopenblas-dev cython libatlas-dev \
m4 libblas-dev python3-dev cmake python3-yaml
(nmt) $ pip install pyyaml  # 설치가 안됐을 수도 있으니까 따로 한번 설치해준다.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;pytorch 설치전 리눅스 환경변수를 만들어준다. 우리의 작은 라즈베리파이는 GPU를 지원할 CUDA 가 필요 있을 리가 없다. “NO_CUDA” 변수를 1로 설정한다. “NO_DISTRIBUTED” 는 뭔지 모르겠다. (알려주세요)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(nmt) $ export NO_CUDA=1
(nmt) $ export NO_DISTRIBUTED=1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;download 폴더를 만들어서 안에 PyTorch 를 clone 한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(nmt) $ mkdir downloads
(nmt) $ cd downloads
(nmt) $ git clone --recursive https://github.com/pytorch/pytorch
(nmt) $ cd pytorch
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;PyTorch 를 빌드한다. 약 2~3시간 걸린다. &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%9D%B8%EC%85%89%EC%85%98&quot;&gt;영화&lt;/a&gt; 한 편을 보고 오면 딱이다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(nmt) $ python3 setup.py build
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;에러가 없을 경우 아래를 계속 진행한다. “-E” 는 여기서 중요한데, 아까 설정한 환경변수를 포함해서 실행하게 만드는 것이다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(nmt) $ sudo -E python3 setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;다음 시간에는 빠르게 앱을 만들어보고 잘 작동하는지 테스트를 해볼 예정이다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://flask-docs-kr.readthedocs.io/ko/latest/installation.html&quot;&gt;flask 한글 튜토리얼&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://onecellboy.tistory.com/220&quot;&gt;리눅스 환경변수 확인하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gist.github.com/fgolemo/b973a3fa1aaa67ac61c480ae8440e754&quot;&gt;라즈베리파이에 파이토치 설치하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 28 Oct 2018 19:22:40 +0900</pubDate>
        <link>http://simonjisu.github.io/flask/2018/10/28/flaskstudy0.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/flask/2018/10/28/flaskstudy0.html</guid>
        
        
        <category>Flask</category>
        
      </item>
    
      <item>
        <title>A Neural Probabilistic Language Model</title>
        <description>&lt;h1 id=&quot;paper-a-neural-probabilistic-language-model&quot;&gt;[PAPER] A Neural Probabilistic Language Model&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;클릭하면 링크를 따라갑니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;paper: &lt;a href=&quot;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot;&gt;A Neural Probabilistic Language Model&lt;/a&gt; - Yoshua Bengio, 2003&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://bit.ly/2OkYFkY&quot;&gt;Slide Share&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bit.ly/2PsEPpg&quot;&gt;Code Repo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/deepnlp_study/blob/master/notebook/01_NNLM.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section&quot;&gt;메인 아이디어&lt;/h2&gt;

&lt;p&gt;기존의 통계 기반의 Language Modeling 은 N-Gram 을 기반으로, 이전 토큰의 나오는 단어를 기반으로 다음 단어의 확률을 극대화 작업이었다. 처음부터 끝까지 보지 않고 N-Gram으로 잘라서 예측하게된 이유는 예측해야할 단어와 아주 오래된 단어간의 상관관계가 적다(혹은 분포가 다르다)라고 생각했기 때문이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(w_t \vert w_1, w_2, \cdots, w_{t-1}) \approx P(w_t \vert t_{t-(n-1)}, \cdots t_{t-1})&lt;/script&gt;

&lt;p&gt;확률을 정의 할때는 Counting 을 사용했다. 예를 들어, “New York” 뒤에 “University” 가 나올  확률을 예측한다고 해보자. 데이터에서 “New York University” 가 등장한 횟수를 세고, “New York” 뒤에 올 수있는 다른 모든 단어의 등장 횟수를 다 Count 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(University \vert New, York) = \dfrac{Count(New, York, University)}{\sum_w Count(New, York, w)}&lt;/script&gt;

&lt;p&gt;해당 방법은 간단하나 문제점이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;첫째, 훈련데이터에서 보지 못한 새로운 단어 조합이 등장하게 되면 확률이 0 이 된다.&lt;/li&gt;
  &lt;li&gt;둘째, N-Gram 의 N 은 작은 수로 적을 수 밖에 없다. 1만개의 단어가 있으면 1-Gram 은 1만, 2-Gram 은 약 5천만 ($C_{10000}^{2}$), 3-Gram 은 약 1700억 ($C_{10000}^{3}$) 이 된다. 즉, N 이 커질수록 계산을 하기 위한 더 많은 컴퓨터 자원이 필요하다. (논문이 나온 2003년때 쯤에는 요즘같이 계산을 하기 위한 GPU 도 없었을 것이다.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서 논문에는 위 두가지 문제점을 해결하기 위해 N 을 더 늘리고, 새로 등장한 단어에 대해서도 예측가능한 모델을 만들고자 했다.&lt;/p&gt;

&lt;p&gt;Yoshua Bengio 교수님이 제안한 모델의 특징은 3 가지로 요약 할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;단어를 m 차원에 벡터와 연관 짓는다.&lt;/li&gt;
    &lt;li&gt;m 차원 벡터로 표현된 단어들의 조건부 확률을 표현한다.&lt;/li&gt;
    &lt;li&gt;조건부 확률과 m 차원의 벡터를 동시에 학습한다.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;아래에서 더 풀어서 설명한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-1&quot;&gt;모델 설명&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/8thdipjnc7bl95f/0826_nnlm.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 가지 단계로 모델이 구성되 있다.&lt;/p&gt;

&lt;h3 id=&quot;distributed-feature-vectors&quot;&gt;1 단계: Distributed feature vectors&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;각 단어를 $C$ 행렬을 통해 $m$ 차원 벡터로 표현한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;단어를 $m$ 차원 실수 벡터로 연관 지어야 한다. 최근에는 이 방법을 임베딩 (Embedding)이라고 하는데 논문에서는 분산된 특징 벡터 (Distributed feature vectors) 라고 했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C(i) \in \Bbb{R}^m&lt;/script&gt;

&lt;p&gt;$C$ 행렬의 $i$ 번째 행을, $i$ 번째 단어의 벡터라고 규정 지었으며, $C$ 의 형태는 $\vert V \vert \times m$ 다.&lt;/p&gt;

&lt;h3 id=&quot;probability-functions&quot;&gt;2 단계: Probability functions&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;$m$ 차원으로 표현된 벡터를 2 층의 신경망을 사용해서 조건부 확률을 구성한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;우선 임베딩된 $m$ 차원의 벡터들을 concatenate 하여 하나의 벡터로 만든다. 이를 context 라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \big( C(w_{t-n+1}), \cdots, C(w_{t-2}), C(w_{t-1}) \big)&lt;/script&gt;

&lt;p&gt;그 후 2층의 신경망에 통과시켜 Softmax 로 최종적인 확률을 구한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = U \tanh(d + Hx)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(w_i = i \vert context) = \dfrac{\exp(y_{w_t})}{\sum_i \exp(y_i)}&lt;/script&gt;

&lt;h3 id=&quot;direct-connection&quot;&gt;기타: direct connection&lt;/h3&gt;

&lt;p&gt;논문에서는 실험적으로 선형적인 관계식을 하나 더 넣어서 context 와 y 사이의 선형관계를 알아내고자 했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \underbrace{b + Wx}_{\text{direct connection}} +U \tanh(d + Hx)&lt;/script&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-2&quot;&gt;실험결과&lt;/h2&gt;

&lt;h3 id=&quot;perplexity&quot;&gt;Perplexity&lt;/h3&gt;

&lt;p&gt;Test Measurement 로 &lt;strong&gt;Perplexity&lt;/strong&gt; 를 선택했다. 정의는 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A measurement of how well a probability distribution or probability model (q) predicts a sample&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;PP = \exp(-\dfrac{1}{N} \sum_{i=1}^N \log_e q(x_i))&lt;/script&gt;

&lt;p&gt;자세히 보면 지수안에 엔트로피 함수가 들어가 있는 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;해당 수식을 말로 풀어보면, 모든 테스트 세트에서 확률 모델 $q$ 의 불확실 정도가 어떻게 되는지를 측정한다. 즉, 이 값이 높을 수록 모델이 예측을 잘 못하며, 낮을 수록 해당테스트 토큰을 확실하게 측정한다는 뜻이다.&lt;/p&gt;

&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;

&lt;p&gt;시간을 측정한 이유는 학습할 파라미터 숫자가 생각보다 많기 때문이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = (b, d, W, U, H, C)&lt;/script&gt;

&lt;p&gt;총 학습해야할 파라미터 수는 $\vert V \vert(1+mn+h)+h(1+(n-1)m)$ 로 계산된다.&lt;/p&gt;

&lt;p&gt;각 행렬의 크기를 아래에 표시해 두었다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} b &amp;= \vert V \vert \\
d &amp;= h \\
U &amp;= \vert V \vert \times h \\
W &amp;= \vert V \vert \times (n-1)m \\
H &amp;= h \times (n-1)m \\
C &amp;= \vert V \vert \times m
\end{aligned} %]]&gt;&lt;/script&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/c975f2j26kzj715/0826_nnlmresult.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;영어 단어 Brown corpus (약 16000개의 단어)에 대해서, 은닉층 유닛수를 100, 임베딩 차원을 30, direct connection이 없고, 5-Gram 을 사용했을 때 결과는 Perplexity 가 제일 낮았다.&lt;/p&gt;

&lt;p&gt;개인적인 실험으로 네이버 영화 corpus [&lt;a href=&quot;https://github.com/e9t/nsmc&quot;&gt;링크&lt;/a&gt;]를 사용하여 평균 perplexity 측정해보았다. [&lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/deepnlp_study/blob/master/notebook/01_NNLM.ipynb&quot;&gt;노트북링크&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;한글 데이터를 사용해 최대한 단어갯수를 줄이려고 문장당 부호 및 단일 한글자음모음을 하나로 제약했다. 따라서 총 약 6만개의 단어가 사용됐다. 10 번의 epoch를 훈련 시킨 결과 Perplexity는 계속 떨어지지만 accuracy 는 20% 이후 상승이 멈췄다. 또한 훈련시간이 34 분 가량 걸렸다. 그만큼 학습할 파라미터가 많다는 뜻이다.&lt;/p&gt;

&lt;p&gt;아래 문장으로 언어모델링을 해보았다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;요즘 나오는 어린이 영화보다 수준 낮은 시나리오 거기다 우리가 아는 윌스미스 보다 어린 윌스미스에 발연기는 보너스&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;input&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;predict&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;target&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;요즘/Noun 나오는/Verb 어린이/Noun 영화/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;가/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;보다/Josa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;나오는/Verb 어린이/Noun 영화/Noun 보다/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;더/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;수준/Noun&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;어린이/Noun 영화/Noun 보다/Josa 수준/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;이/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;낮은/Adjective&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;영화/Noun 보다/Josa 수준/Noun 낮은/Adjective&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;영화/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;시나리오/Noun&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;보다/Josa 수준/Noun 낮은/Adjective 시나리오/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;가/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;거기/Noun&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;수준/Noun 낮은/Adjective 시나리오/Noun 거기/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;에/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;다/Josa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;낮은/Adjective 시나리오/Noun 거기/Noun 다/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;./Punctuation&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;우리/Noun&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;시나리오/Noun 거기/Noun 다/Josa 우리/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;는/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;가/Josa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;거기/Noun 다/Josa 우리/Noun 가/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;뭐/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;아는/Verb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;다/Josa 우리/Noun 가/Josa 아는/Verb&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;사람/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;윌스미스/Noun&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;우리/Noun 가/Josa 아는/Verb 윌스미스/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;인데/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;보다/Verb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;가/Josa 아는/Verb 윌스미스/Noun 보다/Verb&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;가/Eomi&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;어린/Verb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;아는/Verb 윌스미스/Noun 보다/Verb 어린/Verb&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;애/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;윌스미스/Noun&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;윌스미스/Noun 보다/Verb 어린/Verb 윌스미스/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;가/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;에/Josa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;보다/Verb 어린/Verb 윌스미스/Noun 에/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;대한/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;발연기/Noun&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;어린/Verb 윌스미스/Noun 에/Josa 발연기/Noun&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;에/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;는/Josa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;윌스미스/Noun 에/Josa 발연기/Noun 는/Josa&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;좋/Adjective&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;보너스/Noun&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;결과를 살펴보면 문맥은 상당히 못맞추었으나 문장의 구조는 잘 학습한 것을 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Wed, 22 Aug 2018 23:26:14 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/08/22/neuralnetworklm.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/08/22/neuralnetworklm.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Torchtext Tutorial</title>
        <description>&lt;h1 id=&quot;pytorch-torchtext-tutorial&quot;&gt;Pytorch TorchText Tutorial&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;튜토리얼 Notebook: &lt;a href=&quot;https://github.com/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb&quot;&gt;github&lt;/a&gt;, &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb&quot;&gt;nbviewer&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;자연어 처리에서 전처리시 자주 사용하는 패키지 하나를 소개하려고 한다.&lt;/p&gt;

&lt;p&gt;Pytorch 는 데이터를 불러오는 강력한 &lt;a href=&quot;https://pytorch.org/docs/stable/data.html&quot;&gt;Data Loader&lt;/a&gt; 라는 유틸이 있는데, TorchText 는 NLP 분야만을 위한 Data Loader 이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;documentation:&lt;/strong&gt; &lt;a href=&quot;http://torchtext.readthedocs.io/en/latest/index.html&quot;&gt;http://torchtext.readthedocs.io/en/latest/index.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;설치:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install torchtext
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;TorchText 는 자연어 처리에서 아래의 과정을 한번에 쉽게 해준다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;토크나이징(Tokenization)&lt;/li&gt;
  &lt;li&gt;단어장 생성(Build Vocabulary)&lt;/li&gt;
  &lt;li&gt;토큰의 수치화(Numericalize all tokens)&lt;/li&gt;
  &lt;li&gt;데이터 로더 생성(Create Data Loader)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;사용방법&lt;/h2&gt;

&lt;h3 id=&quot;create-field&quot;&gt;1. 필드지정(Create Field)&lt;/h3&gt;

&lt;p&gt;필드란 텐서로 표현 될 수 있는 텍스트 데이터 타입을 처리한다. 각 토큰을 숫자 인덱으로 맵핑시켜주는 단어장(Vocabulary) 객체가 있다. 또한 토큰화 하는 함수, 전처리 등을 지정할 수 있다.&lt;/p&gt;

&lt;p&gt;아래와 같은 문장과 이에 대한 긍정/부정 정도를 분류하는 데이터셋이 있다면,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[&quot;The Importance of Being Earnest , so thick with wit it plays like a reading from Bartlett 's Familiar Quotations&quot;, 
'3']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;텍스트를 뜻하는 &lt;code class=&quot;highlighter-rouge&quot;&gt;TEXT&lt;/code&gt;, 해당 문장의 sentiment 를 뜻하는 &lt;code class=&quot;highlighter-rouge&quot;&gt;LABEL&lt;/code&gt; 필드객체 두 개를 만든다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchtext.data import Field

TEXT = Field(sequential=True,
             use_vocab=True,
             tokenize=str.split,
             lower=True, 
             batch_first=True)  
LABEL = Field(sequential=False,  
              use_vocab=False,   
              preprocessing = lambda x: int(x),  
              batch_first=True)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;sequential:&lt;/strong&gt; TEXT 는 순서가 있는 (sequential) 데이터기 때문에 인자를 True 로 두고, LABEL 데이터는 순서가 필요없기 때문에 False 로 둔다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;use_vocab:&lt;/strong&gt; 단어장(Vocab) 객체를 사용할지의 여부. 텍스트 데이터있는 TEXT 에만 True 로 인자를 전달한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;tokenize:&lt;/strong&gt; 단어의 토크나이징을 맡아줄 함수다. 여기선 “공백”을 기준으로 나누는 함수를 사용했습니다. 한국어의 경우 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;konlpy&lt;/code&gt; 의 토크나이징 함수들을 사용한다. 혹은 개인이 만든 함수도 사용할 수 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;lower:&lt;/strong&gt; 소문자 전환 여부 입니다. 보통 True 로 두며, 단어가 많아질수록 나중에 더 많은 차원에 임베딩해야하기 때문에, 왠만하면 영어는 소문자로 만들어준다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;batch_first:&lt;/strong&gt; 배치를 우선시 하게 되면, tensor 의 크기는 (B, 문장의 최대 길이) 로 만들어진다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;preprocessing:&lt;/strong&gt; 전처리는 토큰화 후, 수치화하기 전 사이에서 작동한다. 여기서는 Label 데이터가 string 타입이기 때문에 int 타입으로 만들어준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;더 자세한 것은 문서를 참조하시길 바란다.&lt;/p&gt;

&lt;h3 id=&quot;create-datasets&quot;&gt;2. 데이터 세트 만들기(Create Datasets)&lt;/h3&gt;

&lt;p&gt;데이터 세트는 위에 지정한 필드에 기반하여 데이터를 불러오는 작업을 한다. 보통 Train, Valid, Test 세트가 있으면 &lt;code class=&quot;highlighter-rouge&quot;&gt;splits&lt;/code&gt; 메서드를 사용해서 아래와 같이 만들어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchtext.data import TabularDataset

train_data = TabularDataset.splits(path='./data/',
					train='train_path',
					valid='valid_path',
					test='test_path',
					format='tsv', 
					fields=[('text', TEXT), ('label', LABEL)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;만약에 없다면? 아래와 같이 객체에 그냥 넣어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_data = TabularDataset(path='./data/examples.tsv', 
				format='tsv', 
				fields=[('text', TEXT), ('label', LABEL)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;fields:&lt;/strong&gt; 아까 만들어준 필드는 리스트 형태로 &lt;code class=&quot;highlighter-rouge&quot;&gt;[('필드이름(임의지정)', 필드객체), ('필드이름(임의지정)', 필드객체)]&lt;/code&gt; 로 넣어준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;build-vocabulary&quot;&gt;3. 단어장 생성(Build vocabulary)&lt;/h3&gt;

&lt;p&gt;토큰과 Interger index 를 매칭시켜주는 단어장을 생성한다. 단, 기본적으로 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;unk&amp;gt;&lt;/code&gt; 토큰을 0, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰을 1 로 만들어준다. 단, 필드지정시, 문장의 시작 토큰(init_token)과, 끝의 토큰(eos_token)을 넣으면 3, 4 번으로 할당된다. 메서드 안에는 생성한 데이터 세트를 넣어준다.&lt;/p&gt;

&lt;p&gt;훈련 데이터를 기반으로 단어장을 생성하려면 아래의 명령어를 입력한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TEXT.build_vocab(train_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;create-data-loader&quot;&gt;4. 데이터 로더 만들기(Create Data Loader)&lt;/h3&gt;

&lt;p&gt;마지막으로 배치 사이즈 만큼 데이터를 불러올 데이터 로더를 만든다. 데이터 세트 때와 마찬가지로 데이터 세트가 분리되어 있다면, &lt;code class=&quot;highlighter-rouge&quot;&gt;splits&lt;/code&gt; 메서드를 사용한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchtext.data import Iterator

train_loader, valid_loader, test_loader = \
	TabularDataset.splits((train_data, valid_data, test_data), 
				batch_size=3, 
				device=None,  # gpu 사용시 &quot;cuda&quot; 입력
				repeat=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;만약에 없다면? 아래와 같이 &lt;code class=&quot;highlighter-rouge&quot;&gt;Iterator&lt;/code&gt; 객체에 그냥 넣어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_loader = Iterator(train_data, 
			batch_size=3, 
			device=None,  # gpu 사용시 &quot;cuda&quot; 입력
			repeat=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이렇게 하면 매 배치 때마다 최대 길이에 따라 알아서 패딩(padding) 작업도 같이 해준다. 패딩이란 문장의 길이를 같게 만들기 위해서 의미없는 토큰 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 를 나머지 길이가 부족한 문장에게 붙여주는 토큰이다. 잘 생각해보면 input 길이를 같게 만들어 주는 과정이다.&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;테스트&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for batch in train_loader:
    break
print(batch.text)
print(batch.label)
=======================================================
tensor([[   643,    191,      4,     43,   1447,      3,   4384,    485,
              7,    207,    892,    107,     43,     85,    408,      3,
            376,     17,      5,   6447,  11035,     37,     98,     43,
            199,   5859,      2,      1,      1,      1,      1,      1,
              1,      1,      1],
        [     3,   4515,     51,    444,      4,   3738,     30,     94,
            957,   3498,     59,    700,  13967,      6,   2287,   4435,
              4,    431,     40,      3,   1201,      7,    486,   1134,
           4120,     59,      5,    166,   1749,    547,      6,   1339,
            144,  14759,      2],
        [    29,      7,    195,    568,    192,     63,    229,     60,
             17,     21,    202,    334,     18,      5,    535,     20,
              4,     15,    628,    231,     52,      9,    303,    195,
           6910,      8,  10136,      8,      3,   2204,   4340,      2,
              1,      1,      1]])
tensor([ 0,  3,  1])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3개의 배치에, 각 토큰에 해당하는 단어의 숫자가 들어가게 되고, 패딩 또한 잘 되었다.&lt;/p&gt;

&lt;p&gt;이처럼 거의 5 줄이면 이 모든 과정을 처리해주는 강력크한 도구다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-2&quot;&gt;만약에 사용하지 않겠다면?&lt;/h2&gt;

&lt;p&gt;노트북: &lt;a href=&quot;https://github.com/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb&quot;&gt;github&lt;/a&gt;, &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb&quot;&gt;nbviewer&lt;/a&gt; 에 상세하게 나만의 데이터 로더를 커스터마이징 하는 방법 또한 적어 두었다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;순수 파이썬 만 사용한 코드&lt;/li&gt;
  &lt;li&gt;파이토치의 Custom Dataset 를 활용한 Data Loader 만들기&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;하지만, 여기서는 소개하지 않겠다. 한 번 TorchText를 사용하게 되면 위 두 가지 방법은 왠만하면 생각도 안날 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-3&quot;&gt;다양한 데이터 세트&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torchtext.datasets&lt;/code&gt; 안에는 자연어 처리에 많이 사용되는 데이터 세트들이 이미 포함돼있다. 여기서는 소개하지 않겠다.&lt;/p&gt;

&lt;p&gt;Documentation 참고: &lt;a href=&quot;http://torchtext.readthedocs.io/en/latest/datasets.html#&quot;&gt;http://torchtext.readthedocs.io/en/latest/datasets.html#&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sentiment Analysis&lt;/li&gt;
  &lt;li&gt;Question Classification&lt;/li&gt;
  &lt;li&gt;Entailment&lt;/li&gt;
  &lt;li&gt;Language Modeling&lt;/li&gt;
  &lt;li&gt;Machine Translation&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 19 Jul 2018 00:18:29 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/07/19/torchtext.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/07/19/torchtext.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Pytorch 의 PackedSequence object 알아보기</title>
        <description>&lt;h1 id=&quot;pytorch--packedsequence-object-&quot;&gt;Pytorch 의 PackedSequence object 알아보기&lt;/h1&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;packedsequence-&quot;&gt;PackedSequence 란?&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;아래의 일련의 과정을 PackedSequence 라고 할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;NLP 에서 매 배치(batch)마다 고정된 문장의 길이로 만들어주기 위해서 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰을 넣어야 한다. 아래 그림의 파란색 영역은 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/ctd209m9zlzs0cw/0705img1.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;사진 출처: &lt;a href=&quot;https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983&quot;&gt;Understanding emotions — from Keras to pyTorch&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그림과 같은 내용을 연산을 하게 되면, 쓸모없는 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰까지 연산을 하게 된다.
따라서 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 를 계산 안하고 효율적으로 진행하기 위해 병렬처리를 하려고한다. 그렇다면 아래의 조건을 만족해야한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN의 히든 스테이트가 이전 타임스텝에 의존해서 최대한 많은 토큰을 병렬적으로 처리해야한다.&lt;/li&gt;
  &lt;li&gt;각 문장의 마지막 토큰이 마지막 타임스텝에서 계산을 멈춰야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아직 어떤 느낌인지 잘 모르겠다면 아래의 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/3ze3svhdz05aakk/0705img3.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, 컴퓨터로 하여금 각 &lt;strong&gt;타임스텝&lt;/strong&gt;(T=배치내에서 문장의 최대 길이) 마다 일련의 단어를 처리해야한다는 뜻이다.&lt;/p&gt;

&lt;p&gt;하지만 $T=2, 3$ 인 부분은 중간에 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt;이 끼어 있어서 어쩔수 없이 연산을 하게 되는데, 이를 방지하기 위해서, 아래의 그림같이 각 배치내에 문장의 길이를 기준으로 &lt;span style=&quot;color: #e87d7d&quot;&gt;정렬(sorting)&lt;/span&gt; 후, 하나의 통합된 배치로 만들어준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/op87oonnoqegn5c/0705img2.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;사진 출처: &lt;a href=&quot;https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983&quot;&gt;Understanding emotions — from Keras to pyTorch&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;data:&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰이 제거후 합병된 데이터&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;batch_sizes:&lt;/strong&gt; 각 타임스텝 마다 배치를 몇개를 넣는지 기록해 둠&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이처럼 PackedSequence 의 &lt;strong&gt;장점&lt;/strong&gt;은 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰을 계산 안하기 때문에 더 빠른 연산을 처리 할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;pytorch---packedsequence&quot;&gt;Pytorch - PackedSequence&lt;/h2&gt;

&lt;p&gt;Pytorch 에서 사용하는 방법은 의외로 간단하다. 실습 코드는 &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/00_Basic/02_PackedSequence.ipynb&quot;&gt;nbviewer&lt;/a&gt; 혹은 &lt;a href=&quot;https://github.com/simonjisu/pytorch_tutorials/blob/master/00_Basic/02_PackedSequence.ipynb&quot;&gt;github&lt;/a&gt;에 있다.&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;과정&lt;/h3&gt;

&lt;p&gt;전처리를 통해 위 배치의 문장들을 숫자로 바꿔주었다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input_seq2idx
============================================
tensor([[  1,  16,   7,  11,  13,   2],
        [  1,  16,   6,  15,   8,   0],
        [ 12,   9,   0,   0,   0,   0],
        [  5,  14,   3,  17,   0,   0],
        [ 10,   0,   0,   0,   0,   0]])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;하단의 코드를 통해서 정렬을 해주고, 각 문장의 길이를 담은 list를 만들어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input_lengths = torch.LongTensor([torch.max(input_seq2idx[i, :].data.nonzero())+1 for i in range(input_seq2idx.size(0))])
input_lengths, sorted_idx = input_lengths.sort(0, descending=True)
input_seq2idx = input_seq2idx[sorted_idx]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;모든 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰의 인덱스인 0 이 밑으로 내려간 것을 알 수 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input_seq2idx, input_lengths
============================================
tensor([[  1,  16,   7,  11,  13,   2],
        [  1,  16,   6,  15,   8,   0],
        [  5,  14,   3,  17,   0,   0],
        [ 12,   9,   0,   0,   0,   0],
        [ 10,   0,   0,   0,   0,   0]])

tensor([ 6,  5,  4,  2,  1])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;torch.nn.utils.rnn&lt;/strong&gt; 에서 &lt;strong&gt;pack_padded_sequence&lt;/strong&gt; 를 사용하면 PackedSequence object를 얻을 수 있다. packed_input 에는 위에서 말한 합병된 데이터와 각 타임스텝의 배치사이즈들이 담겨있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;packed_input = torch.nn.utils.rnn.pack_padded_sequence(input_seq2idx, input_lengths.tolist(), batch_first=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rnn---&quot;&gt;RNN 에서의 사용 방법&lt;/h3&gt;

&lt;p&gt;실수 벡터공간에 임베딩된 문장들을 pack 한 다음에 RNN 에 input을 넣기만 하면 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;embed = nn.Embedding(vocab_size, embedding_size, padding_idx=0)
gru = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=False, batch_first=True)

embeded = embed(input_seq2idx)
packed_input = pack_padded_sequence(embeded, input_lengths.tolist(), batch_first=True)
packed_output, hidden = gru(packed_input)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;packed_output 에는 합병된 output 과 batch_sizes 가 포함되어 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;packed_output[0].size(), packed_output[1]
=========================================================
(torch.Size([18, 2]), tensor([ 5,  4,  3,  3,  2,  1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이를 다시 원래 형태의 &lt;strong&gt;(배치크기, 문장의 최대 길이, 히든크기)&lt;/strong&gt; 로 바꾸려면 &lt;strong&gt;pad_packed_sequence&lt;/strong&gt; 를 사용하면 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)
output.size(), output_lengths
=========================================================
(torch.Size([5, 6, 2]), tensor([ 6,  5,  4,  2,  1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;실습코드에서 출력 결과를 살펴보면 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;pad&amp;gt;&lt;/code&gt; 토큰과 연관된 행은 모드 0으로 채워져 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;rnn-backend--&quot;&gt;RNN Backend 작동 방식&lt;/h2&gt;

&lt;h3 id=&quot;rnn-----&quot;&gt;RNN 안에서 어떤 방법으로 실행되는 것일까?&lt;/h3&gt;

&lt;p&gt;아래의 그림을 살펴보자&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/jl1iymxj6fdtvoe/0705img4.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;은닉층에서는 매 타임스텝마다 batch_sizes 를 참고해서 배치수 만큼 은닉층을 골라서 뒤로 전파한다.&lt;/p&gt;

&lt;p&gt;기존의 RNN 이라면, &lt;strong&gt;(배치크기 $\times$ 문장의 최대 길이 $\times$ 층의 갯수)&lt;/strong&gt; 만큼 연산을 해야하지만, &lt;strong&gt;(실제 토큰의 갯수 $\times$ 층의 갯수)&lt;/strong&gt; 만큼 계산하면 된다. 이 예제로 말하면 $(5 \times 6 \times 1)=30 \rightarrow (18 \times 1)=18$ 로 크게 줄었다.&lt;/p&gt;

&lt;h3 id=&quot;hidden---&quot;&gt;그렇다면 Hidden 어떻게 출력 되는가?&lt;/h3&gt;

&lt;p&gt;기존의 RNN 이라면 마지막 타임스텝 때 hidden vector 만 출력하지만, packed sequence 는 아래의 그림 처럼 골라서 출력하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/e1kjq4jsehbixiq/0705img5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;참고자료: &lt;a href=&quot;https://discuss.pytorch.org/t/lstm-hidden-cell-outputs-and-packed-sequence-for-variable-length-sequence-inputs/1183&quot;&gt;https://discuss.pytorch.org/t/lstm-hidden-cell-outputs-and-packed-sequence-for-variable-length-sequence-inputs/1183&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Jul 2018 09:45:37 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/07/05/packedsequence.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/07/05/packedsequence.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>PyCharm SSH 연결하기</title>
        <description>&lt;h1 id=&quot;pycharm-ssh-&quot;&gt;PyCharm SSH 연결하기&lt;/h1&gt;

&lt;p&gt;최근 딥러닝서버를 만들고 나서 Jupyter Notebook 만 사용했다. 그런데 오늘 파이참에서는 학생들을 위해 매 1년 마다 프로 버젼을 제공해주고 있다는 것을 들었다. 이를 사용해 보기로 했다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;PyCharm For Student: &lt;a href=&quot;https://www.jetbrains.com/student/&quot;&gt;https://www.jetbrains.com/student/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PyCharm Pro를 쓰면 &lt;strong&gt;Run On Remote Server&lt;/strong&gt; 기능을 쓸 수 가 있는데, 현재 운영체제인 Mac OS 에서 코드를 실행하면, 서버 Ubuntu 18.04 LTS 에서 실행 된다는 것이다. 게다가 PyCharm의 디버깅 툴도 사용할수 있다.&lt;/p&gt;

&lt;p&gt;그러나 이를 실행하는 과정이 쉬운 것은 아니었다. 아래 3개의 과정으로 설명하려고 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Interpreter 설정&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deployment 설정&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;프로젝트 연결 설정&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;interpreter&quot;&gt;Interpreter&lt;/h2&gt;

&lt;p&gt;많은 블로그에서 우선 Interpreter 설정을 진행하라고 해서 나도 따라했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/4kqy5xwpdz7qe26/0625_interpreter.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파이참 시작화면에서 오른쪽 아래 &lt;code class=&quot;highlighter-rouge&quot;&gt;Configure &amp;gt; Preferences&lt;/code&gt; 혹은 새로운 프로젝트를 만든 뒤, &lt;code class=&quot;highlighter-rouge&quot;&gt;⌘,&lt;/code&gt;를 누르자. 그러면 위와 같은 화면이 나오는데, &lt;code class=&quot;highlighter-rouge&quot;&gt;Project Interpreter&lt;/code&gt; 를 선택하자.&lt;/p&gt;

&lt;p&gt;처음에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;No interpreter&amp;gt;&lt;/code&gt; 라고 나올텐데, 옆에 &lt;code class=&quot;highlighter-rouge&quot;&gt;톱니바퀴 &amp;gt; add&lt;/code&gt; 를 눌러주자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/mewttyzbf7btzqs/0625_add_interpreter.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Host&lt;/code&gt; : IP&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Port&lt;/code&gt; : 포트번호&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Username&lt;/code&gt; : 사용자 이름&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 세가지 사항을 차례대로 입력한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/r29aktz21vy5e9g/0625_sshkey.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Private Key file&lt;/code&gt; : SSH 에서 Private key 의 위치를 써준다. 보통은 &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.ssh&lt;/code&gt; 폴더 안에 있다.&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Passphrase&lt;/code&gt; : Private Key 의 비밀번호를 넣는다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이렇게 쉽게 성공했으면 얼마나 좋았을까?&lt;/p&gt;

&lt;p&gt;그런데 여기서 아무리 연결을 하려고해도 &lt;span style=&quot;color: #e87d7d&quot;&gt;Authentication Fail&lt;/span&gt; 이라는 빨간 글씨만 보였다.&lt;/p&gt;

&lt;p&gt;구글링 결과 나와 같은 오류를 가진 사람들이 올린글이 하나 있었는데(&lt;a href=&quot;https://bit.ly/2Im44VD&quot;&gt;링크&lt;/a&gt;), 요약하면, &lt;strong&gt;“아무리 패스워드를 정확하게 입력해도 연결이 안된다. 내가 뭘 놓치고 있는거냐?”&lt;/strong&gt; 라는 글이었다.&lt;/p&gt;

&lt;p&gt;나는 아래 댓글의 방안으로 해결 했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/hu4h1mlmsuaxezh/0625_solution.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;요약하면, &lt;strong&gt;deployment server&lt;/strong&gt; 를 먼저 설정한 후에 하라는 말이었다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;deployment&quot;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Deployment&lt;/code&gt; 는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Build, Execution, Deployment&lt;/code&gt; 속성에 있었다. 아무것도 없다면 &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt; 눌러서 새로 만들자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/u59u4f4qcte59dv/0625_deployment.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SFTP&lt;/code&gt; 를 선택한 후&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SFTP host&lt;/code&gt; : IP&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Port&lt;/code&gt; : 포트번호&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Root path&lt;/code&gt; : 루트 패스인데 $HOME 위치를 설정한다.&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;User name&lt;/code&gt; : 사용자 이름&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Auth type&lt;/code&gt; : 패스워드 혹은 Key Pair(나는 ssh 키를 쓰기 때문에 이것을 선택)&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Private key&lt;/code&gt; : 키위치&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Key passphrase&lt;/code&gt; : 키 비밀번호&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 정보를 다 입력하면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Test SFTP connection&lt;/code&gt; 을 눌러봐서 테스트 해본다. 만약 통과가 되면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Apply&lt;/code&gt; 를 누르자!&lt;/p&gt;

&lt;p&gt;그리고 &lt;code class=&quot;highlighter-rouge&quot;&gt;Interpreter&lt;/code&gt; 에 돌아가서 다시 설정해준다.&lt;/p&gt;

&lt;p&gt;접속후 파이썬을 연결 해야하는데, 따로 python을 설치한게 없다면, 기본적으로 2.7 버전인 &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/python&lt;/code&gt; 패스가 설정 될 것이다.&lt;/p&gt;

&lt;p&gt;만약에 자신의 서버에서 Python3 을 따로 설치했다면 서버 terminal 에서 아래와 같이 쳐준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ which python3
/usr/local/bin/python3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;이 위치를 복사해서 쓰자.&lt;/p&gt;

&lt;p&gt;모든게 정상적으로 작동하면, &lt;strong&gt;Connection Sucessfully&lt;/strong&gt; 를 확인 할 수 가 있다.&lt;/p&gt;

&lt;p&gt;아까 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;No interpreter&amp;gt;&lt;/code&gt; 칸에 &lt;code class=&quot;highlighter-rouge&quot;&gt;remote Python 3.6.5 (sftp://[유저이름]@[IP]:[포트]/[파이썬위치])&lt;/code&gt; 가 뜨면 성공한 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section&quot;&gt;프로젝트 연결 설정 확인&lt;/h2&gt;

&lt;p&gt;자신의 프로젝트와 잘 연결 되었는지 확인 해보는 작업을 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/k04w9sa9qplxr1m/0625_openproject.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pycharm에서 새로운 프로젝트를 시작하거나 이미 존재하는 프로젝트를 오픈한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/h8ac2hnryxs9t23/0625_mappings.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫째, &lt;code class=&quot;highlighter-rouge&quot;&gt;⌘,&lt;/code&gt; 를 눌러서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Deployment&lt;/code&gt; 설정에 들어가서 아래의 설정을 해준다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Local Path&lt;/code&gt; : 로컬 PC 의 프로젝트 위치&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Deployment path on Server ***&lt;/code&gt; : 서버의 프로젝트 위치, 이때 앞단에 설정했던 홈 디렉토리 &lt;code class=&quot;highlighter-rouge&quot;&gt;Root Path&lt;/code&gt; 를 빼고 설정해줘야 한다. $HOME/프로젝트위치&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;둘째, 다시 &lt;code class=&quot;highlighter-rouge&quot;&gt;Project Interpreter&lt;/code&gt; 에 접속해서 서버에 있는 파이썬과 연결 됐는지 확인한다.&lt;/p&gt;

&lt;p&gt;셋째, 파일 실행을위해 주 실행파일 &lt;code class=&quot;highlighter-rouge&quot;&gt;main.py&lt;/code&gt;을 선택 후 (없다면 실행할 파일을 선택), 메뉴에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Run &amp;gt; Edit Configuration&lt;/code&gt; 을 선택한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/gl4kjtjep5qjgjg/0625_remotepython.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt; 버튼을 눌러서 새로운 파이썬 실행파일을 연결하자.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Script path&lt;/code&gt;: 스크립트 실행 파일 루트&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Python Interpreter&lt;/code&gt; : Remote Python 으로 설정 됐는지 확인&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Environment Variables&lt;/code&gt; : 딥러닝에서 GPU를 쓰려면 환경을 인식해줘야한다. ㅠㅠ&lt;/p&gt;

  &lt;p&gt;아래 그림과 같이 설정해주자. (그전에 cuda 설치를 못했다면? &lt;a href=&quot;https://simonjisu.github.io/datascience/2018/06/03/gpuserver3.html&quot;&gt;링크&lt;/a&gt;)&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/ere9ckvmt23x343/0625_env.png&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-1&quot;&gt;꿀팁&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;SSH 터미널을 사용하려면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Tools &amp;gt; Start SSH session...&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;로컬에서 변경사항을 자동 업로드 하려면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Tools &amp;gt; Deployment &amp;gt; Automatic uploads(always)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 24 Jun 2018 18:41:07 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/24/pycharmssh.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/24/pycharmssh.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 3 PYTHON &amp; CUDA</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---3&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 3&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;install-python&quot;&gt;Install PYTHON&lt;/h2&gt;

&lt;p&gt;아나콘다를 통하지 않고 소스를 통해 파이썬을 설치하기로 했다. 일단 용량이 작고, 다른 부가 spyder 등 프로그램을 설치하기 싫어서 소스에서 직접 설치하기로 했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;https://www.python.org/&lt;/a&gt; 에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Download &amp;gt; Soruce code &amp;gt; Python 3.6.5 - 2018-03-28&lt;/code&gt; 로 들어가서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Gzipped source tarball&lt;/code&gt; 의 링크를 복사한 후 아래와 같이 쳐주자.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;압축을 헤제시켜주자
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo -zxvf Python-3.6.5.tgz
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;설치해보자
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
make
make test
sudo make install
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;기본적으로 이렇게 진행하면 설치가 완료 된다. (중간에 실패하면 어떤 패키지가 없는지 확인하고 apt-get으로 설치해준다.)&lt;/p&gt;

&lt;p&gt;하지만 &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip3 install numpy&lt;/code&gt; 하게 되면 아래와 같이 “TLS/SSL ~” 이라며 에러가 뜰 수도 있다. 자세히 뭔지는 모르겠지만, 구글링을 통해서 해결했다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.
Collecting
  Could not fetch URL https://pypi.python.org/simple//: There was a problem confirming the ssl certificate: Can't connect to HTTPS URL because the SSL module is not available. - skipping
  Could not find a version that satisfies the requirement (from versions: )
No matching distribution found for
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section&quot;&gt;해결책&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;필수 패키지를 설치한다
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;소스폴더로 돌아가서 다시 설치해준다.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo make
sudo make install
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;마지막으로, pip 를 업그레이드 해준다.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo pip3 install --upgrade pip
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이제 설치가 잘 될 것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;install-cuda-90&quot;&gt;Install CUDA 9.0&lt;/h2&gt;

&lt;p&gt;CUDA-Toolkit 를 설치하면 드라이버를 설치할 필요는 없다고하나 만약에 먼저 설치해야하면 아래와 같이 설치(업데이트) 해주자&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
sudo apt install nvidia-390
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cuda-toolkit&quot;&gt;1. CUDA-Toolkit&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-90-download-archive&quot;&gt;https://developer.nvidia.com/cuda-90-download-archive&lt;/a&gt; 에서 자신에 버젼에 맞는 CUDA-Toolkit 을 받자.&lt;/p&gt;

&lt;p&gt;나는 우분투이기에 &lt;code class=&quot;highlighter-rouge&quot;&gt;Linux &amp;gt; x86_64 &amp;gt; Ubuntu &amp;gt; 17.04 &amp;gt; del[local]&lt;/code&gt; 를 골랐다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Base Installer&lt;/p&gt;

  &lt;p&gt;Patch 1 (Released Jan 25, 2018)&lt;/p&gt;

  &lt;p&gt;Patch 2 (Released Mar 5, 2018)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;세개의 다운로드 링크를 복사한 뒤 &lt;code class=&quot;highlighter-rouge&quot;&gt;wget&lt;/code&gt; 메서드로 받아준다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Base Installer&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb
sudo apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub
sudo apt-get update
sudo apt-get install cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Patch 1&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1604-9-0-local-cublas-performance-update_1.0-1_amd64.deb
sudo apt-get update
sudo apt-get upgrade cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Patch 2&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-2_1.0-1_amd64.deb
sudo apt-get update
sudo apt-get upgrade cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;만약에 쿠다 드라이버 명령어인 &lt;code class=&quot;highlighter-rouge&quot;&gt;nvcc&lt;/code&gt; 를 쓰고 싶다면 &lt;code class=&quot;highlighter-rouge&quot;&gt;./profile&lt;/code&gt; 파일 밑에다 아래 항목을 추가해주면 된다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export PATH=/usr/local/cuda-9.0/bin${PATH:+:$PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;아래 둘중에 하나를 한번 시도해보면 된다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvcc -V
nvidia-smi
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cudnn-&quot;&gt;2. CuDNN 설치&lt;/h3&gt;

&lt;p&gt;CuDNN 을 설치하려면 NVIDIA 회원 가입을 해야한다. 그리고 아래 싸이트에서 받아서 &lt;code class=&quot;highlighter-rouge&quot;&gt;scp&lt;/code&gt; 명령어로 서버로 옮기자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot;&gt;https://developer.nvidia.com/rdp/cudnn-download&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;압축을 해제하고, 파일들을 옮겨주면 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar -xzvf cudnn-9.0-linux-x64-v7.tgz
sudo cp cuda/include/cudnn.h /usr/local/cuda/include
sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;pytorch---&quot;&gt;Pytorch 설치한 후 테스트&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/torch.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이로써 설치 과정을 마치겠다. 컴퓨터 설치는 처음이라 3일 걸렸지만 앞으로는 더 짧아 지겠지…&lt;/p&gt;

&lt;p&gt;만약 오류가 나면 또 업데이트 하겠다.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jun 2018 21:08:24 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/03/gpuserver3.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/03/gpuserver3.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 2 원격 부팅 접속</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---2&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 2&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;after-install&quot;&gt;After Install&lt;/h2&gt;

&lt;p&gt;설치후에 아래 명령어들을 쳐준다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;apt-get(Advanced Packaging Tool) 패키지 명령어 업데이트 및 설치되어 있는 패키지 업그레이드. 설치된 파일은 &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/cache/apt/archive/&lt;/code&gt; 에 저장됨&lt;/li&gt;
  &lt;li&gt;gcc(GNU Compiler Collection) 패키지 설치. 파이썬 설치에 필요&lt;/li&gt;
  &lt;li&gt;make(GNU Make) 패키지 설치. 파이썬 설치시 필요&lt;/li&gt;
  &lt;li&gt;zlib1g-dev 설치. 파이썬 설치시 필요&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt update &amp;amp;&amp;amp; sudo apt upgrade
sudo apt install gcc
sudo apt install make
sudo apt install zlib1g-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;sshsecure-shell-&quot;&gt;SSH(SECURE SHELL) 접속&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;서버&lt;/code&gt; 컴퓨터에서 아래의 사항을 수정:&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo vi /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;ClientAliveInterval 60 : 클라이언트 살아있는지 확인하는 간격&lt;/li&gt;
  &lt;li&gt;ClientAliveCountMax 10 : 클라이언트 응답 없어도 접속 유지하는 횟수&lt;/li&gt;
  &lt;li&gt;PubkeyAuthentication yes : 활성화 시켜야 ssh를 통해서 접속 가능&lt;/li&gt;
  &lt;li&gt;PasswordAuthentication yes : 원격 서버 비밀번호로 로그인 가능하게 것, 나중에 ssh 로만 접속 가능케 하려면 바꿔줘야한다.&lt;/li&gt;
  &lt;li&gt;PermitEmptyPasswords no : 로그인시 빈 비밀번호를 가능케하는 옵션 기본으로 no로 되어 있다. 비밀번호 없이 로그인하게 하려면 yes로 바꿔줄 것, 권장은 안함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ssh-key----&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;로컬&lt;/code&gt; 컴퓨터에서 SSH KEY 생성하고 &lt;code class=&quot;highlighter-rouge&quot;&gt;서버&lt;/code&gt; 컴퓨터로 보내기:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;ssh-keygen:&lt;/strong&gt; SSH 키를 생성한다. 경로 지정을 안할 때 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.ssh&lt;/code&gt; 폴더 안에 &lt;code class=&quot;highlighter-rouge&quot;&gt;id_rsa&lt;/code&gt; 라는 이름으로 생성한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ssh-copy-id:&lt;/strong&gt; SSH 키를 서버로 보낸다. 옵션으로 포트번, 키 디렉토리 등등 설정 가능하다&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen -f [filepath]
ssh-copy-id -i [key_directory] -p [port] [user]@[ip_address]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;scp-&quot;&gt;파일전송 명령어 SCP 사용하기&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;서버 &amp;gt; 로컬
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scp [옵션] [계정명]@[원격지IP주소]:[원본 경로 및 파일] [전송받을 위치]
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;로컬 &amp;gt; 서버
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scp [옵션] [원본 경로 및 파일] [계정명]@[원격지IP주소]:[전송받을 위치]
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다만 주의 해야할 것은 &lt;code class=&quot;highlighter-rouge&quot;&gt;[옵션]&lt;/code&gt; 에다가 로그인 옵션 다 넣어줘야 보내진다는 점을 잊지 말자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;옵션:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;P: 포트&lt;/li&gt;
  &lt;li&gt;i: key&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;wolwake-on-lan&quot;&gt;WOL(Wake-On-Lan)&lt;/h2&gt;

&lt;p&gt;전기세 때문에 원격으로 컴퓨터를 껐다 켯다 하고 싶었다. 찾아보니 WOL 라는 방법이 있었다. 우선 자신의 컴퓨터의 메인보드가 이 기능을 지원해주고, 공유기도 이 기능을 지원해줘야 사용할 수 있다.
컴퓨터 부팅전 &lt;code class=&quot;highlighter-rouge&quot;&gt;BIOS&lt;/code&gt; (내경우는 DEL 키를 눌렀음) 에 들어가서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Wake-On-Lan&lt;/code&gt; 이라는 글귀가 있는지 찾아보고, 있다면 enable 로 바꿔주자. 그리고 아래 명령어를 통해 내컴퓨터의 &lt;code class=&quot;highlighter-rouge&quot;&gt;이더넷 포트(Ethernet port)&lt;/code&gt; 알아보자&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이더넷 포트는 첫째줄 제일 왼쪽에 있을 것이다. 보통 en~~ 로 시작하는 번호다&lt;/p&gt;

&lt;p&gt;그 후, 컴퓨터 내 컴퓨터가 &lt;code class=&quot;highlighter-rouge&quot;&gt;WOL&lt;/code&gt; 기능이 켜졌나 확인 하기 위해, 부팅후 커맨드 라인에 아래와 같이 쳐주자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install ethtool
sudo ethtool [Ethernet port]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Wake-on&lt;/code&gt; 이라는 곳에 &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; 라고 적혀져 있으면 켜진 것이다. 안되있다면 아래의 명령어를 통해 켜주자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ethtool -s [Ethernet port] wol g
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그 다음에 자신의 집의 공유기에 들어가서, WOL 설정을 해주자. NETIS 기준으로 설명 하겠다. IPTIME 은 다른 글들이 많으니 한번 찾아 보길 바란다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;원격 부팅(WOL)&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;사용 IP 목록에서 등록&lt;/code&gt; 누른후, 자신의 컴퓨터 IP 를 선택하고 이름을 지어준 다음 &lt;code class=&quot;highlighter-rouge&quot;&gt;등록&lt;/code&gt; 하게 되면 밑에 하나 등록 될 것이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;외부 연결 포트&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;포트 번호&lt;/code&gt;는 내 컴퓨터의 접속 포트로 했다. (이건 꼭 TCP 통신으로 하는 포트로 해야하는지 모르겠다. 다른 번호를 따로 지정해줄 수 있는지를 확인 못해봄)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/WOL.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 DDNS 서비스 신청한다. 그러면 집 밖에서도 집 공유기에 접속해서 컴퓨터를 킬 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;참고자료: &lt;a href=&quot;http://blog.daum.net/peace20/16779844&quot;&gt;http://blog.daum.net/peace20/16779844&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음 시간에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Python&lt;/code&gt; 설치와 &lt;code class=&quot;highlighter-rouge&quot;&gt;CUDA&lt;/code&gt; 설치를 다뤄보겠다.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jun 2018 10:09:54 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/03/gpuserver2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/03/gpuserver2.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 1 사양 및 우분투 서버 설치</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---1&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 1&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section&quot;&gt;컴퓨터 사양 상세&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;항목&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;상품코드&lt;/th&gt;
      &lt;th&gt;제품명&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;금액&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;수량&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;최종금액&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CPU&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;399920&lt;/td&gt;
      &lt;td&gt;[INTEL] 코어7세대 i5-7600 정품박스 (카비레이크/3.5GHz/6MB/쿨러포함)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;258,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;258,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MAIN&lt;br /&gt;BOARD&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;408703&lt;/td&gt;
      &lt;td&gt;[GIGABYTE] GA-H110M-M.2 듀러블에디션 피씨디렉트 (인텔H110/M-ATX)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71,100원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71,100원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;메모리&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;390790&lt;/td&gt;
      &lt;td&gt;[삼성전자] 삼성 DDR4 16GB PC4-19200&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;183,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;366,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HDD&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;347917&lt;/td&gt;
      &lt;td&gt;[WD] BLUE 2TB WD20EZRZ (3.5HDD/SATA3/5400rpm/64M)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67,730원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67,730원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GPU&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;373864&lt;/td&gt;
      &lt;td&gt;[MSI] GeForce GTX1060 OC D5 6GB 윈드스톰&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;389,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;389,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POWER&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;420859&lt;/td&gt;
      &lt;td&gt;[CORSAIR] CX750 NEW 80PLUS BRONZE (ATX/750W)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;94,370원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;94,370원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;BOX&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;365393&lt;/td&gt;
      &lt;td&gt;[COX] RC 170T USB3.0 (미들타워)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13,500원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13,500원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;OTHER&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3877&lt;/td&gt;
      &lt;td&gt;[컴퓨존] 일반조립비 (하드웨어조립/OS는 설치되지않습니다)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20,000원&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/1.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/2.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/3.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/4.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/5.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/6.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/7.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/8.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/9.jpeg&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2018년 5월 28일 컴퓨존에서 주문해서, 5월 30일 수요일 도착했다. 총비용은 대략 130만원 정도 ㅎㅎ 언른 GPU를 쓰고 싶은 생각에 그날밤 바로 설치를 진행하였다.&lt;/p&gt;

&lt;h2 id=&quot;ubuntu-server-&quot;&gt;Ubuntu Server 설치&lt;/h2&gt;

&lt;p&gt;5월 30일 저녁, 우분투를 설치하려고 하니 버전이 마음에 걸렸다. NVIDIA CUDA TOOLKIT을 보니 리눅스 17.04 버전 까지 지원하는듯 했기 때문이다. 16.04를 설치해야하나? 싶은 찰나에 그냥 최신으로 한번 도전해보기로 했다. 안되면 다시 갈지머..&lt;/p&gt;

&lt;h3 id=&quot;making-a-bootable-ubuntu-usb-disk-tutorial-at-mac-os&quot;&gt;Making a bootable Ubuntu USB disk Tutorial at Mac OS&lt;/h3&gt;

&lt;p&gt;맥에서 부팅 디스크 만들기, 정말 간단하다. &lt;a href=&quot;https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-macos#0&quot;&gt;tutorials.ubuntu.com&lt;/a&gt; 튜토리얼을 따라가면 된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;준비물&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;2GB 이상의 USB Driver&lt;/li&gt;
    &lt;li&gt;Mac OS 컴퓨터&lt;/li&gt;
    &lt;li&gt;우분투 서버 ISO 다운로드 &lt;a href=&quot;https://www.ubuntu.com/download/server&quot;&gt;우분투 서버 다운로드&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;format&quot;&gt;구동 디스크 FORMAT&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;응용프로그램 &amp;gt; 유틸리티 &amp;gt; 디스크 유틸리티 선택&lt;/li&gt;
  &lt;li&gt;MAC OS에 꼽은 USB 를 선택한 다음에 &lt;code class=&quot;highlighter-rouge&quot;&gt;지우기&lt;/code&gt; 를 선택한다.&lt;/li&gt;
  &lt;li&gt;이름을 짓고, &lt;code class=&quot;highlighter-rouge&quot;&gt;MS-DOS(FAT)&lt;/code&gt; 선택한다. (그림에는 Scheme가 있지만 Serria 이후에는 없다는 말이 있음)&lt;/li&gt;
  &lt;li&gt;포맷한다. 지운다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/format_disk.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;etcher-----&quot;&gt;Etcher 를 사용한 시동 디스크 생성&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://etcher.io/&quot;&gt;Etcher&lt;/a&gt; 먼저 받는다. 그후에는 정말 간단하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Select image&lt;/code&gt; 에 다운 받은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ubuntu Server 18.04 ISO&lt;/code&gt; 를 고른다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Select drive&lt;/code&gt; 에 포맷한 디스크를 선택&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Flash!&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/etcher.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 설치 준비 완료되었다.&lt;/p&gt;

&lt;h3 id=&quot;install-ubuntu-server-1804&quot;&gt;Install Ubuntu Server 18.04&lt;/h3&gt;

&lt;p&gt;이제 설치릃 해보자. 설치를 하려면, 최소 한번은 모니터에 연결해서 설치해야한다. 나는 정말 서버만을 생각해서 모니터를 않샀기에… HDMI 케이블로 티비화면으로 연결했다… 덕분에 고생이 두배!&lt;/p&gt;

&lt;p&gt;사실 간단하다. 아까 구운 &lt;code class=&quot;highlighter-rouge&quot;&gt;시동 디스크&lt;/code&gt;를 꼽아주고 부팅을 하면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u4.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u5.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u6.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u7.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u8.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u9.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u10.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u11.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u12.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사진출처: &lt;a href=&quot;https://websiteforstudents.com/install-ubuntu-18-04-lts-server-screenshots/&quot;&gt;https://websiteforstudents.com/install-ubuntu-18-04-lts-server-screenshots/&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;언어선택: 왠만하면 영어로 하자&lt;/li&gt;
  &lt;li&gt;키보드선택: 왠만하면 영어로 가자&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Install Ubuntu&lt;/code&gt; 선택&lt;/li&gt;
  &lt;li&gt;다음&lt;/li&gt;
  &lt;li&gt;특별한 주소가 있으면 작성 아니면, 다음&lt;/li&gt;
  &lt;li&gt;디스크 포맷: 디스크 통째로 포맷한 후에 설치할 것이니 1번&lt;/li&gt;
  &lt;li&gt;디스크 선택&lt;/li&gt;
  &lt;li&gt;마지막 확인&lt;/li&gt;
  &lt;li&gt;정말루?&lt;/li&gt;
  &lt;li&gt;당신의 이름 / 서버 이름 / 유저이름(로그인용) / 패스워드(로그인용) 등&lt;/li&gt;
  &lt;li&gt;설치중… 리붓!&lt;/li&gt;
  &lt;li&gt;완료&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;다음 장에는 설치후에 내가 했던 작업들: &lt;code class=&quot;highlighter-rouge&quot;&gt;원격 부팅과 접속&lt;/code&gt; 을 주로 이야기 하겠다.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jun 2018 22:26:58 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/02/gpuserver.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/02/gpuserver.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: GloVe</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-glove&quot;&gt;All about Word Vectors: GloVe&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;co-occurrence&quot;&gt;Co-occurrence&lt;/h2&gt;

&lt;p&gt;공기(Co-occurrence) 란 무엇인가? 두 개 이상의 어휘가 일정한 범위(range) 혹은 거리(distance) 내에서 함께 출현하는 현상을 말한다. 여기서 어휘는 단어 뿐만 아니라 형태소, 합성어 등의 단위로 의미를 부여할 수 있는 언어 단위다. 그렇다면 왜 &lt;strong&gt;공기 관계&lt;/strong&gt; 를 살피는 것일까?&lt;/p&gt;

&lt;p&gt;공기 관계를 통해 문서나 문장으로 부터 &lt;strong&gt;추상화된 정보&lt;/strong&gt; 를 얻기 위해서다. 이는 자연어처리의 가정을 생각해보면 이해할 수 있을 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;비슷한 맥락에 등장하는 단어들은 유사한 의미를 지니는 경향이 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;때문에, 두 단어가 같이 등장한 횟수가 많아지면 &lt;strong&gt;유사한 의미&lt;/strong&gt; 를 가졌다고 볼 수도 있다는 것이다. 이런 유사한 의미를 추상화된 정보로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;공기(Co-occurrence) 정보를 수집하는 방법은 두 가지다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;window 기반: 품사와 의미(semantic) 정보를 캡쳐할 수 있다.&lt;/li&gt;
  &lt;li&gt;word-document co-occurrence matrix 기반: 조금 더 일반적인 토픽을 추출 할 수 있고, 이는 Latent Semantic Analysis 와 연결 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;example-window-based-co-occurrence-matrix&quot;&gt;Example: Window based co-occurrence matrix&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;I like deep learning.&lt;/p&gt;

  &lt;p&gt;I like NLP.&lt;/p&gt;

  &lt;p&gt;I enjoy flying.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 3 문장을 사용해서, window size = 1 로 지정하는 co-occurrence matrix 를 만들어보자. 무슨 뜻인지는 아래 코드를 실행한 표를 살펴보자.&lt;/p&gt;

&lt;p&gt;단, 한 단어에 대해서 좌측에서 등장했는지 우측에서 등장했는 지는 상관없다(이는 co-occurrence matrix 가 대각을 기준으로 대칭하는 결과를 불러옴)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
import numpy as np
from collections import deque, Counter
from itertools import islice
from scipy.sparse import coo_matrix

flatten = lambda t: [tuple(j) for i in t for j in i]
window = 1

def get_cooccur_list(sentence, window):
    s_len = len(sentence)
    ngram_list = [deque(islice(sentence, i), window+1) for i in range(s_len+1)][2:]
    return ngram_list

sentences = ['I like deep learning .', 'I like NLP .', 'I enjoy flying .']
tokens = [s.split() for s in sentences]
vocab = list(set([w for s in tokens for w in s]))
# print(vocab)
vocab = ['I', 'like', 'enjoy', 'deep', 'learning', 'NLP', 'flying', '.'] # 표와 같은 모양을 만들어주기 위해 다시 지정
vocab2idx = {w: i for i, w in enumerate(vocab)}
tokens_idx = [[vocab2idx.get(w) for w in s] for s in tokens]
co_occurs = [get_cooccur_list(s, window) for s in tokens_idx]

d = Counter()
d.update(flatten(co_occurs))
row, col, data = list(zip(*[[r, c, v] for (r, c), v in d.items()]))
temp = coo_matrix((data, (row, col)), shape=(len(vocab), len(vocab))).toarray()
co_mat = temp.T + temp

pd.DataFrame(co_mat, index=vocab, columns=vocab)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;코드를 실행하면 아래와 같은 표가 나온다. window size 가 1이니까 “I” 주변 한칸에 동시 등장 단어는 “like” 가 2번 “enjoy” 가 1번이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;counts&lt;/th&gt;
      &lt;th&gt;I&lt;/th&gt;
      &lt;th&gt;like&lt;/th&gt;
      &lt;th&gt;enjoy&lt;/th&gt;
      &lt;th&gt;deep&lt;/th&gt;
      &lt;th&gt;learning&lt;/th&gt;
      &lt;th&gt;NLP&lt;/th&gt;
      &lt;th&gt;flying&lt;/th&gt;
      &lt;th&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;I&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;like&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;enjoy&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;deep&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;learning&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NLP&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;flying&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;co-occurrence matrix와 같은 단어 벡터는 어떤 문제점이 있을까?&lt;/p&gt;

&lt;p&gt;첫째로, 단어가 많아지면 벡터가 엄청 길어진다(데이터 차원이 커진)는 것이다. 이에 따른 많은 저장 비용이 들어갈 것이다. 둘째로, sparsity issues가 있을 수 있다(models are less robust).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그렇다면 꼭 하나의 단어로 해야만 하는가? 문서 전체의 단어의 공기 정보를 추출 하는 것은 안되는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이와 같은 생각이 GloVe 를 탄생시켰다.&lt;/p&gt;

&lt;h2 id=&quot;glove&quot;&gt;GloVe&lt;/h2&gt;

&lt;p&gt;Paper: &lt;a href=&quot;https://www.aclweb.org/anthology/D14-1162&quot;&gt;GloVe: Global Vectors for Word Representation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GloVe 의 학습방법은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \dfrac{1}{2} \sum_{i,j=1}^{W} f(P_{ij})(u_i^T v_j - \log P_{ij})^2&lt;/script&gt;

&lt;p&gt;논문해설을 통해서 자세히 보자.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;section&quot;&gt;논문 해설&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;GloVe:&lt;/strong&gt; Global Vectors라고 명칭을 지은 이유는 모델에서 직접적으로 문서 전체의 코퍼스 통계량을 포착할 수있기 때문이다. (the global corpus statistics are captured directly by the model)&lt;/p&gt;

&lt;p&gt;그전에 notation 을 정의해보자.&lt;/p&gt;

&lt;p&gt;Define notation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$X$: 단어간의 공기 매트릭스 (matrix of word-word co-occurrence counts)&lt;/li&gt;
  &lt;li&gt;$X_{ij}$: 단어 $j$ 와 문맥 단어 $i$ 가 같이 등장한 횟수 (the number of times that word $j$ occurs in the context word $i$)&lt;/li&gt;
  &lt;li&gt;$X_i = \sum_k X_{ik}$: 어떤 단어든 문맥 단어 $i$ 와 등장한 횟수 (the number of times any word appears in the context of word $i$)&lt;/li&gt;
  &lt;li&gt;$P_{ij} = P(j \vert i) = X_{ij} / X_i$: 단어 $j$ 와 문맥 단어 $i$ 동시 등장할 확률, 문맥 단어 $i$ 가 주어졌을 때 $j$ 가 등장할 확률 (probability that word $j$ appear in the context word $i$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래의 예시를 보자.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Probability and Ratio&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=solid$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=gas$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=water$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=fashion$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert ice)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.00019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000066&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.003&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert steam)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000022&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.00078&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0022&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert ice)/P(k\vert steam)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8.9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.085&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.96&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;위의 표에 따르면 $i=ice, j=steam$ 일때 $solid$ 와 동시 등장 확률이 높은 단어는 $ice$ 다. 직관적으로 생각해도, 단단한 $ice$ 가 $solid$ 와 연관될 확률이 더 높다. 따러서, 우리는 $P(k\vert ice)/P(k\vert steam)$ 를 구해서, 연관이 있는 단어일 경우 이 비율이 크게 높으며, 아니면 그 반대다. &lt;strong&gt;(엄청 크거나 혹은 엄청 작거나)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이처럼 직접적으로 단어간의 동시등장 확률을 비교하는 것보다. 확률간의 비율을 구하는 것이 &lt;strong&gt;연관성이 없는&lt;/strong&gt; 단어(water &amp;amp; fashion)들로 부터 관련된 단어(solid &amp;amp; gas)를 구별하기 좋으며, &lt;strong&gt;관련성 있는&lt;/strong&gt; 단어(solid &amp;amp; gas)들을 차별화 하기에도 좋다.&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;동시 등장 확률의 비율(ratios of co-occurrence probabilities)&lt;/span&gt;&lt;/strong&gt; 을 모델이 학습하게 하는 것이 바람직 해보인다.&lt;/p&gt;

&lt;p&gt;중요한 것은 이 비율은 3개의 단어 $i, j, k$ 와 연관이 있다. 따라서 아래의 함수를 구성할 수가 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i, w_j, \tilde{w}_k) = \dfrac{P_{ik} }{P_{jk} } \cdots (1)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$w \in \Bbb{R}^d$: word vectors&lt;/li&gt;
  &lt;li&gt;$\tilde{w} \in \Bbb{R}^d$: separate context word vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$(1)$ 식과 같이, 단어 벡터 공간에서 $w_i, w_j, \tilde{w}_k$ 를 input으로 넣었을 때,&lt;/p&gt;

&lt;p&gt;$\dfrac{ P_{ik} }{ P_{jk} }$ 비율을 나타내는 하는 선형구조인 함수를 구하는 것이 목적이다. 그리고 $F$ 를 아래와 같이 변형시켜 본다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F((w_i - w_j)^T \tilde{w}_k) = \dfrac{P_{ik} }{P_{jk} } \cdots (2)&lt;/script&gt;

&lt;p&gt;이로써 선형적인 관계를 포착하고, 양변 모두 스칼라 값으로 정해진 함수가 만들어 졌다. 그러나 단어 $i, j$ 와 $k$ 동시 등장 비율의 &lt;strong&gt;임의적인 차별화&lt;/strong&gt; 를 위해서 어떤 조건들을 만족해야한다. 그 조건들이란 단어 벡터 $w$ 와 문맥 단어 벡터 $\tilde{w}$ 간 서로 자유롭게 교환 될 수가 있어야 한다. 즉, 단어간 공기 매트릭스 $X$의 대칭(symmetric) 특성을 보존해야 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;임의적인 차별화&lt;/strong&gt; 가 무슨 말이냐면, 단어 $k$ 와 $i, j$ 단어 간의 비율을 확인 할때, $i$ 와 $k, j$ (혹은 $j$ 와 $i, k$) 의 관계도 확인 할 수 있어야 된다는 말이다.&lt;/p&gt;

&lt;p&gt;대칭(symmetric) 을 만족하려면 2 단계로 진행 된다. 우선, 두 그룹 $(\Bbb{R}, +)$ 과 $(\Bbb{R}_{&amp;gt;0}, \times)$ 에 대해서 함수 $F$ 가 &lt;strong&gt;homomorphism&lt;/strong&gt; 이어야 한다. (homomorphism 해설: 밑에 &lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt;를 보라), 예를 들어 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i, w_j, \tilde{w}_k) = \dfrac{F(w_i^T \tilde{w}_k) }{F(w_j^T \tilde{w}_k) } \cdots (3)&lt;/script&gt;

&lt;p&gt;$(2)$ 식에 의해서, 아래와 같이 풀 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i^T \tilde{w}_k) = P_{ik} = \dfrac{X_{ik} }{X_i} \cdots (4)&lt;/script&gt;

&lt;p&gt;$(3)$ 식에 만족하는 해답은 $F = \exp$ 임으로, 아래의 식을 도출 해낼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^T \tilde{w}_k = \log(P_{ik}) = \log(X_{ik}) - \log(X_i) \cdots (5)&lt;/script&gt;

&lt;p&gt;다음으로, $(5)$ 식은 $\log(X_i)$ 만 아니였다면 대칭이었을 것이다. $\log(P_{ik})=\log(P_{ki})$ 를 만족하는지 한번 보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\log(P_{ik}) &amp;= \log(X_{ik}) - \log(X_i) \\
\log(P_{ki}) &amp;= \log(X_{ki}) - \log(X_k)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;당연하게도, $\log(X_i) \neq \log(X_k)$ 이기 때문에 $\log(P_{ik}) \neq \log(P_{ki})$ 이다.&lt;/p&gt;

&lt;p&gt;하지만 $\log(X_i)$ 부분은 $k$ 에 대해서 독립적(independent) 이기 때문에, $w_i$ 의 bias $b_i$ 항으로 들어갈 수 있다. 그리고 대칭성을 유지하기 위해서 $\tilde{w}_k$ 의 bias $\tilde{b}_k$ 항도 더해준다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^T \tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik}) \cdots (6)&lt;/script&gt;

&lt;p&gt;$(6)$ 식이 우리의 제일 간단한 선형관계인 solution 이라고 해도 되지만 이는 문제가 좀 있다. $X_{ik} = 0$ 에서 명확하게 정의 되지 않는다. 이를 해결하기 위해서 $X_{ik}+1$ 하는 방법도 있지만, sparsity issue 를 벗어나기 힘들다. 그리고 하나의 큰 약점이 있다면 거의 등장하지 않는 단어들에게 동시 등장 비율이 모두 같을 수 있다는 점이다. 이게 왜 문제가 되냐면, co-occurrence 가 적을 수록 많이 등장하는 단어들 보다 정보 함량이 적고 데이터도 noisy 하기 때문이다.&lt;/p&gt;

&lt;p&gt;연구팀은 새로운 weighted least squares regression model 을 제시하여 문제를 풀고자 했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \dfrac{1}{2} \sum_{i,j=1}^{W} f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2 \cdots (7)&lt;/script&gt;

&lt;p&gt;가중치 함수 (Weighting function) $f(X_{ij})$ 는 아래의 특성을 따라야 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$f(0) = 0$. 만약 $f$ 가 연속함수(continuous function) 라면, $x \rightarrow 0$ 으로 갈때 $\lim_{x\rightarrow 0} f(x) log^2x$ 도 빠르게 수렴한다. 단, 유한한 값이여야 한다.&lt;/li&gt;
  &lt;li&gt;$f(x)$ 는 감소함수가 되면 안된다. (non-decreasing) 이유는 동시 등장이 희박한 단어들의 가중치가 많아져서는 안되기 때문이다.&lt;/li&gt;
  &lt;li&gt;$f(x)$ 는 큰 $x$ 값에 대해서 상대적으로 작은 값이어야 한다. 그 이유는 공기 횟수가 큰 단어들의 가중치가 너무 높게 설정하지 않기 위해서다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;연구팀은 이에 적합한 함수를 찾았다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x) =
\begin{cases} (x/x_{max})^{\alpha} \quad if\ x &lt; x_{max} \\
1 \quad otherwise
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L3_weight_f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림: $f(x)$ with $\alpha = 3/4$ 일때 좋은 성과를 얻었다. 재밌는 것은 Mikolov 논문에서 나온 unigram distribution 에 3/4 승을 해주는 것과 같다는 것을 발견했다.&lt;/p&gt;

&lt;p&gt;조금더 general 한 weighting function 은 아래와 같다. (자세한건 논문 3.1 참고)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{J} = \sum_{i,j} f(X_{ij})(w_i^T \tilde{w}_j - \log X_{ij})^2&lt;/script&gt;

&lt;p&gt;이는 연구팀이 도출한 $(7)$ 식과 같은 식이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;homomorphism&quot;&gt;참고 1: Homomorphism&lt;/h3&gt;

&lt;p&gt;혹시나 틀렸으면 댓글로 이야기 해주세요.&lt;/p&gt;

&lt;p&gt;우선 Group &lt;a href=&quot;https://en.wikipedia.org/wiki/Group_(mathematics)&quot;&gt;(위키 링크)&lt;/a&gt; 이란 것을 알아야한다. 내가 이해한 바로는 &lt;strong&gt;Group $(G, * )$&lt;/strong&gt; 이란, 집합 $G$ 와 연산 $* $ 로 구성되어 있다. 이 연산을 “the Group Law of $G$” 라고 부른다. 집합 $G$ 에 속한 원소 $a, b$ 의 연산을 $a * b$ 라고 표현한다. 또한 아래의 조건들을 만족해야한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Closure: $* $ 연산은 $G$ 에 대해 닫혀 있어야한다. 즉, $a * b$ 연산도 집합 $G$ 에 속해야한다.&lt;/li&gt;
  &lt;li&gt;Associativity: 교환 법칙이 성립해야한다. $(a * b) * c = a * (b * c)$&lt;/li&gt;
  &lt;li&gt;Identity element: 항등원이 존재해야 한다. $a * e = a = e * a$&lt;/li&gt;
  &lt;li&gt;Inverse element: 역원이 존재해야 한다. $a * x = e = x * a$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Group 를 이해 했으면 이제 Homomorphism 을 이해해보자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;정의:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;두 그룹 &lt;span style=&quot;color: #15b23c&quot;&gt;$(G, * )$&lt;/span&gt; 과 &lt;span style=&quot;color: #9013b2&quot;&gt;$(H, @)$&lt;/span&gt; 가 있으면, 모든 &lt;span style=&quot;color: #15b23c&quot;&gt;$x, y \in G$&lt;/span&gt; 에 대해서 $f:$ &lt;span style=&quot;color: #15b23c&quot;&gt;$G$&lt;/span&gt; $\rightarrow$ &lt;span style=&quot;color: #9013b2&quot;&gt;$H$&lt;/span&gt;, &lt;span style=&quot;color: #15b23c&quot;&gt;$f(x * y)$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$f(x) @ f(y)$&lt;/span&gt; 를 만족하는 map 을 말한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예시)&lt;/strong&gt;
두 그룹 &lt;span style=&quot;color: #15b23c&quot;&gt;$(\Bbb{R}, + )$&lt;/span&gt; 와 &lt;span style=&quot;color: #9013b2&quot;&gt;$(\Bbb{R}_{&amp;gt;0}, \times )$&lt;/span&gt; 사이에 어떤 map $f:$ &lt;span style=&quot;color: #15b23c&quot;&gt;$\Bbb{R}$&lt;/span&gt; $\rightarrow$ &lt;span style=&quot;color: #9013b2&quot;&gt;$\Bbb{R}_{&amp;gt;0}$&lt;/span&gt;, $f(x)=e^x$ 가 있다면, $f$ 가 Homomorphism 인지를 밝혀라.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;for any &lt;span style=&quot;color: #15b23c&quot;&gt;$x, y \in \Bbb{R}$&lt;/span&gt;,&lt;/p&gt;

  &lt;p&gt;&lt;span style=&quot;color: #15b23c&quot;&gt;$f(x + y)$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$e^{x+y}$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$e^x \times e^y$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$f(x) \times f(y)$&lt;/span&gt; 임으로&lt;/p&gt;

  &lt;p&gt;Homomorphism 을 만족한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;word2vec 과 glove 관련 포스팅은 &lt;strong&gt;“All about word vectors”&lt;/strong&gt; 시리즈로 마치겠다. 기회가 되면 gensim 의 사용법과, 데이터 차원 축소와 시각화 방법인 t-SNE 을 포스팅 하도록 하겠다.&lt;/p&gt;
</description>
        <pubDate>Wed, 02 May 2018 23:22:05 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/05/02/allaboutwv4.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/05/02/allaboutwv4.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Negative Sampling</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-negative-sampling&quot;&gt;All about Word Vectors: Negative Sampling&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_model_train.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;navie-softmax--&quot;&gt;Navie Softmax 의 단점&lt;/h2&gt;

&lt;p&gt;Navie Softmax 를 최종단에 출력으로 두고 Backpropagation 할때는 큰 단점이 있다.&lt;/p&gt;

&lt;p&gt;사실 Softmax가 그리 값싼 연산은 아니다. 우리가 학습하고 싶은 단어 벡터 1000개가 있다고 가정해보자. 그렇다면 매 window size=2 마다, 다시 말해 총 업데이트 할 5개의 단어 (중심단어 1 + 주변 단어 2 x 2) 를 위해서, $W, W’$ 안에 파라미터를 업데이트 해야하는데, 그 갯수가 최소 $(2 \times d \times 1000)$ 만큼된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown_\theta J_t(\theta) \in \Bbb{R}^{2dV}&lt;/script&gt;

&lt;p&gt;많은 양의 단어에 비해 업데이트 하는 파라미터수는 적기 때문에 gradient matrix $\triangledown_\theta J_t(\theta)$ 가 굉장히 sparse 해질 수 있다 (0이 많다는 소리). Adam 같은 알고리즘은 sparse 한 matrix 에 취약하다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://simonjisu.github.io/deeplearning/2018/01/13/numpywithnn_5.html&quot;&gt;Numpy with NN: Optimizer 편 참고&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그래서 &lt;strong&gt;“window에 실제로 등장하는 단어들만 업데이트 하면 좋지 않을까?”&lt;/strong&gt; 라는 생각을 하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;negative-sampling&quot;&gt;Negative Sampling&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;paper 1: &lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;&gt;Distributed representaions of Words and Phrases and their Compositionality (Mikolov et al. 2013)&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;paper 2: &lt;a href=&quot;https://arxiv.org/abs/1402.3722&quot;&gt;word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;요약하면 아래와 같은 목적함수를 최대화 하는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
J(\theta) &amp;= \dfrac{1}{T}\sum_{t=1}^{T} J_t(\theta)\\
J_t(\theta) &amp;= \underbrace{\log \sigma(u_o^T v_c)}_{(1)} + \underbrace{\sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_c)]}_{(2)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$T$: total num of words&lt;/li&gt;
  &lt;li&gt;$\sigma$: sigmoid function&lt;/li&gt;
  &lt;li&gt;$P(w) = {U(w)^{3/4}} / {Z}$: unigram distribution U(w) raised to the 3/4 power
    &lt;ul&gt;
      &lt;li&gt;The power makes less frequent words be sampled more often&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;말로 풀어보자면, 모든 단어 $T$ 에 대해서 중심단어 $c$ 와 그 주변단어 $o$ 가 같이 나올 확률 &lt;strong&gt;[수식 (1)]&lt;/strong&gt; 을 최대화 하고, 그 주변단어가 아닌 집합에서 sampling 하여 나온 $k$ 개의 단어의 확률 &lt;strong&gt;[수식 (2)]&lt;/strong&gt; 을 최소화 시키는 것이다. (음수가 붙기 때문에 최소하하게 되면 최대화가 된다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section&quot;&gt;상세 논문 설명&lt;/h3&gt;

&lt;p&gt;논문 기준으로 위에 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;표기법&lt;/span&gt;&lt;/strong&gt; 이 조금 다르다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;여기서 &lt;strong&gt;$w$ = center word, $c$ = context&lt;/strong&gt; 다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;출발점은 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$(w, c)$ 세트가 정말로 corpus data로 부터 왔는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;라고 생각하고 아래와 같은 &lt;strong&gt;정의&lt;/strong&gt; 를 하게 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로 부터 왔을 확률&lt;/li&gt;
  &lt;li&gt;$P(D = 0 \vert w, c) = 1 - P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로부터 오지 않았을 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, 우리의 목적은 확률 $P(D = 1\vert\ w, c)$ 를 최대화하는 parameter $\theta$를 찾는 것이기 때문에 아래와 같은 목적함수를 세울 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} &amp;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &amp;\arg \underset{\theta}{\max} \log \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &amp;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;파라미터 $\theta$ 는 단어들의 벡터라고 생각할 수 있다. 즉, 위의 식을 만족하는 어떤 최적의 단어 벡터를 찾는것이다.&lt;/p&gt;

&lt;p&gt;또한, 확률 $P(D=1\vert\ w,c;\theta)$ 은 sigmoid로 아래와 같이 정의 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D=1\vert\ w,c;\theta) = \dfrac{1}{1+e^{-v_c v_w}}&lt;/script&gt;

&lt;p&gt;따라서 우리의 목적함수는 아래와 같이 다시 고쳐 쓸수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} }&lt;/script&gt;

&lt;p&gt;그러나 우리의 목적 함수는 매 $(w, c)$ 세트마다 $P(D=1\vert\ w,c;\theta)=1$ 를 만족하는 trivial solution이 존재한다. $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 $\theta$ (보통 $K$ 가 40이 넘어가면 위 방정식의 값이 0에 가까워짐) 는 모든 값을 똑같이 0으로 만들어 버리기 때문에, 같은 값을 갖지 못하게 하는 매커니즘이 필요하다. ($\theta$ 에 뭘 넣어도 0이 되면 최대값을 찾는 의미가 없어진다, 자세한건 밑에 &lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt; 를 참조) 여기서 “같은 값을 같는다” 라는 말은 단어 벡터가 같은 값을 갖는 것이다.&lt;/p&gt;

&lt;p&gt;따라서, 하나의 방법으로 랜덤 $(w, c)$ 조합을 생성하는 집합 $D’$를 만들어 corpus data 로부터 올 확률 $P(D=1\vert \ w,c;\theta)$ 를 낮게 강제하는 것이다. 즉, $D’$ 에서 생성된 $(w, c)$ 조합은 &lt;strong&gt;corpus data 로부터 오지 않게&lt;/strong&gt; 하는 확률 $P(D=0\vert\ w,c;\theta)$ 을 최대화 하는 것.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} P(D=0\vert\ w,c;\theta) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta) + \underset{(w,c) \in D'}{\sum} \log \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \big(1- \dfrac{1}{1+e^{-v_c v_w} } \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} }
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\sigma(x) = \dfrac{1}{1+e^{-x} }$ 시그모이드 함수로 정의 하면, 아래와 같이 정리 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} } \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \sigma(v_c v_w) + \underset{(w,c) \in D'}{\sum} \log \sigma(- v_c v_w) \quad \cdots (3)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;이는 &lt;span style=&quot;color: #e87d7d&quot;&gt;paper 1&lt;/span&gt; 의 (4) 번 식과 같아지는다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log \sigma(u_c^T v_w) + \sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_w)]&lt;/script&gt;

&lt;p&gt;다른 점이라면, 우리가 만든 (3)식에서는 전체 corpus ($D \cup D’$) 을 포함하지만, Mikolov 논문의 식은 $D$ 에 속하는 $(w, c)$ 조합 하나와 $k$ 개의 다른 $(w, c_j)$ 의 조합을 들었다는 것이다. 구체적으로, $k$ 번의 negative sampling 에서 Mikolov 는 $D’$ 를 $k \times D$ 보다 크게 설정했고, k개의 샘플 $(w, c_1), (w, c_2), \cdots, (w, c_k)$ 에 대해서 $c_j$ 는 &lt;strong&gt;unigram distribution&lt;/strong&gt; 에 &lt;strong&gt;3/4&lt;/strong&gt; 승으로 부터 도출된다. 이는 아래의 분포에서 $(w, c)$ 조합을 추출 하는 것과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{words}(w) = \dfrac{p_{contexts} (c)^{3/4} }{Z}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$p_{words}(w)$, $p_{contexts} (c)$ 는 각각 words and contexts 의 unigram distribution 이다.&lt;/li&gt;
  &lt;li&gt;$Z$ 는 normalization constant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unigram distribution 은 단어가 등장하는 비율에 비례하게 확률을 설정하는 분포다. 예를 들어 “I have a pen. I have an apple. I have a pineapple.” 라는 문장이 있다면, 아래와 같은 분포를 만들 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;I&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;have&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;a&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;pen&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;an&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;apple&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;pineapple&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;여기서 3/4 승을 해주면, 가끔 등장하는 단어는 확률을 높혀주는 효과가 있다. 물론 자주 나오는 단어의 확률도 올라가지만 가끔 등장하는 단어의 상승폭 보다 적다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;a&lt;/th&gt;
      &lt;th&gt;$a^{\frac{3}{4} }$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;apple&lt;/td&gt;
      &lt;td&gt;$\frac{1}{15}=0.067$&lt;/td&gt;
      &lt;td&gt;${\frac{1}{15} }^{\frac{3}{4} }=0.131$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;have&lt;/td&gt;
      &lt;td&gt;$\frac{3}{15}=0.020$&lt;/td&gt;
      &lt;td&gt;${\frac{3}{15} }^{\frac{3}{4} }=0.299$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Mikolov 논문에서는 context는 하나의 단어이기 때문에 $p_{words}(w)$ 는 아래와 동일하다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{words}(w) = p_{contexts} (c) = \dfrac{count(x)}{ \vert text \vert }&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;trivial-solution&quot;&gt;참고 1. Trivial Solution&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} L(\theta;w,c) &amp;= \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } \\
&amp;= \underset{(w,c) \in D}{\sum} \log(1) - \log(1+e^{-v_c v_w}) \\
&amp;= \underset{(w,c) \in D}{\sum} - \log(1+e^{-v_c v_w})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;같은 두 벡터의 내적을 하게 되면 값은 최대가 된다. $\cos$ 값이 1이 되기 때문이다. (여기서는 최대 값이 중요한건 아니지만 값이 커진다는데 의의가 있다.)
&lt;script type=&quot;math/tex&quot;&gt;a\cdot a=\vert a \vert \vert a \vert \cos \theta&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a = np.array([1,2,3,4,5,6,7])
b = np.array([.1,.2,.3,.4,.5,.6,.7])
print(np.dot(a, a))
print(np.dot(a, b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;140&lt;/p&gt;

  &lt;p&gt;14.0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 모든 값들이 $e^{-v_c v_w}$ 를 0으로 만든다면, $L(\theta; w, c)$ 값은 0이 될것이다. 보통 $K$ 가 40 이 상이면, $L(\theta;w,c)$ 의 해는 모두 0 일 것이며 이것을 &lt;strong&gt;trivial solution&lt;/strong&gt; 이라고 한다. 우리의 목적은 단어 벡터 $v_c$ 와 $v_w$ 의 구별이기 때문에, $v_c \not = v_w$ 으로 만들어야한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음 시간에는 말뭉치의 공기정보(co-occurance)를 고려해 단어를 벡터화 시킨 &lt;strong&gt;GloVe&lt;/strong&gt; 에 대해 알아보자.&lt;/p&gt;
</description>
        <pubDate>Tue, 24 Apr 2018 16:14:13 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
  </channel>
</rss>
