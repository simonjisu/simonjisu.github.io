<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soopace</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>https://simonjisu.github.io/</link>
    <atom:link href="https://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 19 Feb 2020 21:41:57 +0900</pubDate>
    <lastBuildDate>Wed, 19 Feb 2020 21:41:57 +0900</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>글또 4기 다짐</title>
        <description>&lt;h1 id=&quot;글또-4기를-시작하며&quot;&gt;글또 4기를 시작하며&lt;/h1&gt;

&lt;p&gt;“글또”가 궁금하다면? &lt;a href=&quot;https://www.notion.so/ac5b18a482fb4df497d4e8257ad4d516&quot;&gt;&lt;strong&gt;글또 Notion 페이지 바로가기&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;작년 여름의 어느날 글또를 시작하면서 세웠던 계획을 다시 돌아보게 됐다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“너는 다 계획이 있구나”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1Y534QRewyuENFNHTtf2WXgZe_5VH91MS&quot; alt=&quot;출처: 영화 기생충 스틸컷&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;출처: 영화 기생충 스틸컷&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;PRML(“Pattern Recognition and Machine Learning” - Christopher Bishop) 책 공부하기&lt;/li&gt;
  &lt;li&gt;NLP 논문 읽기 (BERT 등)&lt;/li&gt;
  &lt;li&gt;새로운 것 배우기: Flask, Spark, Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;대학원 준비&lt;/li&gt;
  &lt;li&gt;장롱면허 탈출&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 5가지 목표 중에서 3개는 달성하고 새로운 것을 배우는 것은 달성하지 못했고 장롱면허도 아직 탈출하지 못했다. 첫번째 목표도 100% 완성하지 못했기 때문에 사실상 50%를 완성했다고 할 수 있다. 다시 돌아보니 글또와는 크게 관련이 없는 목표들이 많았다. 그래서 이번 글또 4기에는 글또에 알맞는 목표를 세워보려고 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;최소 12편 이상의 글 작성하기&lt;/li&gt;
  &lt;li&gt;어떤 주제든 시리즈 글 연재하기(3편 이상)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;글또 4기를 마치고 꼭 두 가지 목표를 완수하기를…&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;글또-4기에서-작성하고-싶은-글&quot;&gt;글또 4기에서 작성하고 싶은 글&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;인지(행동)과학 관련 책 리뷰&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;인지과학 분야의 책을 통해 사람이 어떤 방식으로 생각을 하는지 이해하고, 딥러닝 모델의 설명과 연관지어 떠오르는 아이디어를 정리해보고자 한다. 컴퓨터 비전 분야에서 인간의 시각 시스템을 모방한 Neocognitron(1979, Kunihiko Fukushima)부터 현재의 다양한 Convolution Neural Network 까지, 인간에 대한 연구는 알고리즘을 해석하고 발전시키는데 많은 영향을 줬었다. 사람이 생각하는 과정(인지 프로세스)를 이용해 더 좋은 성능을 내는 모델을 연구하거나, 더 나아가 “설명가능한 딥러닝 모델”의 실마리가 나올 수 있지 않을까 생각하고 있다. 관련 전공이 아니기 때문에, 먼저 사람들이 주로 읽는 도서부터 시작해서 천천히 깊게 탐구해볼 생각이다. 현재 도서 목록으로 다음과 같이 정했는데, 혹시 추천할 만한 책이 있으면 댓글 부탁드린다.&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;생각에 관한 생각&lt;/li&gt;
        &lt;li&gt;넛지&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;데이터 사이언스 대학원 입학과정 및 생활&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;나는 대학원에 운좋은 타이밍에 입학했다고 생각한다. 그래도 차후에 다른 사람에게 도움이 될만한 정보와 신설된 데이터 사이언스 대학원에 입학하게된 계기 및 과정을 쓰려고 한다. 또한, 후회없는 2년(혹은 그 이상..?) 대학원 과정을 만들기 위해서 어떤 것을 계획하고 어떻게 공부를 하는지도 기록할 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;논문 리뷰(NLP, XAI 분야)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;마지막으로 최근에 보고 있는 논문들을 간략하게 정리할 예정이다. 최근 1월~2월 사이에 읽은 Vision 분야에서 사용되고 있는 XAI 관련 논문을 읽었고 이를 정리하고자 한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 02 Feb 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/others/2020/02/02/geultto4.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/others/2020/02/02/geultto4.html</guid>
        
        
        <category>others</category>
        
      </item>
    
      <item>
        <title>[NLP] Attention Is All You Need - 2</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이전 글: &lt;a href=&quot;https://simonjisu.github.io/paper/2020/01/14/attentionisallyouneed.html&quot;&gt;Attention Is All You Need - 1&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-sub-layers&quot;&gt;3. Sub Layers&lt;/h1&gt;

&lt;h2 id=&quot;multi-head-attention&quot;&gt;Multi-Head Attention&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1jpQdv3lFrYNRZ5FbCvcXF4RDtpho0og_&quot; alt=&quot;[그림 1] Multi-Head Attention&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;[그림 1] Multi-Head Attention&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;첫번째 서브층(SubLayer) Multi-Head Attention 의 구조는 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 1&lt;/code&gt; 과 같다. 연구자들은 $d_{model}$ 크기의 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 를 한 번 수행하는 것보다 $h$ 개의 각기 다른 &lt;strong&gt;선형 투영(linear projection)&lt;/strong&gt;을 시켜, 크기가 $d_k$(&lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;), $d_v$(&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt;) 인 텐서를 사용해서 Attention 을 병렬로 수행하는 것이 더 유리한 것을 찾아냈다. 각기 다른 Attention 을 수행한 $h$ 개의 출력값은 하나로 concatenate 후에 최종 선형결합을 통해 다시 $d_{model}$ 크기로 돌아오는데 이를 수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{MultiHead}(Q, K, V) &amp;= \text{Concat}(\text{head}_1, \cdots \text{head}_h)W^O  \\ \text{where head}_i &amp;= \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;W 는 선형결합을 위한 매겨변수이며 각각의 크기는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned} W^Q_i \in \Bbb{R}^{d_{model}\times d_k}, W^K_i\in \Bbb{R}^{d_{model}\times d_k}, W^V_i\in \Bbb{R}^{d_{model}\times d_v}, W^O \in \Bbb{R}^{h*d_v\times d_{model}}\end{aligned}&lt;/script&gt;

&lt;p&gt;그렇다면 이렇게 큰 차원의 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 를 선형 변환 후에 $h$ 개의 Attention 을 나눠서 학습하게 했던 이유는 무엇 일까? 논문에서 이해한 것을 정리하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;일단 선형 변환된 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 를 $h$ 개로 나눠버리는데, 이는 각 토큰을 표현하고 있던 큰 차원의 뉴런들을 $h$ 개의 블록으로 나눴다고 할 수 있다. 이렇게 위치가 상이한 각기 다른 표현 부분공간(representation subspaces) 블록들이 교차하면서(jointly) 정보를 얻게 된다. 이 말은 곧 $h$ 개의 Attention Matrix 가 생기면서 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 간의 토큰들이 더 다양한 관점으로 볼 수 있다는 말이다. 만약에 나누지 않았다면 단 하나의 Attention Matrix 를 생성하면서 이러한 효과를 뭉게버림으로, 선형 변환 층(linear projection)은 학습 과정을 반복하면서 최적의 $h$ 개의 Attention Matrix 를 생성하는 역할을 학습하게 된다.&lt;/p&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/sublayers.py#L11&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;position-wise-feed-forward-networks&quot;&gt;Position-wise Feed-Forward Networks&lt;/h2&gt;

&lt;p&gt;또 다른 서브층으로써 완전 연결층(Fully Connect Layer)인 네트워크를 Encoder, Decoder 뒤에 하나씩 추가했다. 이 완전 연결층은 두 개의 선형변환과 ReLU 활성화 함수를 사용했으며 그 수식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{FFN}(x) = \max(0, xW_1+b_1)W_2+b_2&lt;/script&gt;

&lt;p&gt;아마도 입력 텐서가 각 토큰의 위치별로 차원이 커졌다가 다시 원래 모양으로 줄어들어서 이름이 Position-wise 라고 붙여진 것으로 추정되는데, 차원의 크기가 다음과 같이 변하기 때문이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(B, T, d_{model}) \rightarrow(B, T, d_{ff}) \rightarrow (B, T, d_{model})&lt;/script&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/sublayers.py#L91&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;4-embeddings&quot;&gt;4. Embeddings&lt;/h1&gt;

&lt;h2 id=&quot;input-과-output&quot;&gt;Input 과 Output&lt;/h2&gt;

&lt;p&gt;Embedding 층과 Position Encoding 을 설명하기 전에 입출력이 어떻게 구성되어 있는지를 살펴봐야한다. 기계 번역 문제를 다시 예시로 들어보면, 다음과 같이 수치화된 문장들이 있다. 0 은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Padding&lt;/code&gt; 토큰으로써 데이터 처리를 위해 설정한 문장의 최대 길이에 맞춰서 넣은 인위적인 토큰이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{src} &amp;= \begin{bmatrix}3&amp;6&amp;4&amp;9 \\ 1&amp;3&amp;5&amp;0 \\ 3&amp;2&amp;0&amp;0 \end{bmatrix} \\ \text{trg} &amp;= \begin{bmatrix}2&amp;5&amp;4&amp;0 \\ 2&amp;5&amp;6&amp;0 \\ 2&amp;7&amp;4&amp;9 \end{bmatrix}  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;즉, 위 행렬을 해석하면 현재 Input 데이터는 미니배치가 3, 문장의 최대 길이가 4인 데이터, Target 데이터는 미니배치가 3, 문장의 최대 길이가 4인 데이터다. 각 문장의 토큰들에 순서 인덱스를 부여하여 포지션(Position) 데이터를 얻고자하면 다음과 같다. &lt;code class=&quot;highlighter-rouge&quot;&gt;Padding&lt;/code&gt; 은 인위적으로 넣은 데이터기 때문에 순서가 없어야 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{src_pos} &amp;= \begin{bmatrix}1&amp;2&amp;3&amp;4 \\ 1&amp;2&amp;3&amp;0 \\ 1&amp;2&amp;0&amp;0 \end{bmatrix} \\ \text{trg_pos} &amp;= \begin{bmatrix}1&amp;2&amp;3&amp;0 \\ 1&amp;2&amp;3&amp;0 \\ 1&amp;2&amp;3&amp;4 \end{bmatrix}  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Decoder 의 경우 이전 타임 스텝(t-1)의 토큰들로 다음 타임 스텝(t)의 토큰을 예측하기 때문에 실질적으로 모델에 입력되는 데이터(&lt;code class=&quot;highlighter-rouge&quot;&gt;trg_input&lt;/code&gt;)와 실제 예측해야하는 타겟 데이터(&lt;code class=&quot;highlighter-rouge&quot;&gt;gold&lt;/code&gt;)는 다음과 같다. 즉, 예를 들어 1, 2, 3 포지션에 해당하는 타겟 값을 입력으로 주었을때 2, 3, 4 번 포지션에 해당하는 값을 예측하는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{trg_input} &amp;= \begin{bmatrix} 2&amp;5&amp;4 \\ 2&amp;5&amp;6 \\ 2&amp;7&amp;4 \end{bmatrix} \\ \text{gold} &amp;= \begin{bmatrix}5&amp;4&amp;0 \\ 5&amp;6&amp;0 \\ 7&amp;4&amp;9 \end{bmatrix}  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;토큰에 순서 정보인 포지션을 구하는 이유는 무엇일까? 그 해답은 RNN 의 구동원리에 있는데, RNN 을 Cell 단위로 만들면 다음 코드와 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# create inputs tensor
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_dimension&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# setting rnn cell
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;hidden_dimension&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rnn_cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RNNCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# RNN Layer
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnn_cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# print: torch.Size([10, 2, 6])
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;RNN 의 특징 중 하나는 시퀀스 길이에 상관없이 한 스텝씩 처리하기 때문에 아주 긴 시퀀스도 처리를 할 수 있다. 그러나 이러한 특징은 이전 타입스텝의 정보를 다음 타임스텝에게 전달할 수 있지만 병렬 처리가 불가능 하다. 하지만 Transformer 의 목표중 하나는 시퀀스 데이터의 병렬 처리인데, 즉, 한 번에 지정된 길이의 시퀀스를 모두 모델에게 전달하고 Forward 하게 된다. 그렇다면 시퀀스의 각 토큰간 순서 관계 정보를 모델은 어떻게 알아낼 수 있을까? 바로 &lt;strong&gt;Position Encoding&lt;/strong&gt; 을 통해서 각 토큰의 순서 정보를 &lt;strong&gt;Embedding&lt;/strong&gt; 된 벡터와 결합하여 모델로 전달하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;embedding&quot;&gt;Embedding&lt;/h2&gt;

&lt;p&gt;임베딩은 분절된 토큰들을 고정된 $d_{model}$ 차원의 공간으로 표현해주는 방법이다. Decoder 의 출력층에는 선형변환 층과 Softmax 를 섞어서 예측 토큰의 확률을 구하는 기법을 사용했으며, 임베딩된 벡터에 $\sqrt{d_{model}}$ 를 곱했다.&lt;/p&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/layers.py#L107&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h2&gt;

&lt;p&gt;Positional Encoding 은 상대적이거나 절대적인 위치정보를 부여하는 방법이다. 각 Position Encoding 의 차원의 크기는 더할 수 있게 임베딩된 텐서의 차원 크기인 $d_{model}$과 같고 수식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} PE_{pos, 2i} &amp;= \sin(\frac{pos}{10000^{2i/d_{model}}}) \\ PE_{pos, 2i+1} &amp;= \cos(\frac{pos}{10000^{2i/d_{model}}})\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;결론을 말하자면 각 시퀀스의 순서 인덱서는 PE(Positonal Encoding) 테이블에서 각자의 위치를 조회후에 임베딩된 텐서와 결합하게 된다. pos 는 시퀀스의 위치정보, 예를 들어 텐서의 크기가 $d_{model}$ = 1024 의 경우, 각 1024의 짝수(2i)에 위치한 값들은 sin 함수를 적용하고, 홀수(2i+1) 에 위치한 값들은 cos 함수를 적용한다. PE 테이블을 그리면 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 2&lt;/code&gt; 과 같은데, 자세히 보시면 각 포지션에 해당하는 1 줄(1024 크기의 벡터)값은 모두 차별화 되어있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1IznpVENdNpwyKqJCD0mWnZRcQ2XaIh22&quot; alt=&quot;[그림 2] 최대 길이가 51인 Positional Encoding Table&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;[그림 2] 최대 길이가 51인 Positional Encoding Table&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/layers.py#L82&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음편: [NLP] Attention Is All You Need - 3&lt;/p&gt;
</description>
        <pubDate>Sun, 02 Feb 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/02/02/attentionisallyouneed2.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/02/02/attentionisallyouneed2.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>[NLP] Attention Is All You Need - 1</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;그 동안 LSTM(&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1246450&quot;&gt;Long Short-term Memory&lt;/a&gt;, 1997) 과 GRU(&lt;a href=&quot;https://arxiv.org/abs/1412.3555&quot;&gt;Gated Recurrent Unit&lt;/a&gt;, 2014) 등의 RNN 계열은 언어 모델링, 기계번역 등의 문제와 같이 시퀀스 모델링(sequence modeling)을 하기에 최고의 알고리즘이었다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1si3KMBjwZJ3inzTuoeUUsl7mutbDLNbz&quot; alt=&quot;[그림 1] RNN의 forward propagation&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;[그림 1] RNN의 forward propagation&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;그림 1&lt;/code&gt; 처럼 이전 스텝의 은닉층 유닛인 $h_{t-1}$ 를 현재 스텝의 은닉층 유닛 $h_t$ 로 전달하면서 자연스럽게 시퀀스 데이터의 특징을 유지하지만, 아쉽게도 병렬 처리를 원천적으로 배제한다는 단점이 존재한다. 따라서 만약에 문장이 길어질 수록 훈련 속도가 현저하게 느려진다.&lt;/p&gt;

&lt;p&gt;Input 과 Output 문장의 길이와 관계없이 의존성(dependencies)을 해결해주는 &lt;strong&gt;Attention&lt;/strong&gt; 매커니즘은 시퀀스 모델링 혹은 변환 모델링&lt;span style=&quot;color:gray&quot;&gt;(transduction modeling: 각기 다른 특성을 가진 입력-출력 데이터를 변환하는 문제들, 예를 들어 기계번역)&lt;/span&gt;에서 필수적인 요소가 됐다. 예시로 다음 논문들을 참고하면 좋다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau 2014&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.00887&quot;&gt;Structured Attention Networks, Yoon Kim, 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.01933&quot;&gt;A Decomposable Attention Model for Natural Language Inference, Ankur P. Parikh, 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 두 가지를 결합하여 저자들은 Attention 매커니즘만 활용하여 Input 과 Output 의 의존성을 글로벌하게 처리하고, 병렬화까지 가능한 &lt;code class=&quot;highlighter-rouge&quot;&gt;Transformer&lt;/code&gt;라는 새로운 모델구조를 제안했다.&lt;/p&gt;

&lt;h2 id=&quot;전체-모델구조&quot;&gt;전체 모델구조&lt;/h2&gt;

&lt;p&gt;대부분의 신경망 시퀀스 변환 모델(transduction models)들은 대체로 Encoder 와 Decoder 로 구성된다. Encoder는 심볼로 표현된 입력 시퀀스(비연속적인 토큰들) $x$ 를 연속 공간(Continuous Space) $z$ 로 맵핑 후, $z$ 를 바탕으로 출력 시퀀스 심볼인 $y$ 를 얻는다. 출력 시퀀스는 이전 타임 스텝($t-1$) 시퀀스를 입력으로 다음 타임 스텝($t$)을 출력하는 자기회귀(auto-regressive) 성격을 가진다. 수식으로 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \mathbf{x}&amp;=(x_1, x_2, \cdots, x_n) \rightarrow \mathbf{z}=(z_1, z_2, \cdots, z_n)\\ \mathbf{y}&amp;=(y_1, y_2, \cdots, y_m)\ \text{for}\  y_{t}=f(y_{t-1}, \mathbf{z}) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=15FPAUru5Rm1x3LUu6pcSjaZiuRrBkj97&quot; alt=&quot;[그림 2] 모델구조: Encoder(좌), Decoder(우)&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;[그림 2] 모델구조: Encoder(좌), Decoder(우)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;하지만 &lt;strong&gt;Transformer&lt;/strong&gt; 에서는 한 타임 스텝마다 $y$ 를 출력하지 않고 한번에 처리한다. 저자들이 제안한 전체적인 모델구조는 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 2&lt;/code&gt; 와 같다(전체적인 느낌만 보고 다음으로 넘어가도록 한다).&lt;/p&gt;

&lt;h2 id=&quot;encoder&quot;&gt;Encoder&lt;/h2&gt;

&lt;p&gt;Encoder는 각기 다른 N 개의 “Encoder Layer”라는 층으로 구성되며, 각 층에는 두 개의 서브층(SubLayer)이 존재한다. 첫번째는 Self Attention을 수행하는 “Multi-Head Attention”, 두번째는 일반적인 “Position-wise Feed Forward”로 구성되며, 각 서브층은 Residual Network(&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;Kaiming He, 2015&lt;/a&gt;)처럼 서브층의 입력과 출력을 결합하고, 그 결괏값을 다시 LayerNorm(&lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Jimmy Lei Ba, 2016&lt;/a&gt;) 을 통과시켜 출력을 얻는다. 수식으로 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{LayerNorm}(x + \text{SubLayer}(x))&lt;/script&gt;

&lt;h2 id=&quot;decoder&quot;&gt;Decoder&lt;/h2&gt;

&lt;p&gt;Decoder도 Encoder와 마찬가지로 각기 다른 N 개의 “Decoder Layer” 라는 층으로 구성된다. 다만, Encoder의 출력을 받아서 “Multi-Head Attention”을 수행하는 3번째 서브층이 추가된다. Self Attention을 수행하는 첫번째 “Multi-Head Attention”에서는 뒤에 있는 시퀀스정보로 부터 예측을 하지 않게 이를 가리게 됩니다. 따라서 $i$ 번째 토큰은 $i+1$ 번째 이후의 토큰을 참조하지 않게 됩니다. 나머지는 Encoder와 마찬가지로 잔차 연결(residual connection)을 수행하고 LayerNorm을 통과하게 된다.&lt;/p&gt;

&lt;p&gt;이제부터 모델의 세부 사항을 살펴보면서 저자가 왜 이렇게 사용했는지, 의도가 무엇인지를 알아보려고 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2-scaled-dot-product-attention&quot;&gt;2. Scaled Dot-Product Attention&lt;/h1&gt;

&lt;h2 id=&quot;attention&quot;&gt;Attention&lt;/h2&gt;

&lt;p&gt;Transformer 에서 Attention은 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;query(Q)&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;key(K)&lt;/strong&gt;&lt;/span&gt;-&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;value(V)&lt;/strong&gt;&lt;/span&gt; 세트를 입력으로 집중된 어떤 벡터를 출력하는 함수로 표현할 수 있다. 출력은 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 간의 관계(Attention), 즉 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 의 정보를 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 에 대조 했을 때, 어느 부분을 집중해서 볼 것인지를 계산하고 해당 관계를 &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 와 결합하여 출력을 만든다. 수식으로 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O = \text{Attention}(Q, K, V)&lt;/script&gt;

&lt;p&gt;직관적으로 잘 안떠오르는데, 이게 어떤 느낌인지 알아보기위해 예를 들어보면 다음과 같다.&lt;/p&gt;

&lt;h2 id=&quot;기계번역-문제&quot;&gt;기계번역 문제:&lt;/h2&gt;

&lt;p&gt;영어를 한국어로 번역하는 문제를 예로 들자면, 영어는 소스 문장, 한국어는 타겟 문장이 된다. &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;query(Q)&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;key(K)&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;value(V)&lt;/strong&gt;&lt;/span&gt; 관계는 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 3&lt;/code&gt; 과같이 표현할 수 있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=14tFq4-RDEDFbc9vEABWqiFxG0pI4qq3G&quot; alt=&quot;[그림 3] 기계번역 문제로 Q, K-V 의 관계 알아보기&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;[그림 3] 기계번역 문제로 Q, K-V 의 관계 알아보기&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;query(Q)&lt;/strong&gt;&lt;/span&gt;: 한국어 문장 정보&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;key(K)&lt;/strong&gt;&lt;/span&gt;-&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;value(V)&lt;/strong&gt;&lt;/span&gt; 세트: 인코딩된 영어 문장 정보, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;key(K)&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;value(V)&lt;/strong&gt;&lt;/span&gt; 는 같은 벡터&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 는 우리가 알고 싶어하는 문제라고 생각할 수 있다. 명칭도 “query=질의” 그대로 &lt;strong&gt;“한국어로 변역하기 위해 영어 문장에서 집중적으로 봐야하는 단어는 어느 것인가?”&lt;/strong&gt; 라는 질문을 인코딩된 영어 문장 정보인 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;  한테 물어보게 된다. 그 방법은 이 다음에 소개하도록 하고, 그렇게 얻은 결과인 &lt;strong&gt;A&lt;/strong&gt; 를 &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 와 곱하여 그 단어를 집중적으로 보게한다. 그렇게 Attention의 결과물인 &lt;span style=&quot;color:#49aa71&quot;&gt;&lt;strong&gt;O&lt;/strong&gt;&lt;/span&gt; 를 얻는다.&lt;/p&gt;

&lt;h2 id=&quot;감성-분석-문제&quot;&gt;감성 분석 문제:&lt;/h2&gt;

&lt;p&gt;꼭 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;-&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 가 다른 성격을 가진 시퀀스가 아니어도 된다. 세 토큰 모두 하나의 시퀀스를 가르킬 수도 있으며, 이를 Self-Attention 이라고 한다. 예를 들어 감성 분석(Sentiment Analysis) 문제를 예로 들면, 모델은 문장을 읽고 이를 사전에 정의해 놓은 감성 카테고리로 판단하게 되는 데, 이때 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 모두 같은 문장을 지정하여 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 4&lt;/code&gt;처럼 Attention 을 사용할 수 있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1vFw0wuulHhzu5kwZLQ1QStl24KjnlsgX&quot; alt=&quot;[그림 4] 감성 분류 문제를 통해 Self-Attention 에 대해 알아보기&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;[그림 4] 감성 분류 문제를 통해 Self-Attention 에 대해 알아보기&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;scaled-dot-product-attention&quot;&gt;Scaled Dot-Product Attention&lt;/h2&gt;

&lt;p&gt;Attention을 구하는 방법은 사실 다양하지만 Transformer 에서는 제일 기본적인 “Dot Product” 를 사용했으며, 그 수식은 다음과 같으며, 배치크기를 제외한 Q, K, V 의 크기를 표기해서 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 5&lt;/code&gt; 와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Attention}(Q, K, V) = \text{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V&lt;/script&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1CtBsDHkyU8hmFj2MB0IDhEQO7wCKUEkM&quot; alt=&quot;[그림 5] Q, K, V크기를 표기한 Scaled-Dot Product Attention&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;[그림 5] Q, K, V크기를 표기한 Scaled-Dot Product Attention&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;여기서 주의할 점은 $T_k$ 과 $T_v$가 같다는 점이다. 기계 번역을 예로 들면 소스 문장이 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;-&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 세트이기 때문에 같은 길이의 내용을 담고 있지만 각 토큰이 표현하고 있는 차원만 다를 뿐이다. &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 의 길이는 다를 수 있지만 차원 $d_k$ 로 같다. 두 행렬은 행렬의 곱(matrix multiplication)을 통해서 크기가 $(T_q, T_v)$ 인 점수 행렬 &lt;strong&gt;A&lt;/strong&gt; 를 만들어 낸다.&lt;/p&gt;

&lt;p&gt;행렬 &lt;strong&gt;A&lt;/strong&gt; 는 스케일링(Scaling), 마스킹(Masking) 후 Softmax 를 통해 확률값을 도출한다. 이 행렬의 뜻은 “문제를 해결하기 위해서 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 의 토큰이 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 의 어떤 토큰을 가장 많이 참고해야하는가?” 를 뜻한다. 따라서 확률이 높게 부여된 토큰은 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 의 해당하는 토큰과 연관성이 높다고 할 수 있다. 물론 이 모든 연산은 학습이 가능하도록 DAG(Directed acyclic graph)로 연결되어 있기 때문에 학습 스텝이 진행됨에 따라 풀고자하는 문제에 최적화된 확률을 계속 도출해낸다&lt;span style=&quot;color:gray&quot;&gt;((Masking 은 차후에 다룬다)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;스케일링 작업은 행렬 곱을 구한 &lt;strong&gt;A&lt;/strong&gt; 를 $\sqrt{d_k}$ 로 나누는데, 그 이유는 다음과 같다. 차원의 크기인 $d_k$ 가 커질 수록 행렬의 곱의 수치는 점점 커지고 Softmax 수식에 의해서 그 확률 값 또한 커진다. 따라서 Softmax 의 경사(gradient) 값도 굉장히 작아지는데, 이를 막기위해서 $\frac{1}{\sqrt{d_k}}$ 값을 곱해줘야한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;왜 $\sqrt{d_k}$ 를 나눌까?&lt;/strong&gt; 평균이 0, 표준편차가 1인 랜덤한 값으로 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 로 초기화시키고 확률로 표현된 행렬값 &lt;strong&gt;A&lt;/strong&gt; 의 경사를 구해보면 $d_k$ 가 커짐에 따라서 평균이 0, 분산이 $d_k$ 를 따르는 분포가 된다. 이러한 시뮬레이션을 다음 코드를 통해 알아 볼 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;check_dotproduct_dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampling_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    to check &quot;https://arxiv.org/abs/1706.03762&quot; Paper page 4, annotation 4
    -------------------------------
    To illustrate why the dot products get large, 
    assume that the components of q and k are independent random variables 
    with mean 0 and variance 1.
    Then their dot product has mean 0 and variance d_k
    
    print(&quot;*** notice that the gradient of softmax is y(1-y) ***&quot;)
    for d_k in [10, 100, 1000]:
        check_dotproduct_dist(d_k, sampling_size=100000, seq_len=5, threshold=1e-10)
    
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cal_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampling_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampling_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;size of vector d_k is {d_k}, sampling result, dot product distribution has&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; - mean: {attn.mean().item():.4f}, &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; - var: {attn.var().item():.4f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cal_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;le&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_percent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;count of gradients that smaller than threshod({threshold}) is {g_sum}, {g_percent:.2f}&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;attn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cal_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_sum2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;le&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_percent2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_sum2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;after divide by sqrt(d_k), count of gradients that smaller than threshod({threshold}) is {g_sum2}, {g_percent2:.2f}&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;% &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*** notice that the gradient of softmax is y(1-y) ***&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;check_dotproduct_dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampling_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;시뮬레이션 결과:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;*** notice that the gradient of softmax is y(1-y) ***
size of vector d_k is 10, sampling result, dot product distribution has

 - mean: -0.0004, 
 - var: 9.9979
count of gradients that smaller than threshod(1e-10) is 193, 0.01%
after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.00% 

size of vector d_k is 100, sampling result, dot product distribution has

 - mean: -0.0028, 
 - var: 99.9868
count of gradients that smaller than threshod(1e-10) is 402283, 16.09%
after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.00% 

size of vector d_k is 1000, sampling result, dot product distribution has

 - mean: 0.0029, 
 - var: 999.6312
count of gradients that smaller than threshod(1e-10) is 1737479, 69.50%
after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.00%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/modules.py#L8&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음편: &lt;a href=&quot;https://simonjisu.github.io/paper/2020/02/02/attentionisallyouneed2.html&quot;&gt;[NLP] Attention Is All You Need - 2&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Jan 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/01/14/attentionisallyouneed.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/01/14/attentionisallyouneed.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>What is Data Science?</title>
        <description>&lt;h1 id=&quot;데이터-사이언스란&quot;&gt;데이터 사이언스란?&lt;/h1&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Joma Tech&lt;/code&gt; 라는 분의 유튜브에서 간결하고 명료하게 데이터 사이언스의 유래 및 역할에 대해 설명하여, 이를 정리하고 현재 내 상황, 그리고 나아가야할 방향에 대해 분석해보려고 한다.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/xC-c7E5PK0Y&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;2020.02.02 추가: 데이터관련 직군에 대한 &lt;a href=&quot;https://d2wahc834rj2un.cloudfront.net/Workera%20Report.pdf?fbclid=IwAR0IEBfU-7w231SNnaJFM_DYPEqJQDgOdf5_eCVs0aGsazO9XBWaVxzrbF0&quot;&gt;Report&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;데이터-사이언스와-데이터-사이언티스트의-역할&quot;&gt;데이터 사이언스와 데이터 사이언티스트의 역할&lt;/h2&gt;

&lt;p&gt;동영상에서 소개한 데이터 사이언티스트의 근본은 &lt;strong&gt;“Problem Solver”&lt;/strong&gt;다. 현실에 존재하는 풀기 어려운 문제를 데이터를 사용해서 좋은 방향으로 이끌어가는 것이 데이터 사이언티스트의 역할이다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1Wm5n8IMK5ruCcgu-7ccUxPIH8XOuGx2N&quot; alt=&quot;출처: https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;출처: https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;위 사진은 영상에서 소개한 데이터 사이언스의 계층적 요구에 대해서 사진이다. 피라미드 꼭대기부터 정리해보면 다음과 같다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;번호&lt;/th&gt;
      &lt;th&gt;카테고리&lt;/th&gt;
      &lt;th&gt;업무&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;LEARN/OPTIMIZE&lt;/td&gt;
      &lt;td&gt;AI, Deep Learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;LEARN/OPTIMIZE&lt;/td&gt;
      &lt;td&gt;A/B Testing, Experimentation, Simple ML Algorithm&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;AGGREGATE/LABEL&lt;/td&gt;
      &lt;td&gt;Analytics, Metrics, Segments, Aggregates, Features, Training Data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;EXPLORE/TRANSFORM&lt;/td&gt;
      &lt;td&gt;Cleaning, Anomaly Detection, Prep&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;MOVE/STORE&lt;/td&gt;
      &lt;td&gt;Reliable Data Flow, Infrastructure, Pipelines, ETL(extract, transform, load), Structured and unstructured data storage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;COLLECT&lt;/td&gt;
      &lt;td&gt;Instrumentation, Logging, Sensors, External Data, User generated content&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;동영상에 따르면 스타트업 같은 경우, 리소스가 부족하기 때문에 데이터 사이언티스트는 1~6의 업무를 다 맡게 된다. 중견 기업의 경우 약간의 리소스를 더 사용하여, 데이터의 수집(6)은 소프트웨어 엔지니어가, 데이터의 보관 및 정제 준비(4, 5)는 데이터 엔지니어가, 나머지는 데이터 사이언티스트가 하게 된다. 조금더 큰 기업이라면 탑 3개의 분야를 한번 더 나눠서 데이터 분석, 평가 방법 설정 실험 등(2, 3)은 데이터 사이언스 애널리틱스, AI, Deep Learning 부분(1)의 업무는 리서치 사이언티스트 혹은 코어 데이터 사이언스가 맡게 된다.&lt;/p&gt;

&lt;p&gt;정리하면 이 분야의 직군 분류는 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;소프트웨어 엔지니어(풀스택) - Software Engineer(Full Stack)&lt;/li&gt;
  &lt;li&gt;데이터 엔지니어 - Data Engineer&lt;/li&gt;
  &lt;li&gt;데이터 사이언스 애널릭틱스 - Data Science Analytics&lt;/li&gt;
  &lt;li&gt;리서치 사이언티스트 / 코어 데이터 사이언스 - Research Data Scientist / Core Data Science&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이쯤에서 &lt;del&gt;차후에 가고 싶은&lt;/del&gt; 페이스북(Facebook)의 채용 공고를 몇개 살펴보면 그 역할이 다 다르다는 것을 확인 할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Research Scientist, Artificial Intelligence (PhD)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1D83TD6ZtJrSNwsSYNZVl86IynC0SWnuI&quot; alt=&quot;출처: https://www.facebook.com/careers/jobs/985225105171427&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;출처: https://www.facebook.com/careers/jobs/985225105171427&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Data Scientist, Analytics (PhD)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1c_DpRBhg9yAwQYE59swch4X2OFhSt4qv&quot; alt=&quot;출처: https://www.facebook.com/careers/jobs/387405225294114&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;출처: https://www.facebook.com/careers/jobs/387405225294114&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Data Engineer, Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://drive.google.com/uc?id=1mabjOctjAExkzV2SLpbv7UPo6zMpu_Uu&quot; alt=&quot;출처: https://www.facebook.com/careers/jobs/997860117229177&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;
    &lt;figcaption&gt;출처: https://www.facebook.com/careers/jobs/997860117229177&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;–&lt;/p&gt;

&lt;h2 id=&quot;상황분석-및-나아가야할-방향&quot;&gt;상황분석 및 나아가야할 방향&lt;/h2&gt;

&lt;p&gt;최근 언론에서 이야기하는 부분은 극 소수인 1번 분야이고 지금까지 내가 공부한 방향은 대부분 딥러닝 쪽이었다. 그러나 더 현실적인 문제는 4, 5 번인 데이터 엔지니어 분야, 3번인 데이터 분석 쪽에 있다고 생각한다. 특정 문제를 해결하기 위해 데이터를 수집 및 정제하고, 어플리케이션으로 과정을 경험해보면 좋은 포트폴리오가 될거라고 생각했다.&lt;/p&gt;

&lt;p&gt;그렇다면 앞으로 대학원 2년간 어떤 전략을 짤 것인가?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;기존의 딥러닝 분야의 공부는 계속하되, XAI, NLP 두 분야만 집중적으로 공부한다(+ 수학 및 통계 공부는 지속).&lt;/li&gt;
  &lt;li&gt;데이터 기반 Product를 분석 및 연구
    &lt;ul&gt;
      &lt;li&gt;각 산업별로 관련 회사 리스트업&lt;/li&gt;
      &lt;li&gt;어떤 문제들이 있었고, 어떻게 해결했는지 스터디하기&lt;/li&gt;
      &lt;li&gt;가능하면 업계사람들 많이 만나보기(묻고 싶은 질문지 준비하기)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;간단한 서비스, 웹 어플리케이션을 만들고 개선함으로써 데이터 수집 및 가공하는 연습하기(팀단위로 진행 목표)
    &lt;ul&gt;
      &lt;li&gt;실존하는 해결하지 못한 문제를 찾아보고 괜찮은 서비스 기획해보기&lt;/li&gt;
      &lt;li&gt;SQL 기반의 데이터 파이프라인 설계(ETL 프로세스 설계 및 구현)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 03 Jan 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/datascience/2020/01/03/whatisdatascience.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/datascience/2020/01/03/whatisdatascience.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>2019년도 회고</title>
        <description>&lt;h1 id=&quot;올해의-공부-농사&quot;&gt;올해의 공부 농사&lt;/h1&gt;

&lt;p&gt;올 한해는 작년보다 한 층 더 성장했다고 생각한다. 작년과 비교해서 어떤 목표를 달성했는지 어떤 공부를 더 했는지 GitHub에 정리를 해두었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://github.com/simonjisu/LookBack-Me&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;작년에는 NLP 공부에 매진했다면, 올해는 확률 통계의 기초를 조금더 공고히 다지고, XAI 분야과 조우했다. 물론 쉽지 않지만 앞으로 더 수요가 있는 분야일거라 생각한다. 딥러닝의 성능은 좋지만, 기계가 어떤 결정을 내리는지 아직 명백하게 밝히기 어려운 부분들이 많이 있다. 사람(Human)을 배제한 End-to-End 학습이 현업에서 딥러닝을 도입하지 못하는 이유라고 생각한다.&lt;/p&gt;

&lt;p&gt;XAI 분야는 “기계의 설명력”을 3가지 경우로 응용 할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;기계의 성능이 사람보다 못할 경우, 연구자가 개입해서 성능 부족의 부분을 찾아내는 역할&lt;/li&gt;
  &lt;li&gt;기계의 성능이 사람과 비슷한 경우, 사용자에게 신뢰와 믿음의 근거가 되는 역할&lt;/li&gt;
  &lt;li&gt;기계의 성능이 사람보다 우월한 경우, 기계가 직접 선생님이 되어 다른 기계를 가르치는 역할&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2020년도에는 이 분야를 계속해서 공부하려고 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;올해의-top-4-사건&quot;&gt;올해의 Top 4 사건&lt;/h1&gt;

&lt;p&gt;올해의 개인 Top4 사건을 추리자면 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#1-커넥트재단-퇴사-서울대-데이터-사이언스-대학원-진학&quot;&gt;커넥트재단 퇴사, 서울대 데이터 사이언스 대학원 진학&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-책-출간-및-강의&quot;&gt;책 출간 및 강의&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-글또-3기-ai-collage&quot;&gt;글또 3기, AI Collage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-home-comming-github-블로그-복귀&quot;&gt;Home Comming, GitHub 블로그 복귀&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-커넥트재단-퇴사-서울대-데이터-사이언스-대학원-진학&quot;&gt;1. 커넥트재단 퇴사, 서울대 데이터 사이언스 대학원 진학&lt;/h2&gt;

&lt;p&gt;2018년 2월부터 다니던 네이버 커넥트재단을 2019년 8월에 퇴사했다. 주로 맡은 업무는 강의 퍼블리싱 및 기획 업무였다. 개인적으로 첫 직장에서 좋은 사람들을 만나서 행운이라 생각했다. 그동안 했던 일을 살펴보면 다음과 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;퍼블리싱 업무&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.edwith.org/deepnlp&quot;&gt;딥러닝을 이용한 자연어 처리&lt;/a&gt; - 조경현 교수님&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.edwith.org/deeplearningai1&quot;&gt;DeepLearniNg AI 시리즈&lt;/a&gt; - Andrew Ng&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;프로젝트 기반 학습 콘텐츠 기획 및 매니징&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://deeplearningzerotoall.github.io/season2/&quot;&gt;모두를 위한 딥러닝 시즌 2&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.edwith.org/boostcourse-dl-tensorflow&quot;&gt;텐서플로우로 시작하는 딥러닝 기초&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.edwith.org/boostcourse-dl-pytorch&quot;&gt;파이토치로 시작하는 딥러닝 기초&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;수강신청 데이터를 잘 보기 위한 대시보드 만들기&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.notion.so/simonjisu/87f45a6c9a264f43aa53c843157026ef&quot;&gt;Notion 포스트 링크&lt;/a&gt;, 조만간 다시 이 블로그에 새로 올릴 예정&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;커넥트재단을 다니면서 좋았던 점은 해당 분야의 유명한 사람들을 만날 수 있었다는 것이다. GRU과 기계번역 Encoder-Decoder구조를 제안하신 조경현 교수님의 오프라인 강의를 현장 맨 앞줄에서 들을 수 있었고, 캐나다 Albert 대학의 &lt;a href=&quot;https://www.ualberta.ca/science/about-us/contact-us/faculty-directory/randy-goebel&quot;&gt;Randy Geobel&lt;/a&gt; 교수님을 보고 대가의 생각을 들으면서 다시 한 번 공부뽕(?!)이 차오르는 경험을 했다.&lt;/p&gt;

&lt;p&gt;막상 퇴사하려니 조금 아쉽기도 했지만, 내가 하고싶은 공부를 하기위해 떠났다. 그리하여 올해 하반기에는 대학원에 진학하려고 부단히 노력했다. 최근에 이 분야로 진출하려는 사람들이 많아져 컨택이 쉽지는 않았다(누가 나이 30 대학원생을 받아주려나…). 다행이 이번에 신설된 서울대 데이터 사이언스 대학원에 합격하여 내년부터 하고싶은 공부를 하게 됐다. 앞으로 2년 동안 큰 목표는 다음과 같이 세웠다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;데이터 수집단계 공부를 집중적으로 한다.&lt;/li&gt;
  &lt;li&gt;NLP, 금융 두 개의 도메인에서 XAI를 적용한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;전에 다니던 회사는 비교적 작은 규모의 회사였고, 데이터팀 같은건 없었다. 수강신청 대시보드도 사실 내가 보고싶어서 혼자 만든 작은 프로젝트다. 사람들이 언제 학습을 주로하고 어떤 콘텐츠를 주로 공부하는지 살펴볼 수 있는지 궁금했기 때문이다.&lt;/p&gt;

&lt;p&gt;대부분의 기업에서는 데이터를 수집하는 pipeline이 제대로 갖춰저 있지 않기 때문에, 데이터 수집단계에서 잘 설계해야 원하는 분석을 진행하고, 데이터에서 얻은 인사이트로 의사결정을 할 수 있다. 따라서 첫번째 목표는 데이터 수집을 잘 하기 위한 공부를 할 것이다.&lt;/p&gt;

&lt;p&gt;두번째 목표는 지금까지 공부한 분야에서 최근에 공부한 XAI 분야를 접목하려고 한다. 기계의 의사결정을 잘 설명할 수 있는 모델을 만들어 사람이 보고 기계를 신뢰할 수 있게 만들어 보려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;2-책-출간-및-강의&quot;&gt;2. 책 출간 및 강의&lt;/h2&gt;

&lt;p&gt;2019년도 7월말에 내가 쓴 책 &amp;lt;딥러닝에 목마른 사람들을 위한 PyTorch&amp;gt;가 발간됐다. 책 내용은 집에서 GPU서버를 만드는 법 부터 시작해서, PyTorch로 딥러닝에 입문하는 것이다. 책을 쓰는 작업이 정말 어렵다는 것을 이번 경험으로 부터 깨달았다. 부족한 부분도 많아서 다음에는 조금더 알찬 내용으로 구성해서 다시 gitbook 형태로 써볼까 생각중이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://bit.ly/딥목파&quot;&gt;&amp;lt;딥러닝에 목마른 사람들을 위한 PyTorch&amp;gt; 홈페이지 바로가기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2019년도 말에는 멀티캠퍼스에서 책 내용을 기반으로 강의도 진행했다. 처음해보는 강의라 목소리도 많이 쉬었었다. 사람이 말을 이렇게나 많이 할 수도 있구나 생각했다… 그래도 최선을 다해서 개념 하나하나 설명해드리고 자료도 충분히 많이 준비했다고 생각한다. 3일짜리 강의라 처음 배우시는 분들이 개념을 받아들이기 쉽지는 않았을 거라 생각한다. 다음에 기회가 된다면 책처럼 만들어서 배포해보는 것도 나쁘지 않을 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;3-글또-3기-ai-collage&quot;&gt;3. 글또 3기, AI Collage&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;글또&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2019년도 여름부터 겨울까지 글또 3기에 참가했다. 역시 강제로 due를 정하더니 글을 쓰게 되더라. 이런 규제는 자신에게 긍정적인 효과가 있다고 생각한다. 그래도 초중반까지 꾸준히 글을 써왔는데 후반부에는 바쁘다는 핑계로 계속 안쓰게 됐다. 반성하자. 다음 기수에도 참가할 생각이다. 그때는 정말 한번도 놓치지 않고 예치금을 다 돌려 받으리라.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.facebook.com/groups/375431516259701/&quot;&gt;글또 페이스북 바로가기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;AI Collage&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;또 하나 새로 참가한 것은 모두의 연구소에서 진행하는 AI Collage다. AI Collage는 현실에 있는 6개 분야의 문제를 풀어보고, 최종단계에서는 논문을 써보는 장기 프로젝트다. XAI 팀에 합류하여 여러가지 논문을 읽고 구현을 진행하고 있다. 내년 5월까지 진행되는데 아직 여러 논문을 구현 중에 있다. 내년에는 꼭 논문을 써보려고 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/simonjisu/XAI&quot;&gt;XAI GitHub&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://aic.yangjaehub.com/&quot;&gt;AIC 바로가기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-home-comming-github-블로그-복귀&quot;&gt;4. Home Comming, GitHub 블로그 복귀&lt;/h2&gt;

&lt;p&gt;지난 1년간 Notion을 활용해서 블로그를 만드려고 했다. 깔끔하고 페이지를 정리하는데 좋은 기능들이 많기 때문이다. 그러나 블로그로 활용하기에 부족한 부분이 2가지 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Inline Math 의 부재&lt;/li&gt;
  &lt;li&gt;Google Analytics 의 부재&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Inline Math를 사용할 수 없는 것은 큰 타격이었다. 보통 논문을 보면 수식이 정말 많은데 이를 사용할 수 없다 점은 너무 불편했다. 한줄 짜리 간단한 수식조차 Math Block 으로 만들어야 했기 때문에, 글의 구조를 짜는데 방해가 됐다. Notion 공식 트위터에 많은 사람들이 요청중이긴 하나 언제 개발될지는 아직 미지수다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/notionhq/status/1093334827770699778&quot;&gt;Notion 공식 트위터 Inline Math 요청 바로가기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 GA를 달수 없어서 내 블로그의 유입을 파악할 수가 없었다. 작은 이미지 패킷으로 페이지 조회수를 확인 할 수 있지만, 그럴려면 모든 페이지에 패킷을 달아야하는데 여간 귀찮은 작업이 아니다.&lt;/p&gt;

&lt;p&gt;그래서 다시 복귀했다. 이번달에 블로그 스킨을 다시 깔끔하게 바꾸고 우측의 custom 메뉴바를 으로 2일동안 만들었다. 그렇다고 Notion 이 안좋은 것은 아니다. 빠르게 메모나 기록할 때, 여행 계획을 만들때 이만큼 유용한 도구는 없다. 다만 Notion 기술 블로그를 만드려고 하면 굉장히 비추천한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2020년도-목표&quot;&gt;2020년도 목표&lt;/h1&gt;

&lt;p&gt;2020년이 코앞으로 다가왔다. 내년에는 다음과 같은 목표를 세웠다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PRML 공부 계속하기&lt;/li&gt;
  &lt;li&gt;더 많은 논문을 읽고 블로그에 정리하기&lt;/li&gt;
  &lt;li&gt;글또 4기에 참가하기&lt;/li&gt;
  &lt;li&gt;AI Collage 에서 논문 한편 쓰기&lt;/li&gt;
  &lt;li&gt;데이터 프로세싱에 대한 공부하기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이중 얼만큼 달성할지 모르겠지만, 노력하여 올해보다 더 성장할 수 있는 한해가 되기를 …&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1pCe5D4-wrxlP4v1CqRx_u-_CeGZToDDJ&quot; width=&quot;240px&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;P.S. 닌텐도 스위치도 사자&lt;/del&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Dec 2019 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/others/2019/12/30/lookback2019.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/others/2019/12/30/lookback2019.html</guid>
        
        
        <category>others</category>
        
      </item>
    
      <item>
        <title>[PyTorch] ConvTranspose2d 와 Conv2d 의 관계</title>
        <description>&lt;p&gt;최근 XAI 에 관련된 공부를 하면서 비전쪽의 많은 논문을 살펴보고 있다. “Visualizing and Understanding Convolutional Networks (2013)” 논문에서는 이미지 처리에서는 CNN 알고리즘이 제일 좋지만, 그 이유에 대해서 탐구를 시도한 논문이다. 도대체 Convolution의 필터가 어떤 역할을 하는지, 이들이 어떤 부분을 살펴보는 지를 확인한다. 오늘은 이 논문에서 제안하는 Deconvolutional layers(정확히는 Fractionally-strided convolution 이지만 차후에 언급한다)의 실체를 낱낱이 살펴보도록 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;link: &lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;Visualizing and Understanding Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;convolution-layer&quot;&gt;Convolution layer&lt;/h1&gt;

&lt;p&gt;Deconvolutional layer을 알아보기 전에 합성곱 연산(Convolutional Operation)에 대해 알아볼 필요가 있다. 합성곱 연산은 필터가 조금씩 이동하면서 이미지의 일부와 필터간 연산을 통해 진행된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=17x4ZQ_r0FTa_mlDFiIWvMJcg22vRrBd6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반적으로 CNN 알고리즘은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Convolution&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Activation&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Maxpooling&lt;/code&gt; 과정을 거친다. 예를 들어, 위 그림과 같이 4x4 이미지는 3x3 필터를 통해 2x2 의 선형변환 값을 갖는다(padding=0, stride=1 인 경우). 그리고 활성화 함수를 통과한 뒤에 Maxpool 과정을 거친다. 이때 &lt;code class=&quot;highlighter-rouge&quot;&gt;Convolution&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Activation&lt;/code&gt;을 거쳤을 때 나오는 텐서를 Activation Map이라고 하고, &lt;code class=&quot;highlighter-rouge&quot;&gt;Pooling&lt;/code&gt; 과정을 거쳤을 때 나오는 텐서를 Pooled Map 이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1Y4kIqXn7vUYQgoZWDdprrO-SP9a-Qogs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 논문에서는 그 과정을 역으로 한번 해보는 것을 제안했다. 위 그림처럼 마지막 Pooled Maps 에서 풀링된 위치를 기억했다가(Max Locations “Switches” 부분), 이 위치를 기반으로 역으로 Unpooled Maps 를 재구축한다(이 부분에 관심있는 분들은 이 논문을 한번 살펴보는 것을 추천드린다). 이번 글에서는 그 다음 스텝인 Convolution layer 에서 역으로 돌아가는 방법에 대해서 설명하려고 한다.&lt;/p&gt;

&lt;p&gt;먼저 이미지의 크기를 $N$, 필터(커널)의 크기를 $K$, 패딩의 크기를 $P$, 스트라이드를 $S$ 라고 정의하고, 여러 변수를 정의 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{input image size} &amp;= N \times N =4 \times 4 \\ \text{filter size} &amp;= K\times K = 3 \times 3 \\ \text{padding} &amp;= P = 0 \\ \text{stride} &amp;= S = 1 \\ \text{output image size} &amp;= (\dfrac{N+2P-K}{S}+1, \dfrac{N+2P-K}{S}+1) \\&amp;= 2 \times 2 \\ \text{input image} &amp;: X^{(l)} = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp;x_{14}\\  x_{21} &amp; x_{22} &amp; x_{23} &amp;x_{24}\\ x_{31} &amp; x_{32} &amp; x_{33} &amp;x_{34}\\ x_{41} &amp; x_{42} &amp; x_{43} &amp;x_{44}\end{bmatrix}^{(l)} \\ \text{output image} &amp;: x^{(l+1)} =\begin{bmatrix} x_{11} &amp; x_{12}\\ x_{21} &amp; x_{22} \end{bmatrix}^{(l+1)} \\ \text{filter} &amp;: W = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13}\\  w_{21} &amp; w_{22} &amp; w_{23}\\ w_{31} &amp; w_{32} &amp; w_{33}\end{bmatrix} \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;이제 수식으로 합성곱 연산을 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} x_{pq}^{(l+1)} &amp;= \sum_{p=i}^{K+i-1} \sum_{q=j}^{K+j-1} w_{pq} x_{pq}^{(l)} \quad \text{for }i, j \in (1, 2, \cdots,  N-K+1)\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위 수식으로는 어려워 보이지만 아래와 같은 연산을 &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; 라고 하면 결과는 2x2 행렬이 출력되며 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} X^{(l+1)} &amp;= X^{(l)}*W\\&amp;=\begin{bmatrix}w_{11} x^{(l)}_{11} + w_{12} x^{(l)}_{12} + w_{13} x^{(l)}_{13} + w_{21} x^{(l)}_{21} + w_{22} x^{(l)}_{22} + w_{23} x^{(l)}_{23} + w_{31} x^{(l)}_{31} + w_{32} x^{(l)}_{32} + w_{33} x^{(l)}_{33} &amp; w_{11} x^{(l)}_{12} + w_{12} x^{(l)}_{13} + w_{13} x^{(l)}_{14} + w_{21} x^{(l)}_{22} + w_{22} x^{(l)}_{23} + w_{23} x^{(l)}_{24} + w_{31} x^{(l)}_{32} + w_{32} x^{(l)}_{33} + w_{33} x^{(l)}_{34}\\ w_{11} x^{(l)}_{21} + w_{12} x^{(l)}_{22} + w_{13} x^{(l)}_{23} + w_{21} x^{(l)}_{31} + w_{22} x^{(l)}_{32} + w_{23} x^{(l)}_{33} + w_{31} x^{(l)}_{41} + w_{32} x^{(l)}_{42} + w_{33} x^{(l)}_{43} &amp; w_{11} x^{(l)}_{22} + w_{12} x^{(l)}_{23} + w_{13} x^{(l)}_{24} + w_{21} x^{(l)}_{32} + w_{22} x^{(l)}_{33} + w_{23} x^{(l)}_{34} + w_{31} x^{(l)}_{42} + w_{32} x^{(l)}_{43} + w_{33} x^{(l)}_{44}\end{bmatrix} \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;파이토치에서 Convolution Layer 는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv2d&lt;/code&gt; 로 구현되어 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;link: &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#conv2d&quot;&gt;torch.nn.Conv2d - PyTorch master documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;deconvolution-layer-transposed-convolution-layer&quot;&gt;Deconvolution Layer? Transposed Convolution Layer!&lt;/h1&gt;

&lt;p&gt;저자는 이미 2011 년도에 Deconvolution Layer 를 제안했다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;link: &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.849.3679&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Adaptive Deconvolutional Networks for Mid and High Level Feature Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;간단하게 생각해보면 다음 그림과 같이 필터를 이동시키면서 원래 4x4 이미지(초록색)를 복원하면 될것 같다. 이 과정이 맞는지 이후에 살펴볼 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1R-C4g1zSpculTzC8w00IrM9CNM0vifN_&quot; /&gt;&lt;/p&gt;

&lt;p&gt;흥미로운 것은 파이토치에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;ConvTranspose2d&lt;/code&gt; 라고 구현이 되어 있다. 그리고 다음과 같은 설명이 덧붙여져 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This module can be seen as the gradient of &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv2d&lt;/code&gt; with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;link: &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#convtranspose2d&quot;&gt;torch.nn.ConvTranspose2d - PyTorch master documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;왜 이름이 Deconvolution 이 아닐까? 읽어보면 이 연산은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv2d&lt;/code&gt;의 출력을 입력에 대해 미분을 연산하는 것과 같다고 한다. 미분을 한번 구해보고 이를 C 라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} C = \dfrac{\partial X^{(l+1)}}{\partial X^{(l)}}  &amp;= \dfrac{\partial Vec(X^{(l+1)})}{\partial Vec(X^{(l)})} \\ &amp;= \begin{bmatrix}  \dfrac{\partial x_{11}^{(l+1)}}{\partial x_{11}^{(l)}} &amp; \dfrac{\partial x_{12}^{(l+1)}}{\partial x_{11}^{(l)}} &amp; \dfrac{\partial x_{13}^{(l+1)}}{\partial x_{11}^{(l)}} &amp; \dfrac{\partial x_{14}^{(l+1)}}{\partial x_{11}^{(l)}} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ \dfrac{\partial x_{11}^{(l+1)}}{\partial x_{44}^{(l)}} &amp; \dfrac{\partial x_{12}^{(l+1)}}{\partial x_{44}^{(l)}} &amp; \dfrac{\partial x_{13}^{(l+1)}}{\partial x_{44}^{(l)}} &amp; \dfrac{\partial x_{14}^{(l+1)}}{\partial x_{44}^{(l)}} \end{bmatrix} \\ &amp; = \begin{bmatrix}w_{11} &amp; 0 &amp; 0 &amp; 0\\w_{12} &amp; w_{11} &amp; 0 &amp; 0\\w_{13} &amp; w_{12} &amp; 0 &amp; 0\\0 &amp; w_{13} &amp; 0 &amp; 0\\w_{21} &amp; 0 &amp; w_{11} &amp; 0\\w_{22} &amp; w_{21} &amp; w_{12} &amp; w_{11}\\w_{23} &amp; w_{22} &amp; w_{13} &amp; w_{12}\\0 &amp; w_{23} &amp; 0 &amp; w_{13}\\w_{31} &amp; 0 &amp; w_{21} &amp; 0\\w_{32} &amp; w_{31} &amp; w_{22} &amp; w_{21}\\w_{33} &amp; w_{32} &amp; w_{23} &amp; w_{22}\\0 &amp; w_{33} &amp; 0 &amp; w_{23}\\0 &amp; 0 &amp; w_{31} &amp; 0\\0 &amp; 0 &amp; w_{32} &amp; w_{31}\\0 &amp; 0 &amp; w_{33} &amp; w_{32}\\0 &amp; 0 &amp; 0 &amp; w_{33}\end{bmatrix} \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;이 미분 과정을 파이썬의 &lt;code class=&quot;highlighter-rouge&quot;&gt;sympy&lt;/code&gt; 패키지로 쉽게 만들 수 있다(Jupyter Notebook 에서 사용하길 권장). 다음 코드에서 C 매트릭스를 살펴보면 위와 같은 결과를 얻을 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sympy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatrixSymbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 노트북에서 수학식의 LaTeX 표현 사용
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sympy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_printing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use_latex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mathjax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;convolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply_elementwise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatrixSymbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x^{(l)}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatrixSymbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x^{(l+1)}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatrixSymbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;w&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Convolution Output
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate derivatives &amp;amp; get matrix C
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이 C 행렬은 재밌는 특징을 가진다. 매트릭스 형태의 입력 데이터를 한줄로 핀 후에 매트릭스 연산을 하고, 다시 형태를 변환 시켜주면 Convolution 의 출력값이 나온다. 정말 맞는지 살펴보기 위해서 다음 코드를 실행해보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# forward (1, 16) x (16, 4) = (1, 4) = (2, 2)
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;반대로 출력값을 한줄로 피고 C의 전치행렬과 곱한 후 다시 형태를 변환 시켜주면 입력과 다른 행렬이 나온다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# backward
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;실행하면 다음과 같은 행렬이 나오는데 이 연산 과정을 Deconvolution 연산, 정확히는 &lt;strong&gt;Fractionally-strided convolution&lt;/strong&gt; 혹은 &lt;strong&gt;Transpose convolution&lt;/strong&gt; 이라고 한다. 어떻게 계산된 것이며 어떤 뜻일까?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}w_{11} x^{(l+1)}_{11} &amp; w_{11} x^{(l+1)}_{12} + w_{12} x^{(l+1)}_{11} &amp; w_{12} x^{(l+1)}_{12} + w_{13} x^{(l+1)}_{11} &amp; w_{13} x^{(l+1)}_{12}\\w_{11} x^{(l+1)}_{21} + w_{21} x^{(l+1)}_{11} &amp; w_{11} x^{(l+1)}_{22} + w_{12} x^{(l+1)}_{21} + w_{21} x^{(l+1)}_{12} + w_{22} x^{(l+1)}_{11} &amp; w_{12} x^{(l+1)}_{22} + w_{13} x^{(l+1)}_{21} + w_{22} x^{(l+1)}_{12} + w_{23} x^{(l+1)}_{11} &amp; w_{13} x^{(l+1)}_{22} + w_{23} x^{(l+1)}_{12}\\w_{21} x^{(l+1)}_{21} + w_{31} x^{(l+1)}_{11} &amp; w_{21} x^{(l+1)}_{22} + w_{22} x^{(l+1)}_{21} + w_{31} x^{(l+1)}_{12} + w_{32} x^{(l+1)}_{11} &amp; w_{22} x^{(l+1)}_{22} + w_{23} x^{(l+1)}_{21} + w_{32} x^{(l+1)}_{12} + w_{33} x^{(l+1)}_{11} &amp; w_{23} x^{(l+1)}_{22} + w_{33} x^{(l+1)}_{12}\\w_{31} x^{(l+1)}_{21} &amp; w_{31} x^{(l+1)}_{22} + w_{32} x^{(l+1)}_{21} &amp; w_{32} x^{(l+1)}_{22} + w_{33} x^{(l+1)}_{21} &amp; w_{33} x^{(l+1)}_{22}\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;출력을 계산하는 Convolution 연산 과정에서 &lt;strong&gt;“입력 픽셀”&lt;/strong&gt; 에서 &lt;strong&gt;“출력 픽셀”&lt;/strong&gt; 과 연결된 가중치를 생각하면 편하다. 다음 그림을 살펴보면, 필터(노란색)가 지나가면서, &lt;strong&gt;“입력 픽셀”&lt;/strong&gt; ($x_{12}^{(l)}$)과 &lt;strong&gt;“출력 픽셀”&lt;/strong&gt; ($x_{11}^{(l+1)}$, $x_{12}^{(l+1)}$)사이에 연결된 두 개의 가중치 ($w_{11}$, $w_{12}$)를 통해 연산이 된다. 위 행렬에서 1행 2열에 있는 원소 값과 연관이 있는 것을 확인 할 수 있는데, fractionally-strided convolution 연산은 바로 &lt;strong&gt;“출력 픽셀”&lt;/strong&gt; 에서 &lt;strong&gt;“입력 픽셀”&lt;/strong&gt; 로 방향을 바꿔 연산하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1acZ6YvrW6xooXJd6nYpFYhm-eDHSe2f1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fractionally-strided convolution 연산은 다음 그림과 같다. “fractionally” 의 단어 뜻 처럼 필터가 출력 이미지의 일부분을 걸치면서 이동(stride)하면서 연산된다. 또한 가중치도 기존의 형태와 달리 약간의 변형(transpose)이 된다(정확한 전치행렬은 아니다). 그렇다면 “출력 픽셀”과 연관이 없는 부분은? 0으로 곱해져서 더해진다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1WumIP2aCDNJ4cCWQW_2_Q0e1LCx_WkdQ&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서 다시 정리하면 Convolution Layer의 출력을 입력에 대한 미분을 구해서(C 행렬), 이를 한줄로 편 출력과 곱한 후에 형태를 입력 이미지로 변환해주는 것이 Convolution 의 반대 연산인 Fractionally-strided convolution 이다. 수식으로 다음과 같이 정리 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X^{(l)} = [Vec\big(X^{(l+1)}\big)C^T]^{(N)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$^{(N)}$은 Vector Transpose이며, 이는 다음 노트북을 살펴보자. &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/04_Backpropagation_Matrix_diff.ipynb&quot;&gt;Vector Transpose&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;additional-reference&quot;&gt;Additional Reference&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.07285&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 21:56:38 +0900</pubDate>
        <link>https://simonjisu.github.io/datascience/2019/10/27/convtranspose2d.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/datascience/2019/10/27/convtranspose2d.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>[XAI] Explaining Explanations: An Overview of Interpretability of Machine Learning</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1806.00069&quot;&gt;Explaining Explanations: An Overview of Interpretability of Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;설명가능한(explainable)&lt;/strong&gt; 모델은 &lt;strong&gt;해석(interpretable)&lt;/strong&gt; 이 가능하지만 그 반대는 아니다.&lt;/li&gt;
  &lt;li&gt;설명가능한 시스템(explanatory system)는 머신러닝, 인간과 컴퓨터 상호 작용(Human-computer interaction), 크라우드 소싱, 교육분야, AI 윤리, 기술정책 등 다양한 분야에서 연구되고 적용되고 있다.&lt;/li&gt;
  &lt;li&gt;이 논문에서는 explainable AI 시스템의 전반적인 정의와 분류법을 소개한다.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Section 2&lt;/code&gt;: explanation, interpretability, explainability 에 대해서 정의한다.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Section 3&lt;/code&gt;: 고전적인 AI 접근 방법(causal modeling, constraint reasoning, intelligent user interfaces, planning)을 복습하지만 설명 가능한 딥러닝 모델에 집중할 것이다.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Section 4&lt;/code&gt;: explanation, interpretability, explainability 의 중요한 차이점에 대해서 이야기 한다.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Section 5&lt;/code&gt;: 이 설명들을 통해 무엇이 설명되고 있는가(what is explained by these explanations)에 대한 새로운 분류법을 제시한다. (해석이 잘 안됨…)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2-background-and-foundational-concepts&quot;&gt;2. Background and Foundational Concepts&lt;/h1&gt;

&lt;h2 id=&quot;a-what-is-an-explanation&quot;&gt;A. What is an Explanation?&lt;/h2&gt;

&lt;p&gt;철학적인 문헌에서 “설명을 구성하는 것은 무엇인가”에 대해 많은 논쟁이 있었다. 그중 관심있는 부분은 “어떤 설명이 좋은 설명인가” 혹은 “설명의 진짜 정의는 무엇인가”다. 어떤 사람들은 좋은 설명은 질문에 달려있다고 말한다. 많은 논문이 설명의 기원, 이론 그리고 언어의 토대를 다루고 있다. 그러나 우리의 연구에서 대부분 중요하고 관심있었던 작업은 “왜”라는 질문(Why Questions)이다. 특별히 “어떤 알고리즘에서 알고싶은 것이 무언인가”를 “왜”라는 질문으로 했을 때, 더 이상 질문을 할 수 없을 때까지 질문에 답변을 받을 수 있는 특정 시기를 정량적으로 표현할 수 있다. “왜”라는 질문에는 두 의미가 있는데, “왜(why)” 그리고 “왜 그래야하는가(why should)”이다. 다른 Explainable planning 문헌과 마찬가지로, 철학자들은 “왜 그렇게 하지 말아야 하는지(why-shouldn’t)” 질문과 “왜 그렇게 해야 하는지(why-should)” 질문이 실제로 우리가 원하는 “설명성 요건(explainability requirements)”을 제공할 수 있지에 대해 궁금해 한다.&lt;/p&gt;

&lt;p&gt;어떤 설명이 가장 좋은 설명인지도 철학자들 사이에서 많은 논쟁이 있다. 많은 사람이 말하기를 그것은 추론(inference)이라고 하는데, 학자들 사이에서 많은 논쟁이 있다. 많은 사람이 말하기를 그것은 추론(inference)이라고 하는데, 귀추법(abductive reasoning)을 사용해서 모든 가능성있는 결과에 대해 설명하는 것과 비슷한 관점이다.&lt;/p&gt;

&lt;h2 id=&quot;b-interpretability-vs-completeness&quot;&gt;B. Interpretability vs. Completeness&lt;/h2&gt;

&lt;p&gt;설명(explanation)은 두 가지 방식으로 평가될 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;해석가능성(Interpretability)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;해석가능성의 목표는 내적 시스템이 사람이 알 수 있는 방법으로 구성되는 것이다. 이 목표의 키 요소는 인지, 지식, 인간의 편향이다. 해석가능한 시스템은 사람이 유저에게 이해가능한 쉬운 언어로 구성되고 의미있는 설명(descriptions)을 제공해야한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;완전무결성(completeness)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;완전무결성의 목표는 시스템의 동작을 정확하게 묘사하는 것이다. 설명(explanation)은 시스템의 행동이 더 많은 상황에서 예측 가능해야 완전성이 생긴다. 딥 뉴럴 네트워크를 장착한 컴퓨터 프로그램을 설명할 때, 시스템에서 모든 수학적 연산과 파라미터를 볼 수 있어야 가장 완벽하고 완전한 설명이라고 할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;설명가능한 AI 가 직면한 문제는 완전무결성과 해석가능성을 동시에 만족하는 설명을 만드는 것이 어렵다는 것이다. 정확한 설명은 사람에게 쉽게 해석가능하지 않고, 반대로 해석가능한 설명은 가끔 정확한 예측 파워를 내지 못한다.&lt;/p&gt;

&lt;p&gt;Herman은 단순히 인간의 기준으로 해석가능한 시스템을 평가하는 것을 경계해야한다고 했다. 그 이유는 사람의 평가는 설명을 조금더 간단하게 만들려는 강력하고 구체적인 편향을 불러일으키기 때문이다. 그는 사람 평가에 의존하면, 연구자들이 투명한 시스템 보다는 설득력 있는 시스템을 만들려고 할 것이라 우려했다. 그는 해석가능한 시스템을 만들때 다음과 같은 윤리적인 딜레마를 제기했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;비윤리적으로 유저에게 더 좋은 설득력을 보이기위해서 설명을  조작하는 경우는 언제인가?&lt;/li&gt;
  &lt;li&gt;“투명성 및 윤리”와 “해석가능성에 대한 욕망” 사이의 밸런스를 어떻게 조절할 것인가?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;단순화된 설명의 한계를 유저가 이해하지 못함을 이용해 복잡한 시스템에 단순화된 설명을 제시함으로써 신뢰를 얻으려는 행위은 비윤리적이라고 생각하며, 바람직하지 않은 속성을 숨기도록 그 설명이 최적화되어 있다면 더 나쁘다고 보고 있다. 이러한 설명은 본질적으로 오해의 소지가 있으며,유저에게 위험하고 근거없는 결론을 내리는 결과를 초래할 수 있다.&lt;/p&gt;

&lt;p&gt;이러한 함정을 피하기 위해서 설명(explanations)은 해석가능성과 완전무결함 사이의 트레이드 오프를 허용해야 한다. 단순한 묘사(descriptions)만을 제공하는 것보다 약간의 해석가능성을 대가로 시스템이 자세하고 완전무결한 묘사를 할수 있게 해야한다. 설명 방법은 트레이드 오프의 하나의 점으로 평가될 것이 아니라, 최대 해석가능성부터 최대 완전무결함의 곡선상에서 어떻게 변화하는 가를 봐야한다.&lt;/p&gt;

&lt;h2 id=&quot;c-explainability-of-deep-networks&quot;&gt;C. Explainability of Deep Networks&lt;/h2&gt;

&lt;p&gt;딥 네트워크의 연산에 대한 설명은 네트워크의 데이터 &lt;strong&gt;처리(processing)&lt;/strong&gt; 를 설명하거나, 네트워크 내부에서 데이터의 &lt;strong&gt;표상(representation)&lt;/strong&gt; 을 설명하는 것이다. 처리(processing)에 관한 설명은 “왜 해당 입력이 특정 출력으로 이어지는가?”을 대답하는 것이고 프로그램의 실행 추적을 설명하는 것과 같다. 표상(representation)에 관한 설명은 “네트워크가 어떤 정보를 포함하고 있는가?”를 대답하는 것이고, 프로그램의 내부 데이터 구조를 설명하는 것과 같다.&lt;/p&gt;

&lt;p&gt;해석가능성에 대한 세번째 접근방법은 자신의 행동에 대한 해석을 단순화하도록 설계된 구조를 가진 &lt;strong&gt;설명-생산(explanation-producing) 시스템&lt;/strong&gt; 을 만드는 것이다. 이런 구조는 데이터의 처리, 표상 혹은 사람이 시스템을 쉽게 이해하는 관점에서 설계될 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-review&quot;&gt;3. Review&lt;/h1&gt;

&lt;p&gt;급격한 하위 분야의 확장 그리고 불투명한(opaque) 시스템의 정책 및 법적 파장 때문에, 해석가능성에 대한 연구가 급격이 늘어나고 있다. 모든 분야의 논문을 다루기 어렵기 때문에, 심층 신경망 아키텍처에 대해 집중하고, 다른 분야의 논문를 간단히 강조하려한다.&lt;/p&gt;

&lt;h2 id=&quot;a-explanations-of-deep-network-processing&quot;&gt;A. Explanations of Deep Network Processing&lt;/h2&gt;

&lt;p&gt;보통 심층 네트워크에서는 큰 숫자의 연산을 통해서 결론을 내린다. 예를 들어, 이미지 분류에서 유명한 구조인 ResNet 에서는 이미지 한장을 분류하기 위해 대략 5천만개의 학습된 파라미터와 100억 회의 부동소수점 연산(floating point operations)을 실행한다. 따라서 이러한 프로세스를 설명하기 위해 직면하는 근본적인 문제는 복잡한 연산들을 줄이는 방법을 찾아야한다는 것이다. 이는 기존 모델과 비슷하게 행동하는 &lt;strong&gt;프록시(proxy) 모델&lt;/strong&gt; 을 만들거나, 가장 연관있는 연산을 강조하는 &lt;strong&gt;돌출 맵(salience/saliency map)&lt;/strong&gt; 을 만드는 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;선형 프록시 모델(Linear Proxy Models, LIME)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;프록시 모델 접근 법은 Ribeiro에 의해 잘 설명되어 있다. LIME 을 통해 블랙박스 시스템은 입력의 작은 변화로 인한 행동이 추적되어 설명이 가능하며, 그런 다음 데이터는 전체 모델을 위한 단순 프록시 역할을 하는 지역적 선형 모델(local linear model)을 구성하기 위해 사용된다.  Ribeiro는 다양한 모델 유형과 문제 영역을 거쳐 의사결정에 가장 큰 영향 미치는 입력 영역(regions)을 식별하는데 사용될 수 있음을 보여주고 있다. LIME과 같은 프록시 모델은 예측가능(predictive)하다. 오리지널 시스템에 대한 충성도(faithfulness)에 따라 실행되고 평가될 수 있기 때문이다. 예를 들어, LIME 모델에서 0이 아닌 차원의 개수를 세는 것처럼 모델의 복잡도에 따라서 측정될 수 있다. 프록시 모델은 복잡성(complexity)과 충성도(faithfulness)의 사이를 정량화된 관계를 제공하기 때문에, 방법들은 서로 벤치마킹될 수 있어서 프록시 모델 접근법을 더 매력적으로 만드는 요소다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;의사결정 나무(Decision Trees)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;프록시 모델의 다른 대안 방법으로써 의사결정 나무가 있다. 신경망을 의사결정 나무로 분해하려는 시도는 1990년대부터 최근까지 많은 확장을 거쳤다. 주로 shallow networks에 집중하고, 심층 신경망의 프로세스를 생성하는 것을 목표로 하고있다. CRED 알고리즘의 히든 층을 많이 확장시킨 DeepRED가 그 예시다. DeepRED는 의사결정 나무를 간략화하기 위해 여러 전략을 사용했다. RxREN을 사용해서 불필요한 입력을 다듬고, 트리(a parsimonious decision tree)를 생성하기 위한 통계적 방법인 C4.5 알고리즘을 적용했다. 비록 DeepRED는 원래 네트워크와 비슷한 트리를 생성해냈지만, 그 구조가 꽤나 크고 시행 또한 상당한 시간과 메모리를 차지하기 때문에 확장성이 떨어졌다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://link.springer.com/article/10.1007/s11063-011-9207-8&quot;&gt;Reverse Engineering the Neural Networks for Rule Extraction in Classification Problems&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.01965&quot;&gt;CRED: A Deep Residual Network of Convolutional and Recurrent Units for Earthquake Signal Detection&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.ke.tu-darmstadt.de/publications/papers/DS16DeepRED.pdf&quot;&gt;DeepRED&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://books.google.co.kr/books/about/C4_5.html?id=b3ujBQAAQBAJ&amp;amp;redir_esc=y&quot;&gt;C4.5&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;또 다른 의사결정 나무 방법은 ANN-DT 인데 샘플링 기법을 사용해서 트리를 만든다. 최근접이웃 방법을 사용한 트리 확장 훈련에 샘플링 기법이 사용되는 것이 키 아이디어다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/809084&quot;&gt;ANN-DT: an algorithm for extraction of decision trees from artificial neural networks - IEEE Journals &amp;amp; Magazine&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;자동 규칙 추출(Automatic-Rule Extraction)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;자동 규칙 추출은 의사결정을 요약하기 위한 잘 연구된 또 다른 접근 방법이다. Andrews 는 기존의 규칙 추출 기술을 정리하고 각 규칙에 대한 힘, 반투명성 그리고 퀄리티와 함께 다섯개 차원의 규칙 추출방법에 대한 분류법을 제시했다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/222239090_Survey_and_critique_of_techniques_for_extracting_rules_from_trained_artificial_neural_networks&quot;&gt;Survey_and_critique_of_techniques_for_extracting_rules_from_trained_artificial_neural_networks&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;(하단 원문 생략)&lt;/p&gt;

        &lt;p&gt;Decompositional approaches work on the neuron-level to extract rules to mimic the behavior of individual units. The KT method [28] goes through each neuron, layer-by-layer and applies an if-then rule by finding a threshold. Similar to DeepRED, there is a merging step which creates rules in terms of the inputs rather than the outputs of the preceding layer. This is an exponential approach which is not tangible for deep neural networks. However, a similar approach proposed by Tsukimoto [29] achieves  polynomial-time complexity, and may be more tangible. There has also been work on transforming neural network to fuzzy rules [30], by transforming each neuron into an approximate rule.&lt;/p&gt;

        &lt;p&gt;Pedagogical approaches aim to extract rules by directly mapping inputs to outputs rather than considering the inner workings of a neural network. These treat the network as a black box, and find trends and functions from the inputs to the outputs. Validity Interval Analysis is a type of sensitivity analysis to mimic neural network behavior [31]. This method finds stable intervals, where there is some correlation between the input and the predicted class. Another way to extract rules using sampling methods [32], [33]. Some of these sampling approaches only work on binary input [34] or use genetic algorithms to produce new training examples [35]. Other  approaches aim to reverse engineer the neural network, notably, the RxREN algorithm, which is used in DeepRED[21].&lt;/p&gt;

        &lt;p&gt;Other notable rule-extraction techniques include the MofN algorithm [36], which tries to find rules that explain single neurons by clustering and ignoring insignificant neurons. Similarly, The FERNN [37] algorithm uses the C4.5 algorithm [24]
  and tries to identify the meaningful hidden neurons and inputs to a particular network.
  Although rule-extraction techniques increase the transparency of neural networks, they may not be truly faithful to the model. With that, there are other methods that are focused on creating trust between the user and the model, even if the model is not “sophisicated.”&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;비록 규칙 추출 기술은 신경망의 투명성을 증가시켰으나, 그 규칙이 완전히 모델과 일치하지는 않았다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;돌출 맵(Salience Mapping)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;돌출 맵 접근법은 occlusion procedure 로 예시를 많이 드는데, 네트워크에서 돌출 맵 접근법은 occlusion procedure 로 예시를 많이 드는데, 네트워크에서 입력을 반복적으로 넣으면서 네트워크 출력에 어느 부분이 영향을 줬는지 맵을 만든다. 입력 경사(gradient)를 계산하면서 자연스럽게 돌출 맵을 효율적으로 만들 수 있다. 그러나 이런 미분값들은 중요한 정보를 놓칠 수 있기 때문에, 경사 이외에 다른 정보도 전달할 수 있는 다른 접근 방법도 고려한다. 그 예로써 LRP, DeepLIFT, CAM, Grad-CAM, Integrated gradients, SmoothGrad 를 들수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1807.01696&quot;&gt;Localization Recall Precision (LRP): A New Performance Metric for Object Detection&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.02685&quot;&gt;Learning Important Features Through Propagating Activation Differences&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.04150&quot;&gt;Learning Deep Features for Discriminative Localization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;&gt;Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.01365&quot;&gt;Axiomatic Attribution for Deep Networks&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03825&quot;&gt;SmoothGrad: removing noise by adding noise&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;각 기술은 네트워크에서 뉴런들이 강력하게 활성화 되는 활성화 구역과 출력에 가장 많은 영향을 끼치는 민감도 구역 사이에서 균형을 맞추고 있다. 위 방법들을 비교한 것은 Ancona의 문헌에서 확인 할 수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.06104&quot;&gt;Towards better understanding of gradient-based attribution methods for Deep Neural Networks&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;b-explanations-of-deep-network-representations&quot;&gt;B. Explanations of Deep Network Representations&lt;/h2&gt;

&lt;p&gt;비록 네트워크의 개별 연산숫자는 방대하나, 심층 신경망의 구조는 작은 서브컴포넌트로 잘 정리할 수 있다. ResNet 에서는 100개의 층으로 구성되어 있고 각 픽셀 정보를 64개에서 2048개의 채널로 연산한다. 심층 신경망 표상의 설명 목적은 이런 병목 현상(bottlenecks)을 통해 데이터 흐름의 구조와 역할를 이해하는 것이다. 이는 세밀하게 나눔으로써 달성할 수 있다. 표상(representations)은 &lt;strong&gt;레이어(layer)&lt;/strong&gt;, &lt;strong&gt;유닛(unit)&lt;/strong&gt;, &lt;strong&gt;벡터(vector)&lt;/strong&gt; 로 나눠서 이해할수가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;레이어(Layers)의 역할&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;레이어는 원래 학습된 네트워크의 문제로부터 다른 문제를 해결하는 능력을 테스트함으로써 이해할 수 있다. 예를 들어, Razavian은 ImageNet 데이터를 이용해 학습한 이미지 객체 분류 네트워크의 내부 레이어가 특정 피쳐 벡터(feature vectors)가 다른 어려운 이미지 프로세싱 문제에 재사용할 수 있다는 것을 확인했다. 심지어 심층 벡터(deep representations, 학습된 피쳐벡터를 가르킴)를 SVM과 같은 간단한 모델에 적용하여 전체 네트워크를 훈련 시키지 않고 SOTA(State Of The Art)를 달성했다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1403.6382&quot;&gt;CNN Features off-the-shelf: an Astounding Baseline for Recognition&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;이렇게 네트워크로부터 한 층을 사용하여 새로운 문제를 해결하는 방법을 &lt;strong&gt;전이 학습(transfer learning)&lt;/strong&gt;이라고 한다. 이는 어마어마한 실용적인 시사점을 주는데, 새로운 데이터 세트와 네트워크의 개발없이 다른 새로운 문제들을 해결할 수 있다는 점이다. Yosinksi는 다른 맥락에서 전달 학습 능력을 정량화하기 위한 프레임워크를 설명했다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1411.1792&quot;&gt;How transferable are features in deep neural networks?&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;개별 유닛(Individual Units)의 역할&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;레이어에 포함된 정보는 더 깊에 개별 뉴런과 개별 합성곱 필터로 나눌 수 있다. 개별 유닛의 역할은 개별 유닛의 반응을 최대화 하는 입력 패턴을 시각화(visualization) 함으로써 질적으로 이해할 수 있고, 개별 유닛이 전이 문제를 해결하는 능력을 테스트 함으로써 양적으로도 이해할 수 있다. 시각화는 경사를 이용한 입력 이미지 최적화, 최대 활성화 되는 이미지를 샘플링, 혹은 같은 이미지를 만드는 생성 네트워크를 훈련 시킴으로써 만들 수 있다. 예시로 개별 유닛들이 분할(segmentation) 문제(넓은 범위의 라벨링된 시각화 개념구역을 분할)를 해결하는 능력을 측정하는 &lt;strong&gt;네트워크 해부(network dissection)&lt;/strong&gt; 방법이 있다. 개별 유닛의 새로운 개념을 포착하는 능력을 정량화 함으로써, 네트워크 해부 방법은 네트워크의 개별 유닛이 담긴 정보를 특징지을 수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.05796&quot;&gt;Network Dissection: Quantifying Interpretability of Deep Visual Representations&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;CNN 시각화에서 유닛 표상(unit representations)에 대한 이해를 도울 수 있는 문헌이 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.00614&quot;&gt;Visual Interpretability for Deep Learning: a Survey&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;(하단 원문 생략)&lt;/p&gt;

        &lt;p&gt;A review of explanatory methods focused on understanding unit representations used by visual CNNs can be found in [52], which examines methods for visualization of CNN representations in intermediate network layers, diagnosis of these representations, disentanglement representation units, the creation of explainable models, and semantic middle-to-end learning via human-computer interaction.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;네트워크 가지치기(pruning of networks) 또한 개별 뉴런의 역할을 이해하는 방법 중 하나다. 특별히 큰 네트워크는 최적화에 도움이 되는 초기화(initializations)과 함께 작은 서브네트워크로 구성되어 성공적으로 훈련 시킨다. 해당 문헌은 설명이 용이한 더 작은 네트워크로 같은 문제를 훈련 시킬 수 있는 전략이 존재한다는 것을 증명했다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.03635&quot;&gt;The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;표상된 벡터(Representation Vectors)의 역할&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;개별 유닛을 특징 짓는 것과 유사하게 개별 유닛의 선형 결합(linear combination)으로 구성된 벡터 공간에서 다른 방향들을 특징 짓고 있다. CAVs(Concept Activation Vectors)는 사람이 해석할 수 있는 개념과 일치하는 방향을 식별하고 탐색하는 신경망 표상의 해석을 위한 프레임워크다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.11279&quot;&gt;Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;c-explanation-producing-systems&quot;&gt;C. Explanation-Producing Systems&lt;/h2&gt;

&lt;p&gt;훈련 가능한 네트워크 아키텍처의 일부로 명시적 &lt;strong&gt;어텐션(attention)&lt;/strong&gt;을 포함시키는 방법과 같이, 용이한 설명을 위한 네트워크를 만드는 접근법도 있다. 어텐션은 &lt;strong&gt;분리된 표상(disentangled representations)&lt;/strong&gt;을 학습을 하도록 훈련 시키거나, 직접 &lt;strong&gt;생성가능한 설명(generative explanations)&lt;/strong&gt;을 만들도록 훈련 시킬수도 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;어텐션 네트워크(Attention Networks)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;어텐션 기반의 네트워크는 특정 함수를 학습하는데, 함수는 입력 혹은 내부 피쳐가 네트워크 다른 부분의 정보를 조정하는 것을 볼 수 있게 가중치를 제공한다. 어텐션 기반의 접근법은 비연속적인 순서로 된 문장을 처리하는 기계번역 모델에서 주목할 만한 성공을 거두었다. 그리고 세분화된 이미지(fine-grained image) 분류 분야, VQA(visual question answering) 분야에서도 적용할 수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1411.6447&quot;&gt;The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.03556&quot;&gt;Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;비록 어텐션을 컨트롤 하는 유닛들이 사람이 읽을 수 있는 해석을 만드는 것을 목적으로 훈련하는 것은 아니지만, 어떤 네트워크를 통과하는 정보맵을 확실히 보여준다. 이는 설명의 일종의 형태로 사용될 수 있다는 뜻이다. 사람의 어텐션 데이터 세트가 만들어지기도 했는데 이는 어텐션 시스템이 얼마나 사람의 어텐션과 유사한지 평가할 수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.03556&quot;&gt;Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.08129&quot;&gt;Multimodal Explanations: Justifying Decisions and Pointing to the Evidence&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;어텐션뿐만 아니라 다른 흥미로운 설명을 추출 하는 접근 방법도 있다. 어떤 행동에 부합한 설명을 가지는 네트워크를 만드는 목적으로 어텐션을 명시적으로 훈련 시키는 것이다. Ross 에 의해 제시한 이 기술은 네트워크의 입력에 대한 민감도가 “right for the right reasons”에 부합하는 네트워크를 만들도록 적용 및 측정된다. 이 방법은 네트워크가 내부 추론을 학습할 수 있게 사용될 수 있다. 또한, 이전 사례(instances)에서 발견하지 못한 문제를 새로운 방법으로 스스로 해결하는 일련의 모델 학습에 사용될 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;분리된 표상(Disentangled Representations)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;분리된 표상 속에는 의미있고 다양한 독립적인 요소를 설명하는 개별 차원이 있다. &lt;strong&gt;잠재 요소(latent factor)&lt;/strong&gt; 를 분리하는 문제는 PCA(Principal Component Analysis), ICA(Independent Component Analysis), NMF(Nonnegative Matrix Factorization) 방법으로 해결해왔다. 심층 신경망으로 분리된 표상 학습 시킬수 있다. 그중 하나는 정보이론적 측도(information-theoretic measures)로 입력 확률분포를 네트워크로 학습하는 VAE(Variational Autoencoding)가 있다. Beta-VAE 또한 분리된 요소를 잘 볼수 있는 요소중에 하나다. InfoGAN 은 잠재 요소를 최대한 분리되는 목적으로 학습한다. 순방향 네트워크가 유닛의 분리를 유도하도록 특별한 손실 함수를 제안하고 있다. 이는 CNN에서 개별 유닛들이 해석하기 어려운 패턴의 혼합물 대신 일관성있는 유의미한 패치들을 찾는데 사용될 수 있다. 분리된 유닛으로 네트워크의 추론을 해명할 수 있는 그래프와 의사결정 나무를 만들 수 있다. 캡슐 네트워크(capsule networks)와 같은 아키텍처는 네트워크의 정보를 분리되고 높은 레벨의 개념을 나타내는 조각으로 정리할 수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;Auto-Encoding Variational Bayes&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=Sy2fzU9gl&quot;&gt;beta-VAE: Learning Basic Visual Concepts with a Constrained…&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.00935&quot;&gt;Interpretable Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.04246&quot;&gt;Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.07468&quot;&gt;Unsupervised Learning of Neural Networks to Explain Neural Networks&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.09829&quot;&gt;Dynamic Routing Between Capsules&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;설명 생성(Generated Explanations)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;마지막으로, 심층 네트워크는 시스템의 훈련 일부로 포함해서 인간이 이해가능한 설명을 생성하도록 설계할 수 있다. 설명 생성은 VQA 분야와 세분화된 이미지 분류 문제에서 시스템의 일부로 많이 시연됐다. 두 문제의 시스템에서 “왜냐면(because)”이 들어간 문장을 합성 하면서 자연어로 된 결정(decision)을 설명했다. 해당 설명의 생성기(generator)는 사람이 쓴 설명이 포함된 큰 데이터 셋을 학습했고, 사람이 사용하는 언어로 결정들을 설명했다.&lt;/p&gt;

    &lt;p&gt;시각적 관점과 텍스트 설명이 동시에 포함된 복합적 설명(multimodal explanations)을 생성할 수도 있다. 이 시스템은 2016 VQA 챌린지에서 우승한 모델에 기반해서 일부를 추가하고 단순화 해서 만들었다. QA문제와 내부 어텐션 지도 이외에 시스템은 추가로 설명에 대한 시각적 포인트를 최적화 시킨 두번째 어텐션 맵 그리고 긴 형태의 설명 생성기를 함께 훈련 시켰다. 두 설명 점수 모두 훌륭하게 작동했다. 흥미로운 것은 가독성 있는 설명의 생성은 네트워크의 출력 결과에 의존한다는 것이다. 즉, 네트워크가 이미 결정을 내린 후에 설명이 생성된다는 점이다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.08129&quot;&gt;Multimodal Explanations: Justifying Decisions and Pointing to the Evidence&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.01847&quot;&gt;Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;4-related-work&quot;&gt;4. Related Work&lt;/h1&gt;

&lt;h2 id=&quot;a-interpretability&quot;&gt;A. Interpretability&lt;/h2&gt;

&lt;p&gt;이전에 해석가능성에 대해서 분류법과 모범 사례를 정의해보려는 시도가 있었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.08608&quot;&gt;Towards A Rigorous Science of Interpretable Machine Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;해당 논문의 동기도 본 논문과 비슷하게 해석가능성 분야의 수요가 급격하게 커졌기 때문이다. 그리고 해석가능성에 대한 명확한 정의와 평가기준이 없었다. 저자는 해석가능성을 “사람에게 이해가능한 형태로 설명하는(표현하는) 능력” 으로 정의하고, 다양한 설명가능성에 대한 정의를 제안했다. 그리고 “해석은 설명의 평가를 발견하는 행동이다”라는 개념에 수렴하게 된다. 저자들은 일종의 해석가능한 머신러닝의 정의와 이를 측정할 방법에 합의했다.  해당 논문에 영감을 받아서 해석가능성보다는 설명가능성 관점에서 분류법을 다뤄보기로 한다.&lt;/p&gt;

&lt;p&gt;이 논문의 주요 공헌은 해석가능성 평가를 위한 모드의 분류법(a taxonomy of modes)을 제안한 것이다. &lt;strong&gt;응용 프로그램 기반(application-grounded)&lt;/strong&gt;, &lt;strong&gt;사람 기반(human-grounded)&lt;/strong&gt; 그리고 &lt;strong&gt;기능 기반(functionally grounded)&lt;/strong&gt; 이다. 저자들은 불완전한 문제제기 혹은 최적화 과정에서 평가가 제외 됐을 때 해석가능성이 요구된다고 말하고있다. 그들의 문제제기는 유저와 최적화 문제 사이를 단절 시키는 모델의 불완전성이기 때문에 평가의 접근법들이 중요하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;응용 프로그램 기반(application-grounded)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;첫 번째 접근법인 응용 프로그램 기반 방법은 실제 작업과 실제 사람을 포함하는 것이다. 이 평가는 인간이 만들어낸 설명이 특정 작업에서 다른 인간에게 얼마나 도움이 될 수 있는지를 측정한다. 의사가 진단 시스템을 평가하는 경우를 예시로 들 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;사람 기반(human-grounded)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;두 번째 접근법인 사람 기반 방법은 간단한 문제에 적용되는 사람의 평가 방식을 사용하는 것이다. 이 방법은 응용 프로그램을 테스트할 타겟 커뮤니티를 찾기 어려울 때 주로 사용한다. 또는, 특별한 최종 목표가 실현되기 어려운 경우, 예를 들어 안전 이슈가 중요한 문제에서 오류를 찾아내는 것 등 문제에서도 사용될 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;기능 기반(functionally grounded)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;마지막 방법인 기능 기반 방법은 인간의 주체없이 평가된다. 이 실험 설정에서는 해석가능성에 대한 어떤 공식적인 정의를 증명하기 위해 프록시 또는 단순화된 작업을 사용한다. 저자들은 프록시의 선택이 이 접근법에서 내재된 도전이라는 것을 인정한다. 해석 가능한 모델을 선택하는 것과 모델 행동을 더 잘 표현할 수 있는 덜 해석가능한 프록시 방법을 선택하는 것 사이에는 미묘한 트레이드-오프가 있다. 저자들은 이 점을 인정하고 의사결정 나무를 좋은 해석 가능한 모델로 간략하게 언급한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그런 다음 저자들은 해석가능성 연구에서 개방된 문제, 모범 사례 및 향후 작업에 대해 논의하고, 해석가능성의 발견을 위한 데이터 중심 접근법을 크게 권장했다. 해석가능성의 정의에 대한 이러한 공헌이 있음에도 불구하고, 우리는 모델이 제공하는 설명에 각기 다른 초점을 맞춰 정의하고, 각 설명들이 어떻게 평가되어야 하는지를 포함하여 우리만의 분류법을 구별했다.&lt;/p&gt;

&lt;h2 id=&quot;b-explainable-ai-for-hci&quot;&gt;B. Explainable AI for HCI&lt;/h2&gt;

&lt;p&gt;이전에 설명가능한 AI에 관련된 어떤 논문은 설명가능한 시스템에 관해 상당한 데이터 중심 문헌 분석을 수행했다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.brianlim.net/wordpress/wp-content/uploads/2018/01/chi2018-intelligibility%20final.pdf&quot;&gt;chi2018-intelligibility&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 논문에서 저자들은 기존의 AI 해석가능성 주장을 뛰어넘어, 대신 실제 사용자들에게 효과적이고 실용적인 시스템을 만드는 방법에 초점을 맞췄다. 저자들은 AI 시스템은 “디자인에 따라서 설명가능하다.”라고 주장하면서 세 가지 기여로 이를 제시하고 있다. 설명가능한 AI 연구와 관련된 289 개의 핵심 논문과 12,412 개의 인용 논문에 대한 데이터 중심 네트워크 분석(data-driven network analysis), 네트워크 분석을 이용한 동향 관찰, 그리고 설명가능성과 관련된 HCI 연구의 모범 사례 및 향후 연구에 대한 제안이 그 3 가지다.&lt;/p&gt;

&lt;p&gt;논문은 대부분 문헌 분석에 중점을 두기 때문에, 저자들은 관련 연구분야에서 설명가능한 인공지능(XAI), HCI의 명료성(intelligibility) 및 해석가능성(interpretability), 연구주제의 동향 분석 방법 등 크게 세 가지 분야만 부각시킨다.&lt;/p&gt;

&lt;p&gt;이 논문의 주요 기여는 저자들이 만든 인용 네트워크가 포함된 설명가능한 연구의 상당한 문헌 분석이다. “intelligible”, “interpretable”, “transparency”, “glass box”, “black box”, “scrutable”, “counterfacutals” 그리고 “explainable” 등 키워드의 변형을 문헌에서 검색 및 취합하고, 289 개의 핵심 논문과 12,412 개의 인용 논문을 정리했다. 네트워크 분석을 통해 저자들은 28개의 중요한 클러스터와 9개의 연구 커뮤니티를 발견했다. 이와는 대조적으로, 우리의 연구는 해석가능한 머신러닝과 설명 할 수 있는 딥러닝을 이용한 분류기 연구에 초점을 맞추고 있다.&lt;/p&gt;

&lt;p&gt;같은 코어 논문과 인용 논문의 요약(abstracts) 데이터를 사용하여 LDA 기반의 토픽모델링을 시행한 결과, 저자들은 가장 크고, 중심적이며, 잘 연구된 네트워크가 지능과 주변 시스템(intelligence and ambient systems)이라는 것을 발견했다. 우리의 연구에서 가장 중요한 서브 네트워크는 설명가능한 AI 다. FAT(Fair, Accountable and Transparent) 알고리즘과 iML(Interpretable Machine Learning) 서브 네트워크 그리고 설명 서브 네트워크에 관한 이론들이 이에 해당한다.&lt;/p&gt;

&lt;p&gt;특히, 저자들은 FATML과 해석가능성의 차이점을 이야기하고 있다. FATML은 사회적 이슈에 초점을 맞추고 있는 반면, 해석가능성은 방법에 초점을 맞춘다. 설명의 이론은 인과 관계(causality)와 인지 심리학(cognitive psychology)을 반사실적 추론(conterfactual reasoning)과 인과 설명(causal explanations)의 공통 부분으로 결합시킨다. 이 두 가지는 우리의 분류 분석에서 중요한 요소들이다.&lt;/p&gt;

&lt;p&gt;우리는 이들의 논문 마지막에서 언급한 두 가지 동향이 흥미로웠다(머신러닝 생산 규칙(ML production rules)과 엄격하고 사용 가능한 지식에 대한 로드맵). 저자들은 해석가능성이 적용된 고전적인 AI 방법들의 부족을 지적하며, 현재의 연구에서는 그러한 방법들의 광범위한 적용을 장려한다. 본 논문은 주로 설명가능성에서 HCI 연구 어젠다를 정하는 데 초점을 맞췄지만, 우리 연구와 관련된 많은 논점을 제기한다. 특히, 문헌 분석은 심리학 및 사회과학에서 하위 주제와 하위 학문을 발견했는데, 아직 우리의 분석과 관련이 있는 것으로 확인되지는 않았다.&lt;/p&gt;

&lt;h2 id=&quot;c-explanations-for-black-box-models&quot;&gt;C. Explanations for Black-Box Models&lt;/h2&gt;

&lt;p&gt;최근 블랙박스(black-box) 모델을 설명하는 조사에서는 이해하기 힘든(opaque) 알고리즘을 사용한 주요 문제의 분류를 제공하기 위한 분류법을 소개했다. 조사된 대부분의 방법은 신경망을 기반으로한 알고리즘 이기 때문에 우리의 연구와 연관이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.01933&quot;&gt;A Survey Of Methods For Explaining Black Box Models&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;저자들은 이해하기 힘든 머신러닝 모델에 기초한 의사결정 시스템을 설명하는 방법에 대해 개요를 제공한다. 그들의 분류법은 세세하고, 설명 접근법에 근거해 작은 구성요로 구별된다. 그들은 각 설명 방법에 대해 네 가지 특징을 이야기 했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;당면한 문제의 종류&lt;/li&gt;
  &lt;li&gt;블랙박스를 여는 데 사용되는 설명 능력(The explanatory capability used to open the black box)&lt;/li&gt;
  &lt;li&gt;설명되는 블랙박스 모델의 유형&lt;/li&gt;
  &lt;li&gt;블랙박스 모델에 전달된 입력 데이터의 종류&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그들은 주로 직면하는 문제의 유형에 따라 설명 방법을 나누고, 다음과 같은 네 가지 설명 방법 그룹을 식별한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;블랙박스 모델 설명 방법&lt;/li&gt;
  &lt;li&gt;블랙박스 결과 설명 방법&lt;/li&gt;
  &lt;li&gt;블랙박스 검사 방법&lt;/li&gt;
  &lt;li&gt;투명박스 설계 방법&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그들의 분류 특징과 문제 정의를 사용하여, 그들은 채택된 설명 기능의 유형, 블랙 박스 모델 “개방여부” 및 입력 데이터에 따라 방법을 토론하고 추가로 분류했다. 이들의 목표는 주요 블랙박스 설명 아키텍처를 검토하고 분류하는 것이다. 따라서 이들의 분류는 유사한 문제와 접근방식을 식별하는 지침이 될 수 있다. 우리는 이 연구가 설명 방법의 설계 공간을 탐색하는 데 유용하고 의미있는 기여를 하고 있다는 것을 발견했다. 우리의 분류는 덜 세분화되어 있다. 구현 기법을 세분화하기 보다는 설명 능력의 초점과 각 접근방식이 설명할 수 있는 것을 조사하며, 다양한 유형의 설명가능성(explainability) 방법이 어떻게 평가될 수 있는지를 이해하는 데 중점을 둔다.&lt;/p&gt;

&lt;h2 id=&quot;d-explainability-in-other-domains&quot;&gt;D. Explainability in Other Domains&lt;/h2&gt;

&lt;p&gt;설명가능한 계획법(Explainable Planning)은 planning 커뮤니티에 있던 모델 기반 표상을 활용하는 새로운 학문이다. 수년 전 몇가지 핵심 아이디어가 계획 인식 분야에서 제안됐다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.10256&quot;&gt;Explainable Planning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/adc7/13f0787f9fdef701d63615acd4aad61165a6.pdf&quot;&gt;Generalized plan recognition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;설명가능한 계획법은 계획 알고리즘과 인간의 문제-해결력 사이의 차이를 인정하면서, 사용자와의 의사소통을 위한 친숙하고 일반적인 기반을 촉구한다. 이 논문에서, 저자들은 설명이 대답할 수 있는 다양한 유형의 질문의 개요와 예를 제공한다. 예를 들어, “ 왜 A 행동을 했니?” 혹은 “왜 B 행동을 하지 않았니(DIDN’T)?”, “왜 C 행동을 하지 못했니(CAN’T)?” 등등. 또한 저자들은 계획을 자연어로 표현하는 것은 계획을 설명하는 것과 같지 않다고 강조한다. 설명 요청은 “질문자가 시스템에서 사용할 수 있어야 하며 질문자가 가지지 않았다고 믿는 지식의 일부를 밝혀내기 위한 시도”이다. 이 질문은 결론에서 이야기한다.&lt;/p&gt;

&lt;p&gt;설명 자동 생성(Automatic explanation generation)은 스토리를 이야기 할 수 있는 컴퓨터와 기계와 밀접해 있다. John Reeves의 논문에 따르면, 그는 이야기를 읽고, 특징을 요약하고, 믿음을 추론하며 그리고 갈등과 해결을 이해하기 위해 THUNDER 프로그램을 만들었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Computational-morality%3A-a-process-model-of-belief-Reeves-Dyer/9a25a7794fc25ab039bf6d0662955b3ad84f2e49&quot;&gt;Computational morality: a process model of belief conflict and resolution for story understanding - Semantic Scholar&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다른 연구에서는 스토리 이해를 하는 데 필요한 구조를 나타내는 방법을 검토한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/7763/bd20e69a9b9ef7994adf9aae94aca6ffad0f.pdf&quot;&gt;Story understanding - E. T. Mueller&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Genesis Story-Understanding 시스템은 상위 개념 패턴(higher-level concept patterns)과 상식 규칙(commonsense rules)을 이용하여 스토리를 이해하고, 사용하고, 구성하는 작업 시스템이다. 설명 규칙은 누락된 원인 또는 논리적 연결을 제공하는 데 사용된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://courses.csail.mit.edu/6.803/pdf/manifesto.pdf&quot;&gt;The genesis manifesto: Story understanding and human intelligence&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;인간-로봇 상호작용과 스토리텔링의 교차점에는 언어화(verbalization)가 있다. 즉, 인간과 로봇의 상호작용에 대한 설명을 생성한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.ijcai.org/Proceedings/16/Papers/127.pdf&quot;&gt;Verbalization: Narration of autonomous robot experience&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;유사한 접근방식은 사례 기반 모델 또는 설명 일관성을 사용하는 유괴 추론에서 찾을 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.ijcai.org/Proceedings/93-1/Papers/004.pdf&quot;&gt;Focusing construction and selection of abductive hypotheses&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/f391/61c2bc142d4258dc76d329f47f17f6aeddcf.pdf&quot;&gt;The role of coherence in constructing and evaluating abductive explanations&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;설명 자동 생성은 새로운 사상을 상상하거나 통계적 접근법을 통한 지식의 격차를 메우는 방법으로 뇌와 인지과학에서도 잘 연구된 분야다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0885201414000744&quot;&gt;Imagination and the generation of new ideas&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4041537/&quot;&gt;Theory of Mind: A Neural Prediction Problem&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;5-taxonomy&quot;&gt;5. Taxonomy&lt;/h1&gt;

&lt;p&gt;3가지 다른 카테고리로 접근법을 나눌 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;비록 의사결정 프로세스를 대표 하지는 않지만, 어떤 논문은 방출된 선택에도 적용할 수 있는 정당성의 척도(degree of justification)를 제공하는 설명을 제안한다. 이는 사람이 믿을 만한 정확하고 합리적 시스템의 구축을 위한 설명 요구에 대한 응답(근거)로 사용될 수 있다. 이런 시스템은 보통 데이터의 처리를 모방(emulate)하여 시스템의 입력과 출력 사이의 연결관계 이끌어 낸다.&lt;/li&gt;
  &lt;li&gt;설명의 두 번째 목적은 네트워크 내부의 데이터 표현을 설명하는 것이다. 이것들은 네트워크의 내부 작동에 대한 통찰력을 제공하며, 네트워크 내에서 활성화된 데이터에 대한 설명이나 해석을 용이하게 하는데 사용될 수 있다. 이는 프로그램의 내부 구조를 설명하는 것과 달리, 특정 중간 표현(intermediate representations)이 왜 특정 선택으로 이어졌는 지에 대한 정보를 얻을 수 있다.&lt;/li&gt;
  &lt;li&gt;마지막 설명의 유형은 설명-생성 네트워크다. 이런 네트워크는 스스로 설명을 할 수 있게 구축하며, 불투명한 서브시스템의 해석을 단순화하도록 설계되었다. 처리 및 표현, 혹은 다른 부분이 정당하고 이해하기 쉬운 부분은 서브시스템의 투명성을 높이는 방법은 두 가지 단계다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;우리가 제시하는 분류법은 광범위한 기존 접근법 집합이 주어졌을 때, 머신러닝 시스템의 해석가능성 및 완전무결성의 다양한 척도를 달성하려고 할 때 유용하다. 그러나 동일한 문제를 해결한다고 주장하는 두 가지 뚜렷한 방법이 실제로 매우 다른 질문에 답하고 있을 수 있다. 우리의 분류법은 기존 접근법에 근거하여 문제 공간을 세분화하여 기존 분류보다 더 세밀하게 하려고 한다.&lt;/p&gt;

&lt;p&gt;우리는 다음 표에 리뷰한 방법들에 대한 카테고리를 분류했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1dyldd53JbfefIvlrNH34qqVMgWRdRK95&quot; /&gt;&lt;/p&gt;

&lt;p&gt;주목할 점은 처리(processing)과 해설을 만들어내는(explanation-producing) 역할이 표현(representation)보다 더 많다는 것이다. 우리는 이러한 차이가 “표현에 기반한 모델(representation-based models)은 평가하기 어렵다”라는 사실에 기인한다고 믿는다. 유저-스터디 기반 평가가 항상 적절한 것은 아니다. 특정 표현을 추가하고 제거하는 방법으로 더 좋은 퍼포먼스를 측정하는 수치적 방법은 사용하기 힘들다.&lt;/p&gt;

&lt;p&gt;우리의 분류법의 역할은 여러 카테고리를 걸쳐 연구와 평가를 촉진하는 것이다. 해석의 목적[74]과 유저와의 연결[75]을 평가하는 다른 설명적, 해석적 분류법 대신에, 우리는 집중적으로 방법에 대한 평가를 하려고 한다. 이 방법이 네트워크의 데이터 처리를 설명하는지, 네트워크 내부의 데이터 표현을 설명하는지, 혹은 그 방법을 스스로 설명하는 아키텍쳐인지를 볼 것이다. 이를 통해 그 방법에 대한 추가적인 메타 예측과 통찰력를 얻으려고 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.08608&quot;&gt;74-Towards A Rigorous Science of Interpretable Machine Learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.brianlim.net/wordpress/wp-content/uploads/2018/01/chi2018-intelligibility%20final.pdf&quot;&gt;75-chi2018-intelligibility&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우리는 하위범주를 생성하는 설명 분류법을 신경망 아키텍쳐와 시스템을 디자인하는 하나의 방법으로써 제시하려고한다. 또한 표준화된 평가 지표의 부족을 강조하고, 향후 분류학의 영역 교차 연구를 제안하려고 한다.&lt;/p&gt;

&lt;h1 id=&quot;6-evaluation&quot;&gt;6. Evaluation&lt;/h1&gt;

&lt;p&gt;비록 심층 신경망을 위한 설명을 3 가지로 나눴지만, 셋다 같은 평가기준을 가지지 않는다. 조사된 작업의 대부분은 다음 유형의 설명 중 하나라고 할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;원래 모델과의 완전무결성 비교. 프록시 모델은 설명된 원래 모델에 얼마나 근접했는지에 따러 직접 평가할 수 있다.&lt;/li&gt;
  &lt;li&gt;대체 문제(task)에서 측정했을 때의 완전무결성. 어떤 설명은 모델의 의사결정을 직접적으로 설명하지 않는다. 하지만 다른 어떤 속성을 대신 측정할 수 있다. 예를 들어 모델 민감도는 무차별 측정 보다는 모델의 민감도를 잘 보여주는 돌출 설명(salience explanation)으로 평가할 수 있다.&lt;/li&gt;
  &lt;li&gt;편향(bias)이 있는 모델을 탐지하는 능력. 특정 현상의 민감도(예를 들어, 입력에 대한 특정패턴의 존재 등)를 나타내는 설명은 모델관련 편향의 존재여부를 나타내는 능력을 통해 검증 될 수 있다.&lt;/li&gt;
  &lt;li&gt;사람의 평가. 사람은 설명이 사람의 기대와 얼만큼 일치하는지를 판단하여, 설명을 평가할 수 있다. 사람의 평가는 “인간이 원래 모델의 행동예측을 할 수 있다”라는 관점 혹은 “사람에게 모델의 편향이 들어나면서 도움이 된다”라는 점에서 완전무결성 혹은 대체-작업의 완전무결성을 평가할 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서 두 번째 테이블에서 볼 수 있듯이, 해석가능성과 완전무결성 사이의 트레이드 오프가 프록시 모델의 단순성과 정확성 사이의 균형을 뜻하지는 않는다. 중요한 모델의 편향을 찾아내는 능력 측면에서 다른 작업에 대한 설명 혹은 설명의 평가를 통해서 트레이트 오프를 절충할 수 있다. 각 세 가지 유형의 설명 방법은 완전무결성을 평가할 수 있는 설명을 제공할 뿐만 아니라 모델의 모든 세부 결정을 설명하는 것 보다 더 쉽다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1eXbhIESzWl7pqu8Yf5CEPYa5NstGhDi2&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Processing&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;처리 모델(Processing models)은 에뮬레이션 기반 방법이라고 볼 수 있다. 프록시 방법은 기존 모델을 향한 충실도(faithfulness, 기존 모델에 얼마나 비슷한지)를 통해 평가 된다. 이러한 지표중 몇 개는 다음 논문에 소개되어 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.04938&quot;&gt;“Why Should I Trust You?”: Explaining the Predictions of Any Classifier&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;이 논문의 주요 꼭지는 모델의 완전무결성 평가는 지역적이여야 한다는 것이다. 만약 모델이 전역적으로(globally) 복잡한 심층 신경망이더라도 일부 행동을 근사함으로써(approximating) 지역적 설명을 할 수 있다. 그러므로 처리 모델 설면은 &lt;strong&gt;설명의 “복잡성(complexity)”&lt;/strong&gt;(근본적으로 길이를 최소화)과 &lt;strong&gt;“지역적 완전무결성(local completeness)”&lt;/strong&gt;(실제 분류기와 관련된 해석가능한 표현의 오류)을 최소화 하려는 것이다.&lt;/p&gt;

    &lt;p&gt;민감한 지역을 강조하는 돌출 방법(Salience methods)도 정성적으로 평가된다. 비록 이들은 기존 방법의 출력을 직접적으로 예측하지 않지만, 원래 의도는 모델 민감도를 설명하기 위한 것이기 때문에 충실성(faithfulness)을 평가할 수 있다. 예를 들어 다음 논문에서는 다양한 버전의 일부가 가려진 입력 이미지를 모델에 전달하고 테스트하는 폐쇄(occlusion) 실험을 진리로 둔다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/7a56/72796aeca8605b2e370d8a756a7a311fd171.pdf&quot;&gt;A unified view of gradient-based attribution methods for Deep Neural Networks&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;이 실험은 입력의 어느 부분이 모델의 출력 결과를 가장 많이 변화시키는지에 대해 강력하지만 연산적으로 비효율적인 방법으로 결정한다. 폐쇄성 기반 민감도(occlusion-based sensitivity)와 상관관계가 있는 돌출 맵을 얼마나 밀접하게 만들어 냈는가를 통해 각 돌출 방법(Salience methods)을 평가된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Representation&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;표현 기반 방법은 다른 전이학습과 관련된 작업의 테스트를 통해 표상(representation) 일부의 역할을 특징화(characterize)할 수 있다. 예를 들어 전이 문제(transfer problem)에서 입력 피쳐의 역할을 할 수 있는 표상 층이 있다. 그리고 Network Dissection representation 와 CAV(Concept Activation Vectors) 두 방법은 모두 사람이 이해할 수 있는 특정 개념으로 감지하거나 상호 연관 짓는 능력에 따라 측정된다.&lt;/p&gt;

    &lt;p&gt;개별 표상의 일부가 특징화 되면 설명력을 테스트 할 수 있다. 그들의 활성화 값이 얼만큼 충실하게 특정 네트워크의 편향을 보여주는지를 평가하면서 실행할 수 있다. 예를 들어 CAV(Concept Activation Vectors)는 클래스를 결정짓는 두 가지 다른 유형의 시그널을 담은 데이터로 여러 버전의 구조가 같은 네트워크를 훈련시키면서 이를 평가하게 된다(데이터: 이미지와 다양한 신뢰도를 이름으로 한 클래스 텍스트). CAV의 네트워크 충실도는 두 가지 방법을 통해 식별될 수 있다. 하나는 텍스트 라벨에 의존하는 분류기가 텍스트와 연관되어 높은 CAV 벡터 활성화 값을 보이는가, 다른 하나는 텍스트 라벨에 의존하지 않는 분류기가 낮은 CAV 벡터값을 보이는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Explanation Producing&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;설명 생산 시스템은 얼마나 유저의 기대와 잘 부합되는 지에 따라 평가된다. 예를 들어 네트워크 어텐션은 사람의 어텐션과 비교할 수 있다. 그리고 분리된 표상(disentangled representations)은 잠재된 변수를 가지고 있는 합성된 데이터를 통해 잠재 변수가 복구되었는 지를 확인함으로써 테스트 할 수 있다. 마지막으로 시스템은 명시적으로 사람이 읽을 수 있는 설명을 생성함으로써 테스트 세트 혹은 사람의 평가에서도 똑같이 동작 하는지 확인한다.&lt;/p&gt;

    &lt;p&gt;설명 생산 시스템에서 설명력을 평가하기 어려운 점중에 하나는 시스템 자체가 설명을 만들어 내야 하기 때문에, 반드시 설명의 평가와 함께 시스템에 대한 평가를 병행해야 한다는 것이다. 불합리해 보이는 설명은 시스템이 정보를 합리적인 방법으로 처리하지 못했거나, 설명 생성기가 합리적인 설명을 작성하지 못했음을 통해 나타낼 수 있다. 반대로, 비록 시스템이 불합리한 규칙을 이용해 의사결정을 내리더라도 의사결정 과정에 충실하지 못한 설명 체계는 합리적인 설명을 생산할 수 있다. 합리성에만 의존하는 설명의 평가는 이러한 차별점을 놓칠 수도 있다. 모델과 사용자간의 공백을 메우도록 돕는 여러 유저-학습 디자인을 요약한 논문이 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Paper Links]&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.08608&quot;&gt;Towards A Rigorous Science of Interpretable Machine Learning&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;7-conclusions&quot;&gt;7. Conclusions&lt;/h1&gt;

&lt;p&gt;심층 신경망 커뮤니티에서 보여진 공통적인 관점은 대형 DNN 모델의 완전한(투명한) 설명을 위해 필요한 해석가능성과 이론적 이해 수준이 여전히 부족하다는 점이다. 예를 들어서, Yann LeCunn은 NIPS에서 Ali Rahimi의 “Test of Time” 발표에 대한 대답으로, “엔지니어링의 산물은 항상 이론적인 이해보다 선행되어 왔다.” 라고 이야기했다. 그러나, 우리는 기계 학습 시스템이 회의적인 대중들 사이에서 더 넓은 수용을 얻기 위해서는, 그러한 시스템이 그들의 결정에 대해 만족스러운 설명을 제공하거나 허용할 수 있는 것이 중요하다고 주장한다. 심층 네트워크 처리(deep network processing)의 설명, 심층적인 네트워크 표현(deep network representation)에 대한 설명, 시스템 수준의 설명 생산에 대한 노력으로 이루어진 발전은 유망한 결과를 낳았다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[Talk Link]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://youtu.be/Qi1Yry33TQE&quot;&gt;Ali Rahimi’s talk at NIPS(NIPS 2017 Test-of-time award presentation)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 우리는 설명가능성의 다른 측면을 다루기 위해 취해진 다양한 접근방식이 고립되어 있다는 것을 발견했다. 더 효율적인 설명을 달성하기 위해 다른 범주의 기술을 합병하는 접근 방법과 같은 설명가능성 공간에 대한 작업은 상대적으로 적은 주의를 주고도 특정 범주의 기술의 발전을 야기했다. 설명의 목적과 유형이 주어졌을 때, 어떤 설명 측정기준이 제일 좋은 것인지 알기가 명확하지 않다. 우리는 표적 설명(the targeted explanation)의 목적과 완전무결성에 맞는 다양한 측정기준을 사용하길 권장한다. 커뮤니티에서 다양한 분야의 여러 아이디어를 합치고 함께 작업하고 배우면서, 시스템 설명의 전반적인 상태 눈에 띄게 향상될 것이다. 그 결과로 행동 추정(behavioral extrapolation)을 제공하고, 딥러닝 시스템의 신뢰를 구축하고, 심층 네트워크 연산의 유용한 인사이트 등 시스템 행동의 이해와 증진을 가능케한다.&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Sep 2019 11:04:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2019/09/18/introxai.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2019/09/18/introxai.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>Github pages 로 프로젝트 문서화</title>
        <description>&lt;p&gt;잘 만든 프로젝트를 jupyter notebook 으로 보여주기에는 난잡해 보일 수가 있다. 프로젝트를 정리하고 싶다면 프로젝트 폴더에 docs 를 만들어서 github가 제공하는 웹호스팅 기능을 이용해서 프로젝트 홈페이지를 만들수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;실습환경: Ubuntu Server 18.04 LTS&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;준비과정&quot;&gt;준비과정&lt;/h1&gt;

&lt;h2 id=&quot;install-nodejs--npm&quot;&gt;Install nodejs &amp;amp; npm&lt;/h2&gt;

&lt;p&gt;사용하기 위해서 우선 nodejs 와 노드 패키지 매니저(npm) 를 설치해야한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -
$ sudo apt-get install -y nodejs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;패키지 버전을 체크해보자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ nodejs --version
v10.14.1

$ npm --version
6.4.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;install-gitbook-client&quot;&gt;Install gitbook client&lt;/h2&gt;

&lt;p&gt;gitbook client 를 설치해야한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ npm install gitbook-cli -g
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;initialize-gitbook&quot;&gt;Initialize gitbook&lt;/h2&gt;

&lt;p&gt;이제 본젹적으로 gitbook 을 사용해보자. 처음 프로젝트를 시작하면서 문서화를 생각하는 것은 좋다. 만약에 이미 작업한 프로젝트가 있다면, 중간에 &lt;code class=&quot;highlighter-rouge&quot;&gt;posts&lt;/code&gt; 를 만드는 작업만 유심히 살펴봐도 된다. 프로젝트 디렉토리 구조는 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
\README.md
\SUMMARY.md
\book.json
\notebooks  # 내 프로젝트 쥬피터 노트북
\posts  # 내 프로젝트 관련 포스터
	\chapter-1
		\README.md
		\01_helloworld.md
\docs  # github page 로 가는 html 파일들 등
	\...
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그럼 우선 프로젝트 디렉토리를 만들어보자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ mkdir [프로젝트 디렉토리] &amp;amp;&amp;amp; cd [프로젝트 디렉토리]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 &lt;code class=&quot;highlighter-rouge&quot;&gt;posts&lt;/code&gt; 디렉토리를 만들자. 이 &lt;code class=&quot;highlighter-rouge&quot;&gt;posts&lt;/code&gt; 안에 있는 모든 내용이 향후에 웹페이지로 갈것이다. 만들고 아래 명령어를 시작해보자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[.] $ gitbook init
Installing GitBook 3.2.3
...
warn: no summary file in this book 
info: create README.md 
info: create SUMMARY.md 
info: initialization is finished 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[.] $ ls
README.md  SUMMARY.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;README.md&lt;/strong&gt; 파일은 gitbook 의 첫번째 페이지, &lt;strong&gt;SUMMARY.md&lt;/strong&gt; 는 gitbook 의 목차 역할을 한다. &lt;code class=&quot;highlighter-rouge&quot;&gt;posts&lt;/code&gt; 폴더 안에 첫번째 챕터를 만들어보자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[.] $ mkdir posts &amp;amp;&amp;amp; cd posts
[./posts] $ mkdir chapter-1 &amp;amp;&amp;amp; cd chapter-1
[./posts/chapter-1] $ vi README.md  # 아무거나 쓰고 저장하자
[./posts/chapter-1] $ vi 01_helloworld.md  # 아무거나 쓰고 저장하자
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 SUMMARY.md 에서 목차를 수정해보자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[./posts/chapter-1] $ cd ../..
[.] $ vi SUMMARY.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Summary
  
* [Introduction](README.md)
* [Chapter-1](post/chapter-1/README.md)
	* [01 hello world](post/chapter-1/01_helloworld.md)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;여기까지 왔으면 기본적인 설정은 완료된것이다. &lt;code class=&quot;highlighter-rouge&quot;&gt;gitbook serve&lt;/code&gt; 명령어를 통해 한번 살펴보자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[.] $ gitbook serve
Live reload server started on port: 35729
Press CTRL+C to quit ...

info: 7 plugins are installed 
info: loading plugin &quot;livereload&quot;... OK 
info: loading plugin &quot;highlight&quot;... OK 
info: loading plugin &quot;search&quot;... OK 
info: loading plugin &quot;lunr&quot;... OK 
info: loading plugin &quot;sharing&quot;... OK 
info: loading plugin &quot;fontsettings&quot;... OK 
info: loading plugin &quot;theme-default&quot;... OK 
info: found 3 pages 
info: found 0 asset files 
info: &amp;gt;&amp;gt; generation finished with success in 0.3s ! 

Starting server ...
Serving book on http://localhost:4000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;http://localhost:4000&lt;/strong&gt; 로 접속을 시도해보자. 아래와 같은 화면이 나오면 성공이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/77u9dksoz4tio2x/1204_gitbook.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CTRL+C&lt;/strong&gt; 를 눌러서 꺼주자.&lt;/p&gt;

&lt;h2 id=&quot;initialize-git&quot;&gt;Initialize git&lt;/h2&gt;

&lt;p&gt;테스트 후, 프로젝트 디렉토리에 &lt;code class=&quot;highlighter-rouge&quot;&gt;_book&lt;/code&gt; 라는 폴더가 생성됐을 것이다. 이 폴더를 통햇 github 페이지를 만든다. 이 폴더는 나중에 github 저장소에 올릴 필요가 없기 때문에 .gitignore 에 추가해줘야한다. (밑에 한번 지우는 과정을 거치지만 혹시 모르는 상태에 대비해서 작성해준다.)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[.] $ vi .gitignore
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아래 코드를 넣어주자. &lt;strong&gt;node_modules&lt;/strong&gt; 는 plugin 에 필요한 패키지들을 설치하는 디렉토리인데 올리지 않는다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Book build output
_book
# Dependency packages
node_modules
# Jupyter Notebook checkpoint
.ipynb_checkpoints
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 github 에 올려보도록 한다. 우선 자신의 github 에 [프로젝트 디렉토리]와 같은 이름의 github 저장소를 생성하자. 그리고 git 을 사용하기 위해, 다시 돌아와서 해당 프로젝트 디렉토리를 git 저장소로 만들어준다. (애당초에 repository 를 만들어서 clone 하는 상태에서 시작해도 좋다.)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[.] $ git init
[.](master) $ git remote add origin git@github.com:[사용자이름]/[프로젝트 디렉토리].git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;완료되었으면 저장소에 올려보도록 한다. &lt;code class=&quot;highlighter-rouge&quot;&gt;publish-gitbook.sh&lt;/code&gt; 라는 쉘 스크립트를 만들어준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[.](master) $ vi publish-gitbook.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# remove gitbook old things&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; _book
&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; docs

&lt;span class=&quot;c&quot;&gt;# gitbook init&lt;/span&gt;
gitbook &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; gitbook build

&lt;span class=&quot;c&quot;&gt;# build pages&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;docs
&lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; _book/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; docs/

&lt;span class=&quot;c&quot;&gt;# delete things&lt;/span&gt;
git clean &lt;span class=&quot;nt&quot;&gt;-fx&lt;/span&gt; _book

&lt;span class=&quot;c&quot;&gt;# upload&lt;/span&gt;
git add &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
git commit &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;update docs&quot;&lt;/span&gt;
git push &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;저장소에서-활성화-하기&quot;&gt;저장소에서 활성화 하기&lt;/h2&gt;

&lt;p&gt;자신의 github 저장소의 &lt;strong&gt;Settings&lt;/strong&gt; 에 가서 &lt;strong&gt;Github Pages&lt;/strong&gt; 항목의 &lt;strong&gt;master branch /docs folder&lt;/strong&gt; 를 누르고 &lt;strong&gt;Save&lt;/strong&gt; 를 누르자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/n4kcz94j5z77ia4/1204_repo1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/4s8rsdyl71ph1hd/1204_repo2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 인터넷 주소창에 &lt;strong&gt;https://[사용자이름].github.io/[프로젝트 디렉토리]&lt;/strong&gt; 에 접속하면 아까 보았던 gitbook 모습을 볼 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;customizing&quot;&gt;Customizing&lt;/h1&gt;

&lt;h2 id=&quot;bookjson&quot;&gt;book.json&lt;/h2&gt;

&lt;p&gt;프로젝트 디렉토리에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;book.json&lt;/code&gt; 파일을 생성하여 커스터마이징이 가능하다. 사실 쓰는건 플러그인과 변수 정도이지만, book.json의 기본 설정을 &lt;a href=&quot;https://toolchain.gitbook.com/config.html&quot;&gt;여기&lt;/a&gt;서 참고할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;plugins&quot;&gt;plugins&lt;/h2&gt;

&lt;p&gt;gitbook 에 다양한 플러그인을 설치 할 수 있는데, &lt;a href=&quot;https://plugins.gitbook.com/&quot;&gt;https://plugins.gitbook.com/&lt;/a&gt; 사이트에서 확인 할 수 있다.&lt;/p&gt;

&lt;p&gt;주로 사용하는것은 수식편집이 가능한 “katex”, 방문자 분석을 위한 구글 애널릭틱스 “ga”, 댓글을 달수 있는 “disqus” (회원가입 필요함) 정도다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
	&quot;plugins&quot;: [
		&quot;katex&quot;,
		&quot;disqus&quot;,
		&quot;ga&quot;
		],
	&quot;pluginsConfig&quot;: {
        &quot;disqus&quot;: {
            &quot;shortName&quot;: &quot;XXXXXXX&quot;
        },
        &quot;ga&quot;: {
            &quot;token&quot;: &quot;UA-XXXX-Y&quot;
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://tinydew4.gitbooks.io/gitbook/ko/structure.html&quot;&gt;GitBook Toolchain Documentation for Multi-Languages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.psangwoo.com/coding/2018/01/31/gitbook-on-windows.html&quot;&gt;윈도우에서 깃북 제작 및 깃헙 페이지로 호스팅하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.zhlzzz.com/gitbook/&quot;&gt;GitBook完整教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://beomi.github.io/2017/11/20/Deploy-Gitbook-to-Github-Pages/&quot;&gt;깃헙 Pages에 깃북 배포하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gitbook.zhangjikai.com/plugins.html&quot;&gt;github plugin 설명(중국어)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 04 Dec 2018 22:04:38 +0900</pubDate>
        <link>https://simonjisu.github.io/programming/2018/12/04/buildgithubpages.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/programming/2018/12/04/buildgithubpages.html</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>[비전공자의 Flask-2] 본격 앱 만들기 1</title>
        <description>&lt;h1 id=&quot;폴더-생성부터-데이터베이스-만들기&quot;&gt;폴더 생성부터 데이터베이스 만들기&lt;/h1&gt;

&lt;p&gt;내가 만드려고 하는 앱은 사용자가 어떤 query 를 날리면 이를 모델을 거쳐서 결과를 웹에서 보여주는 간단한 앱이다. 예를 들어, 번역기 같은 앱의 경우를 생각해보자.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;사용자 (웹페이지 방문자)는 번역하고자 하는 글을 쓴고 “번역” 버튼을 클릭한다.&lt;/li&gt;
  &lt;li&gt;번역 클릭 후, 해당 string이 flask를 통해 전송 받으면 이를 모델에 넣어서 결과를 뱉는다.&lt;/li&gt;
  &lt;li&gt;결과를 다시 flask 앱을 통해 사용자에게 보여준다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;구체적으로 &lt;a href=&quot;https://simonjisu.github.io/datascience/2017/08/04/E2EMN.html&quot;&gt;End to End Memory Network&lt;/a&gt; 모델을 활용해서 스토리 내용을 사용자가 선택해서 질문을 던지면 그에 대한 결과를 받는 앱을 만들 것이다.&lt;/p&gt;

&lt;h1 id=&quot;step-0-폴더-생성하기&quot;&gt;Step 0: 폴더 생성하기&lt;/h1&gt;

&lt;p&gt;어플리케이션 개발을 시작하기전에, 어플리케이션에서 사용할 폴더를 만들자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/e2eapp
    /static
    /templates
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;앞으로 이 &lt;strong&gt;“nmtapp”&lt;/strong&gt; 폴더 안에 우리가 사용할 것들을 넣는다. &lt;strong&gt;“static”&lt;/strong&gt; 은 사용자들을 위한 폴더, 이 폴더는 css와 javascript 파일들이 저장되는 곳이다. Flasks는 templates 폴더에서 &lt;a href=&quot;http://jinja.pocoo.org/&quot;&gt;Jinja2&lt;/a&gt; 템플릿을 찾을 것이다.&lt;/p&gt;

&lt;h1 id=&quot;step-1-데이터베이스-스키마&quot;&gt;Step 1: 데이터베이스 스키마&lt;/h1&gt;

&lt;p&gt;데이터베이스 스키마를 생성해야 한다. 우리의 어플리케이션은 단지 하나의 테이블만 필요하며 사용이 매우 쉬운 SQLite를 쓸것이다. 다음의 내용을 schema.sql 이라는 이름의 파일로 방금 생성한 nmtapp 폴더에 저장한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;drop table if exists nmtmain;
create table nmtmain (
  id integer primary key autoincrement,
   question string not null
);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;해당 테이블의 이름은 “nmtmain” 이며, 간략하게 칼럼을 소개하자면, 아래와같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;id: 자동으로 증가되는 정수이며 프라이머리 키(primary key) 이다.&lt;/li&gt;
  &lt;li&gt;query: 사용자들이 입력한 질문.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;데이터베이스 스키마(database schema)란 데이터베이스에서 자료의 구조, 자료의 표현 방법, 자료 간의 관계를 형식 언어로 정의한 구조이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;step-2-어플리케이션-셋업-코드&quot;&gt;Step 2: 어플리케이션 셋업 코드&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/e2eapp
    /static
    /templates
    /schema.sql
    /e2estart.py
    /settings.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;settingspy&quot;&gt;settings.py&lt;/h2&gt;

&lt;p&gt;추가로 앱을 실행하기 위한 파일들을 만든다. 중요한 정보 혹은 환경변수가 있는 파일은 &lt;strong&gt;“settings.py”&lt;/strong&gt; 에 넣기로 한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# settings.py
# configuration
DATABASE = '../data/e2e.db'
DEBUG = True
SECRET_KEY = 'development key'
USERNAME = 'admin'
PASSWORD = 'password'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;클라이언트에서의 세션을 안전하게 보장하기 위해서는 secret_key 가 필요하다. secret_key는 추측이 어렵도록 가능한 복잡하게 선택하여야 한다. 디버그(DEBUG) 플래그는 인터랙티브 디버거를 활성화 시키거나 비활성화 시키는 일을 한다. 운영시스템에서는 디버그 모드를 절대로 활성화 시키지 말아야 한다. 왜냐하면 디버그 모드에서는 사용자가 서버의 코드를 실행할수가 있기 때문이다.&lt;/p&gt;

&lt;h2 id=&quot;nmtstartpy&quot;&gt;nmtstart.py&lt;/h2&gt;

&lt;p&gt;앱을 실행하는 &lt;strong&gt;“e2estart.py”&lt;/strong&gt; 파일을 만든다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# all the imports
import sqlite3
from flask import Flask, request, session, g, redirect, url_for, abort, render_template, flash

# create application
app = Flask(__name__)
app.config.from_pyfile(&quot;./settings.py&quot;, silent=True)

def connect_db():
    return sqlite3.connect(app.config['DATABASE'])

if __name__ == '__main__':
    app.run(host='0.0.0.0')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아직 모르는 import 가 많지만, 차차 배울테니 우선 앱을 만들고 실행하는 부분을 알아보자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;환경설정 불러오기:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config.from_pyfile(&quot;./settings.py&quot;, silent=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;만약에 현재 파일 위치에 환경변수 객체(settings.py 의 내용)를 작성하였으면 &lt;code class=&quot;highlighter-rouge&quot;&gt;from_object(__name__)&lt;/code&gt; 라는 명령어를 쓰면 된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;실행&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(venv) /e2eapp $ python3 e2estart.py  
...
192.168.6.34 - - [11/Nov/2018 19:08:40] &quot;GET / HTTP/1.1&quot; 404 -
192.168.6.34 - - [11/Nov/2018 19:08:40] &quot;GET /favicon.ico HTTP/1.1&quot; 404 -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;지금은 어떤 뷰(view)를 만들지 않았기 때문에, 브라우저에서 페이지를 찾을 수 없다는 404에러를 볼 수 있을 것이다. 이건 나중에 살펴보고, 우선 데이터베이스를 만들고 진행하도록 하자.&lt;/p&gt;

&lt;h1 id=&quot;step-3-데이터베이스-생성하기&quot;&gt;Step 3: 데이터베이스 생성하기&lt;/h1&gt;

&lt;p&gt;현재 만들고자 하는 앱은 관계형 데이터베이스 시스템에 의해 구동되는 어플리케이션이다. 이러한 시스템은 어떻게 데이터를 저장할지에 대한 정보를 가지고 있는 스키마가 필요하다. 그래서 처음으로 서버를 실행하기 전에 스키마를 생성하는 것이 중요하다.&lt;/p&gt;

&lt;p&gt;우선 데이터를 어디다 저장할지 정해보자. 데이터는 앱 밖에 있는 &lt;strong&gt;data&lt;/strong&gt; 폴더를 생성해서 저장하기로 한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/demo
	/venv
	/e2eapp
	/data
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아까 만들어둔 &lt;strong&gt;“schema.sql”&lt;/strong&gt; 파일을 이용하여 sqlite3 명령어를 사용하여 다음과 같이 만들 수 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(venv) /e2eapp $ sqlite3 ../data/e2e.db &amp;lt; schema.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sqlite3 가 설치 안됐을 수도 있다. 아래 명령어를 쳐서 (가상환경 빠져나와서) sqlite3 를 설치하자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install sqlite3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;데이터베이스를 초기화하는 함수를 만들고 싶다면 contextlib 의 closing 함수를 import 한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# import 에 추가
from contextlib import closing

def init_db():
    with closing(connect_db()) as db:
        with app.open_resource('schema.sql') as f:
            db.cursor().executescript(f.read())
        db.commit()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;함수설명&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;closing:&lt;/strong&gt; with 블럭안에서 연결한 커넥션을 유지하도록 도와준다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;app객체의 open_resource:&lt;/strong&gt; 리소스 경로(nmtapp 의 폴더)의 파일을 열고 그 값을 읽을 수 있다. 우리는 이것을 이용하여 데이터베이스에 연결하는 스크립트를 실행시킬 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음 시간에는 데이터베이스와 연결하고, 뷰함수 및 템플릿을 만들어보자.&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://flask-docs-kr.readthedocs.io/ko/latest/installation.html&quot;&gt;flask 한글 튜토리얼&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wikidocs.net/book/1530&quot;&gt;SQLite로 가볍게 배우는 데이터베이스&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 11 Nov 2018 17:39:48 +0900</pubDate>
        <link>https://simonjisu.github.io/programming/2018/11/11/flaskstudy2.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/programming/2018/11/11/flaskstudy2.html</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>[비전공자의 Flask-1] 첫 앱 만들어보기</title>
        <description>&lt;p&gt;설치가 완료 되었으니 빠르게 첫 앱을 만들어보자.&lt;/p&gt;

&lt;h1 id=&quot;hello-world-찍기&quot;&gt;Hello World 찍기&lt;/h1&gt;

&lt;p&gt;컴퓨터 책에 보면 꼭 해보라는 문구가 있다. 그 말을 찍어 볼 것이다.&lt;/p&gt;

&lt;p&gt;hello.py 파일을 하나 만들어 아래와 같이 작성하자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello World!'

if __name__ == '__main__':
    app.run()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;hello.py 를 실행하자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(venv) $ python3 hello.py
 * Serving Flask app &quot;hello&quot; (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;일단 &lt;strong&gt;WARNING&lt;/strong&gt; 이 뜨는데 어떻게 해결하는지는 나중에 살펴보고 통신이 어떻게 되는지 살펴보자.&lt;/p&gt;

&lt;p&gt;특별히 &lt;code class=&quot;highlighter-rouge&quot;&gt;app.run()&lt;/code&gt; 에 host 를 지정하지 않으면 &lt;strong&gt;http://127.0.0.1:5000/&lt;/strong&gt; 내부 주소(localhost) 로 앱이 실행이 된다.&lt;/p&gt;

&lt;p&gt;문제는 라즈베리파이 내부에 인터넷 익스플로어 같은 어플리케이션이(혹은 글쓴이가 찾지를 못한거 일수도, 설령 찾았다 해도 모니터가 없어서 볼수가 없음) 없기 때문에 내부 주소를 통해 해당 페이지에 접속하지 말고 외부 주소로 접속해서 결과를 봐야한다.&lt;/p&gt;

&lt;p&gt;우선 app.run 부분을 아래와 같이 바꿔주고 다시 실행해보자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;app.run(host='0.0.0.0')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 추가로 라즈베리파이를 현재 어떻게 접속하고 있는지를 이해해야한다.&lt;/p&gt;

&lt;p&gt;현재 글쓴이는 &lt;strong&gt;노트북&lt;/strong&gt;을 통해 라즈베리파이의 리눅스 터미널로 접속하고 있는데 이게 어떻게 진행되는 지 알아보자.&lt;/p&gt;

&lt;h2 id=&quot;포트포워딩을-이해보자&quot;&gt;포트포워딩을 이해보자&lt;/h2&gt;

&lt;p&gt;개인별로 사정이 다르지만, 집에서 라즈베리파이 서버를 사용한다고 가정하고 진행하겠다.&lt;/p&gt;

&lt;p&gt;우선 자신의 집의 외부로 연결되는 IP(Internet Protocol) 를 알아야 한다. 인터넷에 “&lt;a href=&quot;http://www.findip.kr/&quot;&gt;IP 확인&lt;/a&gt;” 만 쳐봐도 자신이 접속한 컴퓨터의 IP 를 알수 있다. 보통 해당 고유의 IP 를 통해 집안 곳곳 공유기를 통해 통신한다.&lt;/p&gt;

&lt;p&gt;공유기나 내부 네트워크를 사용해서 인터넷에 접속할 경우 사설 IP(Private IP)라고 하는 특정 주소 범위(192.168.0.1 ~ 192.168.255.254)가 내부적으로 사용되고, 공인 IP 주소를 찾기 힘든 경우가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/s/exbbawgg64w0b75/1030_networkmap.png&quot; height=&quot;480&quot; width=&quot;520&quot; /&gt;&lt;/p&gt;

&lt;p&gt;비전공자라 용어가 정확하지는 않을 수도 있지만, 위 그림처럼 &lt;strong&gt;외부 &amp;gt; 우리집&lt;/strong&gt; 으로 오는 신호는 IP 고, 집 &lt;strong&gt;내부 &amp;gt; 내부&lt;/strong&gt; 로 이동하는 신호는 사설 IP 라고 생각 하면 될것이다. 라즈베리파이가 외부에서 받는 신호는 빨간색 포트를 통과하게 된다. 이정도만 이해하고 넘어가자.&lt;/p&gt;

&lt;p&gt;어쨋든 공유기 홈페이지에서 포트포워딩 작업을 진행해야한다. 공유기 홈페이지를 접속해서 “&lt;strong&gt;포트포워드&lt;/strong&gt;” 라는 단어가 들어간 항목을 찾아가보자. 그리고 자신의 라즈베리 파이가 연결된 &lt;strong&gt;내부 IP 주소&lt;/strong&gt; 를 찾아서 포트를 열어주자, 테스트를 위해 5000 번을 포트로 쓴다. 위 그림의 예시로 들자면 아래와 같다. (물론 예시로 든거기 때문에 똑같이 따라하면 안된다.)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;공유기(벽장) &amp;gt; 거실공유기 : 192.168.54.245, TCP 포트번호(내부/외부)를 5000 으로 설정&lt;/p&gt;

  &lt;p&gt;거실공유기 &amp;gt; 라즈베리파이 : 192.168.0.64, TCP 포트번호(내부/외부)를 5000 으로 설정&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위와 같이 설정시, 내 노트북 크롬에 &lt;code class=&quot;highlighter-rouge&quot;&gt;[외부 IP 주소]:5000&lt;/code&gt; 라고 치면, 해당 통신이 외부신호를 거쳐고, 내부에서 192.168.54.245 &amp;gt; 192.168.0.64 를 거쳐서 라즈베리파이에게 닿게 되고, 아까 실행한 flask app의 결과인 “Hello World!”를 받을 수 있게 된다.&lt;/p&gt;

&lt;p&gt;자세한 포트포워딩 설정 방법은 인터넷에 많으니 잘 찾아보시길 바란다. &lt;a href=&quot;http://studyforus.tistory.com/35&quot;&gt;예시&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;실행하기&quot;&gt;실행하기&lt;/h1&gt;

&lt;p&gt;자, 설정을 마쳤다면 실행하고 있는 노트북의 인터넷 창에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;[외부 IP 주소]:5000&lt;/code&gt; 를 쳐보자. 그리고 라즈베리파이 터미널을 확인하면&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;192.168.6.34 - - [30/Oct/2018 21:23:18] &quot;GET / HTTP/1.1&quot; 200 -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;표시가 뜰텐데, &lt;strong&gt;“정상적으로 신호를 주고 받아서 ‘Hello World!’ 를 보냈어!”&lt;/strong&gt; 라는 뜻이다.&lt;/p&gt;

&lt;p&gt;인터넷 창을 확인해보면 Hello World! 문구가 떠있을 것이다. 야호!&lt;/p&gt;

&lt;p&gt;기본 어플리케이션인 “Hello World!” 를 성공시켰으니 충분히 고생했다고 생각한다. 다음 시간에는 본격적으로 튜토리얼을 따라서 시작해보도록 한다.&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://flask-docs-kr.readthedocs.io/ko/latest/installation.html&quot;&gt;flask 한글 튜토리얼&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 30 Oct 2018 21:25:57 +0900</pubDate>
        <link>https://simonjisu.github.io/programming/2018/10/30/flaskstudy1.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/programming/2018/10/30/flaskstudy1.html</guid>
        
        
        <category>programming</category>
        
      </item>
    
  </channel>
</rss>
