<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soo</title>
    <description>My Blog
</description>
    <link>http://simonjisu.github.io/</link>
    <atom:link href="http://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 17 May 2018 23:40:38 +0900</pubDate>
    <lastBuildDate>Thu, 17 May 2018 23:40:38 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>All about Word Vectors: GloVe</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-glove&quot;&gt;All about Word Vectors: GloVe&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;co-occurrence&quot;&gt;Co-occurrence&lt;/h2&gt;

&lt;p&gt;공기(Co-occurrence) 란 무엇인가? 두 개 이상의 어휘가 일정한 범위(range) 혹은 거리(distance) 내에서 함께 출현하는 현상을 말한다. 여기서 어휘는 단어 뿐만 아니라 형태소, 합성어 등의 단위로 의미를 부여할 수 있는 언어 단위다. 그렇다면 왜 &lt;strong&gt;공기 관계&lt;/strong&gt; 를 살피는 것일까?&lt;/p&gt;

&lt;p&gt;공기 관계를 통해 문서나 문장으로 부터 &lt;strong&gt;추상화된 정보&lt;/strong&gt; 를 얻기 위해서다. 이는 자연어처리의 가정을 생각해보면 이해할 수 있을 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;비슷한 맥락에 등장하는 단어들은 유사한 의미를 지니는 경향이 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;때문에, 두 단어가 같이 등장한 횟수가 많아지면 &lt;strong&gt;유사한 의미&lt;/strong&gt; 를 가졌다고 볼 수도 있다는 것이다. 이런 유사한 의미를 추상화된 정보로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;공기(Co-occurrence) 정보를 수집하는 방법은 두 가지다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;window 기반: 품사와 의미(semantic) 정보를 캡쳐할 수 있다.&lt;/li&gt;
  &lt;li&gt;word-document co-occurrence matrix 기반: 조금 더 일반적인 토픽을 추출 할 수 있고, 이는 Latent Semantic Analysis 와 연결 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;example-window-based-co-occurrence-matrix&quot;&gt;Example: Window based co-occurrence matrix&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;I like deep learning.&lt;/p&gt;

  &lt;p&gt;I like NLP.&lt;/p&gt;

  &lt;p&gt;I enjoy flying.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 3 문장을 사용해서, window size = 1 로 지정하는 co-occurrence matrix 를 만들어보자. 무슨 뜻인지는 아래 코드를 실행한 표를 살펴보자.&lt;/p&gt;

&lt;p&gt;단, 한 단어에 대해서 좌측에서 등장했는지 우측에서 등장했는 지는 상관없다(이는 co-occurrence matrix 가 대각을 기준으로 대칭하는 결과를 불러옴)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
import numpy as np
from collections import deque, Counter
from itertools import islice
from scipy.sparse import coo_matrix

flatten = lambda t: [tuple(j) for i in t for j in i]
window = 1

def get_cooccur_list(sentence, window):
    s_len = len(sentence)
    ngram_list = [deque(islice(sentence, i), window+1) for i in range(s_len+1)][2:]
    return ngram_list

sentences = ['I like deep learning .', 'I like NLP .', 'I enjoy flying .']
tokens = [s.split() for s in sentences]
vocab = list(set([w for s in tokens for w in s]))
# print(vocab)
vocab = ['I', 'like', 'enjoy', 'deep', 'learning', 'NLP', 'flying', '.'] # 표와 같은 모양을 만들어주기 위해 다시 지정
vocab2idx = {w: i for i, w in enumerate(vocab)}
tokens_idx = [[vocab2idx.get(w) for w in s] for s in tokens]
co_occurs = [get_cooccur_list(s, window) for s in tokens_idx]

d = Counter()
d.update(flatten(co_occurs))
row, col, data = list(zip(*[[r, c, v] for (r, c), v in d.items()]))
temp = coo_matrix((data, (row, col)), shape=(len(vocab), len(vocab))).toarray()
co_mat = temp.T + temp

pd.DataFrame(co_mat, index=vocab, columns=vocab)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;코드를 실행하면 아래와 같은 표가 나온다. window size 가 1이니까 “I” 주변 한칸에 동시 등장 단어는 “like” 가 2번 “enjoy” 가 1번이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;counts&lt;/th&gt;
      &lt;th&gt;I&lt;/th&gt;
      &lt;th&gt;like&lt;/th&gt;
      &lt;th&gt;enjoy&lt;/th&gt;
      &lt;th&gt;deep&lt;/th&gt;
      &lt;th&gt;learning&lt;/th&gt;
      &lt;th&gt;NLP&lt;/th&gt;
      &lt;th&gt;flying&lt;/th&gt;
      &lt;th&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;I&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;like&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;enjoy&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;deep&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;learning&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NLP&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;flying&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;co-occurrence matrix와 같은 단어 벡터는 어떤 문제점이 있을까?&lt;/p&gt;

&lt;p&gt;첫째로, 단어가 많아지면 벡터가 엄청 길어진다(데이터 차원이 커진)는 것이다. 이에 따른 많은 저장 비용이 들어갈 것이다. 둘째로, sparsity issues가 있을 수 있다(models are less robust).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그렇다면 꼭 하나의 단어로 해야만 하는가? 문서 전체의 단어의 공기 정보를 추출 하는 것은 안되는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이와 같은 생각이 GloVe 를 탄생시켰다.&lt;/p&gt;

&lt;h2 id=&quot;glove&quot;&gt;GloVe&lt;/h2&gt;

&lt;p&gt;Paper: &lt;a href=&quot;https://www.aclweb.org/anthology/D14-1162&quot;&gt;GloVe: Global Vectors for Word Representation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GloVe 의 학습방법은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \dfrac{1}{2} \sum_{i,j=1}^{W} f(P_{ij})(u_i^T v_j - \log P_{ij})^2&lt;/script&gt;

&lt;p&gt;논문해설을 통해서 자세히 보자.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;section&quot;&gt;논문 해설&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;GloVe:&lt;/strong&gt; Global Vectors라고 명칭을 지은 이유는 모델에서 직접적으로 문서 전체의 코퍼스 통계량을 포착할 수있기 때문이다. (the global corpus statistics are captured directly by the model)&lt;/p&gt;

&lt;p&gt;그전에 notation 을 정의해보자.&lt;/p&gt;

&lt;p&gt;Define notation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$X$: 단어간의 공기 매트릭스 (matrix of word-word co-occurrence counts)&lt;/li&gt;
  &lt;li&gt;$X_{ij}$: 단어 $j$ 와 문맥 단어 $i$ 가 같이 등장한 횟수 (the number of times that word $j$ occurs in the context word $i$)&lt;/li&gt;
  &lt;li&gt;$X_i = \sum_k X_{ik}$: 어떤 단어든 문맥 단어 $i$ 와 등장한 횟수 (the number of times any word appears in the context of word $i$)&lt;/li&gt;
  &lt;li&gt;$P_{ij} = P(j \vert i) = X_{ij} / X_i$: 단어 $j$ 와 문맥 단어 $i$ 동시 등장할 확률, 문맥 단어 $i$ 가 주어졌을 때 $j$ 가 등장할 확률 (probability that word $j$ appear in the context word $i$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래의 예시를 보자.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Probability and Ratio&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=solid$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=gas$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=water$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=fashion$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert ice)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.00019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000066&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.003&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert steam)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000022&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.00078&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0022&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert ice)/P(k\vert steam)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8.9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.085&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.96&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;위의 표에 따르면 $i=ice, j=steam$ 일때 $solid$ 와 동시 등장 확률이 높은 단어는 $ice$ 다. 직관적으로 생각해도, 단단한 $ice$ 가 $solid$ 와 연관될 확률이 더 높다. 따러서, 우리는 $P(k\vert ice)/P(k\vert steam)$ 를 구해서, 연관이 있는 단어일 경우 이 비율이 크게 높으며, 아니면 그 반대다. &lt;strong&gt;(엄청 크거나 혹은 엄청 작거나)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이처럼 직접적으로 단어간의 동시등장 확률을 비교하는 것보다. 확률간의 비율을 구하는 것이 &lt;strong&gt;연관성이 없는&lt;/strong&gt; 단어(water &amp;amp; fashion)들로 부터 관련된 단어(solid &amp;amp; gas)를 구별하기 좋으며, &lt;strong&gt;관련성 있는&lt;/strong&gt; 단어(solid &amp;amp; gas)들을 차별화 하기에도 좋다.&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;동시 등장 확률의 비율(ratios of co-occurrence probabilities)&lt;/span&gt;&lt;/strong&gt; 을 모델이 학습하게 하는 것이 바람직 해보인다.&lt;/p&gt;

&lt;p&gt;중요한 것은 이 비율은 3개의 단어 $i, j, k$ 와 연관이 있다. 따라서 아래의 함수를 구성할 수가 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i, w_j, \tilde{w}_k) = \dfrac{P_{ik} }{P_{jk} } \cdots (1)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$w \in \Bbb{R}^d$: word vectors&lt;/li&gt;
  &lt;li&gt;$\tilde{w} \in \Bbb{R}^d$: separate context word vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$(1)$ 식과 같이, 단어 벡터 공간에서 $w_i, w_j, \tilde{w}_k$ 를 input으로 넣었을 때,&lt;/p&gt;

&lt;p&gt;$\dfrac{ P_{ik} }{ P_{jk} }$ 비율을 나타내는 하는 선형구조인 함수를 구하는 것이 목적이다. 그리고 $F$ 를 아래와 같이 변형시켜 본다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F((w_i - w_j)^T \tilde{w}_k) = \dfrac{P_{ik} }{P_{jk} } \cdots (2)&lt;/script&gt;

&lt;p&gt;이로써 선형적인 관계를 포착하고, 양변 모두 스칼라 값으로 정해진 함수가 만들어 졌다. 그러나 단어 $i, j$ 와 $k$ 동시 등장 비율의 &lt;strong&gt;임의적인 차별화&lt;/strong&gt; 를 위해서 어떤 조건들을 만족해야한다. 그 조건들이란 단어 벡터 $w$ 와 문맥 단어 벡터 $\tilde{w}$ 간 서로 자유롭게 교환 될 수가 있어야 한다. 즉, 단어간 공기 매트릭스 $X$의 대칭(symmetric) 특성을 보존해야 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;임의적인 차별화&lt;/strong&gt; 가 무슨 말이냐면, 단어 $k$ 와 $i, j$ 단어 간의 비율을 확인 할때, $i$ 와 $k, j$ (혹은 $j$ 와 $i, k$) 의 관계도 확인 할 수 있어야 된다는 말이다.&lt;/p&gt;

&lt;p&gt;대칭(symmetric) 을 만족하려면 2 단계로 진행 된다. 우선, 두 그룹 $(\Bbb{R}, +)$ 과 $(\Bbb{R}_{&amp;gt;0}, \times)$ 에 대해서 함수 $F$ 가 &lt;strong&gt;homomorphism&lt;/strong&gt; 이어야 한다. (homomorphism 해설: 밑에 &lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt;를 보라), 예를 들어 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i, w_j, \tilde{w}_k) = \dfrac{F(w_i^T \tilde{w}_k) }{F(w_j^T \tilde{w}_k) } \cdots (3)&lt;/script&gt;

&lt;p&gt;$(2)$ 식에 의해서, 아래와 같이 풀 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i^T \tilde{w}_k) = P_{ik} = \dfrac{X_{ik} }{X_i} \cdots (4)&lt;/script&gt;

&lt;p&gt;$(3)$ 식에 만족하는 해답은 $F = \exp$ 임으로, 아래의 식을 도출 해낼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^T \tilde{w}_k = \log(P_{ik}) = \log(X_{ik}) - \log(X_i) \cdots (5)&lt;/script&gt;

&lt;p&gt;다음으로, $(5)$ 식은 $\log(X_i)$ 만 아니였다면 대칭이었을 것이다. $\log(P_{ik})=\log(P_{ki})$ 를 만족하는지 한번 보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\log(P_{ik}) &amp;= \log(X_{ik}) - \log(X_i) \\
\log(P_{ki}) &amp;= \log(X_{ki}) - \log(X_k)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;당연하게도, $\log(X_i) \neq \log(X_k)$ 이기 때문에 $\log(P_{ik}) \neq \log(P_{ki})$ 이다.&lt;/p&gt;

&lt;p&gt;하지만 $\log(X_i)$ 부분은 $k$ 에 대해서 독립적(independent) 이기 때문에, $w_i$ 의 bias $b_i$ 항으로 들어갈 수 있다. 그리고 대칭성을 유지하기 위해서 $\tilde{w}_k$ 의 bias $\tilde{b}_k$ 항도 더해준다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^T \tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik}) \cdots (6)&lt;/script&gt;

&lt;p&gt;$(6)$ 식이 우리의 제일 간단한 선형관계인 solution 이라고 해도 되지만 이는 문제가 좀 있다. $X_{ik} = 0$ 에서 명확하게 정의 되지 않는다. 이를 해결하기 위해서 $X_{ik}+1$ 하는 방법도 있지만, sparsity issue 를 벗어나기 힘들다. 그리고 하나의 큰 약점이 있다면 거의 등장하지 않는 단어들에게 동시 등장 비율이 모두 같을 수 있다는 점이다. 이게 왜 문제가 되냐면, co-occurrence 가 적을 수록 많이 등장하는 단어들 보다 정보 함량이 적고 데이터도 noisy 하기 때문이다.&lt;/p&gt;

&lt;p&gt;연구팀은 새로운 weighted least squares regression model 을 제시하여 문제를 풀고자 했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \dfrac{1}{2} \sum_{i,j=1}^{W} f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2 \cdots (7)&lt;/script&gt;

&lt;p&gt;가중치 함수 (Weighting function) $f(X_{ij})$ 는 아래의 특성을 따라야 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$f(0) = 0$. 만약 $f$ 가 연속함수(continuous function) 라면, $x \rightarrow 0$ 으로 갈때 $\lim_{x\rightarrow 0} f(x) log^2x$ 도 빠르게 수렴한다. 단, 유한한 값이여야 한다.&lt;/li&gt;
  &lt;li&gt;$f(x)$ 는 감소함수가 되면 안된다. (non-decreasing) 이유는 동시 등장이 희박한 단어들의 가중치가 많아져서는 안되기 때문이다.&lt;/li&gt;
  &lt;li&gt;$f(x)$ 는 큰 $x$ 값에 대해서 상대적으로 작은 값이어야 한다. 그 이유는 공기 횟수가 큰 단어들의 가중치가 너무 높게 설정하지 않기 위해서다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;연구팀은 이에 적합한 함수를 찾았다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x) =
\begin{cases} (x/x_{max})^{\alpha} \quad if\ x &lt; x_{max} \\
1 \quad otherwise
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L3_weight_f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림: $f(x)$ with $\alpha = 3/4$ 일때 좋은 성과를 얻었다. 재밌는 것은 Mikolov 논문에서 나온 unigram distribution 에 3/4 승을 해주는 것과 같다는 것을 발견했다.&lt;/p&gt;

&lt;p&gt;조금더 general 한 weighting function 은 아래와 같다. (자세한건 논문 3.1 참고)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{J} = \sum_{i,j} f(X_{ij})(w_i^T \tilde{w}_j - \log X_{ij})^2&lt;/script&gt;

&lt;p&gt;이는 연구팀이 도출한 $(7)$ 식과 같은 식이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;homomorphism&quot;&gt;참고 1: Homomorphism&lt;/h3&gt;

&lt;p&gt;혹시나 틀렸으면 댓글로 이야기 해주세요.&lt;/p&gt;

&lt;p&gt;우선 Group &lt;a href=&quot;https://en.wikipedia.org/wiki/Group_(mathematics)&quot;&gt;(위키 링크)&lt;/a&gt; 이란 것을 알아야한다. 내가 이해한 바로는 &lt;strong&gt;Group $(G, * )$&lt;/strong&gt; 이란, 집합 $G$ 와 연산 $* $ 로 구성되어 있다. 이 연산을 “the Group Law of $G$” 라고 부른다. 집합 $G$ 에 속한 원소 $a, b$ 의 연산을 $a * b$ 라고 표현한다. 또한 아래의 조건들을 만족해야한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Closure: $* $ 연산은 $G$ 에 대해 닫혀 있어야한다. 즉, $a * b$ 연산도 집합 $G$ 에 속해야한다.&lt;/li&gt;
  &lt;li&gt;Associativity: 교환 법칙이 성립해야한다. $(a * b) * c = a * (b * c)$&lt;/li&gt;
  &lt;li&gt;Identity element: 항등원이 존재해야 한다. $a * e = a = e * a$&lt;/li&gt;
  &lt;li&gt;Inverse element: 역원이 존재해야 한다. $a * x = e = x * a$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Group 를 이해 했으면 이제 Homomorphism 을 이해해보자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;정의:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;두 그룹 &lt;span style=&quot;color: #15b23c&quot;&gt;$(G, * )$&lt;/span&gt; 과 &lt;span style=&quot;color: #9013b2&quot;&gt;$(H, @)$&lt;/span&gt; 가 있으면, 모든 &lt;span style=&quot;color: #15b23c&quot;&gt;$x, y \in G$&lt;/span&gt; 에 대해서 $f:$ &lt;span style=&quot;color: #15b23c&quot;&gt;$G$&lt;/span&gt; $\rightarrow$ &lt;span style=&quot;color: #9013b2&quot;&gt;$H$&lt;/span&gt;, &lt;span style=&quot;color: #15b23c&quot;&gt;$f(x * y)$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$f(x) @ f(y)$&lt;/span&gt; 를 만족하는 map 을 말한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예시)&lt;/strong&gt;
두 그룹 &lt;span style=&quot;color: #15b23c&quot;&gt;$(\Bbb{R}, + )$&lt;/span&gt; 와 &lt;span style=&quot;color: #9013b2&quot;&gt;$(\Bbb{R}_{&amp;gt;0}, \times )$&lt;/span&gt; 사이에 어떤 map $f:$ &lt;span style=&quot;color: #15b23c&quot;&gt;$\Bbb{R}$&lt;/span&gt; $\rightarrow$ &lt;span style=&quot;color: #9013b2&quot;&gt;$\Bbb{R}_{&amp;gt;0}$&lt;/span&gt;, $f(x)=e^x$ 가 있다면, $f$ 가 Homomorphism 인지를 밝혀라.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;for any &lt;span style=&quot;color: #15b23c&quot;&gt;$x, y \in \Bbb{R}$&lt;/span&gt;,&lt;/p&gt;

  &lt;p&gt;&lt;span style=&quot;color: #15b23c&quot;&gt;$f(x + y)$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$e^{x+y}$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$e^x \times e^y$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$f(x) \times f(y)$&lt;/span&gt; 임으로&lt;/p&gt;

  &lt;p&gt;Homomorphism 을 만족한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;word2vec 과 glove 관련 포스팅은 &lt;strong&gt;“All about word vectors”&lt;/strong&gt; 시리즈로 마치겠다. 기회가 되면 gensim 의 사용법과, 데이터 차원 축소와 시각화 방법인 t-SNE 을 포스팅 하도록 하겠다.&lt;/p&gt;
</description>
        <pubDate>Wed, 02 May 2018 23:22:05 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/05/02/allaboutwv4.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/05/02/allaboutwv4.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Negative Sampling</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-negative-sampling&quot;&gt;All about Word Vectors: Negative Sampling&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_model_train.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;navie-softmax--&quot;&gt;Navie Softmax 의 단점&lt;/h2&gt;

&lt;p&gt;Navie Softmax 를 최종단에 출력으로 두고 Backpropagation 할때는 큰 단점이 있다.&lt;/p&gt;

&lt;p&gt;사실 Softmax가 그리 값싼 연산은 아니다. 우리가 학습하고 싶은 단어 벡터 1000개가 있다고 가정해보자. 그렇다면 매 window size=2 마다, 다시 말해 총 업데이트 할 5개의 단어 (중심단어 1 + 주변 단어 2 x 2) 를 위해서, $W, W’$ 안에 파라미터를 업데이트 해야하는데, 그 갯수가 최소 $(2 \times d \times 1000)$ 만큼된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown_\theta J_t(\theta) \in \Bbb{R}^{2dV}&lt;/script&gt;

&lt;p&gt;많은 양의 단어에 비해 업데이트 하는 파라미터수는 적기 때문에 gradient matrix $\triangledown_\theta J_t(\theta)$ 가 굉장히 sparse 해질 수 있다 (0이 많다는 소리). Adam 같은 알고리즘은 sparse 한 matrix 에 취약하다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://simonjisu.github.io/deeplearning/2018/01/13/numpywithnn_5.html&quot;&gt;Numpy with NN: Optimizer 편 참고&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그래서 &lt;strong&gt;“window에 실제로 등장하는 단어들만 업데이트 하면 좋지 않을까?”&lt;/strong&gt; 라는 생각을 하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;negative-sampling&quot;&gt;Negative Sampling&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;paper 1: &lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;&gt;Distributed representaions of Words and Phrases and their Compositionality (Mikolov et al. 2013)&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;paper 2: &lt;a href=&quot;https://arxiv.org/abs/1402.3722&quot;&gt;word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;요약하면 아래와 같은 목적함수를 최대화 하는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
J(\theta) &amp;= \dfrac{1}{T}\sum_{t=1}^{T} J_t(\theta)\\
J_t(\theta) &amp;= \underbrace{\log \sigma(u_o^T v_c)}_{(1)} + \underbrace{\sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_c)]}_{(2)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$T$: total num of words&lt;/li&gt;
  &lt;li&gt;$\sigma$: sigmoid function&lt;/li&gt;
  &lt;li&gt;$P(w) = {U(w)^{3/4}} / {Z}$: unigram distribution U(w) raised to the 3/4 power
    &lt;ul&gt;
      &lt;li&gt;The power makes less frequent words be sampled more often&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;말로 풀어보자면, 모든 단어 $T$ 에 대해서 중심단어 $c$ 와 그 주변단어 $o$ 가 같이 나올 확률 &lt;strong&gt;[수식 (1)]&lt;/strong&gt; 을 최대화 하고, 그 주변단어가 아닌 집합에서 sampling 하여 나온 $k$ 개의 단어의 확률 &lt;strong&gt;[수식 (2)]&lt;/strong&gt; 을 최소화 시키는 것이다. (음수가 붙기 때문에 최소하하게 되면 최대화가 된다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section&quot;&gt;상세 논문 설명&lt;/h3&gt;

&lt;p&gt;논문 기준으로 위에 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;표기법&lt;/span&gt;&lt;/strong&gt; 이 조금 다르다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;여기서 &lt;strong&gt;$w$ = center word, $c$ = context&lt;/strong&gt; 다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;출발점은 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$(w, c)$ 세트가 정말로 corpus data로 부터 왔는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;라고 생각하고 아래와 같은 &lt;strong&gt;정의&lt;/strong&gt; 를 하게 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로 부터 왔을 확률&lt;/li&gt;
  &lt;li&gt;$P(D = 0 \vert w, c) = 1 - P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로부터 오지 않았을 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, 우리의 목적은 확률 $P(D = 1\vert\ w, c)$ 를 최대화하는 parameter $\theta$를 찾는 것이기 때문에 아래와 같은 목적함수를 세울 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} &amp;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &amp;\arg \underset{\theta}{\max} \log \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &amp;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;파라미터 $\theta$ 는 단어들의 벡터라고 생각할 수 있다. 즉, 위의 식을 만족하는 어떤 최적의 단어 벡터를 찾는것이다.&lt;/p&gt;

&lt;p&gt;또한, 확률 $P(D=1\vert\ w,c;\theta)$ 은 sigmoid로 아래와 같이 정의 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D=1\vert\ w,c;\theta) = \dfrac{1}{1+e^{-v_c v_w}}&lt;/script&gt;

&lt;p&gt;따라서 우리의 목적함수는 아래와 같이 다시 고쳐 쓸수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} }&lt;/script&gt;

&lt;p&gt;그러나 우리의 목적 함수는 매 $(w, c)$ 세트마다 $P(D=1\vert\ w,c;\theta)=1$ 를 만족하는 trivial solution이 존재한다. $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 $\theta$ (보통 $K$ 가 40이 넘어가면 위 방정식의 값이 0에 가까워짐) 는 모든 값을 똑같이 0으로 만들어 버리기 때문에, 같은 값을 갖지 못하게 하는 매커니즘이 필요하다. ($\theta$ 에 뭘 넣어도 0이 되면 최대값을 찾는 의미가 없어진다, 자세한건 밑에 &lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt; 를 참조) 여기서 “같은 값을 같는다” 라는 말은 단어 벡터가 같은 값을 갖는 것이다.&lt;/p&gt;

&lt;p&gt;따라서, 하나의 방법으로 랜덤 $(w, c)$ 조합을 생성하는 집합 $D’$를 만들어 corpus data 로부터 올 확률 $P(D=1\vert \ w,c;\theta)$ 를 낮게 강제하는 것이다. 즉, $D’$ 에서 생성된 $(w, c)$ 조합은 &lt;strong&gt;corpus data 로부터 오지 않게&lt;/strong&gt; 하는 확률 $P(D=0\vert\ w,c;\theta)$ 을 최대화 하는 것.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} P(D=0\vert\ w,c;\theta) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta) + \underset{(w,c) \in D'}{\sum} \log \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \big(1- \dfrac{1}{1+e^{-v_c v_w} } \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} }
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\sigma(x) = \dfrac{1}{1+e^{-x} }$ 시그모이드 함수로 정의 하면, 아래와 같이 정리 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} } \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \sigma(v_c v_w) + \underset{(w,c) \in D'}{\sum} \log \sigma(- v_c v_w) \quad \cdots (3)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;이는 &lt;span style=&quot;color: #e87d7d&quot;&gt;paper 1&lt;/span&gt; 의 (4) 번 식과 같아지는다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log \sigma(u_c^T v_w) + \sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_w)]&lt;/script&gt;

&lt;p&gt;다른 점이라면, 우리가 만든 (3)식에서는 전체 corpus ($D \cup D’$) 을 포함하지만, Mikolov 논문의 식은 $D$ 에 속하는 $(w, c)$ 조합 하나와 $k$ 개의 다른 $(w, c_j)$ 의 조합을 들었다는 것이다. 구체적으로, $k$ 번의 negative sampling 에서 Mikolov 는 $D’$ 를 $k \times D$ 보다 크게 설정했고, k개의 샘플 $(w, c_1), (w, c_2), \cdots, (w, c_k)$ 에 대해서 $c_j$ 는 &lt;strong&gt;unigram distribution&lt;/strong&gt; 에 &lt;strong&gt;3/4&lt;/strong&gt; 승으로 부터 도출된다. 이는 아래의 분포에서 $(w, c)$ 조합을 추출 하는 것과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{words}(w) = \dfrac{p_{contexts} (c)^{3/4} }{Z}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$p_{words}(w)$, $p_{contexts} (c)$ 는 각각 words and contexts 의 unigram distribution 이다.&lt;/li&gt;
  &lt;li&gt;$Z$ 는 normalization constant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unigram distribution 은 단어가 등장하는 비율에 비례하게 확률을 설정하는 분포다. 예를 들어 “I have a pen. I have an apple. I have a pineapple.” 라는 문장이 있다면, 아래와 같은 분포를 만들 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;I&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;have&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;a&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;pen&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;an&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;apple&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;pineapple&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;여기서 3/4 승을 해주면, 가끔 등장하는 단어는 확률을 높혀주는 효과가 있다. 물론 자주 나오는 단어의 확률도 올라가지만 가끔 등장하는 단어의 상승폭 보다 적다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;a&lt;/th&gt;
      &lt;th&gt;$a^{\frac{3}{4} }$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;apple&lt;/td&gt;
      &lt;td&gt;$\frac{1}{15}=0.067$&lt;/td&gt;
      &lt;td&gt;${\frac{1}{15} }^{\frac{3}{4} }=0.131$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;have&lt;/td&gt;
      &lt;td&gt;$\frac{3}{15}=0.020$&lt;/td&gt;
      &lt;td&gt;${\frac{3}{15} }^{\frac{3}{4} }=0.299$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Mikolov 논문에서는 context는 하나의 단어이기 때문에 $p_{words}(w)$ 는 아래와 동일하다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{words}(w) = p_{contexts} (c) = \dfrac{count(x)}{ \vert text \vert }&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;trivial-solution&quot;&gt;참고 1. Trivial Solution&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} L(\theta;w,c) &amp;= \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } \\
&amp;= \underset{(w,c) \in D}{\sum} \log(1) - \log(1+e^{-v_c v_w}) \\
&amp;= \underset{(w,c) \in D}{\sum} - \log(1+e^{-v_c v_w})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;같은 두 벡터의 내적을 하게 되면 값은 최대가 된다. $\cos$ 값이 1이 되기 때문이다. (여기서는 최대 값이 중요한건 아니지만 값이 커진다는데 의의가 있다.)
&lt;script type=&quot;math/tex&quot;&gt;a\cdot a=\vert a \vert \vert a \vert \cos \theta&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a = np.array([1,2,3,4,5,6,7])
b = np.array([.1,.2,.3,.4,.5,.6,.7])
print(np.dot(a, a))
print(np.dot(a, b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;140&lt;/p&gt;

  &lt;p&gt;14.0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 모든 값들이 $e^{-v_c v_w}$ 를 0으로 만든다면, $L(\theta; w, c)$ 값은 0이 될것이다. 보통 $K$ 가 40 이 상이면, $L(\theta;w,c)$ 의 해는 모두 0 일 것이며 이것을 &lt;strong&gt;trivial solution&lt;/strong&gt; 이라고 한다. 우리의 목적은 단어 벡터 $v_c$ 와 $v_w$ 의 구별이기 때문에, $v_c \not = v_w$ 으로 만들어야한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음 시간에는 말뭉치의 공기정보(co-occurance)를 고려해 단어를 벡터화 시킨 &lt;strong&gt;GloVe&lt;/strong&gt; 에 대해 알아보자.&lt;/p&gt;
</description>
        <pubDate>Tue, 24 Apr 2018 16:14:13 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Big Little Data 참석후기</title>
        <description>&lt;h1 id=&quot;little-big-data--&quot;&gt;Little Big Data 참석 후기&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/littlebigdata.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;주최: ZEPL
링크: &lt;a href=&quot;https://festa.io/events/21&quot;&gt;festa&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;발표자분들께서 자신들이 겪은 다양한 경험을 들었다. 모든 일이 다 그렇지만, 문제를 파악하고 정의를 어떻게 하며, 방법을 모색하고 해결 후 결과를 다시 한번 정리해보는 사고 프로세스를 배운 것 같다.&lt;/p&gt;

&lt;p&gt;발표 세션을 듣고 일이 있어서 나와야했지만 유익했던 자리.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-1&quot;&gt;상세&lt;/h2&gt;

&lt;p&gt;발표자의 내용이 완벽하게 일치하지 않으며, 제가 중간중간 생각나서 제 생각을 기록한 것도 있습니다. (거의 없긴 하지만)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-2&quot;&gt;극한직업: 한국어 채팅 데이터로 머신러닝 하기&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.scatterlab.co.kr/&quot;&gt;Scatter Lab&lt;/a&gt; 조한석 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;한글 데이터는 문제가 많음:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hell 조사&lt;/li&gt;
  &lt;li&gt;자유로운 언어 변형&lt;/li&gt;
  &lt;li&gt;혀꼬인 소리&lt;/li&gt;
  &lt;li&gt;맞춤법 및 띄어쓰기 오류&lt;/li&gt;
  &lt;li&gt;챗팅에서 쓰이는 단어&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;한국어 데이터는 전처리(Preprocessing)이 80%다.&lt;/p&gt;

&lt;p&gt;일반 오픈소스 형태소 분석기의 한게점: 학습에 사용된 corpus가 잘 정돈된 데이터를 학습했기 때문에, 잘 안되는 경향이 있음.&lt;/p&gt;

&lt;p&gt;그래서 데이터로부터 학습하자! 김현중 님의 &lt;a href=&quot;https://github.com/lovit/soynlp&quot;&gt;soynlp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;다양한 문제들과 문제 정의 및 해결:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Normalize:
    &lt;ul&gt;
      &lt;li&gt;아이디어: 오류가 적다고 생각하는 데이터를 선택 후, 전체 데이터에서 조금 등장한 패턴을 자주 등장하는 것으로 수정하는 방법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;띄어쓰기 교정:
    &lt;ul&gt;
      &lt;li&gt;다음 글자가 띄어쓸지 아닐지 binary Classification&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tokenizing:
    &lt;ul&gt;
      &lt;li&gt;단어 추출 process&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Word Embedding:
    &lt;ul&gt;
      &lt;li&gt;oov 문제 (학습되지 않는 단어는 inference 단계에서 문제)&lt;/li&gt;
      &lt;li&gt;Fasttext 사용: substring 정보 활용 &lt;a href=&quot;https://arxiv.org/abs/1607.01759&quot;&gt;paper&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;한글의 경우: ngam 단위를 글자 / 자음모음으로 하게됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sentence Similarity
    &lt;ul&gt;
      &lt;li&gt;BOW + Word Embedding 방식: 임베딩에 너무 의존하게 됨, 학습된 데이터에 따라서 원하는 결과가 안나올 수도 있음, 따라서 다른 방법을 추가해서 쓰게됨.&lt;/li&gt;
      &lt;li&gt;참고한 논문: &lt;a href=&quot;http://cogcomp.org/papers/SongRo15.pdf&quot;&gt;Unsupervised Sparse Vector Densification for Short Text Similarity&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;팁
    &lt;ul&gt;
      &lt;li&gt;전처리 단계에서 &lt;a href=&quot;https://en.wikipedia.org/wiki/Heaps%27_law&quot;&gt;힙의 법칙(Heap’s Law)&lt;/a&gt; 에 따라서 빈도수가 너무 적은 단어는 과감하게 쳐내기&lt;/li&gt;
      &lt;li&gt;문제 정의를 잘하기, Countbase 모델이 오히려 더 잘 될 수도 있다.&lt;/li&gt;
      &lt;li&gt;unlabel 데이터에 label 을 달아서 인사이트를 얻어보자!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-3&quot;&gt;딥러닝에 필요한 로그 기깔나게 잘 디자인하는 법&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;구글 클라우드 엔지니어 백정상 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;탐색적 데이터 분석(EDA)을 잘 하기위해 어떻게 로그를 쌓는 것이 좋았는가?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로그 디자인&lt;/li&gt;
  &lt;li&gt;명확한 데이터&lt;/li&gt;
  &lt;li&gt;원시 데이터에서 분석 쿼리&lt;/li&gt;
  &lt;li&gt;등등&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 딥러닝은 조금 달랐다!!&lt;/p&gt;

&lt;p&gt;통계적으로 풀지 못하는 문제 생겼는데, 비정상적인 데미지를 만드는 플레이어가 핵유저인지 아닌지? 이상 탐지 문제 (정확히 기억이 안남)&lt;/p&gt;

&lt;p&gt;고전적인 머신러닝 기법으로 풀려고 보니 바운더리 필요하고, 어느정도 데미지가 비정상적인 플레이고, 정상적인 플레이인지 알수 없었음.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;정상적인 데이터가 충분히 많다면, 학습후 비정상 데이터 잡아내기 &lt;strong&gt;(지도학습)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;정상인지 아닌지 확신할 데이터 충분치 않다면, 학습후 클러스터링 해서 아웃라이어 잡기 &lt;strong&gt;(비지도학습)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;데이터가 충분하지 않았기 때문에 2번으로 선택후 가설을 세움.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;가설: 유저들이 평균적으로 내는 데미지에 비해 엄청 크면? –&amp;gt; 비정상&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Feature engineering: feature selection&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;유저 인덱스 들어가면 분류 너무 세분화,&lt;/li&gt;
  &lt;li&gt;페이즈 별로 데미지 얼마 넣었는지 보다 스테이지 완료시 데미지만,&lt;/li&gt;
  &lt;li&gt;캐릭터 직업별로 데미지 넣는 양이 다르니, 직업은 넣자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기까지 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;행복회로! &lt;/span&gt;&lt;/strong&gt; 상상의 나래였던거임.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;-현실-&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;무슨 모델을 쓸 것인가? 꼭 딥러닝을 써야되나?&lt;/p&gt;

&lt;p&gt;오코인코더 써서 대다수 유저와 loss 차이가 많이 나는지 확인! (위에 가설을 검정확인함)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deep AutoEncoder - Compressed Feature Vector: 데이터 복원하는 속성 학습&lt;/li&gt;
  &lt;li&gt;train, 평가 쉬움&lt;/li&gt;
  &lt;li&gt;주의할 것은 &lt;strong&gt;가설&lt;/strong&gt; 이 참이여야만 모델이 정확하다는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;만들면서 생긴 문제들:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;input data 에 따라서 달라짐 &amp;gt; 어떤 feature 를 쓸 것인가?
    &lt;ul&gt;
      &lt;li&gt;꼭 필요함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;훈련방식: 온라인, 배치, 저장된 데이터 &amp;gt; 3개 다해야함
    &lt;ul&gt;
      &lt;li&gt;이유: 왜냐면 많은 사람들이 어뷰징을 쓰게되면 바이어스가 정상으로 학습 될 수도&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;피쳐선택:
    &lt;ul&gt;
      &lt;li&gt;꼭 필요함. 피쳐 늘어날 수록 더 많은 데이터가 필요함&lt;/li&gt;
      &lt;li&gt;EDA: 게임상에서 일어나는 특징적인 패턴을 찾는 데 주력 &amp;gt; 어떤 피쳐가 상관관계가 높지?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로그 디자인: json 으로 일단 저장, nested repeated 정보 어떻게 저장?&lt;/li&gt;
  &lt;li&gt;관리: 용량이 더 커질 수 밖에 없음, 트레이닝 데이터 사이즈를 줄여보는 것도 방법, 콜드데이터는 비용 절감에 주력, 머신러닝에 들어가는 피쳐는 최대한 줄이고 차원을 축소해서 트레이닝 비용을 줄여라.&lt;/li&gt;
  &lt;li&gt;데이터 검증: 관리해야할 로그의 종류와 데이터 타입이 너무 많아짐, 테스트 기반 검증을 진행해야함, 모든 로그 데이터는 검증 로직 테스트를 통과해야 게임 업데이트가 가능 하도록 해야함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론:&lt;/strong&gt; 피쳐가 생명임, 즉 구하려고하는 것과 상관관계가 높은 데이터를 한번 찾아보자&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;협업 방식:&lt;/strong&gt; 처음에는 각 로그별로 정의 문서를 만듬, 나중에는 QA 후 valid 함, 오류나면 커뮤니케이션&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-4&quot;&gt;바닥부터 시작하는 데이터 인프라&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;(전) 레트리카, 변성윤 님&lt;/strong&gt; &lt;a href=&quot;https://zzsza.github.io/&quot;&gt;(블로그)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 대시보드 만들기&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;목표: 팀원들이 조회하고 싶은 데이터를 볼 수 있는 대쉬보드 만들기&lt;/li&gt;
  &lt;li&gt;단순 반복적인 작업을 줄이는 것&lt;/li&gt;
  &lt;li&gt;직접 구현하기 힘들면 오픈소스 툴을 사용하자! (superset)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. 데이터 파이프라인 생성&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;목표 이벤트 레벨까지 데이터를 조회할 수 있는 대시보드&lt;/li&gt;
  &lt;li&gt;문제: 이벤트로그 정리되어 있는가? 테이블 형태가 아니라면 못씀&lt;/li&gt;
  &lt;li&gt;해결: 테이블 형태로 변환&lt;/li&gt;
  &lt;li&gt;문제2: 빅쿼리비용이 너무 크게 나옴&lt;/li&gt;
  &lt;li&gt;해결: 이재광 님(NBT), 데이터를 최대한 줄여라 &amp;gt; flatten table 만들고, 목적에 맞게 데이터 구성(중복하지 않게)&lt;/li&gt;
  &lt;li&gt;기타:
    &lt;ul&gt;
      &lt;li&gt;bigquery vs dataflow&lt;/li&gt;
      &lt;li&gt;task management tool - airflow 도입&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. 음란사진 올라오면?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;처리 프로세스: user &amp;gt; scheduler &amp;gt; docker (NSFW score) &amp;gt; block / unblock&lt;/li&gt;
  &lt;li&gt;사진 말고 비디오, 움짤의 경우? webp 전환후 비디오/움짤의 일부분 만 input으로 집어 넣기&lt;/li&gt;
  &lt;li&gt;콜라보사진: 사람 얼굴 갯수가 많아졌을 때, score가 높음 &amp;gt; 사람 얼굴 갯수로 threshold (정확히 기억이 안남 ㅠㅠ)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. 팁&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;모든것을 만들 필요는 없다.&lt;/li&gt;
  &lt;li&gt;선인의 지혜를 빌리자.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-5&quot;&gt;게임회사 주니어 웹 개발자가 바라본 데이터 분석 이야기&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;넥슨 이준범 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 대시보드&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DB에서 데이터 불러올 때, ORM 굉장히 느리다. (몰랐는데 처음알아따…) 그런데, Pure SQL 썼는데도 느리다 &amp;gt; &lt;strong&gt;“인덱스 타고 있니??”&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DB를 full-scan 하지 않게 만들어야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. 정량적 접근 vs 정성적 접근&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;case1 신규 게임: 데이터가 없는 상황&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;모든 게임에 다 남고 있는 데이터 (접속기록, 계졍명…)&lt;/li&gt;
  &lt;li&gt;게임 유형에 맞는 공통 형식의 로그 -&amp;gt; 쉬움 -&amp;gt; 전부다 잡아낼까?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;case2 게임 특성에 따른 어뷰징&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;어떤 어뷰징 존재한가? 매크로 커뮤니티, 유저들의 신고&lt;/li&gt;
  &lt;li&gt;로그 속에서 패턴 찾아내기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;등등…(나머지 10분 뒤에 내용은 들어도 잘 모르겠어서 정리를 못했다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;그 뒤에 패널 토크에는 어마어마하신 분들이 오신거 같았는데, 못들었다… 다음엔 끝까지 들을 수 있기를~&lt;/p&gt;
</description>
        <pubDate>Sat, 21 Apr 2018 12:47:28 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/04/21/biglittledata.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/04/21/biglittledata.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Word2Vec</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-word2vec&quot;&gt;All about Word Vectors: Word2Vec&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 2 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;p&gt;자연어 처리 공부를 해보신 분이라면 한번쯤 접한 그림이 있을 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_linear-relationships.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“king” - “man” + “woman” = ?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;느낌상 “왕”에서 “남자”라는 속성을 빼주고, “여자”의 속성을 더해주면?&lt;/p&gt;

&lt;p&gt;“queen” 이 나와야할 것 같다. Word Representation은 이런 것을 가능하게 했다.&lt;/p&gt;

&lt;p&gt;이번 시간에는 &lt;strong&gt;Word2vec&lt;/strong&gt; 에 대해서 알아보려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;word2vec&quot;&gt;Word2Vec&lt;/h2&gt;

&lt;p&gt;Word2Vec은 두 가지 알고리즘이 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Skip-grams(SG)
      &lt;ul&gt;
        &lt;li&gt;target 단어를 기반으로 context 단어들을 예측한다. (position independent)&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;Continuous Bag of Words (CBOW)
      &lt;ul&gt;
        &lt;li&gt;context 단어들 집합(bag-of-words context)으로부터 target 단어를 예측한다.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_skipgram1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_skipgram2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_cbow1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_cbow2.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 몇 가지 효율적인 훈련 방법들이 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Two (moderately efficient) training methods (vs Naive Softmax)&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Hierarchical softmax&lt;/li&gt;
    &lt;li&gt;Negative sampling&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://web.stanford.edu/class/cs224n/syllabus.html&quot;&gt;CS224n Lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이번 포스팅에서는 Skip-gram 과 Negative Sampling을 메인으로 소개하겠다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;skip-gram-model-with-naive-softmax&quot;&gt;Skip-gram model with Naive Softmax&lt;/h2&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/1310.4546.pdf&quot;&gt;Distributed Representations of Words and Phrases
and their Compositionality&lt;/a&gt; (Mikolov et al. 2013)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;embedding-look-up&quot;&gt;Embedding Look up&lt;/h3&gt;

&lt;p&gt;모델 설명에 들어가기 앞서 &lt;strong&gt;Embedding Look up&lt;/strong&gt; 이란 것을 알아보자. 이 용어는 이제 여기저기서 많이 나올텐데 알아두면 좋다.&lt;/p&gt;

&lt;p&gt;우리가 하고 싶은 것은 엄청나게 차원이 큰 one-hot vector 를 고정된 작은 차원으로 넣고 싶은 것이다. 어떻게 하면 단어들을 &lt;strong&gt;2-dimension matrix&lt;/strong&gt; 로 표현 할 수 있을까?&lt;/p&gt;

&lt;p&gt;아래 그림의 예를 보자. 8차원 one-hot vector를 3차원으로 만들고 싶다. 그렇다면 $3\times 8$ 행렬을 만들어서 각 column vector 가 하나의 3차원 단어를 표현하면 2-D Matrix 가 되지 않는가? 이 Matrix를 &lt;strong&gt;Embedding Matrix&lt;/strong&gt; 라고 부르기로 하자&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup2.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그렇다면 어떻게 각 단어와 이 Embedding Matrix 를 매칭 시킬수 있을까? 여기서 &lt;strong&gt;내적&lt;/strong&gt; 을 활용하게 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider3&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup4.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup5.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그런데 자세히 보니, one-hot vector의 숫자 $1$ 이 위치한 index 가 Embedding Matrix 의 column vector 의 index 와 같다. 따라서 중복되지 않는 단어사전을 만들고, 각 단어에 대해 index를 메긴 다음, 찾고 싶은 단어를 Embedding Matrix 에서 column vector index 만 &lt;strong&gt;조회(Look up)&lt;/strong&gt; 하면 되는 것이다.&lt;/p&gt;

&lt;ul id=&quot;light-slider4&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup6.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup7.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;코드 예시:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
sentence = &quot;I am going to watch Avengers Infinity War&quot;.split()
embedding_matrix = np.array([[1,2,5,1,9,10,3,4], [5,1,4,1,8,1,2,5], [7,8,1,4,1,6,2,1]])
vocab = {w: i for i, w in enumerate(sentence)}
word = &quot;I&quot;
print(embedding_matrix)
print(&quot;=&quot;*30)
print(&quot;Word:&quot;, word)
print(&quot;Index:&quot;, vocab[word])
print(&quot;Vector:&quot;, embedding_matrix[:, vocab.get(word)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;[[ 1  2  5  1  9 10  3  4]&lt;/p&gt;

  &lt;p&gt;[ 5  1  4  1  8  1  2  5]&lt;/p&gt;

  &lt;p&gt;[ 7  8  1  4  1  6  2  1]]&lt;/p&gt;

  &lt;p&gt;==============================&lt;/p&gt;

  &lt;p&gt;Word: I&lt;/p&gt;

  &lt;p&gt;Index: 0&lt;/p&gt;

  &lt;p&gt;Vector: [1 5 7]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이해가 됐으면 이제 모델로 들어가보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_model_train.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;요약&lt;/h3&gt;

&lt;p&gt;Skip-gram 모델을 한 마디로 설명하자면, 문장의 모든 단어가 한번 씩 중심단어 $c$ 가 되어, $c$ 주변 문맥 단어 $o$ 가 나올 확률을 최대화 하는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;목적&lt;/h3&gt;

&lt;p&gt;각 중심단어 $c$ 에 대해서 아래의 &lt;strong&gt;가능도/우도 (Likelihood)&lt;/strong&gt; 를 구해본다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} p(w_{t+j} | w_t; \theta) \quad \cdots\cdots \quad (1)&lt;/script&gt;

&lt;p&gt;수식을 말로 풀어보자. 각 포지션 $(\prod_{t=1}^{T})$ 의 중심단어 $c$ = $w_t$ 에 대해서, $w_t$ 가 주어졌을 때 다른 문맥단어 $o$ = $w_{t+j}$ 가 나오는 확률 $\big( p (w_{t+j} \vert w_t; \theta) \big)$ 을 가능하게 만드는 $\theta$ 를 구하는 것이다. 단 $j$ 는 윈도우 크기 $m$ 을 넘지 않으며, $0$ 이 될 수 없다.&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;Likelihood&lt;/strong&gt; 를 &lt;strong&gt;최대화&lt;/strong&gt; 하는 것이 우리의 목적이 되겠다.&lt;/p&gt;

&lt;p&gt;그러나 여기서는 우리가 좋아하는 Gradient Descent 를 사용하기 위해서 이 식을 &lt;strong&gt;Negative Log Likelihood&lt;/strong&gt; 로 변형해서 쓰기로한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min J(\theta) = -\dfrac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m,\ j \neq 0} \log p(w_{t+j} | w_t) \quad \cdots\cdots \quad (2)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$(1)$ 식과 $(2)$ 식이 왜 동등한지는 밑에 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt;&lt;/strong&gt; 을 확인하길 바란다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 단어가 등장할 확률 $p(w_{t+j} \vert w_t)$ 는 어떻게 구할 것인가?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Softmax&lt;/strong&gt; 라는 input 값을 0과 1 사이로 만들어 주는 친근한 함수가 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(o|c) = \dfrac{\exp(u_o^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)} \quad \cdots\cdots \quad (3)&lt;/script&gt;

&lt;p&gt;따라서 모델에 있는 모든 파라미터를 $\theta \in \Bbb{R}^{2dV}$ 로 두고, $(2)$ 식을 최적화 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;왜 $\theta \in \Bbb{R}^{2dV}$ 인가?
Center Word 의 Embedding Matrix $W$ Context Words 의 Embedding Matrix $W’$ 두개를 학습 시켜야하기 때문이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;주의 )&lt;/span&gt;&lt;/strong&gt; $W’$ 는 $W$ 의 전치 행렬이 아니라 완전히 새로운 Embedding Matrix 다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;update&quot;&gt;Update&lt;/h3&gt;

&lt;p&gt;Gradient를 통해서 각 파라미터들을 업데이트 하게 된다. $(3)$ 식의 $\log$ 를 취하게 되면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f = \log \dfrac{\exp(u_o^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)}&lt;/script&gt;

&lt;p&gt;이제 $f$ 의 Gradient 를 구해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \dfrac{\partial f}{\partial V_c}
&amp;= \dfrac{\partial }{\partial V_c} \big(\log(\exp(u_o^T V_c)) - \log(\sum_{w=1}^V \exp(u_w^T V_c))\big) \\
&amp;= u_o - \dfrac{1}{\sum_{w=1}^V \exp(u_w^T V_c)}(\sum_{x=1}^V \exp(u_x^T V_c) u_x ) \\
&amp;= u_o - \sum_{x=1}^V \dfrac{\exp(u_x^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)} u_x \\
&amp;= u_o - \sum_{x=1}^V P(x | c) u_x
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$u_o$ : observed word, output context word&lt;/li&gt;
  &lt;li&gt;$P(x\vert c)$: probs context word $x$ given center word $c$&lt;/li&gt;
  &lt;li&gt;$P(x\vert c)u_x$: Expectation of all the context words: likelihood occurance probs $\times$ context vector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;흥미로운 점: &lt;strong&gt;미분 값&lt;/strong&gt; 은 관측된 context word 벡터 $u_o$ 에서 center word $c$ 가 주어졌을 때 나올 수 있는 모든 단어의 기대치를 빼준 다는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;why-mle-is-equivalent-to-minimize-nll&quot;&gt;참고 1: Why MLE is equivalent to minimize NLL?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt; 의 정의:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta|x_1,\cdots,x_n) = f(x_1, \cdots, x_n|\theta) = \prod_{i=1}^n f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;log를 취하게 되면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log L(\theta|x_1,\cdots,x_n) =  \sum_{i=1}^n log f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;MLE(maximum likelihood estimator)&lt;/strong&gt; 의 정의:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MLE} = \underset{\theta}{\arg \max} \sum_{i=1}^n \log f(x_i|\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{x}{\arg \max} (x) = \underset{x}{\arg \min}(-x)&lt;/script&gt;

&lt;p&gt;때문에 우리는 아래의 식을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MLE} = \underset{\theta}{\arg \max} \sum_{i=1}^n \log f(x_i|\theta) = \underset{\theta}{\arg \min} -\sum_{i=1}^n \log f(x_i|\theta)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;왜 log 로 바꾸는 것인가?
    &lt;ol&gt;
      &lt;li&gt;컴퓨터 연산시 곱하기 보다 더하기를 쓰면 &lt;strong&gt;복잡도&lt;/strong&gt; 가 훨씬 줄어들어 계산이 빠르다. ($O(n) \rightarrow O(1)$)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;언더플로우&lt;/strong&gt; 를 방지할수 있다. 언더플로우란 1보다 작은 수를 계속곱하면 0에 가까워져 컴퓨터에서 0 으로 표시되는 현상을 말한다.&lt;/li&gt;
      &lt;li&gt;자연로그함수는 &lt;strong&gt;단조증가함수(monotonic increase function)&lt;/strong&gt; 라서 대소관계가 바뀌지 않는다. 예를 들자면, $5 &amp;lt; 10 \Longleftrightarrow log(5) &amp;lt; log(10)$ 의 관계가 바뀌지 않는 다는 것. 따라서 언제든지 지수를 취해서 다시 원래의 값으로 복귀 가능.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;참고
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/&quot;&gt;why minimize negative log likelihood&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ratsgo.github.io/deep%20learning/2017/09/24/loss/&quot;&gt;(ratsgo 님) 손실함수&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음 시간에는 &lt;strong&gt;Naive Softmax&lt;/strong&gt; 로 훈련 시켰을 때의 단점과 이를 보완 해준 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;Negative Sampling&lt;/span&gt;&lt;/strong&gt; 에 대해서 알아보자.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Apr 2018 10:19:06 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/20/allaboutwv2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/20/allaboutwv2.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Intro</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-intro&quot;&gt;All about Word Vectors: Intro&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 2 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;natural-language-processing&quot;&gt;자연어 처리 (Natural Language Processing)&lt;/h2&gt;
&lt;p&gt;이야기를 하기 앞서서, “언어”를 살펴보자. &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%96%B8%EC%96%B4&quot;&gt;위키백과&lt;/a&gt; 에 따르면 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;언어(言語)에 대한 정의는 여러가지 시도가 있었다. 아래는 그러한 예의 일부이다.&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;사람들이 자신의 머리 속에 있는 생각을 다른 사람에게 나타내는 체계.&lt;/li&gt;
    &lt;li&gt;사물, 행동, 생각, 그리고 상태를 나타내는 체계.&lt;/li&gt;
    &lt;li&gt;사람들이 자신이 가지고 있는 생각을 다른 사람들에게 전달하는 데 사용하는 방법.&lt;/li&gt;
    &lt;li&gt;사람들 사이에 공유되는 의미들의 체계.&lt;/li&gt;
    &lt;li&gt;문법적으로 맞는 말의 집합(절대적이 아님).&lt;/li&gt;
    &lt;li&gt;언어 공동체 내에서 이해될 수 있는 말의 집합.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;위의 예시를 추려내보면 어떤 추상적인 내용을 사람들간의 공통된 약속으로 규정했다는 것이다. 기계한테 어떻게 언어를 처리하도록 알려줘야하나? &lt;strong&gt;자연어 처리&lt;/strong&gt; 는 생각보다 오래된 역사를 가지고 있었다.&lt;/p&gt;

&lt;p&gt;1950년도 이전 부터 자연어를 처리하려는 시도가 꽤 많았던 모양이다. 1954년 조지 타운 실험은 60 개 이상의 러시아어 문장을 영어로 완전 자동 번역하는 작업을 진행했다. 그는 3-5년 안으로 해결 가능하다고 주장했지만 1966 년 ALPAC 보고서에 따르면 실제로 진전이 엄청느려서 연구 자금이 크게 줄었다고 한다. 그리고 최초의 통계 기계 번역 시스템이 개발 된 1980 년대 말까지 기계 번역에 대한 연구는 거의 이루어지지 않았다고 한다. (지금은 너두나두 번역기 만들 수 있지만…)&lt;/p&gt;

&lt;p&gt;또한, 1980년대까지 대부분의 자연어 처리 시스템은 손으로 쓴 복잡한 규칙 세트를 기반으로 했다. 그러나 점차 통계 기반의 자연어 처리 기법이 복잡한 자연어를 모델링 하는데 부상했다. (Reference: &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural-language_processing&quot;&gt;NLP wikipedia&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;또한, 자연어 처리의 기본 가정을 항상 염두하고 공부해야 할 것이다. 좋은 소개글을 링크로 걸어 두었으니 참고하길 바란다.&lt;/p&gt;

&lt;p&gt;참고: ratsgo 님의 블로그 - &lt;a href=&quot;https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/10/frequency/&quot;&gt;idea of statistical semantics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;word-representation&quot;&gt;단어의 표현(Word Representation)&lt;/h2&gt;

&lt;p&gt;어떻게 하면 단어의 “의미”를 표현할 수 있을까?&lt;/p&gt;

&lt;p&gt;가장 간단한 방법은 단어를 종류별로 분류(taxonomy) 하는 것이다.&lt;/p&gt;

&lt;p&gt;영어에는 유명한 &lt;strong&gt;WordNet&lt;/strong&gt; 이라는 프로젝트가 있다. 이는 1985년부터 심리학 교수인 조지 A. 밀러가 지도하는 프린스턴 대학의 인지 과학 연구소에 의해 만들어졌고 유지되고 있다. 기본적으로 상위어(hypernyms) 밑에 동의어(synonym) 세트를 여러개 구성하는 것이다.&lt;/p&gt;

&lt;p&gt;좋긴한데 몇 가지 단점이 있다.&lt;/p&gt;

&lt;p&gt;첫째로, 단어간의 미묘한 차이, 뉘앙스(nuances)를 표현 할 수가 수 없다. 아래의 예를 보자.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from nltk.corpus import wordnet as wn
for synset in wn.synsets(&quot;adept&quot;):
    print(&quot;({})&quot;.format(synset.pos()) + &quot;, &quot;.join([l.name() for l in synset.lemmas()]))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;(n) ace, adept, champion, sensation, maven, mavin, virtuoso, genius, hotshot, star, superstar, whiz, whizz, wizard, wiz
(s) adept, expert, good, practiced, proficient, skillful, skilful&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;“I’m good at deep learning” VS “I’m expert at deep learning” 이 두 문장은 확연히 다른 느낌의 문장이다. 잘하는 것과 전문가의 차이는 사람이 느끼기엔 다르다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;둘째로, 업데이트 비용이 많이 든다. 새로운 단어가 계속 나오면 업데이트 해줘야한다, 즉 구축비용이 쎄다는 것이다.&lt;/p&gt;

&lt;p&gt;셋째로, 사람마다 주관적이기 때문에 명쾌한 기준이 없다.&lt;/p&gt;

&lt;p&gt;마지막으로, 유사도 계산이 어렵다는 점이다. 즉, 같은 상위어에 속해 있는 하위어는 비슷한 것은 알겠는데, 정량적으로 이를 계산할 방법이 없다는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;bag-of-words-representation&quot;&gt;Bag of words representation&lt;/h3&gt;

&lt;p&gt;또다른 방법으로 discrete 된 심볼로 단어를 표현했는데 이를 &lt;strong&gt;one-hot representation&lt;/strong&gt; 라고 하며, 아래와 같이 표현했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;word = [0, 0, 0, 1, 0, 0, 0]&lt;/script&gt;

&lt;p&gt;이러한 방법론을 &lt;strong&gt;Bag of words representation&lt;/strong&gt; 이라 한다. 그러나 이는 두 가지 단점이 있다.&lt;/p&gt;

&lt;p&gt;첫째로, 차원(Dimensionality)의 문제. 단어가 많아 질 수록 벡터가 엄청 길어진다.&lt;/p&gt;

&lt;p&gt;둘째로, 제한적 표현(Localist representation)의 문제. 즉, 단어의 내적의미를 포함하지 않고, 각 단어들이 독립적이다. 예를 들면, “hotel” 과 “motel” 의 유사성을 계산하려고 하면, 0 이 나올 수 밖에 없다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
motel &amp;= \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix} \\
hotel &amp;= \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;hotel \cdot motel^T = 0&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;distributional-similarity-based-representations&quot;&gt;분포 유사성 기반 표현 (Distributional similarity based representations)&lt;/h2&gt;

&lt;p&gt;연구자들은 one-hot vector 와 다른 어떤 유사도를 계산할 수 있는 벡터를 만들고 싶어했다. 따라서 유사도의 정보를 어디서 얻을 수 있을까를 찾기 시작했다. 그리고 어떤 핵심 아이디어를 생각해냈다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;어떤 단어의 “의미”는 그 단어 근처에 자주 출현하는 단어로부터 얻을 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_context.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://web.stanford.edu/class/cs224n/syllabus.html&quot;&gt;CS224n Lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그들은 주변 단어의 정보로 어떤 단어의 의미를 규정하는 시도를 하였고, 이는 modern statistical NLP 에서 많은 각광을 받기 시작했다. 그리고 어떤 단어 $w$ 에 대해서 주변에 나타나는 단어의 집합을 &lt;strong&gt;맥락/문맥(context)&lt;/strong&gt; 이라고 했다.&lt;/p&gt;

&lt;h3 id=&quot;word-vectors&quot;&gt;Word Vectors&lt;/h3&gt;

&lt;p&gt;이전에 0과 1로 채워진 one-hot vector 와 달리 문맥에서 비슷한 단어들을 잘 예측 될 수 있게 단어 타입 별로 촘촘한 벡터(dense vector)를 만든다. 핵심 아이디어는 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Idea:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;We have a large corpus of text&lt;/li&gt;
    &lt;li&gt;Every word in a fixed vocabulary is represented by a vector&lt;/li&gt;
    &lt;li&gt;Go through each &lt;strong&gt;position&lt;/strong&gt; $t$ in the text, which has a &lt;strong&gt;center word&lt;/strong&gt; $c$ and &lt;strong&gt;context (“outside”) words&lt;/strong&gt; $o$&lt;/li&gt;
    &lt;li&gt;Use the similarity of the word vectors for $c$ and $o$ to calculate the probability of $o$ given $c$ (or vice versa)&lt;/li&gt;
    &lt;li&gt;Keep adjusting the word vectors to maximize this probability&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://web.stanford.edu/class/cs224n/syllabus.html&quot;&gt;CS224n Lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;요약하면 방대한 텍스트 데이터를 기반으로, 중심단어 $c$ 가 주어졌을 때, 그 주변단어 $o$ 가 나올 확률 분포를 최대화 하는 것을 구하는 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Word vectors 는 때때로 Word Embeddings, Word Representation 이라고 불린다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이렇게 해서 나온 알고리즘이 &lt;span style=&quot;color: #e87d7d&quot;&gt;“Word2Vec”&lt;/span&gt; 이며, 여기서 잠깐 끊고 다음 글에서 소개하도록 한다.&lt;/p&gt;
</description>
        <pubDate>Thu, 19 Apr 2018 16:41:36 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/19/allaboutwv1.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/19/allaboutwv1.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Bidirectional LSTM + self Attention Model</title>
        <description>&lt;h1 id=&quot;naver-sentiment-movie-corpus-classification&quot;&gt;Naver Sentiment Movie Corpus Classification&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;네이버 영화 감성분류 with Bidirectional LSTM + Self Attention&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;목표&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;영화 리뷰를 통해 긍정인지 부정인지 분류하는 문제 (Many-to-One)&lt;/li&gt;
  &lt;li&gt;사용한 모델: Bidirectional LSTM with Self Attention Model&lt;/li&gt;
  &lt;li&gt;이번 글은 논문과 제가 분석한 모델의 중요 요소를 곁들여 쓴 글입니다.&lt;/li&gt;
  &lt;li&gt;GitHub Code Link: &lt;a href=&quot;https://github.com/simonjisu/nsmc_study&quot;&gt;nsmc_study&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference Paper: &lt;a href=&quot;https://arxiv.org/pdf/1703.03130.pdf&quot;&gt;A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;모델 핵심 부분 설명&lt;/h2&gt;

&lt;p&gt;그림과 수식을 함께 보면 이해하기 쉽다&lt;/p&gt;

&lt;p&gt;어떤 $n$ 개의 토근으로 이루어진 하나의 문장이 있다고 생각해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S = (w_1, w_2, \cdots, w_n)\qquad\qquad (1)&lt;/script&gt;

&lt;p&gt;여기서 $w_i$ 는 one-hot 인코딩된 단어가 아닌, $d$ 차원에 임베딩된 문장에서 $i$ 번째 단어다.&lt;/p&gt;

&lt;p&gt;따라서 $S$ 는 단어 벡터들을 concat 한 $n \times d$ 형태를 가지는 매트릭스다.&lt;/p&gt;

&lt;p&gt;문장 $S$ 는 각기 다른 문장과는 독립적이다. (하나의 문장이 하나의 평점과 세트로 생각하면 된다.) 하나의 문장에서 단어들 간의 관계를 알기 위해서 우리는 bidirectional LSTM 으로 하나의 문장을 처리하게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\overrightarrow{h_t} &amp;= \overrightarrow{LSTM}(w_t, \overrightarrow{h_{t-1}})\qquad\qquad (2) \\
\overleftarrow{h_t} &amp;= \overleftarrow{LSTM}(w_t, \overleftarrow{h_{t-1}})\qquad\qquad (3)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;그후 우리는 각각의 $\overrightarrow{h_t}$ 과 $\overleftarrow{h_t}$ 를 concatenate 하여 하나의 히든 state $h_t$ 를 얻게 된다. 각 unidirectional LSTM(한 쪽 방향 LSTM)의 히든 유닛 크기를 $u$ 라고 하자. 조금 간단하게 표현하기 위해서 모든 $n$ 개의 $h_t$ 들을 $H$ 라고 하며, $n \times 2u$ 의 크기를 가진다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H = (h_1, h_2, \cdots, h_n) \qquad\qquad (4)&lt;/script&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention0.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention4.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우리의 목적은 길이가 변화하는 문장을 어떤 &lt;strong&gt;고정된 크기&lt;/strong&gt; 의 임베딩으로 인코딩 하는 것이다. 이 목적을 달성하기 위해서 $H$ 와 attention 매커니즘이 요구되는 일종의 선형결합을 선택하게 된다. 즉, 아래와 같은 식과 $H$ 를 토대로, 어떤 벡터 $a$ 를 얻게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = softmax(w_{s2} \tanh (W_{s1}H^T)) \qquad\qquad (5)&lt;/script&gt;

&lt;p&gt;여기서 $W_{s1}$ 는 $d_a \times 2u$ 형태를 가진 매트릭스, 그리고 $w_{s2}$ 는 $d_a$ 사이즈를 가진 벡터다. $d_a$ 는 하이퍼파라미터(hyperparameter)로 우리가 정할 수 잇는 변수다. $H$ 의 크기도 $n \times 2u$ 이기 때문에, 벡터 $a$ 는 $n$ 의 크기를 가진다. 또한 $softmax()$ 함수는 모든 weight들의 합을 1로 만들어 준다.&lt;/p&gt;

&lt;p&gt;그후 우리는 LSTM 의 히든상태들의 집합인 $H$ 를 주어진 $a$ 로 곱해서 한 문장을 임베딩한 벡터 $m$ 을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이 벡터 $m$ 은 학습시 한 문장에서 어떤 단어를 중심적으로 보았는지 알 수 있다. 예를 들어 어떤 연관된 단어나 구문 등등.&lt;/p&gt;

&lt;p&gt;문장과 단어의 관계로 추가 설명하자면 아래와 같다.&lt;/p&gt;

&lt;p&gt;각 단어를 input으로 받은 hidden 상태의 노드들은 단어를 통과해서 각 단어의 숨겨진 특성을 대표하고 있다. 학습 시 Task 에 따라 다르겠지만, 분류라고 가정한다면 분류에 도움이 되는 히든 상태는 높은 값을 가지게 될 것이며, 이를 어떤 선형 변환 과정을 거쳐 softmax 취한다는 것은 한 문장에서 분류에 도움이 된 근거 단어 혹은 중요 단어의 확률을 구한다는 것이 된다. (그래서 attention 이라고 하는 것 같다.) 따라서 이는 한 문장에서 &lt;strong&gt;의미적인(semantic)&lt;/strong&gt; 부분을 나타내고 있다고 할 수 있다.&lt;/p&gt;

&lt;p&gt;이 확률 $a$ 를 기존의 hidden 상태와 곱해서 의미부분을 조금더 강조하게 되는 벡터 $m$ 을 구했다고 보면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention5.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention6.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;하지만 한 문장 내에서 중요한 부분 혹은 의미가 있는 부분은 여러군데 일 수가 있다. (여러 의미가 하나의 문장을 구성한다.) 특히 긴 문장일 수록 그렇다. 예를 들어 “아이언맨과 캡틴아메리카” 면 “과”로 이어진, “아이언맨”, “캡틴아메리카” 두 단어는 중요한 의미가 있는 단어 일 수 있다. 따라서 한 문장에서 의미가 있는 부분을 나타내려면 $m$ 이란 벡터를 여러 번 수행해서 문장의 다른 부분까지 커버해야 한다. 이는 우리가 &lt;strong&gt;attention&lt;/strong&gt; 을 &lt;strong&gt;여러번(hops)&lt;/strong&gt; 하게 되는 이유다.&lt;/p&gt;

&lt;p&gt;따라서, 문장에서 우리가 정하는 어떤 수 $r$ 번의 다른 부분을 추출 해낸다고 하면, 기존의 $w_{s2}$ 는 $r \times d_a$ 크기를 가진 $W_{s2}$ 라는 매트릭스로 확장된다. 이에따라 기존에 $a$ 벡터도 $r$ 번을 수행해 concatenate 한 $r \times n$ 크기의 매트릭스 $A$ 가 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A=softmax(W_{s2}tanh(W_{s1}H^T))  \qquad\qquad (6)&lt;/script&gt;

&lt;p&gt;여기서 $softmax()$ 는 input $W_{s2}tanh(W_{s1}H^T)$ 의 2번째 차원을 기준으로 softmax 하게 된다. (즉, 각 row 별로 softmax 해줌)&lt;/p&gt;

&lt;p&gt;사실 $(6)$ 번 수식은 bias 가 없는 2-Layers MLP 로 간주할 수도 있다.&lt;/p&gt;

&lt;p&gt;위에 식에 따라 임베딩된 벡터 $m$ 도 $r \times 2u$ 크기의 매트릭스 $M$ 로 확장된다. 가중치를 담은 매트릭스 $A(r \times n)$ 와 LSTM 의 히든 상태들인 $H(n \times 2u)$를 곱해서 새로운 임베딩 매트릭스 $M$ 을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M=AH  \qquad\qquad (7)&lt;/script&gt;

&lt;p&gt;마지막으로 $M$을 Fully Connected MLP 에 넣어서 하고 싶은 분류를 하면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider3&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention7.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention8.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention9.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;penalization-term&quot;&gt;Penalization Term&lt;/h3&gt;

&lt;p&gt;임베딩된 매트릭스 $M$ 은 $r$ hops 동안 계속해서 같은 유사도 벡터 $a$ 를 곱하게 되면 &lt;strong&gt;중복 문제(redundancy problems)&lt;/strong&gt; 가 생길 수 있다. 즉, 같은 단어 혹은 구문만 계속해서 attention 하게 되는 문제다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/penal.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;그림: 왼쪽(a)은 패널티를 안준 것, 오른쪽(b) 는 준것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, $r$ hops 동안 weight 벡터들의 합을 다양성을 높히는 일종의 패널티를 줘야한다.&lt;/p&gt;

&lt;p&gt;제일 좋은 방법은 $r$ hops 안에 있는 아무 두 벡터 간의 &lt;strong&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0&quot;&gt;쿨백-라이블러 발산 (Kullback–Leibler divergence)&lt;/a&gt;&lt;/strong&gt; 함수를 쓰는 것이다. 매트릭스 $A$ 의 각각의 행(row) 벡터들이 하나의 의미(semantic)를 가지는 단어 혹은 구문이 될 확률분포이기 때문에, 다양한 분포에서 나오는 것은 우리의 목적이 된다. (문장은 여러 단어/구문으로 구성되어 있기때문) 그러므로 KL divergence 값을 &lt;strong&gt;최대&lt;/strong&gt; 로 만들면 중복 문제는 해결된다.&lt;/p&gt;

&lt;p&gt;그러나 논문에서는 위와 같은 경우에 불안정(unstable) 한다는 것을 알아냈다. 논문 저자들은 어림짐작해 보았을 때, KL divergence 를 최대화 할때(보통의 경우 KLD를 최소화 하는 것을 한다.), 매트릭스 $A$ 구하는 단계에서 softmax 시 많은 값들이 0 이거나 아주 작은 값이라서 불안정한 학습을 야기했을 가능성이 있다는 것이다.&lt;/p&gt;

&lt;p&gt;따라서, 논문에서는 매트릭스의 &lt;strong&gt;&lt;a href=&quot;http://mathworld.wolfram.com/FrobeniusNorm.html&quot;&gt;Frobenius norm&lt;/a&gt;&lt;/strong&gt; 을 쓰게 되는데 아래와 같다. ($Norm_2$와 비슷해 보이지만 다르다)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P ={ {\|AA^T - I\|}_F}^2&lt;/script&gt;

&lt;p&gt;이 패널티 값과 기존의 Loss 와 같이 최소화 하는 방향으로 간다. 이 패널티의 뜻은 무엇일까?&lt;/p&gt;

&lt;p&gt;두 개의 다른 유사도 벡터의 합 $a^{i}$ 과 $a^{j}$ 를 생각해보자. Softmax 로 인해서 모든 $a$ 값들의 합은 1이 될 것이다. 따라서 이들을 일종의 이산 확률분포 (discrete probability distribution)에서 나오는 확률질량 함수로 간주할 수 있다.&lt;/p&gt;

&lt;p&gt;매트릭스 $AA^T$ 중, 모든 비대각 $a_{ij}\ (i \neq j)$ 원소에 대해서, 원소의 곱(elementary product)은 아래 두개의 분포를 가지고 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
0&lt; a_{ij} = \sum_{k=1}^{n} a_k^i a_k^j &lt;1 %]]&gt;&lt;/script&gt;

&lt;p&gt;여기서 $a_k^i, a_k^j$ 는 각각 $a^i, a^j$ 의 k 번째 원소다. 제일 극단적인 경우를 생각해보면, $a^i$ 와 $a^j$ 가 일치하지 않다면 (혹은 다른 분포를 나타내고 있다면) 0 이 되고, 완전이 일치해서 같은 단어 혹은 구문을 이야기 하고 있다면 (혹은 같은 분포를 나타내고 있다면) 1 에서 최대값을 가지게 될 것이다.&lt;/p&gt;

&lt;p&gt;따라서, $AA^T$ 의 대각 행렬(같은 단어 혹은 구문)을 대략 1 이 되게 강제한다. $I$ (Identity) 매트릭스를 빼줌으로써 달성하는데, 이는 자기 자신을 제외한 각기 다른 $a^i$ 간 원소들의 합인 $a_{ij}$ 들이 0 으로 최소화되게 만들어 버린다. 즉, 최대한 $a^i$ 간의 분포가 일치하지 않게 만드려고 노력하는 것이다. 이렇게 함으로써 $r$ 번의 hops 마다 각각 다른 단어에 집중하게 만드는 효과를 낼 수 있어서, 중복문제를 해결 할 수가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;네이버 영화 리뷰 테스트 결과 및 시각화&lt;/h2&gt;
&lt;p&gt;총 150000 개의 Train Set과 50000 개의 Test Set 으로 진행했고, 모델에서는 hyperparameter가 많기 때문에 몇 가지 실험을 진행 했다.&lt;/p&gt;

&lt;p&gt;간단한 실험을 위해서 사전에 단어들을 word2vec 으로 학습시키지 않고, mecab 으로 tokenizing 만해서 임베딩 시켰다. (실험을 안해봐서 사실 크게 상관있나 모르겠다. 나중에 여러가지로 실험해볼 예정)&lt;/p&gt;

&lt;p&gt;내가 주로 건드린건 LSTM 에서의 &lt;strong&gt;hidden layer의 갯수&lt;/strong&gt; 와 hops 인 &lt;strong&gt;$r$&lt;/strong&gt; 을 바꾸어 보았다.&lt;/p&gt;

&lt;h3 id=&quot;model-1-1--hidden-layer--5-hops&quot;&gt;model 1: 1 개의 Hidden Layer 와 5번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_1.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-2-1--hidden-layer--20-hops&quot;&gt;model 2: 1 개의 Hidden Layer 와 20번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hops 가 많아지면 긍정/부정을 판단하게 되는 근거도 많아지고, 모델의 정확도도 향상되는 것을 2번에서 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;model-3-3--hidden-layer--5-hops&quot;&gt;model 3: 3 개의 Hidden Layer 와 5번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3번째 모델은 조금 이상하다고 느껴진 것이 있다. 그림을 보면 기계가 문장의 앞뒤만 보고 리뷰가 긍정인지 부정인지 판단했다는 것이다. 그림만 보면 과최적화된 느낌? 정확히 각 층의 layer 값을 보지는 못했지만, 층이 깊어 질 수록 기계가 이전 단계의 layer 에서 추출한 특징들로 학습해서 긍부정을 판단 했을 가능성이 있다. 점수는 높게 나왔으나 사람이 판단하기에는 부적절한 모델&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;향후 해볼 수 있는 과제들&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;전처리 단계에서 임베딩시 다양한 임베딩을 해볼 수 있을 것 같다. 예를 들어 word2vec으로 미리 선학습 후에 만든다던지, 아니면 N-hot 인코딩 (단어 원형 + 품사 + 어미) 등등 시도해볼 수 있는 것은 많다.&lt;/li&gt;
  &lt;li&gt;LSTM Cell 로 구현&lt;/li&gt;
  &lt;li&gt;이와 연관은 좀 덜하지만, CNN으로 분류하는 것과 비교해 성능이 더 잘나올지? &lt;strong&gt;김윤&lt;/strong&gt; 님의 논문 참고 : &lt;a href=&quot;http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf&quot;&gt;링크 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;공부에 도움 주신 분들 및 공부에 도움 되었던 싸이트:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;김성동님: https://github.com/DSKSD&lt;/li&gt;
  &lt;li&gt;같은 논문을 Tensorflow로 구현하신 flrngel님: https://github.com/flrngel/Self-Attentive-tensorflow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;감사합니다.&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Apr 2018 01:01:13 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/04/04/nsmcbidreclstmselfattn.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/04/04/nsmcbidreclstmselfattn.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>Naver AI Colloquium 2018</title>
        <description>&lt;h1 id=&quot;naver-ai-colloquium-2018--&quot;&gt;Naver AI Colloquium 2018 참석 후기&lt;/h1&gt;
&lt;p&gt;&lt;del&gt;+ 클로바스피커 후기&lt;/del&gt;&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;개요&lt;/h2&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/logo.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment3.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://naveraiconf.naver.com/&quot;&gt;네이버 AI 콜로키움&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;운 좋게 기회가 되서 &lt;a href=&quot;http://naveraiconf.naver.com/&quot;&gt;&lt;strong&gt;Naver AI Colloquium 2018&lt;/strong&gt;&lt;/a&gt; 에 다녀왔다.
싸이트에 접속해서 프로그램 표를 자세히 보면 알겠지만, 이번 콜로키움에서는 크게 4가지 주제를 다뤘다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;언어 분야(Search &amp;amp; Natural Language)&lt;/li&gt;
  &lt;li&gt;비전 분야(Computer Vision, Mobility&amp;amp;Location Intelligence)&lt;/li&gt;
  &lt;li&gt;추천 분야(Recommendation)&lt;/li&gt;
  &lt;li&gt;데이터 엔지니어 분야(AI Algorithm, System&amp;amp;Platform)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;나는 언어 분야, 즉 자연어을 다루는 쪽에 관심이 많아서 Track A 만 거의 듣고, 추천 분야 하나 정도 들었다. 각 강의에서 다루는 내용은 차후 하나씩 정리해서 올릴 예정이다.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/naveraicol/professorsungkim.JPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;“모두의 딥러닝”으로 유명하신 Sung Kim 교수님!! (문제가 되면 사진 내리겠습니다~ 댓글 달아주세요)&lt;/p&gt;

&lt;p&gt;인상 깊었던 세션과 그 이유를 몇개 꼽자면, &lt;del&gt;어쩌다보니 다 네이버 직원분들이 발표하신거네ㅎㅎ&lt;/del&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Semantic Matching Model - 김선훈 (NAVER)
시멘틱 매칭의 필요성:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;키워드(단어기반)&lt;/strong&gt; 매칭보다는 &lt;strong&gt;시멘틱(의미기반)&lt;/strong&gt; 매칭이 다양한 표현과 오타를 커버할 수 있는 가능성이 높다.&lt;/li&gt;
      &lt;li&gt;Sementic Gap: 인간이해와 기계이해의 차이, 이것을 줄이는게 큰 과제&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Speed Reading - 서민준 (NAVER)
    &lt;ul&gt;
      &lt;li&gt;Skim-RNN: “속독”에서 나온 아이디어, 중요하지 않은 단어는 적게 업데이트!&lt;/li&gt;
      &lt;li&gt;Big RNN 과 Small RNN 의 결정 짓는 Decision Function&lt;/li&gt;
      &lt;li&gt;Layer를 쌓으면 중요한 정보를 캐치 (마치 글을 두번째 읽을 때는 주요 단어만 보게 되는 것과 같음)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hybrid Natural Language Understanding 모델 - 김경덕 (NAVER)
    &lt;ul&gt;
      &lt;li&gt;문제 정의의 중요성: 잘해야 문제를 해결하기 쉽고 명확하다.&lt;/li&gt;
      &lt;li&gt;팬턴 기반 검색 NLU + 데이터 통계 기반 NLU, 뭐든지 하나만 고르는 것은 아니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;자기학습 기반 챗봇(발표세션은 아님)
    &lt;ul&gt;
      &lt;li&gt;챗봇의 전체 과정:
  Query $\rightarrow$ 언어적 특징 추출 $\rightarrow$ 쿼리 분류기(대화여부 및 도메인 인지) $\rightarrow$ 여러 모델로 부터 답변 생성 $\rightarrow$ Answer&lt;/li&gt;
      &lt;li&gt;N-hot representation: 토큰원형 + 품사태깅 + 어미&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 정도인 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;클로바 스피커(프렌즈) 후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/naveraicol/speaker.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;어쩌다 운좋게 경품에 당첨 되서 받았다. ㅎㅎㅎ 감사합니다.
이놈…생각보다 귀엽다. 아직까지 일본의 프렌즈보다 기능이 덜 있는 것 같다. 라인으로 메세지 보내는 기능 시도해보았는데, 안되드라…&lt;/p&gt;

&lt;p&gt;영상링크: &lt;a href=&quot;https://youtu.be/lK-9yDoHsZ8&quot;&gt;【公式】Clova Friendsができること &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;아무튼 아직 개선할 사항이 많다. 예를 들어, “레미제라블 Do you hear the people sing? 노래틀어줘”라고 말하면, 말씀하신 사항을 찾지 못했다고 대답한다.
어떤 세션에서 들었던것 같은데, 내 생각에는 레미제라블, 노래틀어줘는 노래틀어주는 분야로 의도로 분류되고, 나머지 영어는 번역하는 의도로 분류된 것 같다. (스피커에 말한 내용을 볼 수 있는데, 음성인식을 진짜 제대로 잘 된다. 괜히 1위라고 말한게 아닌듯 ㅋㅋ)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;내년에 또 하게되면 참가하고싶다~&lt;/p&gt;

&lt;p&gt;관심 있었던 세션들을 정리하면 바로 올리겠다.&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Mar 2018 20:14:21 +0900</pubDate>
        <link>http://simonjisu.github.io/naverai2018/2018/03/30/naveraicolloquium2018.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/naverai2018/2018/03/30/naveraicolloquium2018.html</guid>
        
        
        <category>NaverAI2018</category>
        
      </item>
    
      <item>
        <title>RNN &amp; LSTM - 2: Numpy with RNN</title>
        <description>&lt;h1 id=&quot;rnn--lstm----2&quot;&gt;자세하게 설명한 RNN 과 LSTM 시리즈 - 2&lt;/h1&gt;

&lt;h2 id=&quot;numpy--rnn-&quot;&gt;Numpy 로 RNN 만들어보기&lt;/h2&gt;
&lt;p&gt;모든 코드는 Github: &lt;a href=&quot;https://github.com/simonjisu/NUMPYwithNN&quot;&gt;NUMPYwithNN&lt;/a&gt; 에 올려져 있습니다.&lt;/p&gt;

&lt;p&gt;Jupyter Notebook 으로 전체과정 보기: &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/NUMPYwithNN/blob/master/Notebook/Character_Predicting_RNN.ipynb&quot;&gt;링크 &lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;rnn-forward--backward--&quot;&gt;RNN Forward 와 Backward의 계산 그래프&lt;/h2&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_forward0.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_forward1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_forward2.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward0.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward4.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward5.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;backward에서 잊지 말아야 할 부분은 $t=T$일 때(마지막 Step일 때) $d h_T$는 0으로 초기화 되며, 구해진 $d h_{t-1}^{raw}$ 가 이 다음 역전파로 들어가기 전에 이전 단계로 부터 얻은 $dh_{t-1}$ 와 더해져 계산한다는 점이다. 그 이유는 forward 시 다음 step으로 hidden 값($h_t$)을 전파하기 때문이라는 것을 잊지 말자.&lt;/p&gt;

&lt;p&gt;위 그림은 &lt;a href=&quot;https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/&quot;&gt;ratsgo’s blog&lt;/a&gt; 님의 포스트에서 많은 참조를 하고 새로 만들었음을 밝힙니다.&lt;/p&gt;

&lt;h3 id=&quot;bptt--&quot;&gt;참고) BPTT 수식적 이해&lt;/h3&gt;
&lt;p&gt;$tanh$의 미분을 $f(x) = 1 - tanh^2(x)$ 라고 하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&amp;= \dfrac{\partial L}{\partial y_t} \dfrac{\partial y_t}{\partial h_0} + \dfrac{\partial L}{\partial y_{t-1}} \dfrac{\partial y_{t-1}}{\partial h_0} \cdots + \dfrac{\partial L}{\partial y_1} \dfrac{\partial y_1}{\partial h_0}\\
&amp;= \dfrac{\partial L}{\partial y_t} \dfrac{\partial y_t}{\partial h_t} \dfrac{\partial h_t}{\partial a_t} \dfrac{\partial a_t}{\partial h_{t-1}} \cdots \dfrac{\partial a_1}{\partial h_{0}} + \cdots +
\dfrac{\partial L}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial a_1} \dfrac{\partial a_1}{\partial h_0} \\
&amp;= W_{hy} dy_t W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_1) + \cdots + W_{hy} dy_2 W_{hh} f(a_2) W_{hh} f(a_1) + W_{hy} dy_1 W_{hh} f(a_1) \\
&amp;= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{i} \prod_{j=1}^{i} f(a_j) \Big)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위 식을 위에 있는 그림대로 그려보자, 뒤에 $W_{hh} f(a_1)$ 부처 차근차근 묶어서 아래의 식을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\dfrac{\partial L}{\partial h_{0}}
&amp;= W_{hh} f(a_1) \bigg( W_{hy} dy_t  W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_2) + \cdots + W_{hy} dy_2 W_{hh} f(a_2) + W_{hy} dy_1 \bigg) \\
&amp;= W_{hh} f(a_1) \bigg( W_{hh} f(a_2) \Big( W_{hy} dy_t W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_3) + \cdots + W_{hy} dy_2 \Big) + W_{hy} dy_1 \bigg) \\
&amp;= W_{hh} f(a_1) \bigg( W_{hh} f(a_2) \Big( \cdots W_{hh} f(a_{t-1}) \big( \underbrace{W_{hh} f(a_t) (\underbrace{ W_{hy} dy_t }_{dh_t^{raw}} + 0)}_{dh_{t-1}} + \underbrace{ W_{hy} dy_{t-1} }_{dh_{t-1}^{raw}} \big) \cdots + W_{hy} dy_2 \Big) + W_{hy} dy_1 \bigg) \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위에 그림과 비교해보면 이런 식으로 계속 더해진다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;backpropagation-through-time-bptt-&quot;&gt;BackPropagation Through Time (BPTT) 구현&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Single_Layer_RNN&lt;/strong&gt; 의 코드는 &lt;a href=&quot;https://github.com/simonjisu/NUMPYwithNN/blob/master/common/SimpleRNN.py&quot;&gt;여기&lt;/a&gt;에 있습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Layer&lt;/strong&gt; 의 구현을 참고하려면 Github의 &lt;a href=&quot;https://github.com/simonjisu/NUMPYwithNN/blob/master/common/layers.py&quot;&gt;common/layers&lt;/a&gt; 참고하세요!&lt;/li&gt;
  &lt;li&gt;처음 Layer를 짜보시는 분은 &lt;a href=&quot;https://simonjisu.github.io/deeplearning/2017/12/07/numpywithnn_1.html&quot;&gt;Numpy로 짜보는 Neural Network Basic&lt;/a&gt; 시리즈를 참고하세요!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우선 미분한 값의 합을 구하기 위해 각각 Layer의 파라미터와같은 형태(shape)로 만들어 준다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _params_summation_init(self):
    self.params_summ = {}
    self.params_summ['W_xh'] = np.zeros_like(self.params['W_xh'])
    self.params_summ['W_hh'] = np.zeros_like(self.params['W_hh'])
    self.params_summ['W_hy'] = np.zeros_like(self.params['W_hy'])
    self.params_summ['b_h'] = np.zeros_like(self.params['b_h'])
    self.params_summ['b_y'] = np.zeros_like(self.params['b_y'])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;또한, $dh_T$ 를 0으로 초기화 한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dht = np.zeros_like(self.h0)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그후에 총 길이 $T$의 역순으로 각 Layer 의 Back Propagation 을 진행한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for t in np.arange(self.T)[::-1]:
    dout = self.last_layers[t].backward()
    dht_raw = self.layers['Affine_hy'][t].backward(dout)
    dat = self.layers['Activation'][t].backward(dht_raw + dht)
    dht = self.layers['Affine_hh'][t].backward(dat)
    dx = self.layers['Affine_xh'][t].backward(dat)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;또한, 파라미터 $W$ 와 $b$ 의 합도 같이 구해준다. 그 이유는 전편에서 설명되어 있지만, 다시 한번 이야기 하자면, 최종 Loss Function은 각 Output Loss의 평균이기 때문에, 각 Output 마다 파라미터들을 summation 하는 과정이 있다. (평균을 구할때 우선 summation을 한다는 것을 잊지 말자.)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.params_summ['W_xh'] += self.layers['Affine_xh'][t].dW
self.params_summ['W_hh'] += self.layers['Affine_hh'][t].dW
self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
self.params_summ['b_h'] += self.layers['Affine_hh'][t].db
self.params_summ['b_y'] += self.layers['Affine_hy'][t].db
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;전체 Backward 과정&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def backward(self):
    # BPTT
    self._params_summation_init()
    dht = np.zeros_like(self.h0)

    for t in np.arange(self.T)[::-1]:
        dout = self.last_layers[t].backward()
        dht_raw = self.layers['Affine_hy'][t].backward(dout)
        dat = self.layers['Activation'][t].backward(dht_raw + dht)
        dht = self.layers['Affine_hh'][t].backward(dat)
        dx = self.layers['Affine_xh'][t].backward(dat)

        self.params_summ['W_xh'] += self.layers['Affine_xh'][t].dW
        self.params_summ['W_hh'] += self.layers['Affine_hh'][t].dW
        self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
        self.params_summ['b_h'] += self.layers['Affine_hh'][t].db
        self.params_summ['b_y'] += self.layers['Affine_hy'][t].db
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;truncate-backpropagation-through-time-t-bptt&quot;&gt;Truncate BackPropagation Through Time (T-BPTT)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Truncate BackPropagation Through Time (T-BPTT)&lt;/strong&gt; 은 기존 BPTT 에서 과거 모든 미분값을 참조하는 대신 고정된 길이로 참조 할 수 있도록 만든 알고리즘이다.&lt;/p&gt;

&lt;p&gt;왜 이런것을 만들었을 까? BPTT 알고리즘의 미분식을 다시 생각해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&amp;= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{i} \prod_{j=1}^{i} f(a_j) \Big)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위에서 설명했지만, BPTT 과정에서 Time-step이 길어질 수록, 많은 양의 곱셈이 이루어 진다. 계산량을 줄이기 위해서 이런 알고리즘이 나왔을 수 있다.&lt;/p&gt;

&lt;p&gt;다른 접근 방법으로, 학습하고 싶은 Sequence의 일정 길이만큼만 과거를 참조하고 싶기 때문일 수도 있다.&lt;/p&gt;

&lt;p&gt;예를 들어 “I live in Seoul. (중략) I am Korean.” 이라는 문장을 생각해보자. 학습 데이터는 아래와 같을 것이다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[&quot;I&quot;, &quot;live&quot;, &quot;in&quot;, &quot;Seoul&quot;, &quot;.&quot;, (중략), &quot;I&quot;, &quot;am&quot;, &quot;Korean&quot;, &quot;.&quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Forward 할때는 순차적으로 들어갈텐데, Backward 할때는 데이터의 역순으로(“.”, “Korean”) 진행될 것이다. 그러나 내가 한국인이라는 것은 내가 서울에 살고 있기 때문인데, 굳이 앞단의 “I”, “live”, “in” 까지 참조할 필요는 없는 것이다. 그렇다면 위에 식은 아래와 같이 변할 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&amp;= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{k} \prod_{j=k}^{t} f(a_j) \Big) \\
where \quad k &amp;= \max(1, t - truncate)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;t-bptt-&quot;&gt;T-BPTT 구현&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/normal_truncate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림 출처: &lt;a href=&quot;https://r2rt.com/styles-of-truncated-backpropagation.html&quot;&gt;r2rt.com&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def backward_truncate(self):
    # TBPTT
    self._params_summation_init()
    dht = np.zeros_like(self.h0)

    for t in np.arange(self.T)[::-1]:
        dout = self.last_layers[t].backward()
        dht_raw = self.layers['Affine_hy'][t].backward(dout)
        self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
        self.params_summ['b_y'] += self.layers['Affine_hy'][t].db

        for bptt_step in np.arange(max(0, t + 1 - self.bptt_truncate), t + 1)[::-1]:
            dat = self.layers['Activation'][bptt_step].backward(dht_raw + dht)
            dht = self.layers['Affine_hh'][bptt_step].backward(dat)  # dh_t-1
            dx = self.layers['Affine_xh'][bptt_step].backward(dat)  # dx
            self.params_summ['W_xh'] += self.layers['Affine_xh'][bptt_step].dW
            self.params_summ['W_hh'] += self.layers['Affine_hh'][bptt_step].dW
            self.params_summ['b_h'] += self.layers['Affine_hh'][bptt_step].db
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그러나 Tensorflow 에서는 아래와 같이 구현한다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/tensorflow_truncate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림 출처: &lt;a href=&quot;https://r2rt.com/styles-of-truncated-backpropagation.html&quot;&gt;r2rt.com&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;실습&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;목적&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;“hello world! nice to meet you! i love iron-man”&lt;/strong&gt; 을 RNN 으로 학습시키기.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Input&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Output&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;h&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;e&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;e&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;l&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;l&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;l&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;l&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;o&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;⋮&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;⋮&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;⋮&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;a&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;n&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-2&quot;&gt;데이터 및 우리가 만든 패키지 준비&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
from common.SimpleRNN import Single_layer_RNN
from common.optimizer import Adam
from common.train_graph import loss_graph
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = 'hello world! nice to meet you! i love iron-man'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;인코딩 클래스 하나를 만들어서 문자열을 one-hot 인코딩 해준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class chr_coding(object):
    def __init__(self):
        self._dict = None
        self._one_hot_matrix = None
        self._dict_reversed = None

    def fit(self, x):
        if isinstance(x, str):
            x = list(x)

        self._one_hot_matrix = np.eye(len(set(x)))
        self._dict = {d: i for i, d in enumerate(list(set(x)))}
        self._dict_reversed = {v: k for k, v in self._dict.items()}

    def encode(self, x):
        encoded_data = np.array([self._one_hot_matrix[self._dict[d]] for d in x])
        return encoded_data

    def decode(self, x, probs=None):
        if probs is None:
            decoded_data = self._dict_reversed[x]
        else:
            decoded_data = self._dict_reversed[np.argmax(probs)]
        return decoded_data
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;encoder = chr_coding()
encoder.fit(x)
one_hot_data = encoder.encode(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;학습 데이터 x, y를 지정해준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_x = one_hot_data[:-1]
train_y = one_hot_data[1:]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;hyperparameters&quot;&gt;hyperparameters&lt;/h3&gt;

&lt;p&gt;INPUT_SIZE 와 OUTPUT_SIZE 는 중복되지 않는 문자열 사전의 길이라는 것을 잊지 말자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NUM_EPOCHS = 600
PRINT_EPOCH = 30
INPUT_SIZE = one_hot_data.shape[1]
OUTPUT_SIZE = one_hot_data.shape[1]
HIDDEN_SIZE = 20
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;accuracy--train-&quot;&gt;필요한 함수 설정: accuracy 와 train 함수&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def get_accuracy(x, test_string):
    bool_ = np.array(list(x))[1:] == np.array(list(test_string))[1:]
    return bool_.sum() / len(bool_)

def train(rnn, optim, print_epoch=20):
    total_loss_list = []
    total_acc_list = []
    for epoch in range(NUM_EPOCHS):
        test_string = 'h'
        # forward
        total_loss = rnn.loss(train_x, train_y)

        # backward
        rnn.backward()

        optim.update(rnn.params, rnn.params_summ)

        # test string
        predicted_idx = rnn.predict(train_x)
        for idx in predicted_idx:
            test_string += encoder.decode(idx)

        # get accuracy
        acc = get_accuracy(x, test_string)

        if epoch % print_epoch == 0:
            print('#{0}, Loss: {1:.6f}, Acc: {2:.6f}, Test_string: &quot;{3}&quot;'\
                  .format(epoch, total_loss, acc, test_string))
        elif epoch == (NUM_EPOCHS-1):
            print('#{0}, Loss: {1:.6f}, Acc: {2:.6f}, Test_string: &quot;{3}&quot;'\
                  .format(epoch, total_loss, acc, test_string))

        total_loss_list.append(total_loss)
        total_acc_list.append(acc)
    return total_loss_list, total_acc_list
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-3&quot;&gt;학습하기&lt;/h3&gt;

&lt;p&gt;rnn 모델을 만들고, 어떤 방식으로 업데이트 할 것인지 정하자. 여기서는 Adam을 썼다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Optimizer&lt;/strong&gt; 의 설명은 &lt;a href=&quot;https://github.com/simonjisu/NUMPYwithNN/blob/master/common/SimpleRNN.py&quot;&gt;Numpy로 짜보는 Neural Network Basic - 5&lt;/a&gt;에 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rnn = Single_layer_RNN(input_size=INPUT_SIZE,
                       hidden_size=HIDDEN_SIZE,
                       output_size=OUTPUT_SIZE)
optim = Adam()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;학습시키기!&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;total_loss_list, total_acc_list = train(rnn, optim, print_epoch=PRINT_EPOCH)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_bptt.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Loss Graph 도 찍어보자&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loss_graph(train_loss_list=total_loss_list, train_acc_list=total_acc_list)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_bptt_loss.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;공부에 도움 되었던 싸이트:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://gist.github.com/karpathy/d4dee566867f8291f086&quot;&gt;karpathy github RNN part&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/&quot;&gt;ratsgo’s blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 14 Mar 2018 22:05:37 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/03/14/rnnlstm2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/03/14/rnnlstm2.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
      <item>
        <title>RNN &amp; LSTM - 1: RNN</title>
        <description>&lt;h1 id=&quot;rnn--lstm----1&quot;&gt;자세하게 설명한 RNN 과 LSTM 시리즈 - 1&lt;/h1&gt;

&lt;h2 id=&quot;rnnrecurrent-neural-network&quot;&gt;RNN(Recurrent Neural Network)&lt;/h2&gt;
&lt;p&gt;우리가 사는 세상에 연속된 일들, 혹은 시간과 연관된 일은 매우매우 많을 것이다. 예를 들자면, 지금 이 글을 읽은 당신도 앞에 있는 내용을 기억하면서 글을 읽고 있을 것이다. 일반적인 신경망 구조에서는 이 ‘기억’ 이라는 시스템이 존재 하지 않는다. 하지만 RNN은 다르다. 이놈은 ‘기억’을 할 수가 있다. 그렇다면 RNN과 기존 신경망과 어떻게 다른지를 한번 살펴보자.&lt;/p&gt;

&lt;h2 id=&quot;rnn-&quot;&gt;RNN 구조&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/rnn.png&quot; alt=&quot;Drawing&quot; style=&quot;width=500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNN은 중간의 Hidden 층이 순환한다고해서 순환 신경망이라고 한다. 왼쪽의 구조를 펼쳐서 보면, 중간의 Hidden 노드가 어떤 방향으로 계속 이어진 다는 것을 알 수 있다. 이러한 쇠사슬 같은 성격은 RNN으로 하여금 연속된 이벤트와 리스트에 적합한 구조로 만들어 준다.&lt;/p&gt;

&lt;p&gt;이렇게 보면 엄청 어렵게 느껴질 수 있다. 그렇다면 예시를 들어서 RNN이 어떻게 돌아가는지 수학적으로 살펴보자.&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;기본 신경망 구조&lt;/h3&gt;

&lt;p&gt;기존의 신경 구조를 한번 다시 되새겨보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/stick.png&quot; alt=&quot;Drawing&quot; height=&quot;200&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여러개의 노드로 구성된 작은 블럭을 하나의 층이라고 가정하자. 기존의 신경망 구조는 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/basic_nn_mnist.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Input $x$ 가 선형 결합 후, Hidden 에 Activation function을 거쳐 다시 선형결합을 통해 Output $y$를 구해 예측하는 알고리즘이다. 여기서 첫번째 데이터($x_1$)와 그 다음 데이터($x_2$ 등)간의 구조는 독립적이라고 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;forward&quot;&gt;Forward&lt;/h3&gt;
&lt;p&gt;예시로 time step($T$)이 3인 RNN을 살펴보자. (좌우 클릭으로 프로세스 과정 볼 수 있다)&lt;/p&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_0.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_1.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_2.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_3.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_4.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_5.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_6.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_7.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_8.png&quot; /&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;p&gt;Time step = 0 일때, 각각 Layer들의 Weight를 초기화하게 된다. $h_0$ 층은 0으로, 나머지는 Xavier 가중치 초기값으로 초기화한다. 또한 각 가중치는 각각 layer에서 공유하게 된다.
(가중치 초기화를 잊어 버렸다면 &lt;a href=&quot;https://simonjisu.github.io/datascience/2018/01/24/numpywithnn_6.html&quot;&gt;여기&lt;/a&gt;로)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
h_t &amp;= \tanh(W_{hh} h_{t-1}+W_{xh}x_t+b_h) \\
y_t &amp;= W_{hy} h_t + b_y
\end{aligned}
\quad for\ t\ in\ T %]]&gt;&lt;/script&gt;

&lt;p&gt;그리고, 시간이 지날때마 위의 식 처럼 Forward가 진행된다.&lt;/p&gt;

&lt;p&gt;최종 Cost는 모든 Cost Function의 평균으로 구해진다.&lt;/p&gt;

&lt;h3 id=&quot;backward&quot;&gt;Backward&lt;/h3&gt;
&lt;p&gt;RNN에서는 일반적인 신경망과 다른 Backward 알고리즘을 쓴다. 시간 경과에 따른 BackPropagation을 BPTT(BackPropagation Through Time)이라고 부른다.&lt;/p&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back0.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back1.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back2.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back3.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back4.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back5.png&quot; /&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;p&gt;최종적으로 학습 될 값은 Loss Function에서 각 미분한 ${\frac{\partial L}{\partial W}}^{(1)}$, ${\frac{\partial L}{\partial W}}^{(2)}$, ${\frac{\partial L}{\partial W}}^{(3)}$ 의 합으로 구해진다.&lt;/p&gt;

&lt;h3 id=&quot;long-term-dependency-&quot;&gt;장기 의존성(Long-Term Dependency) 문제&lt;/h3&gt;
&lt;p&gt;RNN이 이론상으로는 sequence의 첫번째 항부터 끝까지(즉, $x_1 \cdots x_T$ 까지) 학습 할 수 있을 것으로 보이나, 실제로는 장기기억, 즉 Time Step이 길어 질 수록 예전에 있던 정보를 기억 못한다. 이를 &lt;strong&gt;장기 의존성(Long-Term Dependency)&lt;/strong&gt; 문제라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_bad.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 이유는 우리가 업데이트 하려는 미분 식을 살펴보면 알 수 있다. 예를 들어 $W_{hh}$ 를 업데이트 한다고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\dfrac{\partial L}{\partial W_{hh}}  
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial W_{hh}} + \cdots +
\dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial W_{hh}} \\
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial y_T} \dfrac{\partial y_T}{\partial h_T} \dfrac{\partial h_T}{\partial h_{T-1}}  \cdots \dfrac{\partial h_2}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}} +
\cdots + \dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}} \\
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial y_T} \dfrac{\partial y_T}{\partial h_T} \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}} \dfrac{\partial h_1}{\partial W_{hh}} + \cdots + \dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위의 식중에 $\prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}}$ 부분을 자세히 펼쳐보면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}}
&amp;= \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial a_{T-i+1}} \dfrac{\partial a_{T-i+1}}{\partial h_{T-i}} \\
&amp;= \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial a_{T-i+1}} W_{hh}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;여기서 $a_t=W_{hh}h_{t-1} + W_{xh}x_t + b_h$ 이다.&lt;/p&gt;

&lt;p&gt;앞부분 $\frac{\partial h_{T-i+1}}{\partial a_{T-i+1}}$은 &lt;strong&gt;tanh&lt;/strong&gt; 의 미분 값이다. 아래 그림과 같이 tanh의 미분 값은 0과 1사이의 값이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/tanh.png&quot; style=&quot;width=500px&quot; /&gt;
(그림출처: http://nn.readthedocs.io/en/latest/transfer/)&lt;/p&gt;

&lt;p&gt;뒷부분인 $W_{hh}$의 값들은 세가지 경우가 있다. 1과 같게 되면 Gradient가 수렴될 가능성이 높다. 그러나 1보다 클 경우 gradient가 무한대로 발산하는 &lt;strong&gt;Exploding Gradient&lt;/strong&gt; 문제가 발생한다. 그러나 보통의 경우 $W_{hh}$ 의 값들은 1보다 작다. (아래 논문 참고)&lt;/p&gt;

&lt;p&gt;0과 1사이의 작은 값을 계속 곱하게 되면 0으로 수렴한다. 따라서, 두 가지를 종합 해보았을 때, 출력값과 멀리 떨어진 Time Step일 수록 역전파가 전달 되지 않는 &lt;strong&gt;Vanishing Gradient&lt;/strong&gt; 문제가 생기게 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v28/pascanu13.pdf&quot;&gt;On the difficulty of training recurrent neural networks&lt;/a&gt; 논문에서는 Vanishing &amp;amp; Exploding Gradient 문제를 자세히 다루고 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;장기기억을 하지 못한다는 문제가 생기면서, 이를 해결하기 위해서 몇 가지 방법이 나왔다. 첫째로, Activation Function을 &lt;strong&gt;tanh&lt;/strong&gt; 을 쓰면 기울기가 0과 1사이의 값으로 고정되니 &lt;strong&gt;ReLU&lt;/strong&gt; 를 쓰자는 방법이 있었다. 둘째로, &lt;strong&gt;LSTM&lt;/strong&gt;, &lt;strong&gt;GRU&lt;/strong&gt; 등 새로운 방법들이 등장했다. 이 방법은 다음 시간에 설명하겠다. 더불어 Backward 의 계산 그래프도 같이 첨부하겠다.&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Mar 2018 10:46:04 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/03/07/rnnlstm.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/03/07/rnnlstm.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
      <item>
        <title>NUMPY with NN - 8: Summary</title>
        <description>&lt;h1 id=&quot;numpy--neural-network-basic---8&quot;&gt;Numpy로 짜보는 Neural Network Basic - 8&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;section&quot;&gt;총 정리&lt;/h2&gt;
&lt;p&gt;지금까지 우리는 Neural Network의 기원부터 Feedforward 과정, BackPropogation 과정, 그리고 다양한 학습 관련 기술을 배웠다. 이들을 총 정리해서 Mnist 데이터를 다시 학습 시켜보자.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/simonjisu/ML/tree/master/NeuralNetwork/common&quot;&gt; 모든 코드 링크&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;package-load&quot;&gt;Package Load&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from common.Multilayer import MLP
from dataset.mnist import load_mnist
from common.optimizer import *
import time
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;data-load&quot;&gt;Data Load&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;(60000, 784)&lt;br /&gt;(60000, 10)&lt;br /&gt;(10000, 784)&lt;br /&gt;(10000, 10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;network--optimizer-settings&quot;&gt;Network &amp;amp; Optimizer settings&lt;/h3&gt;

&lt;p&gt;우리의 네트워크는 총 3층이며 Input Size가 784, Hidden node는 각각 100, 50개, Output 은 10(숫자 0~9까지의 손글씨 분류이기 때문)이다.&lt;/p&gt;

&lt;p&gt;활성화 함수는 &lt;strong&gt;ReLu&lt;/strong&gt;, 초기값도 이에 따라 &lt;strong&gt;He&lt;/strong&gt; 를 써준다. 그리고 중간에 Batch Normalization을 써준다.&lt;/p&gt;

&lt;p&gt;가중치 업데이트를 위한 옵티마이저는 &lt;strong&gt;Adam&lt;/strong&gt; 을 쓰고, Loss Function은 &lt;strong&gt;Cross Entropy&lt;/strong&gt; 를 쓰게 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nn = MLP(input_size=784, hidden_size=[100, 50], output_size=10,
         activation='relu', weight_init_std='he', use_batchnorm=True)
optimizer = Adam()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;training--test&quot;&gt;Training &amp;amp; test&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_loss_list = []
train_acc_list = []
test_acc_list = []
epoch_list = []

epoch_num=3000
train_size = x_train.shape[0]
batch_size = 100
epsilon = 1e-6

iter_per_epoch = max(train_size / batch_size, 1)

start = start = time.time()

for epoch in range(epoch_num):
    # get mini batch:
    batch_mask = np.random.choice(train_size, batch_size) # shuffle 효과
    x_batch = x_train[batch_mask]
    y_batch = y_train[batch_mask]

    grads = nn.gradient(x_batch, y_batch)

    optimizer.update(nn.params, grads)

    # 1에폭당 정확도 계산
    if epoch % iter_per_epoch == 0:
        loss = nn.loss(x_batch, y_batch)
        train_loss_list.append(loss)
        train_acc = nn.accuracy(x_train, y_train)
        train_acc_list.append(train_acc)
        test_acc = nn.accuracy(x_test, y_test)
        test_acc_list.append(test_acc)
        epoch_list.append(epoch)
        print('# {0} | loss: {1:.5f} | trian acc: {2:.5f} | test acc: {3:.5f}'.format(epoch, loss, train_acc, test_acc))
    elif epoch == (epoch_num - 1):
        loss = nn.loss(x_batch, y_batch)
        train_loss_list.append(loss)
        train_acc = nn.accuracy(x_train, y_train)
        train_acc_list.append(train_acc)
        test_acc = nn.accuracy(x_test, y_test)
        test_acc_list.append(test_acc)
        epoch_list.append(epoch)
        print('# {0} | loss: {1:.5f} | trian acc: {2:.5f} | test acc: {3:.5f}'.format(epoch, loss, train_acc, test_acc))

end = time.time()
print('total time:', (end - start))        
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;# 0 | loss: 11.06622 | trian acc: 0.11408 | test acc: 0.11910&lt;br /&gt;# 600 | loss: 0.23165 | trian acc: 0.92055 | test acc: 0.92020&lt;br /&gt;# 1200 | loss: 0.19112 | trian acc: 0.93975 | test acc: 0.94180&lt;br /&gt;# 1800 | loss: 0.08235 | trian acc: 0.95188 | test acc: 0.95040&lt;br /&gt;# 2400 | loss: 0.09155 | trian acc: 0.95898 | test acc: 0.95600&lt;br /&gt;# 2999 | loss: 0.09883 | trian acc: 0.96513 | test acc: 0.96150&lt;br /&gt;total time: 27.169809818267822&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/train_test-graph.png&quot; alt=&quot;Drawing&quot; style=&quot;width=500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;시간은 3000 Epoch를 도는데 약 30초가 안걸렸으며, 테스트 결과도 우수하게 나오는 것으로 확인된다. CNN으로 하면 더 높아질 것으로 예상된다.&lt;/p&gt;

&lt;h3 id=&quot;model-check&quot;&gt;Model Check&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def check(x, y, model):
    pred_y = model.predict(x)
    if x.ndim != 2:
        x = x.reshape(28, 28)

    print('Predict Answer: {}'.format(np.argmax(pred_y)))
    print('Real Answer: {}'.format(np.argmax(y)))
    plt.imshow(x, cmap='binary')
    plt.grid(False)
    plt.axis('off')
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;테스트 데이터중 하나 골라서 실험해보자&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;check(x_test[45], y_test[45], nn)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Predict Answer: 5&lt;br /&gt;Real Answer: 5&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/ML/nn/num5.png&quot; alt=&quot;Drawing&quot; height=&quot;100&quot; width=&quot;100&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Thu, 08 Feb 2018 15:13:40 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/02/08/numpywithnn_8.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/02/08/numpywithnn_8.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
  </channel>
</rss>
