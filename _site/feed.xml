<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soo</title>
    <description>My Blog
</description>
    <link>http://simonjisu.github.io/</link>
    <atom:link href="http://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 17 Apr 2018 00:45:34 +0900</pubDate>
    <lastBuildDate>Tue, 17 Apr 2018 00:45:34 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>[cs224n]Lecture 2</title>
        <description>&lt;h1 id=&quot;cs224n-lecture-2-word-vector-&quot;&gt;[CS224n] Lecture 2 Word Vector 정리&lt;/h1&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;how-do-we-represent-the-meaning-of-a-word&quot;&gt;How do we represent the meaning of a word?&lt;/h2&gt;

&lt;p&gt;단어를 어떻게 표현하는게 좋은 것인가? 그전에 &lt;strong&gt;“의미”&lt;/strong&gt; 의 정의를 찾아본다.&lt;/p&gt;

&lt;p&gt;Definition: &lt;strong&gt;meaning&lt;/strong&gt; (webster dictionary)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the idea that is represented by a word, phrase, etc.&lt;/li&gt;
  &lt;li&gt;the idea that a person wants to express by using words, signs, etc.&lt;/li&gt;
  &lt;li&gt;the idea that is expressed in a work of writing, art, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Commononest linguistic way of thinking of meaning:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;signifier(기표) $\Longleftrightarrow$ signified(기의) (idea or thing) = denotation(표시, 명시적의미)
    &lt;ul&gt;
      &lt;li&gt;signifier: 시니피앙(언어가 소리와 그 소리로 표시되는 의미로 성립된다고 할 때, 소리를 가리킴)&lt;/li&gt;
      &lt;li&gt;signified: 시니피에(언어가 소리와 그 소리로 표시되는 의미로 성립된다고 할 때, 의미를 가리킴)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 텍스트 분석에서는 심볼과 그 심볼로 표시되는 의미로 생각하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고)&lt;/strong&gt; 국어사전에서의 &lt;strong&gt;의미&lt;/strong&gt; 정의&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;말이나 글의 뜻.
    &lt;ul&gt;
      &lt;li&gt;단어의 사전적 의미&lt;/li&gt;
      &lt;li&gt;문장의 의미&lt;/li&gt;
      &lt;li&gt;두 단어는 같은 의미로 쓰인다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;행위나 현상이 지닌 뜻.
    &lt;ul&gt;
      &lt;li&gt;삶의 의미&lt;/li&gt;
      &lt;li&gt;역사적 의미&lt;/li&gt;
      &lt;li&gt;의미 있는 웃음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;사물이나 현상의 가치.
    &lt;ul&gt;
      &lt;li&gt;의미 있는 삶을 살다&lt;/li&gt;
      &lt;li&gt;여가를 의미 있게 보내다.&lt;/li&gt;
      &lt;li&gt;의미 없는 행동&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;how-do-we-have-usable-meaning-in-a-computer&quot;&gt;How do we have usable meaning in a computer?&lt;/h2&gt;

&lt;p&gt;Common answer: Use a taxonomy(분류) like WordNet that has hypernyms(상위어)(is-a) relationships and synonym(동의어) sets&lt;/p&gt;

&lt;p&gt;English: wordnet&lt;/p&gt;

&lt;h2 id=&quot;problems-with-this-discrete-representation&quot;&gt;Problems with this discrete representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Great as a resource but missing nuances(의미・소리・색상・감정상의 미묘한 차이, 뉘앙스), e.g., &lt;strong&gt;synonyms&lt;/strong&gt;: 단어간의 미묘한 차이를 넣을 수 없음
    &lt;ul&gt;
      &lt;li&gt;adept, expert, good, practiced, proficient, skillful&lt;/li&gt;
      &lt;li&gt;ex) i’m good (vs expert) at deeplearning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Missing new words (impossible to keep up to date): 매일같이 업데이트 불가능(비용이 너무큼)&lt;/li&gt;
  &lt;li&gt;Subjective: 사람마다 다름, 주관적임&lt;/li&gt;
  &lt;li&gt;Requires human labor to create and adapt: 사람 손을 많이 탐&lt;/li&gt;
  &lt;li&gt;Hard to compute accurate word similarity: 유사도 계산이 어려움&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The vast majority of rele-based and statisical NLP work regards words as atomic symbols: &lt;strong&gt;one-hot-representation&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;word = [0, 0, 0, 1, 0, 0, 0]&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;BAD&lt;/strong&gt; because:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dimensionality: too long when there are a lot lot of words 단어가 많아 질 수록 너무길어짐&lt;/li&gt;
  &lt;li&gt;Localist representation: Doesn’t give inherent notion, independent for each word, which means cannot calculate similarity 단어의 내적의미를 포함하지 않음, 독립적임 (즉, 사람눈에 유사한 단어라도 기계입장에서는 다른 단어일 뿐)
    &lt;ul&gt;
      &lt;li&gt;ex) when someone want to find “Seattle motel”, we have to match and give him “Seattle motel”&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;orthogonal&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
motel &amp;= \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix} \\
hotel &amp;= \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;hotel \cdot motel^T = 0&lt;/script&gt;

&lt;h2 id=&quot;word-vectors&quot;&gt;Word Vectors&lt;/h2&gt;
&lt;p&gt;Build a dense vector for each word type, chosen so that it is good at predicting other words appearing in its context. 문맥에서 비슷한 단어들을 잘 예측 될 수 있게 단어 타입 별로 촘촘한 벡터 (0이 별로없는) 를 만든다.&lt;/p&gt;

&lt;p&gt;그러나 이러한 word vector 가 단어의 개념을 뜻하는 것은 아님, 단지 분포상에서의 의미(distributional meaning)를 뜻함.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Idea:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We have a large corpus of text&lt;/li&gt;
  &lt;li&gt;Every word in a fixed vocabulary is represented by a vector&lt;/li&gt;
  &lt;li&gt;Go through each &lt;strong&gt;position&lt;/strong&gt; $t$ in the text, which has a &lt;strong&gt;center word&lt;/strong&gt; $c$ and &lt;strong&gt;context (“outside”) words&lt;/strong&gt; $o$&lt;/li&gt;
  &lt;li&gt;Use the similarity of the word vectors for $c$ and $o$ to calculate the probability of $o$ given $c$ (or vice versa)&lt;/li&gt;
  &lt;li&gt;Keep adjusting the word vectors to maximize this probability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;요약:&lt;/strong&gt; 모든 단어를 대상으로, 중심단어 $c$ 가 주어졌을 때 그 주변 단어 $o$ 를 나오게 하는 하나의 확률분포을 최대화 시키는 방향으로 학습&lt;/p&gt;

&lt;h2 id=&quot;propose&quot;&gt;Propose&lt;/h2&gt;

&lt;p&gt;For each position of word $c$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max \quad J(\theta) = \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w_{t+j} | w_t; \theta)&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;확률분포 $P(w_{t+j}&lt;/td&gt;
      &lt;td&gt;w_t)$ 를 최대화 한다 라는 것은, 우리가 구하려고 하는 파라미터 $\theta$ (=word vectors) 하에서, 단어 $w_t$ 가 주어졌을 때 주변단어 $w_{t+j}$ 가 나올 확률을 최대화 한다는 뜻이다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Change it to negative log likelihood:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\min \quad J'(\theta) &amp;= -\dfrac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m,\ j \neq 0} \log P(w_{t+j} | w_t) \\
P(o|c) &amp;= \dfrac{\exp(u_o^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;why-mle-is-equivalent-to-nll&quot;&gt;Why MLE is equivalent to NLL?&lt;/h3&gt;

&lt;p&gt;Likelihood 의 정의:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta|x_1,\cdots,x_n) = f(x_1, \cdots, x_n|\theta) = \prod_{i=1}^n f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;log를 취하게 되면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log L(\theta|x_1,\cdots,x_n) =  \sum_{i=1}^n log f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;MLE(maximum likelihood estimator) 의 정의:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MLE} = \underset{\theta}{\arg \max} \sum_{i=1}^n \log f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\underset{x}{\arg \max} (x) = \underset{x}{\arg \max}(-x)&lt;/script&gt; 때문에 우리는 아래의 식을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MLE} = \underset{\theta}{\arg \max} \sum_{i=1}^n \log f(x_i|\theta) = \underset{\theta}{\arg \min} -\sum_{i=1}^n \log f(x_i|\theta)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;왜 log 로 바꾸는 것인가?
  첫째로, 컴퓨터 연산시 곱하기 보다 더하기가 복잡도가 훨씬 줄어들어 계산이 빠르다. ($O(n) \rightarrow O(1)$) 둘째로, 언더플로우 방지. 언더플로우란 1보다 작은 수를 계속곱하면 0에 가까워져 컴퓨터에서 0 으로 표시되는 현상을 말한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;참고&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;why minimize negative log likelihood&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ratsgo.github.io/deep%20learning/2017/09/24/loss/&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;(ratsgo 님) 손실함수&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dot-product--softmax&quot;&gt;Dot product &amp;amp; Softmax&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dot product: similar to calcuate similarity&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u^Tv = u\cdot v = \sum_i u_i v_i&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Softmax&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;softmax(x_i) = \dfrac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}&lt;/script&gt;

&lt;h2 id=&quot;train-compute-all-vector-gradients&quot;&gt;Train: Compute all vector gradients&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;define the set of all parameters in a model in terms of one long vector $\theta \in \Bbb{R}^{2dV}$&lt;/li&gt;
  &lt;li&gt;why $2dV$? Because for each word one is for center word($c$) and the other is for context word($o$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_Skipgram.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;update&quot;&gt;Update?&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f = \log \dfrac{\exp(u_o^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \dfrac{\partial f}{\partial V_c}
&amp;= \dfrac{\partial }{\partial V_c} \big(\log(\exp(u_o^T V_c)) - \log(\sum_{w=1}^V \exp(u_w^T V_c))\big) \\
&amp;= u_o - \dfrac{1}{\sum_{w=1}^V \exp(u_w^T V_c)}(\sum_{x=1}^V \exp(u_x^T V_c) u_x ) \\
&amp;= u_o - \sum_{x=1}^V \dfrac{\exp(u_x^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)} u_x \\
&amp;= u_o - \sum_{x=1}^V P(x | c) u_x
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$u_o$ : observed word, output context word&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$P(x&lt;/td&gt;
          &lt;td&gt;c)$: probs context word $x$ given center word $c$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$P(x&lt;/td&gt;
          &lt;td&gt;c)u_x$: Expectation of all the context words: likelihood occurance probs $\times$ center vector&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 16 Apr 2018 20:54:31 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/16/cs224l2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/16/cs224l2.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Bidirectional LSTM + self Attention Model</title>
        <description>&lt;h1 id=&quot;naver-sentiment-movie-corpus-classification&quot;&gt;Naver Sentiment Movie Corpus Classification&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;네이버 영화 감성분류 with Bidirectional LSTM + Self Attention&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;목표&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;영화 리뷰를 통해 긍정인지 부정인지 분류하는 문제 (Many-to-One)&lt;/li&gt;
  &lt;li&gt;사용한 모델: Bidirectional LSTM with Self Attention Model&lt;/li&gt;
  &lt;li&gt;이번 글은 논문과 제가 분석한 모델의 중요 요소를 곁들여 쓴 글입니다.&lt;/li&gt;
  &lt;li&gt;GitHub Code Link: &lt;a href=&quot;https://github.com/simonjisu/nsmc_study&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;nsmc_study&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;그림이나 글은 퍼가셔도 좋지만, 출처 좀 남겨주세요~&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference Paper: &lt;a href=&quot;https://arxiv.org/pdf/1703.03130.pdf&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;모델 핵심 부분 설명&lt;/h2&gt;

&lt;p&gt;그림과 수식을 함께 보면 이해하기 쉽다&lt;/p&gt;

&lt;p&gt;어떤 $n$ 개의 토근으로 이루어진 하나의 문장이 있다고 생각해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S = (w_1, w_2, \cdots, w_n)\qquad\qquad (1)&lt;/script&gt;

&lt;p&gt;여기서 $w_i$ 는 one-hot 인코딩된 단어가 아닌, $d$ 차원에 임베딩된 문장에서 $i$ 번째 단어다.&lt;/p&gt;

&lt;p&gt;따라서 $S$ 는 단어 벡터들을 concat 한 $n \times d$ 형태를 가지는 매트릭스다.&lt;/p&gt;

&lt;p&gt;문장 $S$ 는 각기 다른 문장과는 독립적이다. (하나의 문장이 하나의 평점과 세트로 생각하면 된다.) 하나의 문장에서 단어들 간의 관계를 알기 위해서 우리는 bidirectional LSTM 으로 하나의 문장을 처리하게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\overrightarrow{h_t} &amp;= \overrightarrow{LSTM}(w_t, \overrightarrow{h_{t-1}})\qquad\qquad (2) \\
\overleftarrow{h_t} &amp;= \overleftarrow{LSTM}(w_t, \overleftarrow{h_{t-1}})\qquad\qquad (3)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;그후 우리는 각각의 $\overrightarrow{h_t}$ 과 $\overleftarrow{h_t}$ 를 concatenate 하여 하나의 히든 state $h_t$ 를 얻게 된다. 각 unidirectional LSTM(한 쪽 방향 LSTM)의 히든 유닛 크기를 $u$ 라고 하자. 조금 간단하게 표현하기 위해서 모든 $n$ 개의 $h_t$ 들을 $H$ 라고 하며, $n \times 2u$ 의 크기를 가진다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H = (h_1, h_2, \cdots, h_n) \qquad\qquad (4)&lt;/script&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention0.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention4.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우리의 목적은 길이가 변화하는 문장을 어떤 &lt;strong&gt;고정된 크기&lt;/strong&gt; 의 임베딩으로 인코딩 하는 것이다. 이 목적을 달성하기 위해서 $H$ 와 attention 매커니즘이 요구되는 일종의 선형결합을 선택하게 된다. 즉, 아래와 같은 식과 $H$ 를 토대로, 어떤 벡터 $a$ 를 얻게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = softmax(w_{s2} \tanh (W_{s1}H^T)) \qquad\qquad (5)&lt;/script&gt;

&lt;p&gt;여기서 $W_{s1}$ 는 $d_a \times 2u$ 형태를 가진 매트릭스, 그리고 $w_{s2}$ 는 $d_a$ 사이즈를 가진 벡터다. $d_a$ 는 하이퍼파라미터(hyperparameter)로 우리가 정할 수 잇는 변수다. $H$ 의 크기도 $n \times 2u$ 이기 때문에, 벡터 $a$ 는 $n$ 의 크기를 가진다. 또한 $softmax()$ 함수는 모든 weight들의 합을 1로 만들어 준다.&lt;/p&gt;

&lt;p&gt;그후 우리는 LSTM 의 히든상태들의 집합인 $H$ 를 주어진 $a$ 로 곱해서 한 문장을 임베딩한 벡터 $m$ 을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이 벡터 $m$ 은 학습시 한 문장에서 어떤 단어를 중심적으로 보았는지 알 수 있다. 예를 들어 어떤 연관된 단어나 구문 등등.&lt;/p&gt;

&lt;p&gt;문장과 단어의 관계로 추가 설명하자면 아래와 같다.&lt;/p&gt;

&lt;p&gt;각 단어를 input으로 받은 hidden 상태의 노드들은 단어를 통과해서 각 단어의 숨겨진 특성을 대표하고 있다. 학습 시 Task 에 따라 다르겠지만, 분류라고 가정한다면 분류에 도움이 되는 히든 상태는 높은 값을 가지게 될 것이며, 이를 어떤 선형 변환 과정을 거쳐 softmax 취한다는 것은 한 문장에서 분류에 도움이 된 근거 단어 혹은 중요 단어의 확률을 구한다는 것이 된다. (그래서 attention 이라고 하는 것 같다.) 따라서 이는 한 문장에서 &lt;strong&gt;의미적인(semantic)&lt;/strong&gt; 부분을 나타내고 있다고 할 수 있다.&lt;/p&gt;

&lt;p&gt;이 확률 $a$ 를 기존의 hidden 상태와 곱해서 의미부분을 조금더 강조하게 되는 벡터 $m$ 을 구했다고 보면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention5.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention6.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;하지만 한 문장 내에서 중요한 부분 혹은 의미가 있는 부분은 여러군데 일 수가 있다. (여러 의미가 하나의 문장을 구성한다.) 특히 긴 문장일 수록 그렇다. 예를 들어 “아이언맨과 캡틴아메리카” 면 “과”로 이어진, “아이언맨”, “캡틴아메리카” 두 단어는 중요한 의미가 있는 단어 일 수 있다. 따라서 한 문장에서 의미가 있는 부분을 나타내려면 $m$ 이란 벡터를 여러 번 수행해서 문장의 다른 부분까지 커버해야 한다. 이는 우리가 &lt;strong&gt;attention&lt;/strong&gt; 을 &lt;strong&gt;여러번(hops)&lt;/strong&gt; 하게 되는 이유다.&lt;/p&gt;

&lt;p&gt;따라서, 문장에서 우리가 정하는 어떤 수 $r$ 번의 다른 부분을 추출 해낸다고 하면, 기존의 $w_{s2}$ 는 $r \times d_a$ 크기를 가진 $W_{s2}$ 라는 매트릭스로 확장된다. 이에따라 기존에 $a$ 벡터도 $r$ 번을 수행해 concatenate 한 $r \times n$ 크기의 매트릭스 $A$ 가 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A=softmax(W_{s2}tanh(W_{s1}H^T))  \qquad\qquad (6)&lt;/script&gt;

&lt;p&gt;여기서 $softmax()$ 는 input $W_{s2}tanh(W_{s1}H^T)$ 의 2번째 차원을 기준으로 softmax 하게 된다. (즉, 각 row 별로 softmax 해줌)&lt;/p&gt;

&lt;p&gt;사실 $(6)$ 번 수식은 bias 가 없는 2-Layers MLP 로 간주할 수도 있다.&lt;/p&gt;

&lt;p&gt;위에 식에 따라 임베딩된 벡터 $m$ 도 $r \times 2u$ 크기의 매트릭스 $M$ 로 확장된다. 가중치를 담은 매트릭스 $A(r \times n)$ 와 LSTM 의 히든 상태들인 $H(n \times 2u)$를 곱해서 새로운 임베딩 매트릭스 $M$ 을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M=AH  \qquad\qquad (7)&lt;/script&gt;

&lt;p&gt;마지막으로 $M$을 Fully Connected MLP 에 넣어서 하고 싶은 분류를 하면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider3&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention7.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention8.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention9.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;penalization-term&quot;&gt;Penalization Term&lt;/h3&gt;

&lt;p&gt;임베딩된 매트릭스 $M$ 은 $r$ hops 동안 계속해서 같은 유사도 벡터 $a$ 를 곱하게 되면 &lt;strong&gt;중복 문제(redundancy problems)&lt;/strong&gt; 가 생길 수 있다. 즉, 같은 단어 혹은 구문만 계속해서 attention 하게 되는 문제다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/penal.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;그림: 왼쪽(a)은 패널티를 안준 것, 오른쪽(b) 는 준것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, $r$ hops 동안 weight 벡터들의 합을 다양성을 높히는 일종의 패널티를 줘야한다.&lt;/p&gt;

&lt;p&gt;제일 좋은 방법은 $r$ hops 안에 있는 아무 두 벡터 간의 &lt;strong&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;쿨백-라이블러 발산 (Kullback–Leibler divergence)&lt;/span&gt;&lt;/a&gt;&lt;/strong&gt; 함수를 쓰는 것이다. 매트릭스 $A$ 의 각각의 행(row) 벡터들이 하나의 의미(semantic)를 가지는 단어 혹은 구문이 될 확률분포이기 때문에, 다양한 분포에서 나오는 것은 우리의 목적이 된다. (문장은 여러 단어/구문으로 구성되어 있기때문) 그러므로 KL divergence 값을 &lt;strong&gt;최대&lt;/strong&gt; 로 만들면 중복 문제는 해결된다.&lt;/p&gt;

&lt;p&gt;그러나 논문에서는 위와 같은 경우에 불안정(unstable) 한다는 것을 알아냈다. 논문 저자들은 어림짐작해 보았을 때, KL divergence 를 최대화 할때(보통의 경우 KLD를 최소화 하는 것을 한다.), 매트릭스 $A$ 구하는 단계에서 softmax 시 많은 값들이 0 이거나 아주 작은 값이라서 불안정한 학습을 야기했을 가능성이 있다는 것이다.&lt;/p&gt;

&lt;p&gt;따라서, 논문에서는 매트릭스의 &lt;strong&gt;&lt;a href=&quot;http://mathworld.wolfram.com/FrobeniusNorm.html&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Frobenius norm&lt;/span&gt;&lt;/a&gt;&lt;/strong&gt; 을 쓰게 되는데 아래와 같다. ($Norm_2$와 비슷해 보이지만 다르다)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P ={ {\|AA^T - I\|}_F}^2&lt;/script&gt;

&lt;p&gt;이 패널티 값과 기존의 Loss 와 같이 최소화 하는 방향으로 간다. 이 패널티의 뜻은 무엇일까?&lt;/p&gt;

&lt;p&gt;두 개의 다른 유사도 벡터의 합 $a^{i}$ 과 $a^{j}$ 를 생각해보자. Softmax 로 인해서 모든 $a$ 값들의 합은 1이 될 것이다. 따라서 이들을 일종의 이산 확률분포 (discrete probability distribution)에서 나오는 확률질량 함수로 간주할 수 있다.&lt;/p&gt;

&lt;p&gt;매트릭스 $AA^T$ 중, 모든 비대각 $a_{ij}\ (i \neq j)$ 원소에 대해서, 원소의 곱(elementary product)은 아래 두개의 분포를 가지고 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
0&lt; a_{ij} = \sum_{k=1}^{n} a_k^i a_k^j &lt;1 %]]&gt;&lt;/script&gt;

&lt;p&gt;여기서 $a_k^i, a_k^j$ 는 각각 $a^i, a^j$ 의 k 번째 원소다. 제일 극단적인 경우를 생각해보면, $a^i$ 와 $a^j$ 가 일치하지 않다면 (혹은 다른 분포를 나타내고 있다면) 0 이 되고, 완전이 일치해서 같은 단어 혹은 구문을 이야기 하고 있다면 (혹은 같은 분포를 나타내고 있다면) 1 에서 최대값을 가지게 될 것이다.&lt;/p&gt;

&lt;p&gt;따라서, $AA^T$ 의 대각 행렬(같은 단어 혹은 구문)을 대략 1 이 되게 강제한다. $I$ (Identity) 매트릭스를 빼줌으로써 달성하는데, 이는 자기 자신을 제외한 각기 다른 $a^i$ 간 원소들의 합인 $a_{ij}$ 들이 0 으로 최소화되게 만들어 버린다. 즉, 최대한 $a^i$ 간의 분포가 일치하지 않게 만드려고 노력하는 것이다. 이렇게 함으로써 $r$ 번의 hops 마다 각각 다른 단어에 집중하게 만드는 효과를 낼 수 있어서, 중복문제를 해결 할 수가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;네이버 영화 리뷰 테스트 결과 및 시각화&lt;/h2&gt;
&lt;p&gt;총 150000 개의 Train Set과 50000 개의 Test Set 으로 진행했고, 모델에서는 hyperparameter가 많기 때문에 몇 가지 실험을 진행 했다.&lt;/p&gt;

&lt;p&gt;간단한 실험을 위해서 사전에 단어들을 word2vec 으로 학습시키지 않고, mecab 으로 tokenizing 만해서 임베딩 시켰다. (실험을 안해봐서 사실 크게 상관있나 모르겠다. 나중에 여러가지로 실험해볼 예정)&lt;/p&gt;

&lt;p&gt;내가 주로 건드린건 LSTM 에서의 &lt;strong&gt;hidden layer의 갯수&lt;/strong&gt; 와 hops 인 &lt;strong&gt;$r$&lt;/strong&gt; 을 바꾸어 보았다.&lt;/p&gt;

&lt;h3 id=&quot;model-1-1--hidden-layer--5-hops&quot;&gt;model 1: 1 개의 Hidden Layer 와 5번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_1.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-2-1--hidden-layer--20-hops&quot;&gt;model 2: 1 개의 Hidden Layer 와 20번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hops 가 많아지면 긍정/부정을 판단하게 되는 근거도 많아지고, 모델의 정확도도 향상되는 것을 2번에서 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;model-3-3--hidden-layer--5-hops&quot;&gt;model 3: 3 개의 Hidden Layer 와 5번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3번째 모델은 조금 이상하다고 느껴진 것이 있다. 그림을 보면 기계가 문장의 앞뒤만 보고 리뷰가 긍정인지 부정인지 판단했다는 것이다. 그림만 보면 과최적화된 느낌? 정확히 각 층의 layer 값을 보지는 못했지만, 층이 깊어 질 수록 기계가 이전 단계의 layer 에서 추출한 특징들로 학습해서 긍부정을 판단 했을 가능성이 있다. 점수는 높게 나왔으나 사람이 판단하기에는 부적절한 모델&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;향후 해볼 수 있는 과제들&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;전처리 단계에서 임베딩시 다양한 임베딩을 해볼 수 있을 것 같다. 예를 들어 word2vec으로 미리 선학습 후에 만든다던지, 아니면 N-hot 인코딩 (단어 원형 + 품사 + 어미) 등등 시도해볼 수 있는 것은 많다.&lt;/li&gt;
  &lt;li&gt;LSTM Cell 로 구현&lt;/li&gt;
  &lt;li&gt;이와 연관은 좀 덜하지만, CNN으로 분류하는 것과 비교해 성능이 더 잘나올지? &lt;strong&gt;김윤&lt;/strong&gt; 님의 논문 참고 : &lt;a href=&quot;http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크 &lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;공부에 도움 주신 분들 및 공부에 도움 되었던 싸이트:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;김성동님: &lt;span style=&quot;color: #7d7ee8&quot;&gt;https://github.com/DSKSD&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;같은 논문을 Tensorflow로 구현하신 flrngel님: &lt;span style=&quot;color: #7d7ee8&quot;&gt;https://github.com/flrngel/Self-Attentive-tensorflow&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;감사합니다.&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Apr 2018 01:01:13 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/04/04/nsmcbidreclstmselfattn.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/04/04/nsmcbidreclstmselfattn.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>Naver AI Colloquium 2018</title>
        <description>&lt;h1 id=&quot;naver-ai-colloquium-2018--&quot;&gt;Naver AI Colloquium 2018 참석 후기&lt;/h1&gt;
&lt;p&gt;&lt;del&gt;+ 클로바스피커 후기&lt;/del&gt;&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;개요&lt;/h2&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/logo.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment3.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://naveraiconf.naver.com/&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;네이버 AI 콜로키움&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;운 좋게 기회가 되서 &lt;a href=&quot;http://naveraiconf.naver.com/&quot;&gt;&lt;strong&gt;Naver AI Colloquium 2018&lt;/strong&gt;&lt;/a&gt; 에 다녀왔다.
싸이트에 접속해서 프로그램 표를 자세히 보면 알겠지만, 이번 콜로키움에서는 크게 4가지 주제를 다뤘다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;언어 분야(Search &amp;amp; Natural Language)&lt;/li&gt;
  &lt;li&gt;비전 분야(Computer Vision, Mobility&amp;amp;Location Intelligence)&lt;/li&gt;
  &lt;li&gt;추천 분야(Recommendation)&lt;/li&gt;
  &lt;li&gt;데이터 엔지니어 분야(AI Algorithm, System&amp;amp;Platform)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;나는 언어 분야, 즉 자연어을 다루는 쪽에 관심이 많아서 Track A 만 거의 듣고, 추천 분야 하나 정도 들었다. 각 강의에서 다루는 내용은 차후 하나씩 정리해서 올릴 예정이다.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/naveraicol/professorsungkim.JPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;“모두의 딥러닝”으로 유명하신 Sung Kim 교수님!! (문제가 되면 사진 내리겠습니다~ 댓글 달아주세요)&lt;/p&gt;

&lt;p&gt;인상 깊었던 세션과 그 이유를 몇개 꼽자면, &lt;del&gt;어쩌다보니 다 네이버 직원분들이 발표하신거네ㅎㅎ&lt;/del&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Semantic Matching Model - 김선훈 (NAVER)
시멘틱 매칭의 필요성:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;키워드(단어기반)&lt;/strong&gt; 매칭보다는 &lt;strong&gt;시멘틱(의미기반)&lt;/strong&gt; 매칭이 다양한 표현과 오타를 커버할 수 있는 가능성이 높다.&lt;/li&gt;
      &lt;li&gt;Sementic Gap: 인간이해와 기계이해의 차이, 이것을 줄이는게 큰 과제&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Speed Reading - 서민준 (NAVER)
    &lt;ul&gt;
      &lt;li&gt;Skim-RNN: “속독”에서 나온 아이디어, 중요하지 않은 단어는 적게 업데이트!&lt;/li&gt;
      &lt;li&gt;Big RNN 과 Small RNN 의 결정 짓는 Decision Function&lt;/li&gt;
      &lt;li&gt;Layer를 쌓으면 중요한 정보를 캐치 (마치 글을 두번째 읽을 때는 주요 단어만 보게 되는 것과 같음)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hybrid Natural Language Understanding 모델 - 김경덕 (NAVER)
    &lt;ul&gt;
      &lt;li&gt;문제 정의의 중요성: 잘해야 문제를 해결하기 쉽고 명확하다.&lt;/li&gt;
      &lt;li&gt;팬턴 기반 검색 NLU + 데이터 통계 기반 NLU, 뭐든지 하나만 고르는 것은 아니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;자기학습 기반 챗봇(발표세션은 아님)
    &lt;ul&gt;
      &lt;li&gt;챗봇의 전체 과정:
  Query $\rightarrow$ 언어적 특징 추출 $\rightarrow$ 쿼리 분류기(대화여부 및 도메인 인지) $\rightarrow$ 여러 모델로 부터 답변 생성 $\rightarrow$ Answer&lt;/li&gt;
      &lt;li&gt;N-hot representation: 토큰원형 + 품사태깅 + 어미&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 정도인 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;클로바 스피커(프렌즈) 후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/naveraicol/speaker.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;어쩌다 운좋게 경품에 당첨 되서 받았다. ㅎㅎㅎ 감사합니다.
이놈…생각보다 귀엽다. 아직까지 일본의 프렌즈보다 기능이 덜 있는 것 같다. 라인으로 메세지 보내는 기능 시도해보았는데, 안되드라…&lt;/p&gt;

&lt;p&gt;영상링크: &lt;a href=&quot;https://youtu.be/lK-9yDoHsZ8&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;【公式】Clova Friendsができること &lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;아무튼 아직 개선할 사항이 많다. 예를 들어, “레미제라블 Do you hear the people sing? 노래틀어줘”라고 말하면, 말씀하신 사항을 찾지 못했다고 대답한다.
어떤 세션에서 들었던것 같은데, 내 생각에는 레미제라블, 노래틀어줘는 노래틀어주는 분야로 의도로 분류되고, 나머지 영어는 번역하는 의도로 분류된 것 같다. (스피커에 말한 내용을 볼 수 있는데, 음성인식을 진짜 제대로 잘 된다. 괜히 1위라고 말한게 아닌듯 ㅋㅋ)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;내년에 또 하게되면 참가하고싶다~&lt;/p&gt;

&lt;p&gt;관심 있었던 세션들을 정리하면 바로 올리겠다.&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Mar 2018 20:14:21 +0900</pubDate>
        <link>http://simonjisu.github.io/naverai2018/2018/03/30/naveraicolloquium2018.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/naverai2018/2018/03/30/naveraicolloquium2018.html</guid>
        
        
        <category>NaverAI2018</category>
        
      </item>
    
      <item>
        <title>RNN &amp; LSTM - 2: Numpy with RNN</title>
        <description>&lt;h1 id=&quot;rnn--lstm----2&quot;&gt;자세하게 설명한 RNN 과 LSTM 시리즈 - 2&lt;/h1&gt;

&lt;h2 id=&quot;numpy--rnn-&quot;&gt;Numpy 로 RNN 만들어보기&lt;/h2&gt;
&lt;p&gt;모든 코드는 Github: &lt;a href=&quot;https://github.com/simonjisu/NUMPYwithNN&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;NUMPYwithNN&lt;/span&gt;&lt;/a&gt; 에 올려져 있습니다.&lt;/p&gt;

&lt;p&gt;Jupyter Notebook 으로 전체과정 보기: &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/NUMPYwithNN/blob/master/Notebook/Character_Predicting_RNN.ipynb&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크 &lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;rnn-forward--backward--&quot;&gt;RNN Forward 와 Backward의 계산 그래프&lt;/h2&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_forward0.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_forward1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_forward2.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward0.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward4.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/graph_backward5.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;backward에서 잊지 말아야 할 부분은 $t=T$일 때(마지막 Step일 때) $d h_T$는 0으로 초기화 되며, 구해진 $d h_{t-1}^{raw}$ 가 이 다음 역전파로 들어가기 전에 이전 단계로 부터 얻은 $dh_{t-1}$ 와 더해져 계산한다는 점이다. 그 이유는 forward 시 다음 step으로 hidden 값($h_t$)을 전파하기 때문이라는 것을 잊지 말자.&lt;/p&gt;

&lt;p&gt;위 그림은 &lt;a href=&quot;https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;ratsgo’s blog&lt;/span&gt;&lt;/a&gt; 님의 포스트에서 많은 참조를 하고 새로 만들었음을 밝힙니다.&lt;/p&gt;

&lt;h3 id=&quot;bptt--&quot;&gt;참고) BPTT 수식적 이해&lt;/h3&gt;
&lt;p&gt;$tanh$의 미분을 $f(x) = 1 - tanh^2(x)$ 라고 하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&amp;= \dfrac{\partial L}{\partial y_t} \dfrac{\partial y_t}{\partial h_0} + \dfrac{\partial L}{\partial y_{t-1}} \dfrac{\partial y_{t-1}}{\partial h_0} \cdots + \dfrac{\partial L}{\partial y_1} \dfrac{\partial y_1}{\partial h_0}\\
&amp;= \dfrac{\partial L}{\partial y_t} \dfrac{\partial y_t}{\partial h_t} \dfrac{\partial h_t}{\partial a_t} \dfrac{\partial a_t}{\partial h_{t-1}} \cdots \dfrac{\partial a_1}{\partial h_{0}} + \cdots +
\dfrac{\partial L}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial a_1} \dfrac{\partial a_1}{\partial h_0} \\
&amp;= W_{hy} dy_t W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_1) + \cdots + W_{hy} dy_2 W_{hh} f(a_2) W_{hh} f(a_1) + W_{hy} dy_1 W_{hh} f(a_1) \\
&amp;= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{i} \prod_{j=1}^{i} f(a_j) \Big)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위 식을 위에 있는 그림대로 그려보자, 뒤에 $W_{hh} f(a_1)$ 부처 차근차근 묶어서 아래의 식을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\dfrac{\partial L}{\partial h_{0}}
&amp;= W_{hh} f(a_1) \bigg( W_{hy} dy_t  W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_2) + \cdots + W_{hy} dy_2 W_{hh} f(a_2) + W_{hy} dy_1 \bigg) \\
&amp;= W_{hh} f(a_1) \bigg( W_{hh} f(a_2) \Big( W_{hy} dy_t W_{hh} f(a_t) W_{hh} f(a_{t-1}) \cdots W_{hh} f(a_3) + \cdots + W_{hy} dy_2 \Big) + W_{hy} dy_1 \bigg) \\
&amp;= W_{hh} f(a_1) \bigg( W_{hh} f(a_2) \Big( \cdots W_{hh} f(a_{t-1}) \big( \underbrace{W_{hh} f(a_t) (\underbrace{ W_{hy} dy_t }_{dh_t^{raw}} + 0)}_{dh_{t-1}} + \underbrace{ W_{hy} dy_{t-1} }_{dh_{t-1}^{raw}} \big) \cdots + W_{hy} dy_2 \Big) + W_{hy} dy_1 \bigg) \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위에 그림과 비교해보면 이런 식으로 계속 더해진다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;backpropagation-through-time-bptt-&quot;&gt;BackPropagation Through Time (BPTT) 구현&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Single_Layer_RNN&lt;/strong&gt; 의 코드는 &lt;a href=&quot;https://github.com/simonjisu/NUMPYwithNN/blob/master/common/SimpleRNN.py&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;여기&lt;/span&gt;&lt;/a&gt;에 있습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Layer&lt;/strong&gt; 의 구현을 참고하려면 Github의 &lt;a href=&quot;https://github.com/simonjisu/NUMPYwithNN/blob/master/common/layers.py&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;common/layers&lt;/span&gt;&lt;/a&gt; 참고하세요!&lt;/li&gt;
  &lt;li&gt;처음 Layer를 짜보시는 분은 &lt;a href=&quot;https://simonjisu.github.io/deeplearning/2017/12/07/numpywithnn_1.html&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Numpy로 짜보는 Neural Network Basic&lt;/span&gt;&lt;/a&gt; 시리즈를 참고하세요!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우선 미분한 값의 합을 구하기 위해 각각 Layer의 파라미터와같은 형태(shape)로 만들어 준다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _params_summation_init(self):
    self.params_summ = {}
    self.params_summ['W_xh'] = np.zeros_like(self.params['W_xh'])
    self.params_summ['W_hh'] = np.zeros_like(self.params['W_hh'])
    self.params_summ['W_hy'] = np.zeros_like(self.params['W_hy'])
    self.params_summ['b_h'] = np.zeros_like(self.params['b_h'])
    self.params_summ['b_y'] = np.zeros_like(self.params['b_y'])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;또한, $dh_T$ 를 0으로 초기화 한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dht = np.zeros_like(self.h0)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그후에 총 길이 $T$의 역순으로 각 Layer 의 Back Propagation 을 진행한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for t in np.arange(self.T)[::-1]:
    dout = self.last_layers[t].backward()
    dht_raw = self.layers['Affine_hy'][t].backward(dout)
    dat = self.layers['Activation'][t].backward(dht_raw + dht)
    dht = self.layers['Affine_hh'][t].backward(dat)
    dx = self.layers['Affine_xh'][t].backward(dat)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;또한, 파라미터 $W$ 와 $b$ 의 합도 같이 구해준다. 그 이유는 전편에서 설명되어 있지만, 다시 한번 이야기 하자면, 최종 Loss Function은 각 Output Loss의 평균이기 때문에, 각 Output 마다 파라미터들을 summation 하는 과정이 있다. (평균을 구할때 우선 summation을 한다는 것을 잊지 말자.)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.params_summ['W_xh'] += self.layers['Affine_xh'][t].dW
self.params_summ['W_hh'] += self.layers['Affine_hh'][t].dW
self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
self.params_summ['b_h'] += self.layers['Affine_hh'][t].db
self.params_summ['b_y'] += self.layers['Affine_hy'][t].db
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;전체 Backward 과정&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def backward(self):
    # BPTT
    self._params_summation_init()
    dht = np.zeros_like(self.h0)

    for t in np.arange(self.T)[::-1]:
        dout = self.last_layers[t].backward()
        dht_raw = self.layers['Affine_hy'][t].backward(dout)
        dat = self.layers['Activation'][t].backward(dht_raw + dht)
        dht = self.layers['Affine_hh'][t].backward(dat)
        dx = self.layers['Affine_xh'][t].backward(dat)

        self.params_summ['W_xh'] += self.layers['Affine_xh'][t].dW
        self.params_summ['W_hh'] += self.layers['Affine_hh'][t].dW
        self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
        self.params_summ['b_h'] += self.layers['Affine_hh'][t].db
        self.params_summ['b_y'] += self.layers['Affine_hy'][t].db
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;truncate-backpropagation-through-time-t-bptt&quot;&gt;Truncate BackPropagation Through Time (T-BPTT)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Truncate BackPropagation Through Time (T-BPTT)&lt;/strong&gt; 은 기존 BPTT 에서 과거 모든 미분값을 참조하는 대신 고정된 길이로 참조 할 수 있도록 만든 알고리즘이다.&lt;/p&gt;

&lt;p&gt;왜 이런것을 만들었을 까? BPTT 알고리즘의 미분식을 다시 생각해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&amp;= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{i} \prod_{j=1}^{i} f(a_j) \Big)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위에서 설명했지만, BPTT 과정에서 Time-step이 길어질 수록, 많은 양의 곱셈이 이루어 진다. 계산량을 줄이기 위해서 이런 알고리즘이 나왔을 수 있다.&lt;/p&gt;

&lt;p&gt;다른 접근 방법으로, 학습하고 싶은 Sequence의 일정 길이만큼만 과거를 참조하고 싶기 때문일 수도 있다.&lt;/p&gt;

&lt;p&gt;예를 들어 “I live in Seoul. (중략) I am Korean.” 이라는 문장을 생각해보자. 학습 데이터는 아래와 같을 것이다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[&quot;I&quot;, &quot;live&quot;, &quot;in&quot;, &quot;Seoul&quot;, &quot;.&quot;, (중략), &quot;I&quot;, &quot;am&quot;, &quot;Korean&quot;, &quot;.&quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Forward 할때는 순차적으로 들어갈텐데, Backward 할때는 데이터의 역순으로(“.”, “Korean”) 진행될 것이다. 그러나 내가 한국인이라는 것은 내가 서울에 살고 있기 때문인데, 굳이 앞단의 “I”, “live”, “in” 까지 참조할 필요는 없는 것이다. 그렇다면 위에 식은 아래와 같이 변할 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dh_{0} = \dfrac{\partial L}{\partial h_{0}}
&amp;= \sum_{i=1}^{t} \Big( dy_i W_{hy} {(W_{hh})}^{k} \prod_{j=k}^{t} f(a_j) \Big) \\
where \quad k &amp;= \max(1, t - truncate)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;t-bptt-&quot;&gt;T-BPTT 구현&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/normal_truncate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림 출처: &lt;a href=&quot;https://r2rt.com/styles-of-truncated-backpropagation.html&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;r2rt.com&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def backward_truncate(self):
    # TBPTT
    self._params_summation_init()
    dht = np.zeros_like(self.h0)

    for t in np.arange(self.T)[::-1]:
        dout = self.last_layers[t].backward()
        dht_raw = self.layers['Affine_hy'][t].backward(dout)
        self.params_summ['W_hy'] += self.layers['Affine_hy'][t].dW
        self.params_summ['b_y'] += self.layers['Affine_hy'][t].db

        for bptt_step in np.arange(max(0, t + 1 - self.bptt_truncate), t + 1)[::-1]:
            dat = self.layers['Activation'][bptt_step].backward(dht_raw + dht)
            dht = self.layers['Affine_hh'][bptt_step].backward(dat)  # dh_t-1
            dx = self.layers['Affine_xh'][bptt_step].backward(dat)  # dx
            self.params_summ['W_xh'] += self.layers['Affine_xh'][bptt_step].dW
            self.params_summ['W_hh'] += self.layers['Affine_hh'][bptt_step].dW
            self.params_summ['b_h'] += self.layers['Affine_hh'][bptt_step].db
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그러나 Tensorflow 에서는 아래와 같이 구현한다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/tensorflow_truncate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림 출처: &lt;a href=&quot;https://r2rt.com/styles-of-truncated-backpropagation.html&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;r2rt.com&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;실습&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;목적&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;“hello world! nice to meet you! i love iron-man”&lt;/strong&gt; 을 RNN 으로 학습시키기.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Input&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Output&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;h&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;e&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;e&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;l&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;l&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;l&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;l&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;o&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;⋮&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;⋮&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;⋮&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;a&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;→&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;n&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-2&quot;&gt;데이터 및 우리가 만든 패키지 준비&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
from common.SimpleRNN import Single_layer_RNN
from common.optimizer import Adam
from common.train_graph import loss_graph
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = 'hello world! nice to meet you! i love iron-man'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;인코딩 클래스 하나를 만들어서 문자열을 one-hot 인코딩 해준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class chr_coding(object):
    def __init__(self):
        self._dict = None
        self._one_hot_matrix = None
        self._dict_reversed = None

    def fit(self, x):
        if isinstance(x, str):
            x = list(x)

        self._one_hot_matrix = np.eye(len(set(x)))
        self._dict = {d: i for i, d in enumerate(list(set(x)))}
        self._dict_reversed = {v: k for k, v in self._dict.items()}

    def encode(self, x):
        encoded_data = np.array([self._one_hot_matrix[self._dict[d]] for d in x])
        return encoded_data

    def decode(self, x, probs=None):
        if probs is None:
            decoded_data = self._dict_reversed[x]
        else:
            decoded_data = self._dict_reversed[np.argmax(probs)]
        return decoded_data
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;encoder = chr_coding()
encoder.fit(x)
one_hot_data = encoder.encode(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;학습 데이터 x, y를 지정해준다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_x = one_hot_data[:-1]
train_y = one_hot_data[1:]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;hyperparameters&quot;&gt;hyperparameters&lt;/h3&gt;

&lt;p&gt;INPUT_SIZE 와 OUTPUT_SIZE 는 중복되지 않는 문자열 사전의 길이라는 것을 잊지 말자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NUM_EPOCHS = 600
PRINT_EPOCH = 30
INPUT_SIZE = one_hot_data.shape[1]
OUTPUT_SIZE = one_hot_data.shape[1]
HIDDEN_SIZE = 20
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;accuracy--train-&quot;&gt;필요한 함수 설정: accuracy 와 train 함수&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def get_accuracy(x, test_string):
    bool_ = np.array(list(x))[1:] == np.array(list(test_string))[1:]
    return bool_.sum() / len(bool_)

def train(rnn, optim, print_epoch=20):
    total_loss_list = []
    total_acc_list = []
    for epoch in range(NUM_EPOCHS):
        test_string = 'h'
        # forward
        total_loss = rnn.loss(train_x, train_y)

        # backward
        rnn.backward()

        optim.update(rnn.params, rnn.params_summ)

        # test string
        predicted_idx = rnn.predict(train_x)
        for idx in predicted_idx:
            test_string += encoder.decode(idx)

        # get accuracy
        acc = get_accuracy(x, test_string)

        if epoch % print_epoch == 0:
            print('#{0}, Loss: {1:.6f}, Acc: {2:.6f}, Test_string: &quot;{3}&quot;'\
                  .format(epoch, total_loss, acc, test_string))
        elif epoch == (NUM_EPOCHS-1):
            print('#{0}, Loss: {1:.6f}, Acc: {2:.6f}, Test_string: &quot;{3}&quot;'\
                  .format(epoch, total_loss, acc, test_string))

        total_loss_list.append(total_loss)
        total_acc_list.append(acc)
    return total_loss_list, total_acc_list
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-3&quot;&gt;학습하기&lt;/h3&gt;

&lt;p&gt;rnn 모델을 만들고, 어떤 방식으로 업데이트 할 것인지 정하자. 여기서는 Adam을 썼다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Optimizer&lt;/strong&gt; 의 설명은 &lt;a href=&quot;https://github.com/simonjisu/NUMPYwithNN/blob/master/common/SimpleRNN.py&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Numpy로 짜보는 Neural Network Basic - 5&lt;/span&gt;&lt;/a&gt;에 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rnn = Single_layer_RNN(input_size=INPUT_SIZE,
                       hidden_size=HIDDEN_SIZE,
                       output_size=OUTPUT_SIZE)
optim = Adam()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;학습시키기!&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;total_loss_list, total_acc_list = train(rnn, optim, print_epoch=PRINT_EPOCH)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_bptt.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Loss Graph 도 찍어보자&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loss_graph(train_loss_list=total_loss_list, train_acc_list=total_acc_list)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_bptt_loss.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;공부에 도움 되었던 싸이트:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://gist.github.com/karpathy/d4dee566867f8291f086&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;karpathy github RNN part&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;ratsgo’s blog&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 14 Mar 2018 22:05:37 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/03/14/rnnlstm2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/03/14/rnnlstm2.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
      <item>
        <title>RNN &amp; LSTM - 1: RNN</title>
        <description>&lt;h1 id=&quot;rnn--lstm----1&quot;&gt;자세하게 설명한 RNN 과 LSTM 시리즈 - 1&lt;/h1&gt;

&lt;h2 id=&quot;rnnrecurrent-neural-network&quot;&gt;RNN(Recurrent Neural Network)&lt;/h2&gt;
&lt;p&gt;우리가 사는 세상에 연속된 일들, 혹은 시간과 연관된 일은 매우매우 많을 것이다. 예를 들자면, 지금 이 글을 읽은 당신도 앞에 있는 내용을 기억하면서 글을 읽고 있을 것이다. 일반적인 신경망 구조에서는 이 ‘기억’ 이라는 시스템이 존재 하지 않는다. 하지만 RNN은 다르다. 이놈은 ‘기억’을 할 수가 있다. 그렇다면 RNN과 기존 신경망과 어떻게 다른지를 한번 살펴보자.&lt;/p&gt;

&lt;h2 id=&quot;rnn-&quot;&gt;RNN 구조&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/rnn.png&quot; alt=&quot;Drawing&quot; style=&quot;width=500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNN은 중간의 Hidden 층이 순환한다고해서 순환 신경망이라고 한다. 왼쪽의 구조를 펼쳐서 보면, 중간의 Hidden 노드가 어떤 방향으로 계속 이어진 다는 것을 알 수 있다. 이러한 쇠사슬 같은 성격은 RNN으로 하여금 연속된 이벤트와 리스트에 적합한 구조로 만들어 준다.&lt;/p&gt;

&lt;p&gt;이렇게 보면 엄청 어렵게 느껴질 수 있다. 그렇다면 예시를 들어서 RNN이 어떻게 돌아가는지 수학적으로 살펴보자.&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;기본 신경망 구조&lt;/h3&gt;

&lt;p&gt;기존의 신경 구조를 한번 다시 되새겨보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/stick.png&quot; alt=&quot;Drawing&quot; height=&quot;200&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여러개의 노드로 구성된 작은 블럭을 하나의 층이라고 가정하자. 기존의 신경망 구조는 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/basic_nn_mnist.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Input $x$ 가 선형 결합 후, Hidden 에 Activation function을 거쳐 다시 선형결합을 통해 Output $y$를 구해 예측하는 알고리즘이다. 여기서 첫번째 데이터($x_1$)와 그 다음 데이터($x_2$ 등)간의 구조는 독립적이라고 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;forward&quot;&gt;Forward&lt;/h3&gt;
&lt;p&gt;예시로 time step($T$)이 3인 RNN을 살펴보자. (좌우 클릭으로 프로세스 과정 볼 수 있다)&lt;/p&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_0.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_1.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_2.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_3.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_4.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_5.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_6.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_7.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_8.png&quot; /&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;p&gt;Time step = 0 일때, 각각 Layer들의 Weight를 초기화하게 된다. $h_0$ 층은 0으로, 나머지는 Xavier 가중치 초기값으로 초기화한다. 또한 각 가중치는 각각 layer에서 공유하게 된다.
(가중치 초기화를 잊어 버렸다면 &lt;a href=&quot;https://simonjisu.github.io/datascience/2018/01/24/numpywithnn_6.html&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;여기&lt;/span&gt;&lt;/a&gt;로)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
h_t &amp;= \tanh(W_{hh} h_{t-1}+W_{xh}x_t+b_h) \\
y_t &amp;= W_{hy} h_t + b_y
\end{aligned}
\quad for\ t\ in\ T %]]&gt;&lt;/script&gt;

&lt;p&gt;그리고, 시간이 지날때마 위의 식 처럼 Forward가 진행된다.&lt;/p&gt;

&lt;p&gt;최종 Cost는 모든 Cost Function의 평균으로 구해진다.&lt;/p&gt;

&lt;h3 id=&quot;backward&quot;&gt;Backward&lt;/h3&gt;
&lt;p&gt;RNN에서는 일반적인 신경망과 다른 Backward 알고리즘을 쓴다. 시간 경과에 따른 BackPropagation을 BPTT(BackPropagation Through Time)이라고 부른다.&lt;/p&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back0.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back1.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back2.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back3.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back4.png&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_back5.png&quot; /&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;p&gt;최종적으로 학습 될 값은 Loss Function에서 각 미분한 ${\frac{\partial L}{\partial W}}^{(1)}$, ${\frac{\partial L}{\partial W}}^{(2)}$, ${\frac{\partial L}{\partial W}}^{(3)}$ 의 합으로 구해진다.&lt;/p&gt;

&lt;h3 id=&quot;long-term-dependency-&quot;&gt;장기 의존성(Long-Term Dependency) 문제&lt;/h3&gt;
&lt;p&gt;RNN이 이론상으로는 sequence의 첫번째 항부터 끝까지(즉, $x_1 \cdots x_T$ 까지) 학습 할 수 있을 것으로 보이나, 실제로는 장기기억, 즉 Time Step이 길어 질 수록 예전에 있던 정보를 기억 못한다. 이를 &lt;strong&gt;장기 의존성(Long-Term Dependency)&lt;/strong&gt; 문제라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/rnn_bad.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 이유는 우리가 업데이트 하려는 미분 식을 살펴보면 알 수 있다. 예를 들어 $W_{hh}$ 를 업데이트 한다고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\dfrac{\partial L}{\partial W_{hh}}  
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial W_{hh}} + \cdots +
\dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial W_{hh}} \\
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial y_T} \dfrac{\partial y_T}{\partial h_T} \dfrac{\partial h_T}{\partial h_{T-1}}  \cdots \dfrac{\partial h_2}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}} +
\cdots + \dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}} \\
&amp;= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial y_T} \dfrac{\partial y_T}{\partial h_T} \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}} \dfrac{\partial h_1}{\partial W_{hh}} + \cdots + \dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위의 식중에 $\prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}}$ 부분을 자세히 펼쳐보면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}}
&amp;= \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial a_{T-i+1}} \dfrac{\partial a_{T-i+1}}{\partial h_{T-i}} \\
&amp;= \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial a_{T-i+1}} W_{hh}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;여기서 $a_t=W_{hh}h_{t-1} + W_{xh}x_t + b_h$ 이다.&lt;/p&gt;

&lt;p&gt;앞부분 $\frac{\partial h_{T-i+1}}{\partial a_{T-i+1}}$은 &lt;strong&gt;tanh&lt;/strong&gt; 의 미분 값이다. 아래 그림과 같이 tanh의 미분 값은 0과 1사이의 값이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/rnn/tanh.png&quot; style=&quot;width=500px&quot; /&gt;
(그림출처: http://nn.readthedocs.io/en/latest/transfer/)&lt;/p&gt;

&lt;p&gt;뒷부분인 $W_{hh}$의 값들은 세가지 경우가 있다. 1과 같게 되면 Gradient가 수렴될 가능성이 높다. 그러나 1보다 클 경우 gradient가 무한대로 발산하는 &lt;strong&gt;Exploding Gradient&lt;/strong&gt; 문제가 발생한다. 그러나 보통의 경우 $W_{hh}$ 의 값들은 1보다 작다. (아래 논문 참고)&lt;/p&gt;

&lt;p&gt;0과 1사이의 작은 값을 계속 곱하게 되면 0으로 수렴한다. 따라서, 두 가지를 종합 해보았을 때, 출력값과 멀리 떨어진 Time Step일 수록 역전파가 전달 되지 않는 &lt;strong&gt;Vanishing Gradient&lt;/strong&gt; 문제가 생기게 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v28/pascanu13.pdf&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;On the difficulty of training recurrent neural networks&lt;/span&gt;&lt;/a&gt; 논문에서는 Vanishing &amp;amp; Exploding Gradient 문제를 자세히 다루고 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;장기기억을 하지 못한다는 문제가 생기면서, 이를 해결하기 위해서 몇 가지 방법이 나왔다. 첫째로, Activation Function을 &lt;strong&gt;tanh&lt;/strong&gt; 을 쓰면 기울기가 0과 1사이의 값으로 고정되니 &lt;strong&gt;ReLU&lt;/strong&gt; 를 쓰자는 방법이 있었다. 둘째로, &lt;strong&gt;LSTM&lt;/strong&gt;, &lt;strong&gt;GRU&lt;/strong&gt; 등 새로운 방법들이 등장했다. 이 방법은 다음 시간에 설명하겠다. 더불어 Backward 의 계산 그래프도 같이 첨부하겠다.&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Mar 2018 10:46:04 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/03/07/rnnlstm.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/03/07/rnnlstm.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
      <item>
        <title>NUMPY with NN - 8: Summary</title>
        <description>&lt;h1 id=&quot;numpy--neural-network-basic---8&quot;&gt;Numpy로 짜보는 Neural Network Basic - 8&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;section&quot;&gt;총 정리&lt;/h2&gt;
&lt;p&gt;지금까지 우리는 Neural Network의 기원부터 Feedforward 과정, BackPropogation 과정, 그리고 다양한 학습 관련 기술을 배웠다. 이들을 총 정리해서 Mnist 데이터를 다시 학습 시켜보자.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/simonjisu/ML/tree/master/NeuralNetwork/common&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt; 모든 코드 링크&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;package-load&quot;&gt;Package Load&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from common.Multilayer import MLP
from dataset.mnist import load_mnist
from common.optimizer import *
import time
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;data-load&quot;&gt;Data Load&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;(60000, 784)&lt;br /&gt;(60000, 10)&lt;br /&gt;(10000, 784)&lt;br /&gt;(10000, 10)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;network--optimizer-settings&quot;&gt;Network &amp;amp; Optimizer settings&lt;/h3&gt;

&lt;p&gt;우리의 네트워크는 총 3층이며 Input Size가 784, Hidden node는 각각 100, 50개, Output 은 10(숫자 0~9까지의 손글씨 분류이기 때문)이다.&lt;/p&gt;

&lt;p&gt;활성화 함수는 &lt;strong&gt;ReLu&lt;/strong&gt;, 초기값도 이에 따라 &lt;strong&gt;He&lt;/strong&gt; 를 써준다. 그리고 중간에 Batch Normalization을 써준다.&lt;/p&gt;

&lt;p&gt;가중치 업데이트를 위한 옵티마이저는 &lt;strong&gt;Adam&lt;/strong&gt; 을 쓰고, Loss Function은 &lt;strong&gt;Cross Entropy&lt;/strong&gt; 를 쓰게 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nn = MLP(input_size=784, hidden_size=[100, 50], output_size=10,
         activation='relu', weight_init_std='he', use_batchnorm=True)
optimizer = Adam()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;training--test&quot;&gt;Training &amp;amp; test&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_loss_list = []
train_acc_list = []
test_acc_list = []
epoch_list = []

epoch_num=3000
train_size = x_train.shape[0]
batch_size = 100
epsilon = 1e-6

iter_per_epoch = max(train_size / batch_size, 1)

start = start = time.time()

for epoch in range(epoch_num):
    # get mini batch:
    batch_mask = np.random.choice(train_size, batch_size) # shuffle 효과
    x_batch = x_train[batch_mask]
    y_batch = y_train[batch_mask]

    grads = nn.gradient(x_batch, y_batch)

    optimizer.update(nn.params, grads)

    # 1에폭당 정확도 계산
    if epoch % iter_per_epoch == 0:
        loss = nn.loss(x_batch, y_batch)
        train_loss_list.append(loss)
        train_acc = nn.accuracy(x_train, y_train)
        train_acc_list.append(train_acc)
        test_acc = nn.accuracy(x_test, y_test)
        test_acc_list.append(test_acc)
        epoch_list.append(epoch)
        print('# {0} | loss: {1:.5f} | trian acc: {2:.5f} | test acc: {3:.5f}'.format(epoch, loss, train_acc, test_acc))
    elif epoch == (epoch_num - 1):
        loss = nn.loss(x_batch, y_batch)
        train_loss_list.append(loss)
        train_acc = nn.accuracy(x_train, y_train)
        train_acc_list.append(train_acc)
        test_acc = nn.accuracy(x_test, y_test)
        test_acc_list.append(test_acc)
        epoch_list.append(epoch)
        print('# {0} | loss: {1:.5f} | trian acc: {2:.5f} | test acc: {3:.5f}'.format(epoch, loss, train_acc, test_acc))

end = time.time()
print('total time:', (end - start))        
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;# 0 | loss: 11.06622 | trian acc: 0.11408 | test acc: 0.11910&lt;br /&gt;# 600 | loss: 0.23165 | trian acc: 0.92055 | test acc: 0.92020&lt;br /&gt;# 1200 | loss: 0.19112 | trian acc: 0.93975 | test acc: 0.94180&lt;br /&gt;# 1800 | loss: 0.08235 | trian acc: 0.95188 | test acc: 0.95040&lt;br /&gt;# 2400 | loss: 0.09155 | trian acc: 0.95898 | test acc: 0.95600&lt;br /&gt;# 2999 | loss: 0.09883 | trian acc: 0.96513 | test acc: 0.96150&lt;br /&gt;total time: 27.169809818267822&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/train_test-graph.png&quot; alt=&quot;Drawing&quot; style=&quot;width=500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;시간은 3000 Epoch를 도는데 약 30초가 안걸렸으며, 테스트 결과도 우수하게 나오는 것으로 확인된다. CNN으로 하면 더 높아질 것으로 예상된다.&lt;/p&gt;

&lt;h3 id=&quot;model-check&quot;&gt;Model Check&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def check(x, y, model):
    pred_y = model.predict(x)
    if x.ndim != 2:
        x = x.reshape(28, 28)

    print('Predict Answer: {}'.format(np.argmax(pred_y)))
    print('Real Answer: {}'.format(np.argmax(y)))
    plt.imshow(x, cmap='binary')
    plt.grid(False)
    plt.axis('off')
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;테스트 데이터중 하나 골라서 실험해보자&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;check(x_test[45], y_test[45], nn)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Predict Answer: 5&lt;br /&gt;Real Answer: 5&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/ML/nn/num5.png&quot; alt=&quot;Drawing&quot; height=&quot;100&quot; width=&quot;100&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Thu, 08 Feb 2018 15:13:40 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/02/08/numpywithnn_8.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/02/08/numpywithnn_8.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
      <item>
        <title>NUMPY with NN - 7: Batch Normalization</title>
        <description>&lt;h1 id=&quot;numpy--neural-network-basic---7&quot;&gt;Numpy로 짜보는 Neural Network Basic - 7&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;part-3&quot;&gt;학습관련 기술 Part 3&lt;/h2&gt;

&lt;h3 id=&quot;batch-normalization&quot;&gt;배치 정규화 (Batch Normalization)&lt;/h3&gt;
&lt;p&gt;배치 정규화란 미니배치 단위로 선형합인 &lt;strong&gt;$a$&lt;/strong&gt; 값을 정규화하는 것이다. 즉, 미니배치에 한해서 데이터 분포가 평균이 0 분산이 1이 되도록 한다. 이는 데이터 분포가 덜 치우치게 하는 효과가 있어서 가중치 초기화 값의 영향을 덜 받게한다. 또한, 학습속도를 증가시키고 regularizer 역할을 하여 Overfitting을 방지함으로 Dropout의 필요성을 줄인다. &lt;del&gt;자세한 내용은 논문을 참고하자!&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;기본적인 아이디어는 아래와 같다. $D$ 차원의 미니배치 데이터 $x = (x^{(1)}, \cdots, x^{(k)}, \cdots, x^{(D)})$에 대해서 각각의 평균과 분산을 구한 후, 정규화를 통해 새로운 $x^{(k)}$ ($\hat{x}^{(k)}$) 를 구한 후에 Scaling($\gamma$) 과 Shifting($\beta$)을 거쳐 새로운 $y$ 를 기존의 선형합성 곱인 $a$ 를 대신해 활성화 함수에 넣는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/6/batch_norm_idea.png&quot; alt=&quot;Drawing&quot; style=&quot;width=500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서, 하나의 Hidden Layer 는 $Affine \rightarrow BatchNorm \rightarrow Activation$ 으로 구성된다.&lt;/p&gt;

&lt;h3 id=&quot;backpropogation-&quot;&gt;배치 정규화의 BackPropogation 이해하기&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/NN_batchnorm.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;forward&quot;&gt;Forward:&lt;/h4&gt;

&lt;p&gt;x 부터 out 까지 차근차근 진행해보자. 헷갈리지 말아야할 점은 위에 공식에서 $i$ 는 batch를 iteration 한 것이라는 점이다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Forward Process
# step-1: mu (D,)
mu = x.mean(axis=0)
# step-2: xmu (N, D)
xmu = x - mu
# step-3: sq (N, D)
sq = xmu**2
# step-4: var (D,)
var = np.mean(sq, axis=0)
# step-5: std (D,)
std = np.sqrt(var + 1e-6)
# step-6: invstd (D,)
invstd = 1.0 / std
# step-7: xhat (N, D)
xhat = xmu * invstd
# step-8: scale (N, D)
scale = gamma * xhat
# step-9: out (N, D)
out = scale + beta
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;backward&quot;&gt;Backward:&lt;/h4&gt;

&lt;p&gt;우리의 목표는 $\dfrac{\partial L}{\partial x}, \dfrac{\partial L}{\partial \gamma}, \dfrac{\partial L}{\partial \beta}$ 를 구해서, $\dfrac{\partial L}{\partial x}$ 는 Affine Layer로 역전파 시키고 $\gamma, \beta$ 는 학습 시키는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step-9:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $out(scale, \beta) = scale + \beta$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;더하기 노드의 역전파는 그대로 흘러간다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases} dscale = \dfrac{\partial L}{\partial scale} = \dfrac{\partial L}{\partial out} \dfrac{\partial out}{\partial scale} = 1 * dout \\
\\
d\beta = \dfrac{\partial L}{\partial \beta} = \dfrac{\partial L}{\partial out} \dfrac{\partial out}{\partial \beta} = 1 * \sum_i^N dout \end{cases}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-8:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $scale(\gamma, \hat{x}_i) = \gamma \ * \ \hat{x}_i$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;곱의 노드의 역전파는 들어왔던 신호를 역으로 곱해서 흘려 보낸다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}
d\hat{x}_i = \dfrac{\partial L}{\partial \hat{x}_i} = \dfrac{\partial L}{\partial scale} \dfrac{\partial scale}{\partial \hat{x}_i} = 1 * \sum_i^N dout \\
\\
d\gamma = \dfrac{\partial L}{\partial \gamma} = \dfrac{\partial L}{\partial scale} \dfrac{\partial scale}{\partial \gamma} = \sum_i^N dout \ * \ \hat{x}_i
\end{cases}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-7:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $\hat{x}_i(xmu, invstd) = xmu \ * \ invstd$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$xmu$는 윗쪽(step-7 $\rightarrow$ step-2)과 아래쪽(step-3 $\rightarrow$ step-2) 으로 두 번 돌아가기 때문에 첨자를 단다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}
dxmu_1= \dfrac{\partial L}{\partial xmu_1} = \dfrac{\partial L}{\partial \hat{x}_i} \dfrac{\partial \hat{x}_i}{\partial xmu_1} = d\hat{x}_i \ * \ invstd \\
\\
dinvstd = \dfrac{\partial L}{\partial \hat{x}_i} = \dfrac{\partial L}{\partial \hat{x}_i} \dfrac{\partial \hat{x}_i}{\partial invstd} = d\hat{x}_i \ * \ xmu
\end{cases}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-6:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $invstd(\sigma) = \dfrac{1}{\sigma}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$f(x) = \dfrac{1}{x}$ 의 미분은 $f’(x) = -\dfrac{1}{x^2} = -f(x)^2$ 이기 때문에 아래와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d\sigma = \dfrac{\partial L}{\partial \sigma} = \dfrac{\partial L}{\partial invstd} \dfrac{\partial invstd}{\partial \sigma} = dinvstd \ * \ (-invstd^2)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-5:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $\sigma(var) = \sqrt{var + \epsilon}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$f(x) = \sqrt{x + \epsilon}$ 의 미분은 $f’(x) = -\dfrac{1}{2}(x+\epsilon)^{-\frac{1}{2}}$ 이기 때문에 아래와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dvar = \dfrac{\partial L}{\partial var} = \dfrac{\partial L}{\partial \sigma} \dfrac{\partial \sigma}{\partial var} = d\sigma \ * \ (-\dfrac{1}{2}(var+\epsilon)^{-\frac{1}{2}})&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-4:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $var(sq) = \dfrac{1}{N} \sum_i^N sq$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$f(x) = \dfrac{1}{N} \sum_i^N x_i$ 의 미분은 $f’(x) = \dfrac{1}{N} \sum_i^N 1$ 이기 때문에 아래와 같다. 단, x의 형상(shape)이 같아야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
dsq = \dfrac{\partial L}{\partial sq} = \dfrac{\partial L}{\partial var} \dfrac{\partial var}{\partial sq} = \dfrac{1}{N} dvar \ * \ \begin{bmatrix} 1 &amp; \cdots &amp; 1 \\ \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; \cdots &amp; 1 \end{bmatrix}_{(N, D)} = \dfrac{1}{N} dvar \ * \ ones(N, D) %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-3:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $sq = xmu^2$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$f(x) = x^2$ 의 미분은 $f’(x) = 2x$ 이기 때문에 아래와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dxmu_2 = \dfrac{\partial L}{\partial xmu_2} = \dfrac{\partial L}{\partial sq} \dfrac{\partial sq}{\partial xmu_2} = dsq \ * \ 2 \ xmu&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-2:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $xmu = x_i - \mu$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$dxmu = dxmu_1 + dxmu_2$ 로 정의 된다. 곱의 미분 법칙 생각해보면 된다. $h(x) = f(x) g(x)$ 를 $x$ 에 대해서 미분하면 $f’(x)g(x) + f(x)g’(x)$ 기 때문이다. &lt;br /&gt;
또한 이것도 덧셈과 마찬가지로 그대로 흘러 보내는다 밑에 쪽은 -1 을 곱해서 흘려 보낸다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}
dx_1= \dfrac{\partial L}{\partial x_1} = \dfrac{\partial L}{\partial xmu} \dfrac{\partial xmu}{\partial x_1} = dmu \ * \ 1 \\
\\
d\mu = \dfrac{\partial L}{\partial \mu} = \dfrac{\partial L}{\partial xmu} \dfrac{\partial xmu}{\partial \mu} = \sum_i^N dxmu \ * \ (-1)
\end{cases}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-1:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forward : $\mu = \dfrac{1}{N} \sum_i^N x_i$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;step-4에서 설명했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
dx_2 = \dfrac{\partial L}{\partial x_2} = \dfrac{\partial L}{\partial \mu} \dfrac{\partial \mu}{\partial x_2} = \dfrac{1}{N} d\mu \ * \ \begin{bmatrix} 1 &amp; \cdots &amp; 1 \\ \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; \cdots &amp; 1 \end{bmatrix}_{(N, D)} = \dfrac{1}{N} d\mu \ * \ ones(N, D) %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step-0:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;최종적으로 구하는 $dx = \dfrac{\partial L}{\partial x} = dx_1 + dx_2$ 로 정의 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Backward Process
# step-9: out = scale + beta
dbeta = dout.sum(axis=0)
dscale = dout
# step-8: scale = gamma * xhat
dgamma = np.sum(xhat * dout, axis=0)
dxhat = gamma * dscale
# step-7: xhat = xmu * invstd
dxmu1 = dxhat * invstd
dinvstd = np.sum(dxhat * xmu, axis=0)
# step-6: invstd = 1 / std
dstd = dinvstd * (-invstd**2)
# step-5: std = np.sqrt(var + 1e-6)
dvar = -0.5 * dstd * (1 / np.sqrt(var + 1e-6))
# step-4: var = sum(sq)
dsq = (1.0 / batch_size) * np.ones(input_shape) * dvar
# step-3: sq = xmu**2
dxmu2 = dsq * 2 * xmu
# step-2: xmu = x - mu
dxmu = dxmu1 + dxmu2
dmu = -1 * np.sum(dxmu, axis=0)
dx1 = dxmu * 1
# step-1: mu = mean(x)
dx2 = (1.0 / batch_size) * np.ones(input_shape) * dmu
# step-0:
dx = dx1 + dx2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section&quot;&gt;실제 구현&lt;/h4&gt;
&lt;p&gt;그러나 실제 구현 시에는 training 과 testing을 나눠서 아래와 같이 진행된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/6/batch_norm_al.png&quot; alt=&quot;Drawing&quot; style=&quot;width=500px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;backpropogation---&quot;&gt;첨부: Backpropogation 전체 미분 수학식&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;수식의 이해는 이분의 블로그에서 많은 참조를 했다. Blog: [&lt;a href=&quot;http://cthorey.github.io/backpropagation/&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Clement Thorey&lt;/span&gt;&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Y &amp;= \gamma \hat{X} + \beta \\
\hat{X} &amp;= (X - \mu)(\sigma^2+\epsilon)^{-1/2}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;size:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Y, \hat{X}, X &amp;= (N, D) \\
\mu, \sigma, \gamma, \beta &amp;= (D,)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;$N$은 미니 배치 싸이즈고, $D$는 데이터의 차원 수다.&lt;/p&gt;

&lt;p&gt;Matrix 로 정의한 수식을 다시 원소별로 표기를 정의 해보자. 매트릭스 $Y, X, \hat{X}$ 와 벡터 $\gamma, \beta$ 그리고 위에 수식은 아래와 같이 다시 정의 해볼 수 있다. (왜 매트릭스와 벡터인지는 Forward 과정에 나와있다. 각 차원별로 평균과 분산을 구하는걸 잊지말자)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
y_{kl} &amp;= \gamma_l \hat{x}_{kl} + \beta_l \\
\hat{x}_{kl} &amp;= (x_{kl} - \mu_l)(\sigma_l^2+\epsilon)^{-1/2}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;where\quad \mu_l = \dfrac{1}{N} \sum_{p=1}^{N} x_{pl} , \quad \sigma_l^2 = \dfrac{1}{N} \sum_{p=1}^{N} (x_{pl}-\mu_l)^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;with\quad k = [1, \cdots, N] \ ,\  l = [1, \cdots, D]&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이제 우리고 구하려고 하는 미분 값들$(\dfrac{\partial L}{\partial x}, \dfrac{\partial L}{\partial \gamma}, \dfrac{\partial L}{\partial \beta})$을 하나씩 구해보자.&lt;/p&gt;

&lt;h4 id=&quot;xij---&quot;&gt;$x_{ij}$ 에 대한 미분&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\dfrac{\partial L}{\partial x_{ij}}
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial x_{ij}} \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \hat{x}_{kl}} \dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \cdot \gamma_l \cdot \dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} = \dfrac{\partial f}{\partial {x}_{ij}} g + f \dfrac{\partial g}{\partial {x}_{ij}} \quad where \quad \begin{cases} f = (x_{kl} - \mu_l) \\ g = (\sigma_l^2+\epsilon)^{-1/2} \end{cases}&lt;/script&gt; 에 대한 미분을 구해보자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;우선 분자 $f = (x_{kl} - \mu_l)$ 에 대한 미분을 하면 아래와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial f}{\partial {x}_{ij}} = \delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl}&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_{m,n} = \begin{cases} 1 \quad where \quad m = n \\ 0 \quad otherwise \end{cases}&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;$\delta_{m,n}$ 은 앞첨자 $m$ 이 뒷첨자 $n$과 같다면 1이 된다는 뜻이다.&lt;/p&gt;

&lt;p&gt;즉, 여기서 $i$ 가 $[1 \cdots k \cdots D]$ 까지, $j$ 가 $[1 \cdots l \cdots D]$ 까지 iteration 할 것인데, 오직 $i=k, j=l$ 일때만 앞 항인 $\delta_{il} \delta_{jl} = 1$ 이 될 것이고, $j=l$ 일때만 뒷항인 $\frac{1}{N} \delta_{jl} = \frac{1}{N}$ 이 될 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;분모 $g = (\sigma_l^2+\epsilon)^{-1/2}$ 에 대한 미분은 아래와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial g}{\partial {x}_{ij}} = -\dfrac{1}{2}(\sigma_l^2 + \epsilon)^{-3/2} \dfrac{\partial \sigma_l^2}{\partial x_{ij}}&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} where \quad \sigma_l^2
&amp;= \dfrac{1}{N} \sum_{p=1}^{N} (x_{pl}-\mu_l)^2 \\
\dfrac{\partial \sigma_l^2}{\partial x_{ij}}
&amp;= \dfrac{1}{N} \sum_{p=1}^{N} 2(x_{pl}-\mu_l)(\delta_{ip} \delta_{jl} - \frac{1}{N} \delta_{jl}) \\
&amp;= \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl} - \dfrac{2}{N^2} \sum_{p=1}^N (x_{pl}-\mu_l) \delta_{jl} \\
&amp; = \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl} - \dfrac{2}{N} \delta_{jl} (\dfrac{1}{N}  \sum_{p=1}^N  (x_{pl}-\mu_l)) \cdots (1) \\
&amp; = \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;(1) 번 식을 잠깐 이야기 하면 $\dfrac{1}{N} \sum_{p=1}^N  (x_{pl}-\mu_l) = 0$ 인것은 어떤 값들을 평균을 빼고 다시 평균 시키면 0이 된다.&lt;/p&gt;

&lt;p&gt;$e.g)\quad \frac{(1-2)+(2-2)+(3-2)}{3}=0$&lt;/p&gt;

&lt;p&gt;이제 드디어 &lt;script type=&quot;math/tex&quot;&gt;\dfrac{\hat{x}_{kl}}{\partial {x}_{ij}}&lt;/script&gt; 에 대해 구할수 있다. 곱의 미분 법칙을 사용하면 아래와 같이 전개 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \dfrac{\hat{x}_{kl}}{\partial {x}_{ij}}
&amp;= (\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2}  -\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;최종적으로 우리의 목적 &lt;script type=&quot;math/tex&quot;&gt;\dfrac{\partial L}{\partial x_{ij}}&lt;/script&gt; 를 구해보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\dfrac{\partial L}{\partial x_{ij}}
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \cdot \gamma_l \cdot [(\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2} - \dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [(\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2}] - \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l (\delta_{ik} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l \delta_{jl}(\sigma_l^2+\epsilon)^{-1/2} - \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&amp;= \dfrac{\partial L}{\partial y_{ij}} \gamma_l \delta_{ii} \delta_{jj} (\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_k \dfrac{\partial L}{\partial y_{kj}} \gamma_l \delta_{jj}(\sigma_j^2+\epsilon)^{-1/2} - \dfrac{1}{N} \sum_{k} \dfrac{\partial L}{\partial y_{kj}} \gamma_l [ (x_{kj} - \mu_j)(\sigma_j^2 + \epsilon)^{-3/2} (x_{ij}-\mu_j) \delta_{jj}] \cdots (2) \\
&amp;= \dfrac{\partial L}{\partial y_{ij}} \gamma_l (\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_k \dfrac{\partial L}{\partial y_{kj}} \gamma_l (\sigma_j^2+\epsilon)^{-1/2} - \dfrac{1}{N} \sum_{k} \dfrac{\partial L}{\partial y_{kj}} \gamma_l (x_{kj} - \mu_j)(\sigma_j^2 + \epsilon)^{-3/2} (x_{ij}-\mu_j) \\
&amp;= \dfrac{1}{N} \gamma_l (\sigma_l^2+\epsilon)^{-1/2} [N \dfrac{\partial L}{\partial y_{ij}} - \sum_k \dfrac{\partial L}{\partial y_{kj}} - (x_{ij}-\mu_j)(\sigma_j^2 + \epsilon)^{-1} \sum_{k} \dfrac{\partial L}{\partial y_{kj}}(x_{kj} - \mu_j)]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;(2) 번 식으로 도출 되는 과정을 잘 살펴보면, 각 항마다 곱으로 구성되어 있다. 첫번째 항은 $\sum_{k, l}$ 에서 오직 $k=i, l=j$ 일때 남아 있고 나머지는 전부다 0 이고, 두번째 항은 오직 $l=j$ 일때 남아있고 나머지는 전부다 0 이다. 그리고 마지막도 마친가지로 $l=j$ 일때만 남아있는다.&lt;/p&gt;

&lt;h4 id=&quot;gammaj---&quot;&gt;$\gamma_j$ 에 대한 미분&lt;/h4&gt;

&lt;p&gt;위에 까지 이해했으면 $\gamma_l$ 에 대한 미분은 간단하다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\dfrac{\partial L}{\partial \gamma_j}
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \gamma_j} \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \hat{x}_{kl} \delta_{jl} \\
&amp;= \sum_k \dfrac{\partial L}{\partial y_{kj}} \hat{x}_{kj} \\
&amp;= \sum_k \dfrac{\partial L}{\partial y_{kj}} (x_{kj} - \mu_j)(\sigma_j^2+\epsilon)^{-1/2}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;betaj---&quot;&gt;$\beta_j$ 에 대한 미분&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\dfrac{\partial L}{\partial \beta_j}
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \gamma_j} \\
&amp;= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \delta_{jl} \\
&amp;= \sum_k \dfrac{\partial L}{\partial y_{kj}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;여기서 우리는 왜 위에 step-9, 8 코드 구현에서 dgamma와 dbeta를 summation 하는지 알 수 있다.&lt;/p&gt;

&lt;p&gt;다음 마지막 시간에는 모든걸 종합해서 학습하는 과정을 코드로 살펴보자.&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jan 2018 12:54:15 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/01/25/numpywithnn_7.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/01/25/numpywithnn_7.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
      <item>
        <title>NUMPY with NN - 6: Weight Initialization</title>
        <description>&lt;h1 id=&quot;numpy--neural-network-basic---6&quot;&gt;Numpy로 짜보는 Neural Network Basic - 6&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;part-2&quot;&gt;학습관련 기술 Part 2&lt;/h2&gt;

&lt;h3 id=&quot;weight-initialization&quot;&gt;가중치 초기값 설정(Weight Initialization)&lt;/h3&gt;
&lt;p&gt;이전에 활성화 함수가 왜 중요한지 이야기 했었다. 다시 한 번 이야기 하면, 비 선형 활성화 함수를 사용해서 선형으로만 표현할 수 없는 값을 표현할 수 있게 되며, 그로 인해 은닉층을 쌓는 의미가 생긴다. 이런 비 선형 함수의 가중치 미분을 구해서 학습하고자 하는 파라미터를 업데이트 하게 된다.&lt;/p&gt;

&lt;p&gt;[&lt;a href=&quot;https://simonjisu.github.io/datascience/2017/12/08/numpywithnn_2.html&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Numpy로 짜보는 Neural Network Basic - 2&lt;/span&gt;&lt;/a&gt;] 참고&lt;/p&gt;

&lt;p&gt;그렇다면 Sigmoid를 예를 들어서 이야기 해보자, 아래 그림은 Sigmoid 함수와 미분의 그래프다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \sigma(a) &amp; = \dfrac{1}{1+e^{-a}} \\ \sigma'(a) &amp;= \sigma(a)(1 - \sigma(a))\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/6/sigmoid_prime.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;선형결합을 통해 구해진 값 a은 뉴런을 거쳐서 Sigmoid로 활성화 된 함수는 대부분 $[0, 1]$ 사이의 값을 가지게 될 것이다. 선형결합을 통해 구해진 값이 조금만 커져도 (약 $[-5, 5]$ 이외 값) Gradient 값이 0으로 되는 경우가 많아진다. 이를 &lt;strong&gt;Gradient Vanishing Problem&lt;/strong&gt;, 즉 가중치 0에 가까워져 업데이트 안되는 현상을 말한다. 따라서 가중치 초기 값이 엄청 작게 설정 했다고 해도 dot product해진 값이 커지면 이런 현상이 일어날 수가 있다.&lt;/p&gt;

&lt;h4 id=&quot;mnist--&quot;&gt;Mnist 데이터로 살펴보기&lt;/h4&gt;

&lt;p&gt;실제로 활성화 함수 값이 어떤 분포인지 mnist 데이터로 살펴보자. 내가 만든 뉴럴 네트워크의 구조는 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} Input_{(784)}
&amp;\rightarrow [Affine1 \rightarrow Activation1]_{hidden1: (100)} \\
&amp;\rightarrow [Affine2 \rightarrow Activation2]_{hidden2: (50)} \\
&amp;\rightarrow [Affine3 \rightarrow SoftmaxwithLoss]_{output:(10)_{}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Hidden Node 는 각 100개와 50개로 설정하고 Activation Fucntion은 simgoid로 했다.
아래는 200 epoch까지 중간에 Activation 값들의 분포를 찍어본 것이다. 가중치 초기 값은 1을 곱한 것으로써 랜덤 Initialization 되었다고 생각하면 된다.(= $W$에다 1을 곱했다.)&lt;/p&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/6/sig_act1.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;

&lt;p&gt;대부분의 값이 0과 1로 이루어져 있다는 것을 알 수 있다. 이는 즉 대부분의 값이 미분을 했을 때 0이될 가능성이 높다는 뜻이다.&lt;/p&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/6/sig_back1.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;

&lt;p&gt;앞쪽의 Layer로 backward 할 수록 가중치 미분 값이 0에 가까워지는 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;우리는 &lt;a href=&quot;https://simonjisu.github.io/datascience/2017/12/15/numpywithnn_4.html&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;4편&lt;/span&gt;&lt;/a&gt;에서 가중치 값을 0.01 초기화 시켰더니 학습이 전혀 안된 모습을 볼 수 있었다. 그렇다면 0.01로 가중치 값을 초기화 하면 어떻게 될까?&lt;/p&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/6/sig_act2.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;

&lt;p&gt;가중치 활성화 값이 0.5로 치우쳐져 두 레이어의 분포가 거의 비슷해졌다. 다수의 뉴런이 같은 값을 출력하고 있다는 뜻으로 우리가 비선형함수를 써서 예측 불가능하게 만드려고 한 노력을 물거품으로 만들어 버렸다. 즉, 위에서 이야기 했던 층을 여러게 쌓은 의미가 없어진다. 이를 &lt;strong&gt;“표현력이 제한된다”&lt;/strong&gt; 라고 말한다.&lt;/p&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/6/sig_back2.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;

&lt;p&gt;가중치의 미분 값들이 대부분 0 근처에 있는 것을 확인 할 수가 있다.&lt;/p&gt;

&lt;p&gt;따라서 초기 값을 잘 설정해주어야 하는데, Sigmoid 함수에 대해서 자주 사용하는 Xavier 초기 값이 있다.&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=&quot;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W \sim Uniform(n_{in}, n_{out})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Var(W)=\dfrac{1}{n_{in}}&lt;/script&gt;

&lt;p&gt;즉 가중치 초기화를 할때 각 파라미터 $W$ 에 대하여 $\dfrac{1}{\sqrt{n_{in}}}$ 를 곱해주는 것이다.&lt;/p&gt;

&lt;p&gt;아래는 Xavier 가중치 초기값을 설정했을 때의 Activation 분포다. 두 층이 전혀 다른 분포가 되어있는 것을 볼 수가 있다.&lt;/p&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/6/sig_act3.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/6/sig_back3.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;

&lt;p&gt;각각의 가중치 초기화 값에 대한 학습 결과를 살펴보자. 총 10000번의 epochs를 돌린 결과다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;학습결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/6/sig1.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$w_{std} = 1$: 처음에 빠른것 같지만 나중에 굉장히 천천히 학습 되는 것을 확인 할 수 있다. &lt;br /&gt; 또한 매번 실행시 학습 속도가 다르다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/6/sig2.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$w_{std} = 0.01$: 학습이 전혀 안되는 것을 확인 할 수 있다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/6/sig3.png&quot; alt=&quot;Drawing&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$w_{std} = \dfrac{1}{\sqrt{n}}$: test 성적이 조금 더 좋아 졌다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;relu&quot;&gt;활성화 함수를 바꿔보자: ReLu&lt;/h4&gt;
&lt;p&gt;ReLu의 미분 값은 $x &amp;gt; 0$ 에서 $1$ 이고 나머지는 $0$ 이다. ReLu는 &lt;strong&gt;He&lt;/strong&gt; 라는 초기값을 설정하게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W \sim Uniform(n_{in}, n_{out})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Var(W)=\dfrac{2}{n_{in}}&lt;/script&gt;

&lt;p&gt;즉, 가중치 $W$ 에 $\sqrt{\dfrac{2}{n_{in}}}$ 을 곱하게 된다. 아래 동영상을 보면 0보다 큰 부분에서 꾸준히 활성화 되는 모습을 볼 수 있다.&lt;/p&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/6/relu_act3.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/6/relu_back3.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;

&lt;p&gt;다음 시간에는 배치 정규화에 대해서 알아보자.&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Jan 2018 12:31:37 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/01/24/numpywithnn_6.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/01/24/numpywithnn_6.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
      <item>
        <title>NUMPY with NN - 5: Optimizer</title>
        <description>&lt;h1 id=&quot;numpy--neural-network-basic---5&quot;&gt;Numpy로 짜보는 Neural Network Basic - 5&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;part-1&quot;&gt;학습관련 기술 Part 1&lt;/h2&gt;

&lt;h3 id=&quot;optimizer&quot;&gt;Optimizer&lt;/h3&gt;

&lt;p&gt;손실 함수 값을 가능한 낮게 만들어 매개변수 최적값을 찾는 과정을 &lt;strong&gt;최적화&lt;/strong&gt; 라고 한다. 여기서 몇가지 방법을 한번 살펴본다.&lt;/p&gt;

&lt;h4 id=&quot;sgd--&quot;&gt;SGD(확률적 경사 하강법)&lt;/h4&gt;
&lt;p&gt;$W \leftarrow W - \eta \dfrac{\partial L}{\partial W}$&lt;/p&gt;

&lt;p&gt;$\eta$ 는 학습률로 얼만큼 가중치를 업데이트 할지 정하는 하이퍼파라미터다. 즉 우리가 미리 정해줘야하는 변수다. 그러나 SGD 알고리즘에서는 이 변수에 따라서 학습되는 모양이 다르다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class SGD(object):
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;장점&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;일부 데이터로 업데이트를 해서 진동이 심할 수도 있지만, 전체 데이터의 Gradient를 구하는 것보다 빠르다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;단점&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;learning rate에 따라서 global min을 찾지 못하고 local min에 갇힐 가능서 존재&lt;/li&gt;
  &lt;li&gt;Oscilation(발진 현상): 해에 접근 할 수록 수렴 속도($\dfrac{\partial L}{\partial W}$)가 느려짐, 따라서 협곡 같은 모양에서 헤매는 경우 존재한다. 그렇다고 lr을 너무 높히면 발산 할 수도 있음(loss값이 커지는 현상)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래와 같은 함수의 최적값을 찾아보자.&lt;/p&gt;

&lt;p&gt;$f(x, y) = \dfrac{1}{20} x^2 + y^2$&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def f(x, y):
    return np.array((1/20)*(x**2) + (y**2))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;$f$ 를 미분하면 아래와 같다.&lt;/p&gt;

&lt;p&gt;$\dfrac{\partial f}{\partial x}, \dfrac{\partial f}{\partial y} = \dfrac{x}{10}, 2y$&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def f_prime(x, y, grads=None):
    if grads is None:
        grads = {}

    grads['x'] = (1/10)*x
    grads['y'] = 2*y
    return grads
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;시작은 &lt;strong&gt;(-7, 2)&lt;/strong&gt; 점부터 시작한다고 하면 아래처럼 그림으로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/fgraph.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 함수의 최저점은 (0, 0) 점으로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;이제 learning rate 를 0.1 과 0.9로 각각 정해서 SGD를 적요해보자. 총 30 epoch동안 Gradient를 구하고 이를 조금씩 업데이트 하는 방식을 취했다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Video&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Graph&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;video controls=&quot;controls&quot; style=&quot;width: 400px;&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/SGD_0.1.mp4&quot; /&gt; &amp;lt;/source&amp;gt;&lt;/video&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/SGD_0.1.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;learning rate 가 0.1 일때 학습이 조금씩 진행 되는 것을 볼 수 있다. 그러나 epoch 횟수가 너무 적어 최적의 값까지 도달을 못했다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Video&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Graph&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;video controls=&quot;controls&quot; style=&quot;width: 400px;&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/SGD_0.9.mp4&quot; /&gt; &amp;lt;/source&amp;gt;&lt;/video&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/SGD_0.9.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;learning rate 가 0.9 일때 학습이 크게 진행 되는 것을 볼 수 있다. 그러나 변동이 심해서 크게 흔들리면서 최저점으로 가는 모습을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;학습률이 다르다는 것은 한 번 나아갈때 폭의 길이를 보면 그 차이를 알 수 가 있다.&lt;/p&gt;

&lt;h4 id=&quot;momentum&quot;&gt;Momentum&lt;/h4&gt;
&lt;p&gt;$v \leftarrow \gamma v - \eta \dfrac{\partial L}{\partial W}$&lt;/p&gt;

&lt;p&gt;$W \leftarrow W + v$&lt;/p&gt;

&lt;p&gt;모멘텀 방식은 gradient 방향에 일종의 관성을 더해줘서 기존의 이동 방향에 힘들 실어줘 더 이동할 수 있게 만들어준다. $v$ 의 초기값은 0으로 설정하고 진행한다. 따라서 첫 step이 후 기존에 이동했던 방향을 저장해둔 $v$ 가 추가로 저장 되어 다음 step에 더해져 조금 더 움직이게 된다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Video&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Graph&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;video controls=&quot;controls&quot; style=&quot;width: 400px;&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/Momentum.mp4&quot; /&gt; &amp;lt;/source&amp;gt;&lt;/video&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/Momentum.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;learning rate가 0.1 일때 SGD보다 더 많이 가는 것을 알 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Video&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Graph&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;video controls=&quot;controls&quot; style=&quot;width: 400px;&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/Momentum_0.9.mp4&quot; /&gt; &amp;lt;/source&amp;gt;&lt;/video&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/Momentum_0.9.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;learning rate가 0.9 일때 주변을 헤매면서 가는 모습을 볼 수 있다. 하이퍼파라미터를 잘 조정해야 학습이 빠르게 진행 된 다는 것을 알 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;adagrad&quot;&gt;Adagrad&lt;/h4&gt;
&lt;p&gt;$h \leftarrow h + \dfrac{\partial L}{\partial W} \odot \dfrac{\partial L}{\partial W}$&lt;/p&gt;

&lt;p&gt;$W \leftarrow W - \eta \dfrac{1}{\sqrt{h +\epsilon}} \dfrac{\partial L}{\partial W}$&lt;/p&gt;

&lt;p&gt;학습률($\eta$)에 대한 고민이 많이지자 이를 해결해보기 위해 나온 알고리즘이 AdaGrad 다.&lt;/p&gt;

&lt;p&gt;학습률을 처음에 크게 했다 나중에 차차 줄여가는 &lt;strong&gt;학습률 감소(learning rate decay)&lt;/strong&gt; 기술이 이 알고리즘의 특징이다. 각각의 매개변수에 맞춤형 학습률 값을 맞춰 줄 수가 있다.&lt;/p&gt;

&lt;p&gt;$\odot$ 는 여기서 dot product가 아닌 element-wise multiplication를 말한다. 수식을 보면 gradient를 제곱하여 h에 저장한다. 업데이트시 여태까지 저장해온 gradient 제곱 값을 분모로 두게 된다. 따라서 시간이 지날 수록 gradient 누적 값이 큰 것은 learning rate 가 반대로 작아지게 된서 학습률이 조정 된다. 이를 적응적으로(adaptive) 학습률을 조정한다고 한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Adagrad(object):
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        self.epsilon = 1e-6  # 0으로 나누눈 것을 방지

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + self.epsilon)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Video&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Graph&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;video controls=&quot;controls&quot; style=&quot;width: 400px;&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/Adagrad.mp4&quot; /&gt; &amp;lt;/source&amp;gt;&lt;/video&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/Adagrad.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;학습률을 1.5로 크게 주었는데도 차차 감소하면서 학습되는 과정을 볼 수 가 있다.&lt;/p&gt;

&lt;p&gt;그러나 이렇게 좋아보이는 방법도 &lt;strong&gt;단점&lt;/strong&gt; 이 있다.&lt;/p&gt;

&lt;p&gt;과거의 기울기 값들을 전부 누적해서 더하기 때문에 h 값이 많이 커지게 되면 학습률 부분($\dfrac{1}{\sqrt{h +\epsilon}}$)이 1에 가까워져 업데이트 할 때 발산하는 형태로 가기 때문에 더 이상 학습이 진행이 안되는 상황이 발생할 수 있다.&lt;/p&gt;

&lt;p&gt;이를 개선하기 위해서 RMSProp과 Adadelta라는 방법이 있다. (코드는 기본 알고리즘 원리만 구현해놨다. 구체적으로 효율적인 학습을 위해서 조금씩 변형이 가해진다. 논문 참조 할 것, &lt;del&gt;아직 이해중&lt;/del&gt;)&lt;/p&gt;

&lt;p&gt;RMSProp:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class RMSProp(object):
    def __init__(self, lr=0.01, gamma=0.9):
    &quot;&quot;&quot;G는 이동평균의 개념으로 과거 1보다 작은 gamma값을 곱해서 서서히 잊게 하고 새로운 값을 조금씩 더 해준다.&quot;&quot;&quot;
        self.lr = lr
        self.gamma = gamma  # decay term
        self.G = None
        self.epsilon = 1e-6  # 0으로 나누눈 것을 방지

    def update(self, params, grads):
        if self.G is None:
            self.G = {}
            for key, val in params.items():
                self.G[key] = np.zeros_like(val)

        for key in params.keys():
            self.G[key] += self.gamma * self.G[key] + (1 - self.gamma) * (grads[key] * grads[key])
            params[key] -= self.lr * grads[key] / np.sqrt(self.G[key] + self.epsilon)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;AdaDelta:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class AdaDelta(object):
    def __init__(self, gamma=0.9):
        &quot;&quot;&quot;
        https://arxiv.org/pdf/1212.5701
        &quot;&quot;&quot;
        self.gamma = gamma  # decay term
        self.G = None  # accumulated gradients
        self.s = None  # accumulated updates
        self.del_W = None
        self.epsilon = 1e-6  # 0으로 나누눈 것을 방지
        self.iter = 0

    def update(self, params, grads):
        if (self.G is None) | (self.s is None) | (self.del_W is None):
            # Initialize accumulation variables
            self.G = {}
            self.s = {}  
            self.del_W = {}
            for key, val in params.items():
                self.G[key] = np.zeros_like(val)
                self.s[key] = np.zeros_like(val)
                self.del_W[key] = np.zeros_like(val)

        for key in params.keys():
            self.G[key] += self.gamma * self.G[key] + (1 - self.gamma) * (grads[key] * grads[key])
            self.del_W[key] = -(np.sqrt(self.s[key] + self.epsilon) / np.sqrt(self.G[key] + self.epsilon)) * grads[key]
            self.s[key] += self.gamma * self.s[key] + (1 - self.gamma) * self.del_W[key]**2
            params[key] += self.del_W[key]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h4 id=&quot;adamadaptive-moment-estimation&quot;&gt;Adam(Adaptive Moment Estimation)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Adam&lt;/strong&gt; (Adaptive Moment Estimation)은 RMSProp과 Momentum 방식을 합친 것 같은 알고리즘이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/Algorithm_Adam.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://arxiv.org/abs/1412.6980v8&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;https://arxiv.org/abs/1412.6980v8&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$m_t$: the exponential moving averages of the gradient (Momentum쪽)&lt;/li&gt;
  &lt;li&gt;$v_t$: the squared gradient (RMSProp쪽)&lt;/li&gt;
  &lt;li&gt;$\beta_1$: the exponential decay rates for $m_t$, 보통 0.9 취함&lt;/li&gt;
  &lt;li&gt;$\beta_2$: the exponential decay rates for $v_t$, 보통 0.999 취함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;알고리즘 그대로 짜는게 아니라 조금더 효율적인 계산을 하기 위해서 아래와 같은 내용을 이해하고 보정해줘야 한다…(자세한 건 논문에 더 있음)&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;initialization-bias-correction&quot;&gt;추가 설명:(18.01.16) Initialization Bias Correction&lt;/h4&gt;
&lt;p&gt;우리가 구한 $m_t$, $v_t$ 값이 초기 값이 0으로 설정하고, $\beta$ 도 1에 가깝기 때문에 처음에 적용하는 gradient($g_t$) 값이 적용이 잘 안되서(즉, 업데이트가 안된다), 초기 epoch에서는 학습 진행이 안되는 경우가 있다.&lt;/p&gt;

&lt;p&gt;이는 $m_t$, $v_t$ 값이 실제로 $g_t$, $g_t^2$ 가 맞는지 확인하는 작업이 필요하다. 따라서 각각 기대값(Expectation)을 씌워서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}
E[m_t] = E[g_t] \\
E[v_t] = E[g_t^2]
\end{cases}&lt;/script&gt;

&lt;p&gt;가 성립하는지 확인해야 된다. $v_t$를 보면,&lt;/p&gt;

&lt;p&gt;$v_0 = 0$ (0 vector) 으로 초기 값을 주었기 때문에, $t = 1 \cdots t$ 까지 아래와 같이 정리해서 쓸 수가 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
v_0 &amp;= 0 \\
v_1 &amp;= \beta_2 v_0 + (1-\beta_2) g_1^2 = (1-\beta_2) g_1^2 \\
v_2 &amp;= \beta_2 v_1 + (1-\beta_2) g_2^2 = \beta_2 (1-\beta_2) g_1^2 + (1-\beta_2) g_2^2 = (1-\beta_2)(\beta_2^1 g_1^2 + \beta_2^0 g_2^2)\\
\vdots \\
v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 = (1-\beta_2)(\beta_2^{t-1} g_1^2 + \cdots + \beta_2^0 g_t^2) = (1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i}g_i^2 \cdots (1)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;(1) 식에서 $g_i^2$ 를 $g_i^2 - g_t^2 + g_t^2$ 로 바꿔 줄 수가 있다. 그후 양변에 Expectation을 취하게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
E[v_t] &amp;= E[(1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i}(g_i^2 - g_t^2 + g_t^2))] \\
&amp;= E[(1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i}g_t^2] + E[(1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i}(g_i^2 - g_t^2))] \\
&amp;= E[g_t^2](1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i} + \zeta \\
&amp;= E[g_t^2](1-\beta_2)(\beta_2^{t-1} + \cdots + \beta_2^{0}) + \zeta \\
&amp;= E[g_t^2]\{(\beta_2^{t-1} + \cdots + \beta_2^{0}) - (\beta_2^{t} + \cdots + \beta_2^{1})\} + \zeta \\
&amp;= E[g_t^2](1-\beta_2^t) + \zeta \cdots (2)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;$E[g_t^2]$가 stationary 할때 $\zeta = 0$ 이 되고, 아니더라도 $\zeta$ 값은 이동평균의 특성상 따라 멀리 있는 $\beta_2^{t-i}$ 값이 아주 작아 0에 가까워 진다. 따라서 (2) 식만 남게 되는데, 우리가 원하는 $E[g_t^2]$ 를 구하기 위해서는 $E[g_t^2] = \dfrac{E[v_t]}{1-\beta_2^t}$ 를 해주면 초기값 0으로 설정하게 되어 생긴 bias를 조정 할 수 있게 된다.&lt;/p&gt;

&lt;p&gt;수식의 이해는 아래 블로그에서 도움을 조금 받았습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dalpo0814.tistory.com/29#comment5316278&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;http://dalpo0814.tistory.com&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;기존 알고리즘 코드:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Adam(object):
    &quot;&quot;&quot;Adam (http://arxiv.org/abs/1412.6980v8)&quot;&quot;&quot;

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.unbias_m = None
        self.v = None
        self.unbias_v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1

        for key in params.keys():
            self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)

            self.unbias_m = self.m[key] / (1 - self.beta1**self.iter) # correct bias
            self.unbias_v = self.v[key] / (1 - self.beta2**self.iter) # correct bias
            params[key] -= self.lr * self.unbias_m / (np.sqrt(self.unbias_v) + 1e-7)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;아래는 다른 사람의 코드를 따와서 개조했다. 출처: &lt;a href=&quot;https://github.com/WegraLee/deep-learning-from-scratch/blob/master/common/optimizer.py&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;https://github.com/WegraLee/deep-learning-from-scratch/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Adam(object):
    &quot;&quot;&quot;Adam (http://arxiv.org/abs/1412.6980v8)&quot;&quot;&quot;

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1
        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)

        for key in params.keys():
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])

            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;조금 더 효율 적으로 개선된 것을 볼 수 있다. &lt;strong&gt;lr_t&lt;/strong&gt; 는 위에 unbias 항들을 넣어서 정리해주면 아래와 같이 정의 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
&amp;= \beta_1 m_{t-1} + m_{t-1} - m_{t-1} + (1-\beta_1) g_t\\
&amp;= m_{t-1} - (1-\beta_1) m_{t-1} + (1-\beta_1) g_t\\
&amp;= m_{t-1} + (1-\beta_1)(g_t-m_{t-1})\\
v_t &amp;= v_{t-1} + (1-\beta_1)(g_t^2-v_{t-1}) \\
\alpha_t &amp;= \alpha \dfrac{\sqrt{1-\beta_2^t}}{1-\beta_1} \\
\theta_t &amp; \leftarrow \theta_{t-1} - \alpha_t \dfrac{m_t}{\sqrt{v_t} + \epsilon}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;signal-to-noisesnr&quot;&gt;Signal-to-Noise(SNR)&lt;/h4&gt;
&lt;p&gt;보통의 경우 $\hat{v}_t$ (gradient 제곱의 지수 평균) 이 $\hat{m}_t$ (gradient의 지수 평균) 보다 크기 때문에 $\dfrac{\hat{m}_t}{\sqrt{\hat{v}_t}} \leq 1$ ($\epsilon = 0$ 이라 가정) 가 되서 learning rate 보다 작은 값으로 업데이트 될 것이라는 점이다.&lt;/p&gt;

&lt;p&gt;이를 논문에서는 $\dfrac{\hat{m}_t}{\sqrt{\hat{v}_t}}$ 를 &lt;strong&gt;signal-to-noise ratio(SNR)&lt;/strong&gt; 라고 하며, SNR 값이 작아질 수록 step size도 0에 근접하게 된다. 즉, learning rate 가 점점 작아져 자동적으로 수렴하게 된다는 이야기다. 지금까지 고민하던 고정 학습률의 고민을 해결해 준다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;step size : $\Delta_t = \theta_t - \theta_{t-1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 조금 주의할 점은 데이터가 굉장히 sparse한 데이터 경우, 대부분의 $m_{t-1}$, $v_{t-1}$ 의 값은 0이 될 것이고, epoch($t$) 가 커질수록 $\hat{m}_t$, $\hat{v}_t$ 는 그 시점에서의 gradient 로 구성되어 있게 된다. 따라서 업데이트 식은 아래와 같게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_t \leftarrow \theta_{t-1} - \alpha \dfrac{1-\beta_1}{\sqrt{1-\beta_2}}&lt;/script&gt;

&lt;p&gt;이런 상황에서는 $\dfrac{1-\beta_1}{\sqrt{1-\beta_2}}$ 값이 1 보다 크기 때문에($beta_1 = 0.9, \beta_2 = 0.999$, 계산하면 약 3.16) 발산할 가능성이 높아진다. 이런 상황은 거의 드물다고 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Video&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Graph&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;video controls=&quot;controls&quot; style=&quot;width: 400px;&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot;&gt; &lt;source type=&quot;video/mp4&quot; src=&quot;/assets/ML/nn/Adam.mp4&quot; /&gt; &amp;lt;/source&amp;gt; &lt;/video&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/ML/nn/Adam.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;논문 결론 부에는 Adam 알고리즘이 큰 데이터 셋이나 고차원 파라미터 공간을 학습하는데 효율적이다라고 이야기 하고 있다.&lt;/p&gt;

&lt;p&gt;다음 시간에는 가중치 초기화와 배치 노말라이제이션에 대에서 이야기 해보도록 하겠다.&lt;/p&gt;
</description>
        <pubDate>Sat, 13 Jan 2018 14:17:06 +0900</pubDate>
        <link>http://simonjisu.github.io/deeplearning/2018/01/13/numpywithnn_5.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deeplearning/2018/01/13/numpywithnn_5.html</guid>
        
        
        <category>DeepLearning</category>
        
      </item>
    
      <item>
        <title>DeepMindNLP 강의 정리 1</title>
        <description>&lt;h1 id=&quot;word-vectors-and-lexical-semantics&quot;&gt;Word Vectors and Lexical Semantics&lt;/h1&gt;

&lt;h2 id=&quot;how-to-represent-words&quot;&gt;How to represent Words&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Natural language text = sequences of discrete symbols 이산 기호들의 배열(시퀀스)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Navie representaion: one hot vectors $\in$ $R^{vocabulary}$, one hot 인코딩된 벡터들로 표현 아주큼&lt;/p&gt;

    &lt;p&gt;words = [‘딥마인드’, ‘워드’, ‘벡터’]
  df = pd.DataFrame(np.eye(len(words)), index=words, dtype=np.int)
  df&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;0&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;딥마인드&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;워드&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;벡터&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Classical IR: document and query vectors are superpositions of word vectors
&lt;script type=&quot;math/tex&quot;&gt;\hat{d_q}=\underset{d}{\arg \max} \sim(d,q)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly for word classification problems(e.g. Navie Bayes topic models)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Issues: sparse, orthogonal representations, semantically weak&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-similarity--&quot;&gt;Semantic similarity 의미론적 유사성&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;더 풍부하게 단어를 표현하고 싶다!!&lt;/li&gt;
  &lt;li&gt;Distributional semantics: 분산 의미론
    &lt;ul&gt;
      &lt;li&gt;Idea: produce dense vector representations based on the contex/use of words&lt;/li&gt;
      &lt;li&gt;Approaches:
        &lt;ul&gt;
          &lt;li&gt;count-based&lt;/li&gt;
          &lt;li&gt;predictive&lt;/li&gt;
          &lt;li&gt;task-based&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;count-based-methods&quot;&gt;Count-based methods&lt;/h3&gt;
&lt;p&gt;Define a basis vocabulary C of context words&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;고를 때는 linguistic intutition(언어적 직관, 주관적인) / statistics of the corpus 에 의해 고름&lt;/li&gt;
  &lt;li&gt;이것을 하는 이유는 a, the 같은 의미와 무관한 function word를 포함시키지 않기 위함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Define a word window size $w$.&lt;/p&gt;

&lt;p&gt;Count the basis vocabulary words occurring $w$ words to the left or right of each instance of a target word in the corpus&lt;/p&gt;

&lt;p&gt;From a vector representation of the target word based on these counts&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from collections import Counter, defaultdict
from operator import itemgetter

def get_vocabulary_dict(contexts, stopwords):
vocabulary = Counter()
for sentence in contexts:
    words = [word for word in sentence.split() if word not in stopwords]
    vocabulary.update(words)
return vocabulary

def represent_vector(contexts_words, vocabulary):
vocab_len = len(vocabulary)
word2idx = {w: i for i, w in enumerate(vocabulary)}
count_based_vector = defaultdict()

for key_word, context_w in contexts_words.items():
    temp = np.zeros(vocab_len, dtype=np.int)
    for w in context_w:
        temp[word2idx[w]] += 1
    count_based_vector[key_word] = temp
return count_based_vector, word2idx

contexts = ['and the cute kitten purred and then',
        'the cute furry cat purred and miaowed',
        'that the small kitten miaowed and she',
        'the loud furry dog ran and bit']
stopwords=['and', 'then', 'she', 'that', 'the', 'cat', 'dog', 'kitten']
contexts_words = {'kitten': {'cute', 'purred', 'small', 'miaowed'},
              'cat': {'cute', 'furry', 'miaowed'},
              'dog': {'loud', 'furry', 'ran', 'bit'}}

vocabulary = get_vocabulary_dict(contexts, stopwords)
count_based_vector, word2idx = represent_vector(contexts_words, vocabulary)

word_idx_list = [w for i, w in sorted([(i, w) for w, i in word2idx.items()], key=itemgetter(0))]
df = pd.DataFrame(count_based_vector, index=word_idx_list)
df.T
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;cute&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;purred&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;furry&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;miaowed&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;small&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;loud&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ran&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;bit&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cat&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;dog&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;kitten&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Compare as similarity kernel:
$cosine(u, v) = \dfrac{u\cdot v}{|u|\times|v|}$&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def cosine(u, v):
    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))

print('kitten-cat:', cosine(df['kitten'], df['cat']))
print('kitten-dog:',cosine(df['kitten'], df['dog']))
print('cat-dog:',cosine(df['cat'], df['dog']))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;kitten-cat: 0.57735026919&lt;/p&gt;

  &lt;p&gt;kitten-dog: 0.0&lt;/p&gt;

  &lt;p&gt;cat-dog: 0.288675134595&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Count-based method는 Navie Approach으로 접근&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Not all features are equal: we must distinguish counts that are high, because they are informative from those that are just independently frequent contexts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Many Normalisation methods: TF-IDF, PMI, etc&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some remove the need for norm-invariant similarity metrics&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But… perhaps there are easier ways to address this problem of count-based mothods(and others, e.g. choice of basis context)&lt;/p&gt;

&lt;h3 id=&quot;neural-embedding-models&quot;&gt;Neural Embedding Models&lt;/h3&gt;
&lt;p&gt;Learning count based vecotrs produces an embedding matrix in $R^{|vocab|\times|context|}$&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;cute&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;purred&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;furry&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;miaowed&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;small&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;loud&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ran&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;bit&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cat&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;dog&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;kitten&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Rows are word vectors, so we can retrieve them with one hot vectors in ${0,1}^{&lt;/td&gt;
      &lt;td&gt;vocab&lt;/td&gt;
      &lt;td&gt;}$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;onehot_{cat} = \begin{bmatrix} 0 \newline 1 \newline 0 \end{bmatrix}, cat=onehot_{cat}^TE&lt;/script&gt;

&lt;p&gt;Symbols = unique vectors. Representation = embedding symbols with $E$&lt;/p&gt;

&lt;h4 id=&quot;generic-idea-behind-embedding-learning&quot;&gt;Generic(포괄적인) idea behind embedding learning:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Collect instances $t_i \in inst(t)$ of a word $t$ of vocab $V$&lt;/li&gt;
  &lt;li&gt;For each instance, collect its context words $c(t_i)$ (e.g. k-word window)&lt;/li&gt;
  &lt;li&gt;Define some score function $score(t_i, c(t_i); \theta, E)$ with upper bound on output&lt;/li&gt;
  &lt;li&gt;Define a loss:
&lt;script type=&quot;math/tex&quot;&gt;L=-\sum_{t\in V}\sum_{t_i \in inst(t)}score(t_i, c(t_i);\theta,E)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Estimate:
&lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}, \hat{E}=\underset{\theta, E}{\arg \min}\ L&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Use the estimated $E$ as your embedding matrix&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;problems-scoring-function&quot;&gt;Problems: Scoring function&lt;/h4&gt;

&lt;p&gt;Easy to design a useless scorer(e.g. ignore input, output upper bound)&lt;/p&gt;

&lt;p&gt;Implicitly define is useful&lt;/p&gt;

&lt;p&gt;Ideally, scorer:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Embeds $t_i$ with $E$&lt;/li&gt;
  &lt;li&gt;Produces a score which is a function of how well $t_i$ is accounted for by $c(t_i)$, and/or vice versa&lt;/li&gt;
  &lt;li&gt;Requires the word to account for the context(or the reverse) more than another word in the same place.&lt;/li&gt;
  &lt;li&gt;Produces a loss which is differentiable w.r.t. $\theta$ and $E$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;cwcollobert-et-al-2011&quot;&gt;C&amp;amp;W(Collobert et al. 2011)&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;paper&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interpretation: representations carry information about what neighbouring representations should look like&lt;/p&gt;

&lt;p&gt;where it belongs? 같은 정보를 포함&lt;/p&gt;

&lt;h4 id=&quot;cbow-mikolov-et-al-2013&quot;&gt;CBoW (Mikolov et al. 2013)&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;paper&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Embed context words. Add them.&lt;/p&gt;

&lt;p&gt;Project back to vocabulary size. Softmax.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;softmax(l)_i=\dfrac{e^{l_i}}{\sum_{j}e^{l_i}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray} P(t_i|context(t_i) &amp; = &amp; softmax(\sum_{t_j\in context(t_i)} onehot_{t_j}^{t}\cdot E\cdot W_v) \newline
&amp; = &amp; softmax((\sum_{t_j\in context(t_i)} onehot_{t_j}^{t}\cdot E)\cdot W_v) \end{eqnarray} %]]&gt;&lt;/script&gt;

&lt;p&gt;Minimize Negative Log Likelihood:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{data} = -\sum_{t_i \in data}\log P(t_i|context(t_i))&lt;/script&gt;

&lt;p&gt;장점:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;All linear, so very fast. Basically a cheap way of applying one matrix to all inputs.&lt;/li&gt;
  &lt;li&gt;Historically, negative sampling used instead of expensive softmax.&lt;/li&gt;
  &lt;li&gt;NLL(negative log-likelihood) minimisation is more stable and is fast enough today&lt;/li&gt;
  &lt;li&gt;Variants: postion specific matrix per input(Ling et al. 2015)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;skip-gram-mikolov-et-al-2013&quot;&gt;Skip-gram (Mikolov et al. 2013)&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;paper&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Target word predicts context words.&lt;/p&gt;

&lt;p&gt;Embed target word.&lt;/p&gt;

&lt;p&gt;Project into vocabulary. Softmax.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(t_j|t_i) = softmax(onehot_{t_i}^T\cdot E \cdot W_v)&lt;/script&gt;

&lt;p&gt;Learn to estimate Likelihood of context words.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\log P(context(t_i)|t_i) = -\log \prod_{t_j\in context(t_i)}P(t_j|t_i) - \sum_{t_j\in context(t_i)}\log P(t_j|t_i)&lt;/script&gt;

&lt;p&gt;장점:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fast: One embedding versus $C$ (size of contexts) embeddings&lt;/li&gt;
  &lt;li&gt;Just read off probabilities from softmax&lt;/li&gt;
  &lt;li&gt;Similiar variants to CBoW possible: position specific projections&lt;/li&gt;
  &lt;li&gt;Trade off between efficiency and more structured notion of context&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section&quot;&gt;기타&lt;/h4&gt;
&lt;p&gt;Word Embedding 하는 목적이 뭐냐? dense 한 vector 를 얻는 거다&lt;/p&gt;

&lt;p&gt;Word2Vec은 딥러닝이 아니라 shallow model(얕은 모델: 층이 하나밖에 없는)이다.&lt;/p&gt;

&lt;p&gt;Word2Vec == PMI Matrix factorization of count based models(Levy and Goldberg, 2014)&lt;/p&gt;

&lt;h3 id=&quot;specific-benefits-of-neural-approaches&quot;&gt;Specific Benefits of Neural Approaches&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Easy to learn, especially with good linear algebra libraries.&lt;/li&gt;
  &lt;li&gt;Highly parallel problem: minibatching, GPUs, distributed models.&lt;/li&gt;
  &lt;li&gt;Can predict other discrete aspects of context(dependencies, POS tags, etc). Can estimate these probabilities with counts, but sparsity quickly becomes a problems.&lt;/li&gt;
  &lt;li&gt;Can predict/condition on continuous contexts: e.g. images.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;evaluating-word-representations&quot;&gt;Evaluating Word Representations&lt;/h3&gt;
&lt;p&gt;Intrinsic Evaluation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;WordSim-353 (Finkelstein et al. 2003)&lt;/li&gt;
  &lt;li&gt;SimLex-999 (Hill et al 2016, but has been around since 2014)&lt;/li&gt;
  &lt;li&gt;Word analogy task (Mikolov et al. 2013)&lt;/li&gt;
  &lt;li&gt;Embedding visualisation (nearest neighbours, T-SNE projection)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;t-SNE visualize, word 2 dimension cluster: &lt;span style=&quot;color: #e87d7d&quot;&gt; http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Extrinsic Evaluation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Simply: do your embeddings improve performance on other task(s).&lt;/li&gt;
  &lt;li&gt;More …&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;task-based-embedding-learning&quot;&gt;Task-based Embedding Learning&lt;/h3&gt;
&lt;p&gt;Just saw methods for learning $E$ through minimising a loss.&lt;/p&gt;

&lt;p&gt;One use for $E$ is to get input features to a neural network from words.&lt;/p&gt;

&lt;p&gt;Neural network parameters are updated using gradients on loss $L(x, y, \theta)$:
&lt;script type=&quot;math/tex&quot;&gt;\theta_{t+1} = update(\theta_t, \triangledown_{\theta}L(x, y, \theta_t))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If $E \subseteq \theta$ then this update can modify $E$ (if we let it):
&lt;script type=&quot;math/tex&quot;&gt;E_{t+1} = update(E_t, \triangledown_E L(x, y, \theta_t))&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;task-based-features-bow-classifiers&quot;&gt;Task-based Features: Bow Classifiers&lt;/h4&gt;
&lt;p&gt;Classify sentences/documents based on a variable number of word representations&lt;/p&gt;

&lt;p&gt;Simplest options: bag of vectors
&lt;script type=&quot;math/tex&quot;&gt;P(C|D)=softmax(W_C \sum_{t_i \in D} embed_E(t_i))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Projection into logits (input to softmax) canbe arbitrarily complex. E.g.:
&lt;script type=&quot;math/tex&quot;&gt;P(C|D)=softmax(W_C \cdot \sigma (\sum_{t_i \in D} embed_E(t_i)))&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$C$: class&lt;/li&gt;
  &lt;li&gt;$D$: document&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example tasks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sentiment analysis: tweets, movie reviews&lt;/li&gt;
  &lt;li&gt;Document classification: 20 Newsgroups&lt;/li&gt;
  &lt;li&gt;Author identification&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;task-based-features-bilingual-features&quot;&gt;Task-based Features: Bilingual Features&lt;/h4&gt;
&lt;p&gt;linguistic general approach: translations&lt;/p&gt;

&lt;p&gt;데이터가 많으면 그냥 pre-trained할 필요 없이 Embedding을 만든(random initialize) 담에 같이 train하면 됨, 만약에 데이터가 충분치 않다면, 미리 training하는 것이 좋아 보임&lt;/p&gt;

&lt;h2 id=&quot;torch-word2vec-&quot;&gt;Torch로 word2vec 짜보기&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torch.utils.data as data_utils
from torch.utils.data import DataLoader
from scipy.spatial.distance import cosine
import matplotlib.pylab as plt
from collections import Counter, defaultdict, deque
from nltk.tokenize import word_tokenize
from operator import itemgetter

class WORD2VEC(nn.Module):
    def __init__(self, N, half_window_size, lr, mode='cbow'):
        &quot;&quot;&quot;
        V: vocab_size
        N: hidden layer size(word vector size)
        window_size: how many words that you want to see near target word
        mode: cbow / skipgram
        &quot;&quot;&quot;
        super(WORD2VEC, self).__init__()

        self.V = None
        self.N = N
        # vocab and data setting
        self.half_window_size = half_window_size
        self.vocab_count = Counter()
        self.vocab2idx = defaultdict()
        self.vocab2idx['NULL'] = 0
        self.lr = lr

    def build_network(self):
        # network setting
        self.i2h = nn.Embedding(self.V, self.N, padding_idx=0)  # Embedding
        self.h2o = nn.Linear(self.N, self.V)
        self.softmax = nn.Softmax(dim=1)

    def get_vocabulary(self, corpus_list):
        for sentence in corpus_list:
            self.vocab_count.update(sentence)
        for i, w in enumerate(self.vocab_count.keys()):
            self.vocab2idx[w] = i + 1
        self.idx2vocab = {i: w for w, i in self.vocab2idx.items()}

    def generate_batch(self, sentence):
        # sentence size와 window size 결정 조건 확인(추가할것)
        target_words = []
        batch_windows = []

        # add padding data
        batch_sentence = ['NULL'] * self.half_window_size + sentence + ['NULL'] * self.half_window_size
        for i, target_word in enumerate(sentence):
            target_words.append(target_word)
            center_idx = i + self.half_window_size
            window = deque(maxlen=self.half_window_size * 2)
            window.extendleft(reversed(batch_sentence[i:center_idx]))
            window.extend(batch_sentence[center_idx + 1:center_idx + 1 + self.half_window_size])
            batch_windows.append(window)

        return batch_windows, target_words

    def data_transfer(self, corpus_list):
        &quot;&quot;&quot;batch_data = [windows(list), target(list)]&quot;&quot;&quot;
        batch_data = []
        for sentence in corpus_list:
            batch_windows, target_words = self.generate_batch(sentence)
            for window, target in zip(batch_windows, target_words):
                idxed_window = [self.vocab2idx[word] for word in window]
                idxed_target = [self.vocab2idx[target]]
                batch_data.append([idxed_window, idxed_target])
        return batch_data

    def tokenize_corpus(self, corpus):
        &quot;&quot;&quot;문장에 부호를 제거하고 단어 단위로 tokenize 한다&quot;&quot;&quot;
        check = ['.', '!', ':', ',', '(', ')', '?', '@', '#', '[', ']', '-', '+', '=', '_']
        corpus_list = []
        for sentence in corpus:
            temp = word_tokenize(sentence)
            temp = [word.lower() for word in temp if word not in check]
            corpus_list.append(temp)
        return corpus_list

    def fit(self, corpus):
        &quot;&quot;&quot;
        corpus를 학습시킬 데이터로 전환시켜준다. 모든 데이터는 단어의 vocab2idx를 근거해서 바뀐다.
        Vocab이 설정되면 네트워크도 같이 설정된다.
        batch_data = [window, target]
        &quot;&quot;&quot;
        corpus_list = self.tokenize_corpus(corpus)
        self.get_vocabulary(corpus_list)
        self.V = len(self.vocab2idx)
        batch_data = self.data_transfer(corpus_list)
        self.build_network()
        print('fit done!')
        return batch_data

    def forward(self, X):
        embed = self.i2h(X)  # batch x V x N
        h = Variable(embed.data.mean(dim=1))  # batch x N
        output = self.h2o(h)  # batch x V
        probs = self.softmax(output)  # batch x V
        return output, probs

#################################################
# Sample Data
#################################################

def create_sample_data():
    corpus = ['the king loves the queen',
              'the queen loves the king',
              'the dwarf hates the king',
              'the queen hates the dwarf',
              'the dwarf poisons the king',
              'the dwarf poisons the queen',]

    return corpus


def get_data_loader(batch_data, batch_size, num_workers, shuffle=False):
    features = torch.LongTensor([batch_data[i][0] for i in range(len(batch_data))])
    targets = torch.LongTensor([batch_data[i][1] for i in range(len(batch_data))])
    data = data_utils.TensorDataset(features, targets)

    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)

    return loader

#################################################
# Train
#################################################

def word2vec_train(corpus, N, half_window_size=2, lr=0.01, n_epoch=1000, batch_size=10, print_epoch=100, num_workers=2, shuffle=False):
    &quot;&quot;&quot;본격적으로 데이터를 학습한다&quot;&quot;&quot;
    word2vec = WORD2VEC(N=N, half_window_size=half_window_size, lr=lr)
    batch_data = word2vec.fit(corpus)
    loader = get_data_loader(batch_data, batch_size, num_workers, shuffle)

    F = nn.CrossEntropyLoss()
    optimizer = optim.SGD(word2vec.parameters(), lr=word2vec.lr)

    loss_list = []
    for epoch in range(n_epoch):

        for batch_X, batch_y in loader:
            optimizer.zero_grad()
            batch_X = Variable(batch_X)
            batch_y = Variable(batch_y)

            output, probs = word2vec.forward(batch_X)
            loss = F(output, batch_y.squeeze(-1))  # must be 1-d tensor in labels

            loss.backward()
            optimizer.step()
        loss_list.append(loss.data[0])

        if epoch % print_epoch == 0:
            print('#{}| loss:{}'.format(epoch, loss.data[0]))

    return word2vec, loss_list
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Training은 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;corpus = create_sample_data()
N = 2
half_window_size = 2
lr = 0.01
n_epoch = 3000
print_epoch = 200
batch_size = 4
num_workers=2
shuffle=False
word2vec, loss_list = word2vec_train(corpus, N=N, half_window_size=half_window_size,
  lr=lr, n_epoch=n_epoch, batch_size=batch_size, print_epoch=print_epoch,
  num_workers=num_workers, shuffle=shuffle)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/Deepnlp/lec1/Loss.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2차원으로 embedding 했으니 평면에 그려보았다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/Deepnlp/lec1/vector.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;조금더 큰 데이터를 그냥 CBOW 혹은 Skip-gram으로 학습 시킬 경우 속도가 아주 느린 것을 발견 할 수 가 있다. 이는 말뭉치가 많아질 수록 단어의 수도 많아 지기 때문에, 말단에 Hierarchical Softmax와 Negative Sampling 방법을 쓴다고 한다. 김범수님의 블로그 &lt;a href=&quot;https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/&quot;&gt;[&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크&lt;/span&gt;]&lt;/a&gt; 참조&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Jan 2018 21:39:31 +0900</pubDate>
        <link>http://simonjisu.github.io/deepnlp/2018/01/02/deepmindnlp1.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/deepnlp/2018/01/02/deepmindnlp1.html</guid>
        
        
        <category>Deepnlp</category>
        
      </item>
    
  </channel>
</rss>
