<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soo</title>
    <description>My Blog
</description>
    <link>http://simonjisu.github.io/</link>
    <atom:link href="http://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 03 Jun 2018 22:34:07 +0900</pubDate>
    <lastBuildDate>Sun, 03 Jun 2018 22:34:07 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 3 PYTHON &amp; CUDA</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---3&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 3&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;install-python&quot;&gt;Install PYTHON&lt;/h2&gt;

&lt;p&gt;아나콘다를 통하지 않고 소스를 통해 파이썬을 설치하기로 했다. 일단 용량이 작고, 다른 부가 spyder 등 프로그램을 설치하기 싫어서 소스에서 직접 설치하기로 했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;https://www.python.org/&lt;/a&gt; 에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Download &amp;gt; Soruce code &amp;gt; Python 3.6.5 - 2018-03-28&lt;/code&gt; 로 들어가서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Gzipped source tarball&lt;/code&gt; 의 링크를 복사한 후 아래와 같이 쳐주자.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;압축을 헤제시켜주자
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo -zxvf Python-3.6.5.tgz
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;설치해보자
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
make
make test
sudo make install
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;기본적으로 이렇게 진행하면 설치가 완료 된다. (중간에 실패하면 어떤 패키지가 없는지 확인하고 apt-get으로 설치해준다.)&lt;/p&gt;

&lt;p&gt;하지만 &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip3 install numpy&lt;/code&gt; 하게 되면 아래와 같이 “TLS/SSL ~” 이라며 에러가 뜰 수도 있다. 자세히 뭔지는 모르겠지만, 구글링을 통해서 해결했다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.
Collecting
  Could not fetch URL https://pypi.python.org/simple//: There was a problem confirming the ssl certificate: Can't connect to HTTPS URL because the SSL module is not available. - skipping
  Could not find a version that satisfies the requirement (from versions: )
No matching distribution found for
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section&quot;&gt;해결책&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;필수 패키지를 설치한다
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;소스폴더로 돌아가서 다시 설치해준다.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo make
sudo make install
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;마지막으로, pip 를 업그레이드 해준다.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo pip3 install --upgrade pip
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이제 설치가 잘 될 것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;install-cuda-90&quot;&gt;Install CUDA 9.0&lt;/h2&gt;

&lt;p&gt;CUDA-Toolkit 를 설치하면 드라이버를 설치할 필요는 없다고하나 만약에 먼저 설치해야하면 아래와 같이 설치(업데이트) 해주자&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
sudo apt install nvidia-390
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cuda-toolkit&quot;&gt;1. CUDA-Toolkit&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-90-download-archive&quot;&gt;https://developer.nvidia.com/cuda-90-download-archive&lt;/a&gt; 에서 자신에 버젼에 맞는 CUDA-Toolkit 을 받자.&lt;/p&gt;

&lt;p&gt;나는 우분투이기에 &lt;code class=&quot;highlighter-rouge&quot;&gt;Linux &amp;gt; x86_64 &amp;gt; Ubuntu &amp;gt; 17.04 &amp;gt; del[local]&lt;/code&gt; 를 골랐다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Base Installer&lt;/p&gt;

  &lt;p&gt;Patch 1 (Released Jan 25, 2018)&lt;/p&gt;

  &lt;p&gt;Patch 2 (Released Mar 5, 2018)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;세개의 다운로드 링크를 복사한 뒤 &lt;code class=&quot;highlighter-rouge&quot;&gt;wget&lt;/code&gt; 메서드로 받아준다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Base Installer&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb
sudo apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub
sudo apt-get update
sudo apt-get install cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Patch 1&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1604-9-0-local-cublas-performance-update_1.0-1_amd64.deb
sudo apt-get update
sudo apt-get upgrade cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Patch 2&lt;/strong&gt; 설치&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i cuda-repo-ubuntu1604-9-0-local-cublas-performance-update-2_1.0-1_amd64.deb
sudo apt-get update
sudo apt-get upgrade cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;만약에 쿠다 드라이버 명령어인 &lt;code class=&quot;highlighter-rouge&quot;&gt;nvcc&lt;/code&gt; 를 쓰고 싶다면 &lt;code class=&quot;highlighter-rouge&quot;&gt;./profile&lt;/code&gt; 파일 밑에다 아래 항목을 추가해주면 된다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export PATH=/usr/local/cuda-9.0/bin${PATH:+:$PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;아래 둘중에 하나를 한번 시도해보면 된다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvcc -V
nvidia-smi
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cudnn-&quot;&gt;2. CuDNN 설치&lt;/h3&gt;

&lt;p&gt;CuDNN 을 설치하려면 NVIDIA 회원 가입을 해야한다. 그리고 아래 싸이트에서 받아서 &lt;code class=&quot;highlighter-rouge&quot;&gt;scp&lt;/code&gt; 명령어로 서버로 옮기자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot;&gt;https://developer.nvidia.com/rdp/cudnn-download&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;압축을 해제하고, 파일들을 옮겨주면 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar -xzvf cudnn-9.0-linux-x64-v7.tgz
sudo cp cuda/include/cudnn.h /usr/local/cuda/include
sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;pytorch---&quot;&gt;Pytorch 설치한 후 테스트&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/torch.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이로써 설치 과정을 마치겠다. 컴퓨터 설치는 처음이라 3일 걸렸지만 앞으로는 더 짧아 지겠지…&lt;/p&gt;

&lt;p&gt;만약 오류가 나면 또 업데이트 하겠다.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jun 2018 21:08:24 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/03/gpuserver3.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/03/gpuserver3.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 2 원격 부팅 접속</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---2&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 2&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;after-install&quot;&gt;After Install&lt;/h2&gt;

&lt;p&gt;설치후에 아래 명령어들을 쳐준다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;apt-get(Advanced Packaging Tool) 패키지 명령어 업데이트 및 설치되어 있는 패키지 업그레이드. 설치된 파일은 &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/cache/apt/archive/&lt;/code&gt; 에 저장됨&lt;/li&gt;
  &lt;li&gt;gcc(GNU Compiler Collection) 패키지 설치. 파이썬 설치에 필요&lt;/li&gt;
  &lt;li&gt;make(GNU Make) 패키지 설치. 파이썬 설치시 필요&lt;/li&gt;
  &lt;li&gt;zlib1g-dev 설치. 파이썬 설치시 필요&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt update &amp;amp;&amp;amp; sudo apt upgrade
sudo apt install gcc
sudo apt install make
sudo apt install zlib1g-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;sshsecure-shell-&quot;&gt;SSH(SECURE SHELL) 접속&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;서버&lt;/code&gt; 컴퓨터에서 아래의 사항을 수정:&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo vi /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;ClientAliveInterval 60 : 클라이언트 살아있는지 확인하는 간격&lt;/li&gt;
  &lt;li&gt;ClientAliveCountMax 10 : 클라이언트 응답 없어도 접속 유지하는 횟수&lt;/li&gt;
  &lt;li&gt;PubkeyAuthentication yes : 활성화 시켜야 ssh를 통해서 접속 가능&lt;/li&gt;
  &lt;li&gt;PasswordAuthentication yes : 원격 서버 비밀번호로 로그인 가능하게 것, 나중에 ssh 로만 접속 가능케 하려면 바꿔줘야한다.&lt;/li&gt;
  &lt;li&gt;PermitEmptyPasswords no : 로그인시 빈 비밀번호를 가능케하는 옵션 기본으로 no로 되어 있다. 비밀번호 없이 로그인하게 하려면 yes로 바꿔줄 것, 권장은 안함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ssh-key----&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;로컬&lt;/code&gt; 컴퓨터에서 SSH KEY 생성하고 &lt;code class=&quot;highlighter-rouge&quot;&gt;서버&lt;/code&gt; 컴퓨터로 보내기:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;ssh-keygen:&lt;/strong&gt; SSH 키를 생성한다. 경로 지정을 안할 때 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.ssh&lt;/code&gt; 폴더 안에 &lt;code class=&quot;highlighter-rouge&quot;&gt;id_rsa&lt;/code&gt; 라는 이름으로 생성한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ssh-copy-id:&lt;/strong&gt; SSH 키를 서버로 보낸다. 옵션으로 포트번, 키 디렉토리 등등 설정 가능하다&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen -f [filepath]
ssh-copy-id -i [key_directory] -p [port] [user]@[ip_address]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;scp-&quot;&gt;파일전송 명령어 SCP 사용하기&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;서버 &amp;gt; 로컬
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scp [옵션] [계정명]@[원격지IP주소]:[원본 경로 및 파일] [전송받을 위치]
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;로컬 &amp;gt; 서버
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scp [옵션] [원본 경로 및 파일] [계정명]@[원격지IP주소]:[전송받을 위치]
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다만 주의 해야할 것은 &lt;code class=&quot;highlighter-rouge&quot;&gt;[옵션]&lt;/code&gt; 에다가 로그인 옵션 다 넣어줘야 보내진다는 점을 잊지 말자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;옵션:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;P: 포트&lt;/li&gt;
  &lt;li&gt;i: key&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;wolwake-on-lan&quot;&gt;WOL(Wake-On-Lan)&lt;/h2&gt;

&lt;p&gt;전기세 때문에 원격으로 컴퓨터를 껐다 켯다 하고 싶었다. 찾아보니 WOL 라는 방법이 있었다. 우선 자신의 컴퓨터의 메인보드가 이 기능을 지원해주고, 공유기도 이 기능을 지원해줘야 사용할 수 있다.
컴퓨터 부팅전 &lt;code class=&quot;highlighter-rouge&quot;&gt;BIOS&lt;/code&gt; (내경우는 DEL 키를 눌렀음) 에 들어가서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Wake-On-Lan&lt;/code&gt; 이라는 글귀가 있는지 찾아보고, 있다면 enable 로 바꿔주자. 그리고 아래 명령어를 통해 내컴퓨터의 &lt;code class=&quot;highlighter-rouge&quot;&gt;이더넷 포트(Ethernet port)&lt;/code&gt; 알아보자&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이더넷 포트는 첫째줄 제일 왼쪽에 있을 것이다. 보통 en~~ 로 시작하는 번호다&lt;/p&gt;

&lt;p&gt;그 후, 컴퓨터 내 컴퓨터가 &lt;code class=&quot;highlighter-rouge&quot;&gt;WOL&lt;/code&gt; 기능이 켜졌나 확인 하기 위해, 부팅후 커맨드 라인에 아래와 같이 쳐주자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install ethtool
sudo ethtool [Ethernet port]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Wake-on&lt;/code&gt; 이라는 곳에 &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; 라고 적혀져 있으면 켜진 것이다. 안되있다면 아래의 명령어를 통해 켜주자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ethtool -s [Ethernet port] wol g
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그 다음에 자신의 집의 공유기에 들어가서, WOL 설정을 해주자. NETIS 기준으로 설명 하겠다. IPTIME 은 다른 글들이 많으니 한번 찾아 보길 바란다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;원격 부팅(WOL)&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;사용 IP 목록에서 등록&lt;/code&gt; 누른후, 자신의 컴퓨터 IP 를 선택하고 이름을 지어준 다음 &lt;code class=&quot;highlighter-rouge&quot;&gt;등록&lt;/code&gt; 하게 되면 밑에 하나 등록 될 것이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;외부 연결 포트&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;포트 번호&lt;/code&gt;는 내 컴퓨터의 접속 포트로 했다. (이건 꼭 TCP 통신으로 하는 포트로 해야하는지 모르겠다. 다른 번호를 따로 지정해줄 수 있는지를 확인 못해봄)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/WOL.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 DDNS 서비스 신청한다. 그러면 집 밖에서도 집 공유기에 접속해서 컴퓨터를 킬 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;참고자료: &lt;a href=&quot;http://blog.daum.net/peace20/16779844&quot;&gt;http://blog.daum.net/peace20/16779844&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음 시간에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Python&lt;/code&gt; 설치와 &lt;code class=&quot;highlighter-rouge&quot;&gt;CUDA&lt;/code&gt; 설치를 다뤄보겠다.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jun 2018 10:09:54 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/03/gpuserver2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/03/gpuserver2.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>개인 딥러닝용 서버 설치 과정기 - 1 사양 및 우분투 서버 설치</title>
        <description>&lt;h1 id=&quot;install-ubuntu-1804-gpu-server-for-deeplearning---1&quot;&gt;Install Ubuntu 18.04 GPU Server For DeepLearning - 1&lt;/h1&gt;

&lt;p&gt;개인 딥러닝용 서버 설치 과정과 삽질을 담은 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section&quot;&gt;컴퓨터 사양 상세&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;항목&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;상품코드&lt;/th&gt;
      &lt;th&gt;제품명&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;금액&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;수량&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;최종금액&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CPU&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;399920&lt;/td&gt;
      &lt;td&gt;[INTEL] 코어7세대 i5-7600 정품박스 (카비레이크/3.5GHz/6MB/쿨러포함)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;258,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;258,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MAIN&lt;br /&gt;BOARD&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;408703&lt;/td&gt;
      &lt;td&gt;[GIGABYTE] GA-H110M-M.2 듀러블에디션 피씨디렉트 (인텔H110/M-ATX)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71,100원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71,100원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;메모리&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;390790&lt;/td&gt;
      &lt;td&gt;[삼성전자] 삼성 DDR4 16GB PC4-19200&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;183,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;366,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HDD&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;347917&lt;/td&gt;
      &lt;td&gt;[WD] BLUE 2TB WD20EZRZ (3.5HDD/SATA3/5400rpm/64M)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67,730원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67,730원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GPU&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;373864&lt;/td&gt;
      &lt;td&gt;[MSI] GeForce GTX1060 OC D5 6GB 윈드스톰&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;389,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;389,000원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POWER&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;420859&lt;/td&gt;
      &lt;td&gt;[CORSAIR] CX750 NEW 80PLUS BRONZE (ATX/750W)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;94,370원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;94,370원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;BOX&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;365393&lt;/td&gt;
      &lt;td&gt;[COX] RC 170T USB3.0 (미들타워)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13,500원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13,500원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;OTHER&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3877&lt;/td&gt;
      &lt;td&gt;[컴퓨존] 일반조립비 (하드웨어조립/OS는 설치되지않습니다)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20,000원&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20,000원&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/1.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/2.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/3.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/4.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/5.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/6.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/7.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/8.jpeg&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/9.jpeg&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2018년 5월 28일 컴퓨존에서 주문해서, 5월 30일 수요일 도착했다. 총비용은 대략 130만원 정도 ㅎㅎ 언른 GPU를 쓰고 싶은 생각에 그날밤 바로 설치를 진행하였다.&lt;/p&gt;

&lt;h2 id=&quot;ubuntu-server-&quot;&gt;Ubuntu Server 설치&lt;/h2&gt;

&lt;p&gt;5월 30일 저녁, 우분투를 설치하려고 하니 버전이 마음에 걸렸다. NVIDIA CUDA TOOLKIT을 보니 리눅스 17.04 버전 까지 지원하는듯 했기 때문이다. 16.04를 설치해야하나? 싶은 찰나에 그냥 최신으로 한번 도전해보기로 했다. 안되면 다시 갈지머..&lt;/p&gt;

&lt;h3 id=&quot;making-a-bootable-ubuntu-usb-disk-tutorial-at-mac-os&quot;&gt;Making a bootable Ubuntu USB disk Tutorial at Mac OS&lt;/h3&gt;

&lt;p&gt;맥에서 부팅 디스크 만들기, 정말 간단하다. &lt;a href=&quot;https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-macos#0&quot;&gt;tutorials.ubuntu.com&lt;/a&gt; 튜토리얼을 따라가면 된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;준비물&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;2GB 이상의 USB Driver&lt;/li&gt;
    &lt;li&gt;Mac OS 컴퓨터&lt;/li&gt;
    &lt;li&gt;우분투 서버 ISO 다운로드 &lt;a href=&quot;https://www.ubuntu.com/download/server&quot;&gt;우분투 서버 다운로드&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;format&quot;&gt;구동 디스크 FORMAT&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;응용프로그램 &amp;gt; 유틸리티 &amp;gt; 디스크 유틸리티 선택&lt;/li&gt;
  &lt;li&gt;MAC OS에 꼽은 USB 를 선택한 다음에 &lt;code class=&quot;highlighter-rouge&quot;&gt;지우기&lt;/code&gt; 를 선택한다.&lt;/li&gt;
  &lt;li&gt;이름을 짓고, &lt;code class=&quot;highlighter-rouge&quot;&gt;MS-DOS(FAT)&lt;/code&gt; 선택한다. (그림에는 Scheme가 있지만 Serria 이후에는 없다는 말이 있음)&lt;/li&gt;
  &lt;li&gt;포맷한다. 지운다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/format_disk.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;etcher-----&quot;&gt;Etcher 를 사용한 시동 디스크 생성&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://etcher.io/&quot;&gt;Etcher&lt;/a&gt; 먼저 받는다. 그후에는 정말 간단하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Select image&lt;/code&gt; 에 다운 받은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ubuntu Server 18.04 ISO&lt;/code&gt; 를 고른다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Select drive&lt;/code&gt; 에 포맷한 디스크를 선택&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Flash!&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/gpuserver/etcher.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 설치 준비 완료되었다.&lt;/p&gt;

&lt;h3 id=&quot;install-ubuntu-server-1804&quot;&gt;Install Ubuntu Server 18.04&lt;/h3&gt;

&lt;p&gt;이제 설치릃 해보자. 설치를 하려면, 최소 한번은 모니터에 연결해서 설치해야한다. 나는 정말 서버만을 생각해서 모니터를 않샀기에… HDMI 케이블로 티비화면으로 연결했다… 덕분에 고생이 두배!&lt;/p&gt;

&lt;p&gt;사실 간단하다. 아까 구운 &lt;code class=&quot;highlighter-rouge&quot;&gt;시동 디스크&lt;/code&gt;를 꼽아주고 부팅을 하면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u4.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u5.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u6.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u7.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u8.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u9.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u10.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u11.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ds/gpuserver/u12.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사진출처: &lt;a href=&quot;https://websiteforstudents.com/install-ubuntu-18-04-lts-server-screenshots/&quot;&gt;https://websiteforstudents.com/install-ubuntu-18-04-lts-server-screenshots/&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;언어선택: 왠만하면 영어로 하자&lt;/li&gt;
  &lt;li&gt;키보드선택: 왠만하면 영어로 가자&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Install Ubuntu&lt;/code&gt; 선택&lt;/li&gt;
  &lt;li&gt;다음&lt;/li&gt;
  &lt;li&gt;특별한 주소가 있으면 작성 아니면, 다음&lt;/li&gt;
  &lt;li&gt;디스크 포맷: 디스크 통째로 포맷한 후에 설치할 것이니 1번&lt;/li&gt;
  &lt;li&gt;디스크 선택&lt;/li&gt;
  &lt;li&gt;마지막 확인&lt;/li&gt;
  &lt;li&gt;정말루?&lt;/li&gt;
  &lt;li&gt;당신의 이름 / 서버 이름 / 유저이름(로그인용) / 패스워드(로그인용) 등&lt;/li&gt;
  &lt;li&gt;설치중… 리붓!&lt;/li&gt;
  &lt;li&gt;완료&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;다음 장에는 설치후에 내가 했던 작업들: &lt;code class=&quot;highlighter-rouge&quot;&gt;원격 부팅과 접속&lt;/code&gt; 을 주로 이야기 하겠다.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jun 2018 22:26:58 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/06/02/gpuserver.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/06/02/gpuserver.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: GloVe</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-glove&quot;&gt;All about Word Vectors: GloVe&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;co-occurrence&quot;&gt;Co-occurrence&lt;/h2&gt;

&lt;p&gt;공기(Co-occurrence) 란 무엇인가? 두 개 이상의 어휘가 일정한 범위(range) 혹은 거리(distance) 내에서 함께 출현하는 현상을 말한다. 여기서 어휘는 단어 뿐만 아니라 형태소, 합성어 등의 단위로 의미를 부여할 수 있는 언어 단위다. 그렇다면 왜 &lt;strong&gt;공기 관계&lt;/strong&gt; 를 살피는 것일까?&lt;/p&gt;

&lt;p&gt;공기 관계를 통해 문서나 문장으로 부터 &lt;strong&gt;추상화된 정보&lt;/strong&gt; 를 얻기 위해서다. 이는 자연어처리의 가정을 생각해보면 이해할 수 있을 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;비슷한 맥락에 등장하는 단어들은 유사한 의미를 지니는 경향이 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;때문에, 두 단어가 같이 등장한 횟수가 많아지면 &lt;strong&gt;유사한 의미&lt;/strong&gt; 를 가졌다고 볼 수도 있다는 것이다. 이런 유사한 의미를 추상화된 정보로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;공기(Co-occurrence) 정보를 수집하는 방법은 두 가지다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;window 기반: 품사와 의미(semantic) 정보를 캡쳐할 수 있다.&lt;/li&gt;
  &lt;li&gt;word-document co-occurrence matrix 기반: 조금 더 일반적인 토픽을 추출 할 수 있고, 이는 Latent Semantic Analysis 와 연결 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;example-window-based-co-occurrence-matrix&quot;&gt;Example: Window based co-occurrence matrix&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;I like deep learning.&lt;/p&gt;

  &lt;p&gt;I like NLP.&lt;/p&gt;

  &lt;p&gt;I enjoy flying.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 3 문장을 사용해서, window size = 1 로 지정하는 co-occurrence matrix 를 만들어보자. 무슨 뜻인지는 아래 코드를 실행한 표를 살펴보자.&lt;/p&gt;

&lt;p&gt;단, 한 단어에 대해서 좌측에서 등장했는지 우측에서 등장했는 지는 상관없다(이는 co-occurrence matrix 가 대각을 기준으로 대칭하는 결과를 불러옴)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
import numpy as np
from collections import deque, Counter
from itertools import islice
from scipy.sparse import coo_matrix

flatten = lambda t: [tuple(j) for i in t for j in i]
window = 1

def get_cooccur_list(sentence, window):
    s_len = len(sentence)
    ngram_list = [deque(islice(sentence, i), window+1) for i in range(s_len+1)][2:]
    return ngram_list

sentences = ['I like deep learning .', 'I like NLP .', 'I enjoy flying .']
tokens = [s.split() for s in sentences]
vocab = list(set([w for s in tokens for w in s]))
# print(vocab)
vocab = ['I', 'like', 'enjoy', 'deep', 'learning', 'NLP', 'flying', '.'] # 표와 같은 모양을 만들어주기 위해 다시 지정
vocab2idx = {w: i for i, w in enumerate(vocab)}
tokens_idx = [[vocab2idx.get(w) for w in s] for s in tokens]
co_occurs = [get_cooccur_list(s, window) for s in tokens_idx]

d = Counter()
d.update(flatten(co_occurs))
row, col, data = list(zip(*[[r, c, v] for (r, c), v in d.items()]))
temp = coo_matrix((data, (row, col)), shape=(len(vocab), len(vocab))).toarray()
co_mat = temp.T + temp

pd.DataFrame(co_mat, index=vocab, columns=vocab)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;코드를 실행하면 아래와 같은 표가 나온다. window size 가 1이니까 “I” 주변 한칸에 동시 등장 단어는 “like” 가 2번 “enjoy” 가 1번이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;counts&lt;/th&gt;
      &lt;th&gt;I&lt;/th&gt;
      &lt;th&gt;like&lt;/th&gt;
      &lt;th&gt;enjoy&lt;/th&gt;
      &lt;th&gt;deep&lt;/th&gt;
      &lt;th&gt;learning&lt;/th&gt;
      &lt;th&gt;NLP&lt;/th&gt;
      &lt;th&gt;flying&lt;/th&gt;
      &lt;th&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;I&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;like&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;enjoy&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;deep&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;learning&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NLP&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;flying&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;co-occurrence matrix와 같은 단어 벡터는 어떤 문제점이 있을까?&lt;/p&gt;

&lt;p&gt;첫째로, 단어가 많아지면 벡터가 엄청 길어진다(데이터 차원이 커진)는 것이다. 이에 따른 많은 저장 비용이 들어갈 것이다. 둘째로, sparsity issues가 있을 수 있다(models are less robust).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그렇다면 꼭 하나의 단어로 해야만 하는가? 문서 전체의 단어의 공기 정보를 추출 하는 것은 안되는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이와 같은 생각이 GloVe 를 탄생시켰다.&lt;/p&gt;

&lt;h2 id=&quot;glove&quot;&gt;GloVe&lt;/h2&gt;

&lt;p&gt;Paper: &lt;a href=&quot;https://www.aclweb.org/anthology/D14-1162&quot;&gt;GloVe: Global Vectors for Word Representation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GloVe 의 학습방법은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \dfrac{1}{2} \sum_{i,j=1}^{W} f(P_{ij})(u_i^T v_j - \log P_{ij})^2&lt;/script&gt;

&lt;p&gt;논문해설을 통해서 자세히 보자.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;section&quot;&gt;논문 해설&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;GloVe:&lt;/strong&gt; Global Vectors라고 명칭을 지은 이유는 모델에서 직접적으로 문서 전체의 코퍼스 통계량을 포착할 수있기 때문이다. (the global corpus statistics are captured directly by the model)&lt;/p&gt;

&lt;p&gt;그전에 notation 을 정의해보자.&lt;/p&gt;

&lt;p&gt;Define notation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$X$: 단어간의 공기 매트릭스 (matrix of word-word co-occurrence counts)&lt;/li&gt;
  &lt;li&gt;$X_{ij}$: 단어 $j$ 와 문맥 단어 $i$ 가 같이 등장한 횟수 (the number of times that word $j$ occurs in the context word $i$)&lt;/li&gt;
  &lt;li&gt;$X_i = \sum_k X_{ik}$: 어떤 단어든 문맥 단어 $i$ 와 등장한 횟수 (the number of times any word appears in the context of word $i$)&lt;/li&gt;
  &lt;li&gt;$P_{ij} = P(j \vert i) = X_{ij} / X_i$: 단어 $j$ 와 문맥 단어 $i$ 동시 등장할 확률, 문맥 단어 $i$ 가 주어졌을 때 $j$ 가 등장할 확률 (probability that word $j$ appear in the context word $i$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래의 예시를 보자.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Probability and Ratio&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=solid$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=gas$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=water$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$k=fashion$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert ice)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.00019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000066&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.003&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert steam)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000022&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.00078&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0022&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.000018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$P(k\vert ice)/P(k\vert steam)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8.9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.085&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.96&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;위의 표에 따르면 $i=ice, j=steam$ 일때 $solid$ 와 동시 등장 확률이 높은 단어는 $ice$ 다. 직관적으로 생각해도, 단단한 $ice$ 가 $solid$ 와 연관될 확률이 더 높다. 따러서, 우리는 $P(k\vert ice)/P(k\vert steam)$ 를 구해서, 연관이 있는 단어일 경우 이 비율이 크게 높으며, 아니면 그 반대다. &lt;strong&gt;(엄청 크거나 혹은 엄청 작거나)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이처럼 직접적으로 단어간의 동시등장 확률을 비교하는 것보다. 확률간의 비율을 구하는 것이 &lt;strong&gt;연관성이 없는&lt;/strong&gt; 단어(water &amp;amp; fashion)들로 부터 관련된 단어(solid &amp;amp; gas)를 구별하기 좋으며, &lt;strong&gt;관련성 있는&lt;/strong&gt; 단어(solid &amp;amp; gas)들을 차별화 하기에도 좋다.&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;동시 등장 확률의 비율(ratios of co-occurrence probabilities)&lt;/span&gt;&lt;/strong&gt; 을 모델이 학습하게 하는 것이 바람직 해보인다.&lt;/p&gt;

&lt;p&gt;중요한 것은 이 비율은 3개의 단어 $i, j, k$ 와 연관이 있다. 따라서 아래의 함수를 구성할 수가 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i, w_j, \tilde{w}_k) = \dfrac{P_{ik} }{P_{jk} } \cdots (1)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$w \in \Bbb{R}^d$: word vectors&lt;/li&gt;
  &lt;li&gt;$\tilde{w} \in \Bbb{R}^d$: separate context word vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$(1)$ 식과 같이, 단어 벡터 공간에서 $w_i, w_j, \tilde{w}_k$ 를 input으로 넣었을 때,&lt;/p&gt;

&lt;p&gt;$\dfrac{ P_{ik} }{ P_{jk} }$ 비율을 나타내는 하는 선형구조인 함수를 구하는 것이 목적이다. 그리고 $F$ 를 아래와 같이 변형시켜 본다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F((w_i - w_j)^T \tilde{w}_k) = \dfrac{P_{ik} }{P_{jk} } \cdots (2)&lt;/script&gt;

&lt;p&gt;이로써 선형적인 관계를 포착하고, 양변 모두 스칼라 값으로 정해진 함수가 만들어 졌다. 그러나 단어 $i, j$ 와 $k$ 동시 등장 비율의 &lt;strong&gt;임의적인 차별화&lt;/strong&gt; 를 위해서 어떤 조건들을 만족해야한다. 그 조건들이란 단어 벡터 $w$ 와 문맥 단어 벡터 $\tilde{w}$ 간 서로 자유롭게 교환 될 수가 있어야 한다. 즉, 단어간 공기 매트릭스 $X$의 대칭(symmetric) 특성을 보존해야 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;임의적인 차별화&lt;/strong&gt; 가 무슨 말이냐면, 단어 $k$ 와 $i, j$ 단어 간의 비율을 확인 할때, $i$ 와 $k, j$ (혹은 $j$ 와 $i, k$) 의 관계도 확인 할 수 있어야 된다는 말이다.&lt;/p&gt;

&lt;p&gt;대칭(symmetric) 을 만족하려면 2 단계로 진행 된다. 우선, 두 그룹 $(\Bbb{R}, +)$ 과 $(\Bbb{R}_{&amp;gt;0}, \times)$ 에 대해서 함수 $F$ 가 &lt;strong&gt;homomorphism&lt;/strong&gt; 이어야 한다. (homomorphism 해설: 밑에 &lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt;를 보라), 예를 들어 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i, w_j, \tilde{w}_k) = \dfrac{F(w_i^T \tilde{w}_k) }{F(w_j^T \tilde{w}_k) } \cdots (3)&lt;/script&gt;

&lt;p&gt;$(2)$ 식에 의해서, 아래와 같이 풀 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w_i^T \tilde{w}_k) = P_{ik} = \dfrac{X_{ik} }{X_i} \cdots (4)&lt;/script&gt;

&lt;p&gt;$(3)$ 식에 만족하는 해답은 $F = \exp$ 임으로, 아래의 식을 도출 해낼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^T \tilde{w}_k = \log(P_{ik}) = \log(X_{ik}) - \log(X_i) \cdots (5)&lt;/script&gt;

&lt;p&gt;다음으로, $(5)$ 식은 $\log(X_i)$ 만 아니였다면 대칭이었을 것이다. $\log(P_{ik})=\log(P_{ki})$ 를 만족하는지 한번 보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\log(P_{ik}) &amp;= \log(X_{ik}) - \log(X_i) \\
\log(P_{ki}) &amp;= \log(X_{ki}) - \log(X_k)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;당연하게도, $\log(X_i) \neq \log(X_k)$ 이기 때문에 $\log(P_{ik}) \neq \log(P_{ki})$ 이다.&lt;/p&gt;

&lt;p&gt;하지만 $\log(X_i)$ 부분은 $k$ 에 대해서 독립적(independent) 이기 때문에, $w_i$ 의 bias $b_i$ 항으로 들어갈 수 있다. 그리고 대칭성을 유지하기 위해서 $\tilde{w}_k$ 의 bias $\tilde{b}_k$ 항도 더해준다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^T \tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik}) \cdots (6)&lt;/script&gt;

&lt;p&gt;$(6)$ 식이 우리의 제일 간단한 선형관계인 solution 이라고 해도 되지만 이는 문제가 좀 있다. $X_{ik} = 0$ 에서 명확하게 정의 되지 않는다. 이를 해결하기 위해서 $X_{ik}+1$ 하는 방법도 있지만, sparsity issue 를 벗어나기 힘들다. 그리고 하나의 큰 약점이 있다면 거의 등장하지 않는 단어들에게 동시 등장 비율이 모두 같을 수 있다는 점이다. 이게 왜 문제가 되냐면, co-occurrence 가 적을 수록 많이 등장하는 단어들 보다 정보 함량이 적고 데이터도 noisy 하기 때문이다.&lt;/p&gt;

&lt;p&gt;연구팀은 새로운 weighted least squares regression model 을 제시하여 문제를 풀고자 했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \dfrac{1}{2} \sum_{i,j=1}^{W} f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2 \cdots (7)&lt;/script&gt;

&lt;p&gt;가중치 함수 (Weighting function) $f(X_{ij})$ 는 아래의 특성을 따라야 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$f(0) = 0$. 만약 $f$ 가 연속함수(continuous function) 라면, $x \rightarrow 0$ 으로 갈때 $\lim_{x\rightarrow 0} f(x) log^2x$ 도 빠르게 수렴한다. 단, 유한한 값이여야 한다.&lt;/li&gt;
  &lt;li&gt;$f(x)$ 는 감소함수가 되면 안된다. (non-decreasing) 이유는 동시 등장이 희박한 단어들의 가중치가 많아져서는 안되기 때문이다.&lt;/li&gt;
  &lt;li&gt;$f(x)$ 는 큰 $x$ 값에 대해서 상대적으로 작은 값이어야 한다. 그 이유는 공기 횟수가 큰 단어들의 가중치가 너무 높게 설정하지 않기 위해서다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;연구팀은 이에 적합한 함수를 찾았다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x) =
\begin{cases} (x/x_{max})^{\alpha} \quad if\ x &lt; x_{max} \\
1 \quad otherwise
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L3_weight_f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림: $f(x)$ with $\alpha = 3/4$ 일때 좋은 성과를 얻었다. 재밌는 것은 Mikolov 논문에서 나온 unigram distribution 에 3/4 승을 해주는 것과 같다는 것을 발견했다.&lt;/p&gt;

&lt;p&gt;조금더 general 한 weighting function 은 아래와 같다. (자세한건 논문 3.1 참고)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{J} = \sum_{i,j} f(X_{ij})(w_i^T \tilde{w}_j - \log X_{ij})^2&lt;/script&gt;

&lt;p&gt;이는 연구팀이 도출한 $(7)$ 식과 같은 식이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;homomorphism&quot;&gt;참고 1: Homomorphism&lt;/h3&gt;

&lt;p&gt;혹시나 틀렸으면 댓글로 이야기 해주세요.&lt;/p&gt;

&lt;p&gt;우선 Group &lt;a href=&quot;https://en.wikipedia.org/wiki/Group_(mathematics)&quot;&gt;(위키 링크)&lt;/a&gt; 이란 것을 알아야한다. 내가 이해한 바로는 &lt;strong&gt;Group $(G, * )$&lt;/strong&gt; 이란, 집합 $G$ 와 연산 $* $ 로 구성되어 있다. 이 연산을 “the Group Law of $G$” 라고 부른다. 집합 $G$ 에 속한 원소 $a, b$ 의 연산을 $a * b$ 라고 표현한다. 또한 아래의 조건들을 만족해야한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Closure: $* $ 연산은 $G$ 에 대해 닫혀 있어야한다. 즉, $a * b$ 연산도 집합 $G$ 에 속해야한다.&lt;/li&gt;
  &lt;li&gt;Associativity: 교환 법칙이 성립해야한다. $(a * b) * c = a * (b * c)$&lt;/li&gt;
  &lt;li&gt;Identity element: 항등원이 존재해야 한다. $a * e = a = e * a$&lt;/li&gt;
  &lt;li&gt;Inverse element: 역원이 존재해야 한다. $a * x = e = x * a$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Group 를 이해 했으면 이제 Homomorphism 을 이해해보자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;정의:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;두 그룹 &lt;span style=&quot;color: #15b23c&quot;&gt;$(G, * )$&lt;/span&gt; 과 &lt;span style=&quot;color: #9013b2&quot;&gt;$(H, @)$&lt;/span&gt; 가 있으면, 모든 &lt;span style=&quot;color: #15b23c&quot;&gt;$x, y \in G$&lt;/span&gt; 에 대해서 $f:$ &lt;span style=&quot;color: #15b23c&quot;&gt;$G$&lt;/span&gt; $\rightarrow$ &lt;span style=&quot;color: #9013b2&quot;&gt;$H$&lt;/span&gt;, &lt;span style=&quot;color: #15b23c&quot;&gt;$f(x * y)$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$f(x) @ f(y)$&lt;/span&gt; 를 만족하는 map 을 말한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예시)&lt;/strong&gt;
두 그룹 &lt;span style=&quot;color: #15b23c&quot;&gt;$(\Bbb{R}, + )$&lt;/span&gt; 와 &lt;span style=&quot;color: #9013b2&quot;&gt;$(\Bbb{R}_{&amp;gt;0}, \times )$&lt;/span&gt; 사이에 어떤 map $f:$ &lt;span style=&quot;color: #15b23c&quot;&gt;$\Bbb{R}$&lt;/span&gt; $\rightarrow$ &lt;span style=&quot;color: #9013b2&quot;&gt;$\Bbb{R}_{&amp;gt;0}$&lt;/span&gt;, $f(x)=e^x$ 가 있다면, $f$ 가 Homomorphism 인지를 밝혀라.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;for any &lt;span style=&quot;color: #15b23c&quot;&gt;$x, y \in \Bbb{R}$&lt;/span&gt;,&lt;/p&gt;

  &lt;p&gt;&lt;span style=&quot;color: #15b23c&quot;&gt;$f(x + y)$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$e^{x+y}$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$e^x \times e^y$&lt;/span&gt; $=$ &lt;span style=&quot;color: #9013b2&quot;&gt;$f(x) \times f(y)$&lt;/span&gt; 임으로&lt;/p&gt;

  &lt;p&gt;Homomorphism 을 만족한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;word2vec 과 glove 관련 포스팅은 &lt;strong&gt;“All about word vectors”&lt;/strong&gt; 시리즈로 마치겠다. 기회가 되면 gensim 의 사용법과, 데이터 차원 축소와 시각화 방법인 t-SNE 을 포스팅 하도록 하겠다.&lt;/p&gt;
</description>
        <pubDate>Wed, 02 May 2018 23:22:05 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/05/02/allaboutwv4.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/05/02/allaboutwv4.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Negative Sampling</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-negative-sampling&quot;&gt;All about Word Vectors: Negative Sampling&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 3 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_model_train.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;navie-softmax--&quot;&gt;Navie Softmax 의 단점&lt;/h2&gt;

&lt;p&gt;Navie Softmax 를 최종단에 출력으로 두고 Backpropagation 할때는 큰 단점이 있다.&lt;/p&gt;

&lt;p&gt;사실 Softmax가 그리 값싼 연산은 아니다. 우리가 학습하고 싶은 단어 벡터 1000개가 있다고 가정해보자. 그렇다면 매 window size=2 마다, 다시 말해 총 업데이트 할 5개의 단어 (중심단어 1 + 주변 단어 2 x 2) 를 위해서, $W, W’$ 안에 파라미터를 업데이트 해야하는데, 그 갯수가 최소 $(2 \times d \times 1000)$ 만큼된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown_\theta J_t(\theta) \in \Bbb{R}^{2dV}&lt;/script&gt;

&lt;p&gt;많은 양의 단어에 비해 업데이트 하는 파라미터수는 적기 때문에 gradient matrix $\triangledown_\theta J_t(\theta)$ 가 굉장히 sparse 해질 수 있다 (0이 많다는 소리). Adam 같은 알고리즘은 sparse 한 matrix 에 취약하다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://simonjisu.github.io/deeplearning/2018/01/13/numpywithnn_5.html&quot;&gt;Numpy with NN: Optimizer 편 참고&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그래서 &lt;strong&gt;“window에 실제로 등장하는 단어들만 업데이트 하면 좋지 않을까?”&lt;/strong&gt; 라는 생각을 하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;negative-sampling&quot;&gt;Negative Sampling&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;paper 1: &lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;&gt;Distributed representaions of Words and Phrases and their Compositionality (Mikolov et al. 2013)&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;paper 2: &lt;a href=&quot;https://arxiv.org/abs/1402.3722&quot;&gt;word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;요약하면 아래와 같은 목적함수를 최대화 하는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
J(\theta) &amp;= \dfrac{1}{T}\sum_{t=1}^{T} J_t(\theta)\\
J_t(\theta) &amp;= \underbrace{\log \sigma(u_o^T v_c)}_{(1)} + \underbrace{\sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_c)]}_{(2)}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$T$: total num of words&lt;/li&gt;
  &lt;li&gt;$\sigma$: sigmoid function&lt;/li&gt;
  &lt;li&gt;$P(w) = {U(w)^{3/4}} / {Z}$: unigram distribution U(w) raised to the 3/4 power
    &lt;ul&gt;
      &lt;li&gt;The power makes less frequent words be sampled more often&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;말로 풀어보자면, 모든 단어 $T$ 에 대해서 중심단어 $c$ 와 그 주변단어 $o$ 가 같이 나올 확률 &lt;strong&gt;[수식 (1)]&lt;/strong&gt; 을 최대화 하고, 그 주변단어가 아닌 집합에서 sampling 하여 나온 $k$ 개의 단어의 확률 &lt;strong&gt;[수식 (2)]&lt;/strong&gt; 을 최소화 시키는 것이다. (음수가 붙기 때문에 최소하하게 되면 최대화가 된다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section&quot;&gt;상세 논문 설명&lt;/h3&gt;

&lt;p&gt;논문 기준으로 위에 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;표기법&lt;/span&gt;&lt;/strong&gt; 이 조금 다르다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;여기서 &lt;strong&gt;$w$ = center word, $c$ = context&lt;/strong&gt; 다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;출발점은 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$(w, c)$ 세트가 정말로 corpus data로 부터 왔는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;라고 생각하고 아래와 같은 &lt;strong&gt;정의&lt;/strong&gt; 를 하게 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로 부터 왔을 확률&lt;/li&gt;
  &lt;li&gt;$P(D = 0 \vert w, c) = 1 - P(D = 1 \vert w, c)$ : $(w, c)$ 가 corpus data로부터 오지 않았을 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, 우리의 목적은 확률 $P(D = 1\vert\ w, c)$ 를 최대화하는 parameter $\theta$를 찾는 것이기 때문에 아래와 같은 목적함수를 세울 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} &amp;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &amp;\arg \underset{\theta}{\max} \log \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \\
= &amp;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;파라미터 $\theta$ 는 단어들의 벡터라고 생각할 수 있다. 즉, 위의 식을 만족하는 어떤 최적의 단어 벡터를 찾는것이다.&lt;/p&gt;

&lt;p&gt;또한, 확률 $P(D=1\vert\ w,c;\theta)$ 은 sigmoid로 아래와 같이 정의 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D=1\vert\ w,c;\theta) = \dfrac{1}{1+e^{-v_c v_w}}&lt;/script&gt;

&lt;p&gt;따라서 우리의 목적함수는 아래와 같이 다시 고쳐 쓸수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} }&lt;/script&gt;

&lt;p&gt;그러나 우리의 목적 함수는 매 $(w, c)$ 세트마다 $P(D=1\vert\ w,c;\theta)=1$ 를 만족하는 trivial solution이 존재한다. $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 $\theta$ (보통 $K$ 가 40이 넘어가면 위 방정식의 값이 0에 가까워짐) 는 모든 값을 똑같이 0으로 만들어 버리기 때문에, 같은 값을 갖지 못하게 하는 매커니즘이 필요하다. ($\theta$ 에 뭘 넣어도 0이 되면 최대값을 찾는 의미가 없어진다, 자세한건 밑에 &lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt; 를 참조) 여기서 “같은 값을 같는다” 라는 말은 단어 벡터가 같은 값을 갖는 것이다.&lt;/p&gt;

&lt;p&gt;따라서, 하나의 방법으로 랜덤 $(w, c)$ 조합을 생성하는 집합 $D’$를 만들어 corpus data 로부터 올 확률 $P(D=1\vert \ w,c;\theta)$ 를 낮게 강제하는 것이다. 즉, $D’$ 에서 생성된 $(w, c)$ 조합은 &lt;strong&gt;corpus data 로부터 오지 않게&lt;/strong&gt; 하는 확률 $P(D=0\vert\ w,c;\theta)$ 을 최대화 하는 것.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} P(D=0\vert\ w,c;\theta) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\prod} P(D=1\vert\ w,c;\theta) \underset{(w,c) \in D'}{\prod} \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log P(D=1\vert\ w,c;\theta) + \underset{(w,c) \in D'}{\sum} \log \big(1- P(D=1\vert\ w,c;\theta) \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \big(1- \dfrac{1}{1+e^{-v_c v_w} } \big) \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} }
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\sigma(x) = \dfrac{1}{1+e^{-x} }$ 시그모이드 함수로 정의 하면, 아래와 같이 정리 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } + \underset{(w,c) \in D'}{\sum} \log \dfrac{1}{1+e^{v_c v_w} } \\
&amp;= \arg \underset{\theta}{\max} \underset{(w,c) \in D}{\sum} \log \sigma(v_c v_w) + \underset{(w,c) \in D'}{\sum} \log \sigma(- v_c v_w) \quad \cdots (3)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;이는 &lt;span style=&quot;color: #e87d7d&quot;&gt;paper 1&lt;/span&gt; 의 (4) 번 식과 같아지는다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log \sigma(u_c^T v_w) + \sum_{i=1}^{k} \mathbb{E}_{j \backsim P(w)} [\log \sigma(-u_j^T v_w)]&lt;/script&gt;

&lt;p&gt;다른 점이라면, 우리가 만든 (3)식에서는 전체 corpus ($D \cup D’$) 을 포함하지만, Mikolov 논문의 식은 $D$ 에 속하는 $(w, c)$ 조합 하나와 $k$ 개의 다른 $(w, c_j)$ 의 조합을 들었다는 것이다. 구체적으로, $k$ 번의 negative sampling 에서 Mikolov 는 $D’$ 를 $k \times D$ 보다 크게 설정했고, k개의 샘플 $(w, c_1), (w, c_2), \cdots, (w, c_k)$ 에 대해서 $c_j$ 는 &lt;strong&gt;unigram distribution&lt;/strong&gt; 에 &lt;strong&gt;3/4&lt;/strong&gt; 승으로 부터 도출된다. 이는 아래의 분포에서 $(w, c)$ 조합을 추출 하는 것과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{words}(w) = \dfrac{p_{contexts} (c)^{3/4} }{Z}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$p_{words}(w)$, $p_{contexts} (c)$ 는 각각 words and contexts 의 unigram distribution 이다.&lt;/li&gt;
  &lt;li&gt;$Z$ 는 normalization constant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unigram distribution 은 단어가 등장하는 비율에 비례하게 확률을 설정하는 분포다. 예를 들어 “I have a pen. I have an apple. I have a pineapple.” 라는 문장이 있다면, 아래와 같은 분포를 만들 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;I&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;have&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;a&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;pen&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;an&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;apple&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;pineapple&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3/15&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;여기서 3/4 승을 해주면, 가끔 등장하는 단어는 확률을 높혀주는 효과가 있다. 물론 자주 나오는 단어의 확률도 올라가지만 가끔 등장하는 단어의 상승폭 보다 적다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;a&lt;/th&gt;
      &lt;th&gt;$a^{\frac{3}{4} }$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;apple&lt;/td&gt;
      &lt;td&gt;$\frac{1}{15}=0.067$&lt;/td&gt;
      &lt;td&gt;${\frac{1}{15} }^{\frac{3}{4} }=0.131$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;have&lt;/td&gt;
      &lt;td&gt;$\frac{3}{15}=0.020$&lt;/td&gt;
      &lt;td&gt;${\frac{3}{15} }^{\frac{3}{4} }=0.299$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Mikolov 논문에서는 context는 하나의 단어이기 때문에 $p_{words}(w)$ 는 아래와 동일하다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{words}(w) = p_{contexts} (c) = \dfrac{count(x)}{ \vert text \vert }&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;trivial-solution&quot;&gt;참고 1. Trivial Solution&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} L(\theta;w,c) &amp;= \underset{(w,c) \in D}{\sum} \log \dfrac{1}{1+e^{-v_c v_w} } \\
&amp;= \underset{(w,c) \in D}{\sum} \log(1) - \log(1+e^{-v_c v_w}) \\
&amp;= \underset{(w,c) \in D}{\sum} - \log(1+e^{-v_c v_w})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;같은 두 벡터의 내적을 하게 되면 값은 최대가 된다. $\cos$ 값이 1이 되기 때문이다. (여기서는 최대 값이 중요한건 아니지만 값이 커진다는데 의의가 있다.)
&lt;script type=&quot;math/tex&quot;&gt;a\cdot a=\vert a \vert \vert a \vert \cos \theta&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a = np.array([1,2,3,4,5,6,7])
b = np.array([.1,.2,.3,.4,.5,.6,.7])
print(np.dot(a, a))
print(np.dot(a, b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;140&lt;/p&gt;

  &lt;p&gt;14.0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, $v_c = v_w$ 이며, $\forall v_c,\ v_w$ 에 대해 $v_c \cdot v_w = K$ 를 만족하는 모든 값들이 $e^{-v_c v_w}$ 를 0으로 만든다면, $L(\theta; w, c)$ 값은 0이 될것이다. 보통 $K$ 가 40 이 상이면, $L(\theta;w,c)$ 의 해는 모두 0 일 것이며 이것을 &lt;strong&gt;trivial solution&lt;/strong&gt; 이라고 한다. 우리의 목적은 단어 벡터 $v_c$ 와 $v_w$ 의 구별이기 때문에, $v_c \not = v_w$ 으로 만들어야한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음 시간에는 말뭉치의 공기정보(co-occurance)를 고려해 단어를 벡터화 시킨 &lt;strong&gt;GloVe&lt;/strong&gt; 에 대해 알아보자.&lt;/p&gt;
</description>
        <pubDate>Tue, 24 Apr 2018 16:14:13 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/24/allaboutwv3.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Big Little Data 참석후기</title>
        <description>&lt;h1 id=&quot;little-big-data--&quot;&gt;Little Big Data 참석 후기&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ds/littlebigdata.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;주최: ZEPL
링크: &lt;a href=&quot;https://festa.io/events/21&quot;&gt;festa&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;발표자분들께서 자신들이 겪은 다양한 경험을 들었다. 모든 일이 다 그렇지만, 문제를 파악하고 정의를 어떻게 하며, 방법을 모색하고 해결 후 결과를 다시 한번 정리해보는 사고 프로세스를 배운 것 같다.&lt;/p&gt;

&lt;p&gt;발표 세션을 듣고 일이 있어서 나와야했지만 유익했던 자리.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-1&quot;&gt;상세&lt;/h2&gt;

&lt;p&gt;발표자의 내용이 완벽하게 일치하지 않으며, 제가 중간중간 생각나서 제 생각을 기록한 것도 있습니다. (거의 없긴 하지만)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-2&quot;&gt;극한직업: 한국어 채팅 데이터로 머신러닝 하기&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.scatterlab.co.kr/&quot;&gt;Scatter Lab&lt;/a&gt; 조한석 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;한글 데이터는 문제가 많음:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hell 조사&lt;/li&gt;
  &lt;li&gt;자유로운 언어 변형&lt;/li&gt;
  &lt;li&gt;혀꼬인 소리&lt;/li&gt;
  &lt;li&gt;맞춤법 및 띄어쓰기 오류&lt;/li&gt;
  &lt;li&gt;챗팅에서 쓰이는 단어&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;한국어 데이터는 전처리(Preprocessing)이 80%다.&lt;/p&gt;

&lt;p&gt;일반 오픈소스 형태소 분석기의 한게점: 학습에 사용된 corpus가 잘 정돈된 데이터를 학습했기 때문에, 잘 안되는 경향이 있음.&lt;/p&gt;

&lt;p&gt;그래서 데이터로부터 학습하자! 김현중 님의 &lt;a href=&quot;https://github.com/lovit/soynlp&quot;&gt;soynlp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;다양한 문제들과 문제 정의 및 해결:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Normalize:
    &lt;ul&gt;
      &lt;li&gt;아이디어: 오류가 적다고 생각하는 데이터를 선택 후, 전체 데이터에서 조금 등장한 패턴을 자주 등장하는 것으로 수정하는 방법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;띄어쓰기 교정:
    &lt;ul&gt;
      &lt;li&gt;다음 글자가 띄어쓸지 아닐지 binary Classification&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tokenizing:
    &lt;ul&gt;
      &lt;li&gt;단어 추출 process&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Word Embedding:
    &lt;ul&gt;
      &lt;li&gt;oov 문제 (학습되지 않는 단어는 inference 단계에서 문제)&lt;/li&gt;
      &lt;li&gt;Fasttext 사용: substring 정보 활용 &lt;a href=&quot;https://arxiv.org/abs/1607.01759&quot;&gt;paper&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;한글의 경우: ngam 단위를 글자 / 자음모음으로 하게됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sentence Similarity
    &lt;ul&gt;
      &lt;li&gt;BOW + Word Embedding 방식: 임베딩에 너무 의존하게 됨, 학습된 데이터에 따라서 원하는 결과가 안나올 수도 있음, 따라서 다른 방법을 추가해서 쓰게됨.&lt;/li&gt;
      &lt;li&gt;참고한 논문: &lt;a href=&quot;http://cogcomp.org/papers/SongRo15.pdf&quot;&gt;Unsupervised Sparse Vector Densification for Short Text Similarity&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;팁
    &lt;ul&gt;
      &lt;li&gt;전처리 단계에서 &lt;a href=&quot;https://en.wikipedia.org/wiki/Heaps%27_law&quot;&gt;힙의 법칙(Heap’s Law)&lt;/a&gt; 에 따라서 빈도수가 너무 적은 단어는 과감하게 쳐내기&lt;/li&gt;
      &lt;li&gt;문제 정의를 잘하기, Countbase 모델이 오히려 더 잘 될 수도 있다.&lt;/li&gt;
      &lt;li&gt;unlabel 데이터에 label 을 달아서 인사이트를 얻어보자!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-3&quot;&gt;딥러닝에 필요한 로그 기깔나게 잘 디자인하는 법&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;구글 클라우드 엔지니어 백정상 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;탐색적 데이터 분석(EDA)을 잘 하기위해 어떻게 로그를 쌓는 것이 좋았는가?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로그 디자인&lt;/li&gt;
  &lt;li&gt;명확한 데이터&lt;/li&gt;
  &lt;li&gt;원시 데이터에서 분석 쿼리&lt;/li&gt;
  &lt;li&gt;등등&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 딥러닝은 조금 달랐다!!&lt;/p&gt;

&lt;p&gt;통계적으로 풀지 못하는 문제 생겼는데, 비정상적인 데미지를 만드는 플레이어가 핵유저인지 아닌지? 이상 탐지 문제 (정확히 기억이 안남)&lt;/p&gt;

&lt;p&gt;고전적인 머신러닝 기법으로 풀려고 보니 바운더리 필요하고, 어느정도 데미지가 비정상적인 플레이고, 정상적인 플레이인지 알수 없었음.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;정상적인 데이터가 충분히 많다면, 학습후 비정상 데이터 잡아내기 &lt;strong&gt;(지도학습)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;정상인지 아닌지 확신할 데이터 충분치 않다면, 학습후 클러스터링 해서 아웃라이어 잡기 &lt;strong&gt;(비지도학습)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;데이터가 충분하지 않았기 때문에 2번으로 선택후 가설을 세움.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;가설: 유저들이 평균적으로 내는 데미지에 비해 엄청 크면? –&amp;gt; 비정상&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Feature engineering: feature selection&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;유저 인덱스 들어가면 분류 너무 세분화,&lt;/li&gt;
  &lt;li&gt;페이즈 별로 데미지 얼마 넣었는지 보다 스테이지 완료시 데미지만,&lt;/li&gt;
  &lt;li&gt;캐릭터 직업별로 데미지 넣는 양이 다르니, 직업은 넣자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기까지 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;행복회로! &lt;/span&gt;&lt;/strong&gt; 상상의 나래였던거임.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;-현실-&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;무슨 모델을 쓸 것인가? 꼭 딥러닝을 써야되나?&lt;/p&gt;

&lt;p&gt;오코인코더 써서 대다수 유저와 loss 차이가 많이 나는지 확인! (위에 가설을 검정확인함)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deep AutoEncoder - Compressed Feature Vector: 데이터 복원하는 속성 학습&lt;/li&gt;
  &lt;li&gt;train, 평가 쉬움&lt;/li&gt;
  &lt;li&gt;주의할 것은 &lt;strong&gt;가설&lt;/strong&gt; 이 참이여야만 모델이 정확하다는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;만들면서 생긴 문제들:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;input data 에 따라서 달라짐 &amp;gt; 어떤 feature 를 쓸 것인가?
    &lt;ul&gt;
      &lt;li&gt;꼭 필요함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;훈련방식: 온라인, 배치, 저장된 데이터 &amp;gt; 3개 다해야함
    &lt;ul&gt;
      &lt;li&gt;이유: 왜냐면 많은 사람들이 어뷰징을 쓰게되면 바이어스가 정상으로 학습 될 수도&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;피쳐선택:
    &lt;ul&gt;
      &lt;li&gt;꼭 필요함. 피쳐 늘어날 수록 더 많은 데이터가 필요함&lt;/li&gt;
      &lt;li&gt;EDA: 게임상에서 일어나는 특징적인 패턴을 찾는 데 주력 &amp;gt; 어떤 피쳐가 상관관계가 높지?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로그 디자인: json 으로 일단 저장, nested repeated 정보 어떻게 저장?&lt;/li&gt;
  &lt;li&gt;관리: 용량이 더 커질 수 밖에 없음, 트레이닝 데이터 사이즈를 줄여보는 것도 방법, 콜드데이터는 비용 절감에 주력, 머신러닝에 들어가는 피쳐는 최대한 줄이고 차원을 축소해서 트레이닝 비용을 줄여라.&lt;/li&gt;
  &lt;li&gt;데이터 검증: 관리해야할 로그의 종류와 데이터 타입이 너무 많아짐, 테스트 기반 검증을 진행해야함, 모든 로그 데이터는 검증 로직 테스트를 통과해야 게임 업데이트가 가능 하도록 해야함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론:&lt;/strong&gt; 피쳐가 생명임, 즉 구하려고하는 것과 상관관계가 높은 데이터를 한번 찾아보자&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;협업 방식:&lt;/strong&gt; 처음에는 각 로그별로 정의 문서를 만듬, 나중에는 QA 후 valid 함, 오류나면 커뮤니케이션&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-4&quot;&gt;바닥부터 시작하는 데이터 인프라&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;(전) 레트리카, 변성윤 님&lt;/strong&gt; &lt;a href=&quot;https://zzsza.github.io/&quot;&gt;(블로그)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 대시보드 만들기&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;목표: 팀원들이 조회하고 싶은 데이터를 볼 수 있는 대쉬보드 만들기&lt;/li&gt;
  &lt;li&gt;단순 반복적인 작업을 줄이는 것&lt;/li&gt;
  &lt;li&gt;직접 구현하기 힘들면 오픈소스 툴을 사용하자! (superset)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. 데이터 파이프라인 생성&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;목표 이벤트 레벨까지 데이터를 조회할 수 있는 대시보드&lt;/li&gt;
  &lt;li&gt;문제: 이벤트로그 정리되어 있는가? 테이블 형태가 아니라면 못씀&lt;/li&gt;
  &lt;li&gt;해결: 테이블 형태로 변환&lt;/li&gt;
  &lt;li&gt;문제2: 빅쿼리비용이 너무 크게 나옴&lt;/li&gt;
  &lt;li&gt;해결: 이재광 님(NBT), 데이터를 최대한 줄여라 &amp;gt; flatten table 만들고, 목적에 맞게 데이터 구성(중복하지 않게)&lt;/li&gt;
  &lt;li&gt;기타:
    &lt;ul&gt;
      &lt;li&gt;bigquery vs dataflow&lt;/li&gt;
      &lt;li&gt;task management tool - airflow 도입&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. 음란사진 올라오면?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;처리 프로세스: user &amp;gt; scheduler &amp;gt; docker (NSFW score) &amp;gt; block / unblock&lt;/li&gt;
  &lt;li&gt;사진 말고 비디오, 움짤의 경우? webp 전환후 비디오/움짤의 일부분 만 input으로 집어 넣기&lt;/li&gt;
  &lt;li&gt;콜라보사진: 사람 얼굴 갯수가 많아졌을 때, score가 높음 &amp;gt; 사람 얼굴 갯수로 threshold (정확히 기억이 안남 ㅠㅠ)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. 팁&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;모든것을 만들 필요는 없다.&lt;/li&gt;
  &lt;li&gt;선인의 지혜를 빌리자.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-5&quot;&gt;게임회사 주니어 웹 개발자가 바라본 데이터 분석 이야기&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;넥슨 이준범 님&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 대시보드&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DB에서 데이터 불러올 때, ORM 굉장히 느리다. (몰랐는데 처음알아따…) 그런데, Pure SQL 썼는데도 느리다 &amp;gt; &lt;strong&gt;“인덱스 타고 있니??”&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DB를 full-scan 하지 않게 만들어야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. 정량적 접근 vs 정성적 접근&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;case1 신규 게임: 데이터가 없는 상황&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;모든 게임에 다 남고 있는 데이터 (접속기록, 계졍명…)&lt;/li&gt;
  &lt;li&gt;게임 유형에 맞는 공통 형식의 로그 -&amp;gt; 쉬움 -&amp;gt; 전부다 잡아낼까?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;case2 게임 특성에 따른 어뷰징&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;어떤 어뷰징 존재한가? 매크로 커뮤니티, 유저들의 신고&lt;/li&gt;
  &lt;li&gt;로그 속에서 패턴 찾아내기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;등등…(나머지 10분 뒤에 내용은 들어도 잘 모르겠어서 정리를 못했다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;그 뒤에 패널 토크에는 어마어마하신 분들이 오신거 같았는데, 못들었다… 다음엔 끝까지 들을 수 있기를~&lt;/p&gt;
</description>
        <pubDate>Sat, 21 Apr 2018 12:47:28 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/04/21/biglittledata.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/04/21/biglittledata.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Word2Vec</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-word2vec&quot;&gt;All about Word Vectors: Word2Vec&lt;/h1&gt;

&lt;hr /&gt;
&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 2 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;p&gt;자연어 처리 공부를 해보신 분이라면 한번쯤 접한 그림이 있을 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_linear-relationships.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“king” - “man” + “woman” = ?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;느낌상 “왕”에서 “남자”라는 속성을 빼주고, “여자”의 속성을 더해주면?&lt;/p&gt;

&lt;p&gt;“queen” 이 나와야할 것 같다. Word Representation은 이런 것을 가능하게 했다.&lt;/p&gt;

&lt;p&gt;이번 시간에는 &lt;strong&gt;Word2vec&lt;/strong&gt; 에 대해서 알아보려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;word2vec&quot;&gt;Word2Vec&lt;/h2&gt;

&lt;p&gt;Word2Vec은 두 가지 알고리즘이 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Skip-grams(SG)
      &lt;ul&gt;
        &lt;li&gt;target 단어를 기반으로 context 단어들을 예측한다. (position independent)&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;Continuous Bag of Words (CBOW)
      &lt;ul&gt;
        &lt;li&gt;context 단어들 집합(bag-of-words context)으로부터 target 단어를 예측한다.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_skipgram1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_skipgram2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_cbow1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_cbow2.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 몇 가지 효율적인 훈련 방법들이 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Two (moderately efficient) training methods (vs Naive Softmax)&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Hierarchical softmax&lt;/li&gt;
    &lt;li&gt;Negative sampling&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://web.stanford.edu/class/cs224n/syllabus.html&quot;&gt;CS224n Lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이번 포스팅에서는 Skip-gram 과 Negative Sampling을 메인으로 소개하겠다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;skip-gram-model-with-naive-softmax&quot;&gt;Skip-gram model with Naive Softmax&lt;/h2&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/1310.4546.pdf&quot;&gt;Distributed Representations of Words and Phrases
and their Compositionality&lt;/a&gt; (Mikolov et al. 2013)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;embedding-look-up&quot;&gt;Embedding Look up&lt;/h3&gt;

&lt;p&gt;모델 설명에 들어가기 앞서 &lt;strong&gt;Embedding Look up&lt;/strong&gt; 이란 것을 알아보자. 이 용어는 이제 여기저기서 많이 나올텐데 알아두면 좋다.&lt;/p&gt;

&lt;p&gt;우리가 하고 싶은 것은 엄청나게 차원이 큰 one-hot vector 를 고정된 작은 차원으로 넣고 싶은 것이다. 어떻게 하면 단어들을 &lt;strong&gt;2-dimension matrix&lt;/strong&gt; 로 표현 할 수 있을까?&lt;/p&gt;

&lt;p&gt;아래 그림의 예를 보자. 8차원 one-hot vector를 3차원으로 만들고 싶다. 그렇다면 $3\times 8$ 행렬을 만들어서 각 column vector 가 하나의 3차원 단어를 표현하면 2-D Matrix 가 되지 않는가? 이 Matrix를 &lt;strong&gt;Embedding Matrix&lt;/strong&gt; 라고 부르기로 하자&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup2.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그렇다면 어떻게 각 단어와 이 Embedding Matrix 를 매칭 시킬수 있을까? 여기서 &lt;strong&gt;내적&lt;/strong&gt; 을 활용하게 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider3&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup4.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup5.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그런데 자세히 보니, one-hot vector의 숫자 $1$ 이 위치한 index 가 Embedding Matrix 의 column vector 의 index 와 같다. 따라서 중복되지 않는 단어사전을 만들고, 각 단어에 대해 index를 메긴 다음, 찾고 싶은 단어를 Embedding Matrix 에서 column vector index 만 &lt;strong&gt;조회(Look up)&lt;/strong&gt; 하면 되는 것이다.&lt;/p&gt;

&lt;ul id=&quot;light-slider4&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup6.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nlp/L2_embedlookup7.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;코드 예시:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
sentence = &quot;I am going to watch Avengers Infinity War&quot;.split()
embedding_matrix = np.array([[1,2,5,1,9,10,3,4], [5,1,4,1,8,1,2,5], [7,8,1,4,1,6,2,1]])
vocab = {w: i for i, w in enumerate(sentence)}
word = &quot;I&quot;
print(embedding_matrix)
print(&quot;=&quot;*30)
print(&quot;Word:&quot;, word)
print(&quot;Index:&quot;, vocab[word])
print(&quot;Vector:&quot;, embedding_matrix[:, vocab.get(word)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;[[ 1  2  5  1  9 10  3  4]&lt;/p&gt;

  &lt;p&gt;[ 5  1  4  1  8  1  2  5]&lt;/p&gt;

  &lt;p&gt;[ 7  8  1  4  1  6  2  1]]&lt;/p&gt;

  &lt;p&gt;==============================&lt;/p&gt;

  &lt;p&gt;Word: I&lt;/p&gt;

  &lt;p&gt;Index: 0&lt;/p&gt;

  &lt;p&gt;Vector: [1 5 7]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이해가 됐으면 이제 모델로 들어가보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_model_train.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;요약&lt;/h3&gt;

&lt;p&gt;Skip-gram 모델을 한 마디로 설명하자면, 문장의 모든 단어가 한번 씩 중심단어 $c$ 가 되어, $c$ 주변 문맥 단어 $o$ 가 나올 확률을 최대화 하는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;목적&lt;/h3&gt;

&lt;p&gt;각 중심단어 $c$ 에 대해서 아래의 &lt;strong&gt;가능도/우도 (Likelihood)&lt;/strong&gt; 를 구해본다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} p(w_{t+j} | w_t; \theta) \quad \cdots\cdots \quad (1)&lt;/script&gt;

&lt;p&gt;수식을 말로 풀어보자. 각 포지션 $(\prod_{t=1}^{T})$ 의 중심단어 $c$ = $w_t$ 에 대해서, $w_t$ 가 주어졌을 때 다른 문맥단어 $o$ = $w_{t+j}$ 가 나오는 확률 $\big( p (w_{t+j} \vert w_t; \theta) \big)$ 을 가능하게 만드는 $\theta$ 를 구하는 것이다. 단 $j$ 는 윈도우 크기 $m$ 을 넘지 않으며, $0$ 이 될 수 없다.&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;Likelihood&lt;/strong&gt; 를 &lt;strong&gt;최대화&lt;/strong&gt; 하는 것이 우리의 목적이 되겠다.&lt;/p&gt;

&lt;p&gt;그러나 여기서는 우리가 좋아하는 Gradient Descent 를 사용하기 위해서 이 식을 &lt;strong&gt;Negative Log Likelihood&lt;/strong&gt; 로 변형해서 쓰기로한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min J(\theta) = -\dfrac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m,\ j \neq 0} \log p(w_{t+j} | w_t) \quad \cdots\cdots \quad (2)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$(1)$ 식과 $(2)$ 식이 왜 동등한지는 밑에 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;참고 1&lt;/span&gt;&lt;/strong&gt; 을 확인하길 바란다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 단어가 등장할 확률 $p(w_{t+j} \vert w_t)$ 는 어떻게 구할 것인가?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Softmax&lt;/strong&gt; 라는 input 값을 0과 1 사이로 만들어 주는 친근한 함수가 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(o|c) = \dfrac{\exp(u_o^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)} \quad \cdots\cdots \quad (3)&lt;/script&gt;

&lt;p&gt;따라서 모델에 있는 모든 파라미터를 $\theta \in \Bbb{R}^{2dV}$ 로 두고, $(2)$ 식을 최적화 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;왜 $\theta \in \Bbb{R}^{2dV}$ 인가?
Center Word 의 Embedding Matrix $W$ Context Words 의 Embedding Matrix $W’$ 두개를 학습 시켜야하기 때문이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;주의 )&lt;/span&gt;&lt;/strong&gt; $W’$ 는 $W$ 의 전치 행렬이 아니라 완전히 새로운 Embedding Matrix 다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;update&quot;&gt;Update&lt;/h3&gt;

&lt;p&gt;Gradient를 통해서 각 파라미터들을 업데이트 하게 된다. $(3)$ 식의 $\log$ 를 취하게 되면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f = \log \dfrac{\exp(u_o^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)}&lt;/script&gt;

&lt;p&gt;이제 $f$ 의 Gradient 를 구해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \dfrac{\partial f}{\partial V_c}
&amp;= \dfrac{\partial }{\partial V_c} \big(\log(\exp(u_o^T V_c)) - \log(\sum_{w=1}^V \exp(u_w^T V_c))\big) \\
&amp;= u_o - \dfrac{1}{\sum_{w=1}^V \exp(u_w^T V_c)}(\sum_{x=1}^V \exp(u_x^T V_c) u_x ) \\
&amp;= u_o - \sum_{x=1}^V \dfrac{\exp(u_x^T V_c)}{\sum_{w=1}^V \exp(u_w^T V_c)} u_x \\
&amp;= u_o - \sum_{x=1}^V P(x | c) u_x
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$u_o$ : observed word, output context word&lt;/li&gt;
  &lt;li&gt;$P(x\vert c)$: probs context word $x$ given center word $c$&lt;/li&gt;
  &lt;li&gt;$P(x\vert c)u_x$: Expectation of all the context words: likelihood occurance probs $\times$ context vector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;흥미로운 점: &lt;strong&gt;미분 값&lt;/strong&gt; 은 관측된 context word 벡터 $u_o$ 에서 center word $c$ 가 주어졌을 때 나올 수 있는 모든 단어의 기대치를 빼준 다는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;why-mle-is-equivalent-to-minimize-nll&quot;&gt;참고 1: Why MLE is equivalent to minimize NLL?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt; 의 정의:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta|x_1,\cdots,x_n) = f(x_1, \cdots, x_n|\theta) = \prod_{i=1}^n f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;log를 취하게 되면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log L(\theta|x_1,\cdots,x_n) =  \sum_{i=1}^n log f(x_i|\theta)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;MLE(maximum likelihood estimator)&lt;/strong&gt; 의 정의:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MLE} = \underset{\theta}{\arg \max} \sum_{i=1}^n \log f(x_i|\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{x}{\arg \max} (x) = \underset{x}{\arg \min}(-x)&lt;/script&gt;

&lt;p&gt;때문에 우리는 아래의 식을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MLE} = \underset{\theta}{\arg \max} \sum_{i=1}^n \log f(x_i|\theta) = \underset{\theta}{\arg \min} -\sum_{i=1}^n \log f(x_i|\theta)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;왜 log 로 바꾸는 것인가?
    &lt;ol&gt;
      &lt;li&gt;컴퓨터 연산시 곱하기 보다 더하기를 쓰면 &lt;strong&gt;복잡도&lt;/strong&gt; 가 훨씬 줄어들어 계산이 빠르다. ($O(n) \rightarrow O(1)$)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;언더플로우&lt;/strong&gt; 를 방지할수 있다. 언더플로우란 1보다 작은 수를 계속곱하면 0에 가까워져 컴퓨터에서 0 으로 표시되는 현상을 말한다.&lt;/li&gt;
      &lt;li&gt;자연로그함수는 &lt;strong&gt;단조증가함수(monotonic increase function)&lt;/strong&gt; 라서 대소관계가 바뀌지 않는다. 예를 들자면, $5 &amp;lt; 10 \Longleftrightarrow log(5) &amp;lt; log(10)$ 의 관계가 바뀌지 않는 다는 것. 따라서 언제든지 지수를 취해서 다시 원래의 값으로 복귀 가능.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;참고
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/&quot;&gt;why minimize negative log likelihood&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ratsgo.github.io/deep%20learning/2017/09/24/loss/&quot;&gt;(ratsgo 님) 손실함수&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음 시간에는 &lt;strong&gt;Naive Softmax&lt;/strong&gt; 로 훈련 시켰을 때의 단점과 이를 보완 해준 &lt;strong&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;Negative Sampling&lt;/span&gt;&lt;/strong&gt; 에 대해서 알아보자.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Apr 2018 10:19:06 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/20/allaboutwv2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/20/allaboutwv2.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>All about Word Vectors: Intro</title>
        <description>&lt;h1 id=&quot;all-about-word-vectors-intro&quot;&gt;All about Word Vectors: Intro&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;본 포스팅은 &lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n&lt;/a&gt; Lecture 2 강의내용을 기반으로 강의 내용 이해를 돕고자 작성 됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;natural-language-processing&quot;&gt;자연어 처리 (Natural Language Processing)&lt;/h2&gt;
&lt;p&gt;이야기를 하기 앞서서, “언어”를 살펴보자. &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%96%B8%EC%96%B4&quot;&gt;위키백과&lt;/a&gt; 에 따르면 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;언어(言語)에 대한 정의는 여러가지 시도가 있었다. 아래는 그러한 예의 일부이다.&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;사람들이 자신의 머리 속에 있는 생각을 다른 사람에게 나타내는 체계.&lt;/li&gt;
    &lt;li&gt;사물, 행동, 생각, 그리고 상태를 나타내는 체계.&lt;/li&gt;
    &lt;li&gt;사람들이 자신이 가지고 있는 생각을 다른 사람들에게 전달하는 데 사용하는 방법.&lt;/li&gt;
    &lt;li&gt;사람들 사이에 공유되는 의미들의 체계.&lt;/li&gt;
    &lt;li&gt;문법적으로 맞는 말의 집합(절대적이 아님).&lt;/li&gt;
    &lt;li&gt;언어 공동체 내에서 이해될 수 있는 말의 집합.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;위의 예시를 추려내보면 어떤 추상적인 내용을 사람들간의 공통된 약속으로 규정했다는 것이다. 기계한테 어떻게 언어를 처리하도록 알려줘야하나? &lt;strong&gt;자연어 처리&lt;/strong&gt; 는 생각보다 오래된 역사를 가지고 있었다.&lt;/p&gt;

&lt;p&gt;1950년도 이전 부터 자연어를 처리하려는 시도가 꽤 많았던 모양이다. 1954년 조지 타운 실험은 60 개 이상의 러시아어 문장을 영어로 완전 자동 번역하는 작업을 진행했다. 그는 3-5년 안으로 해결 가능하다고 주장했지만 1966 년 ALPAC 보고서에 따르면 실제로 진전이 엄청느려서 연구 자금이 크게 줄었다고 한다. 그리고 최초의 통계 기계 번역 시스템이 개발 된 1980 년대 말까지 기계 번역에 대한 연구는 거의 이루어지지 않았다고 한다. (지금은 너두나두 번역기 만들 수 있지만…)&lt;/p&gt;

&lt;p&gt;또한, 1980년대까지 대부분의 자연어 처리 시스템은 손으로 쓴 복잡한 규칙 세트를 기반으로 했다. 그러나 점차 통계 기반의 자연어 처리 기법이 복잡한 자연어를 모델링 하는데 부상했다. (Reference: &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural-language_processing&quot;&gt;NLP wikipedia&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;또한, 자연어 처리의 기본 가정을 항상 염두하고 공부해야 할 것이다. 좋은 소개글을 링크로 걸어 두었으니 참고하길 바란다.&lt;/p&gt;

&lt;p&gt;참고: ratsgo 님의 블로그 - &lt;a href=&quot;https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/10/frequency/&quot;&gt;idea of statistical semantics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;word-representation&quot;&gt;단어의 표현(Word Representation)&lt;/h2&gt;

&lt;p&gt;어떻게 하면 단어의 “의미”를 표현할 수 있을까?&lt;/p&gt;

&lt;p&gt;가장 간단한 방법은 단어를 종류별로 분류(taxonomy) 하는 것이다.&lt;/p&gt;

&lt;p&gt;영어에는 유명한 &lt;strong&gt;WordNet&lt;/strong&gt; 이라는 프로젝트가 있다. 이는 1985년부터 심리학 교수인 조지 A. 밀러가 지도하는 프린스턴 대학의 인지 과학 연구소에 의해 만들어졌고 유지되고 있다. 기본적으로 상위어(hypernyms) 밑에 동의어(synonym) 세트를 여러개 구성하는 것이다.&lt;/p&gt;

&lt;p&gt;좋긴한데 몇 가지 단점이 있다.&lt;/p&gt;

&lt;p&gt;첫째로, 단어간의 미묘한 차이, 뉘앙스(nuances)를 표현 할 수가 수 없다. 아래의 예를 보자.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from nltk.corpus import wordnet as wn
for synset in wn.synsets(&quot;adept&quot;):
    print(&quot;({})&quot;.format(synset.pos()) + &quot;, &quot;.join([l.name() for l in synset.lemmas()]))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;(n) ace, adept, champion, sensation, maven, mavin, virtuoso, genius, hotshot, star, superstar, whiz, whizz, wizard, wiz
(s) adept, expert, good, practiced, proficient, skillful, skilful&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;“I’m good at deep learning” VS “I’m expert at deep learning” 이 두 문장은 확연히 다른 느낌의 문장이다. 잘하는 것과 전문가의 차이는 사람이 느끼기엔 다르다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;둘째로, 업데이트 비용이 많이 든다. 새로운 단어가 계속 나오면 업데이트 해줘야한다, 즉 구축비용이 쎄다는 것이다.&lt;/p&gt;

&lt;p&gt;셋째로, 사람마다 주관적이기 때문에 명쾌한 기준이 없다.&lt;/p&gt;

&lt;p&gt;마지막으로, 유사도 계산이 어렵다는 점이다. 즉, 같은 상위어에 속해 있는 하위어는 비슷한 것은 알겠는데, 정량적으로 이를 계산할 방법이 없다는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;bag-of-words-representation&quot;&gt;Bag of words representation&lt;/h3&gt;

&lt;p&gt;또다른 방법으로 discrete 된 심볼로 단어를 표현했는데 이를 &lt;strong&gt;one-hot representation&lt;/strong&gt; 라고 하며, 아래와 같이 표현했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;word = [0, 0, 0, 1, 0, 0, 0]&lt;/script&gt;

&lt;p&gt;이러한 방법론을 &lt;strong&gt;Bag of words representation&lt;/strong&gt; 이라 한다. 그러나 이는 두 가지 단점이 있다.&lt;/p&gt;

&lt;p&gt;첫째로, 차원(Dimensionality)의 문제. 단어가 많아 질 수록 벡터가 엄청 길어진다.&lt;/p&gt;

&lt;p&gt;둘째로, 제한적 표현(Localist representation)의 문제. 즉, 단어의 내적의미를 포함하지 않고, 각 단어들이 독립적이다. 예를 들면, “hotel” 과 “motel” 의 유사성을 계산하려고 하면, 0 이 나올 수 밖에 없다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
motel &amp;= \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix} \\
hotel &amp;= \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;hotel \cdot motel^T = 0&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;distributional-similarity-based-representations&quot;&gt;분포 유사성 기반 표현 (Distributional similarity based representations)&lt;/h2&gt;

&lt;p&gt;연구자들은 one-hot vector 와 다른 어떤 유사도를 계산할 수 있는 벡터를 만들고 싶어했다. 따라서 유사도의 정보를 어디서 얻을 수 있을까를 찾기 시작했다. 그리고 어떤 핵심 아이디어를 생각해냈다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;어떤 단어의 “의미”는 그 단어 근처에 자주 출현하는 단어로부터 얻을 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nlp/L2_context.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://web.stanford.edu/class/cs224n/syllabus.html&quot;&gt;CS224n Lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그들은 주변 단어의 정보로 어떤 단어의 의미를 규정하는 시도를 하였고, 이는 modern statistical NLP 에서 많은 각광을 받기 시작했다. 그리고 어떤 단어 $w$ 에 대해서 주변에 나타나는 단어의 집합을 &lt;strong&gt;맥락/문맥(context)&lt;/strong&gt; 이라고 했다.&lt;/p&gt;

&lt;h3 id=&quot;word-vectors&quot;&gt;Word Vectors&lt;/h3&gt;

&lt;p&gt;이전에 0과 1로 채워진 one-hot vector 와 달리 문맥에서 비슷한 단어들을 잘 예측 될 수 있게 단어 타입 별로 촘촘한 벡터(dense vector)를 만든다. 핵심 아이디어는 아래와 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Idea:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;We have a large corpus of text&lt;/li&gt;
    &lt;li&gt;Every word in a fixed vocabulary is represented by a vector&lt;/li&gt;
    &lt;li&gt;Go through each &lt;strong&gt;position&lt;/strong&gt; $t$ in the text, which has a &lt;strong&gt;center word&lt;/strong&gt; $c$ and &lt;strong&gt;context (“outside”) words&lt;/strong&gt; $o$&lt;/li&gt;
    &lt;li&gt;Use the similarity of the word vectors for $c$ and $o$ to calculate the probability of $o$ given $c$ (or vice versa)&lt;/li&gt;
    &lt;li&gt;Keep adjusting the word vectors to maximize this probability&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://web.stanford.edu/class/cs224n/syllabus.html&quot;&gt;CS224n Lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;요약하면 방대한 텍스트 데이터를 기반으로, 중심단어 $c$ 가 주어졌을 때, 그 주변단어 $o$ 가 나올 확률 분포를 최대화 하는 것을 구하는 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Word vectors 는 때때로 Word Embeddings, Word Representation 이라고 불린다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이렇게 해서 나온 알고리즘이 &lt;span style=&quot;color: #e87d7d&quot;&gt;“Word2Vec”&lt;/span&gt; 이며, 여기서 잠깐 끊고 다음 글에서 소개하도록 한다.&lt;/p&gt;
</description>
        <pubDate>Thu, 19 Apr 2018 16:41:36 +0900</pubDate>
        <link>http://simonjisu.github.io/nlp/2018/04/19/allaboutwv1.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/nlp/2018/04/19/allaboutwv1.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Bidirectional LSTM + self Attention Model</title>
        <description>&lt;h1 id=&quot;naver-sentiment-movie-corpus-classification&quot;&gt;Naver Sentiment Movie Corpus Classification&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;네이버 영화 감성분류 with Bidirectional LSTM + Self Attention&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;목표&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;영화 리뷰를 통해 긍정인지 부정인지 분류하는 문제 (Many-to-One)&lt;/li&gt;
  &lt;li&gt;사용한 모델: Bidirectional LSTM with Self Attention Model&lt;/li&gt;
  &lt;li&gt;이번 글은 논문과 제가 분석한 모델의 중요 요소를 곁들여 쓴 글입니다.&lt;/li&gt;
  &lt;li&gt;GitHub Code Link: &lt;a href=&quot;https://github.com/simonjisu/nsmc_study&quot;&gt;nsmc_study&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference Paper: &lt;a href=&quot;https://arxiv.org/pdf/1703.03130.pdf&quot;&gt;A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;모델 핵심 부분 설명&lt;/h2&gt;

&lt;p&gt;그림과 수식을 함께 보면 이해하기 쉽다&lt;/p&gt;

&lt;p&gt;어떤 $n$ 개의 토근으로 이루어진 하나의 문장이 있다고 생각해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S = (w_1, w_2, \cdots, w_n)\qquad\qquad (1)&lt;/script&gt;

&lt;p&gt;여기서 $w_i$ 는 one-hot 인코딩된 단어가 아닌, $d$ 차원에 임베딩된 문장에서 $i$ 번째 단어다.&lt;/p&gt;

&lt;p&gt;따라서 $S$ 는 단어 벡터들을 concat 한 $n \times d$ 형태를 가지는 매트릭스다.&lt;/p&gt;

&lt;p&gt;문장 $S$ 는 각기 다른 문장과는 독립적이다. (하나의 문장이 하나의 평점과 세트로 생각하면 된다.) 하나의 문장에서 단어들 간의 관계를 알기 위해서 우리는 bidirectional LSTM 으로 하나의 문장을 처리하게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\overrightarrow{h_t} &amp;= \overrightarrow{LSTM}(w_t, \overrightarrow{h_{t-1}})\qquad\qquad (2) \\
\overleftarrow{h_t} &amp;= \overleftarrow{LSTM}(w_t, \overleftarrow{h_{t-1}})\qquad\qquad (3)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;그후 우리는 각각의 $\overrightarrow{h_t}$ 과 $\overleftarrow{h_t}$ 를 concatenate 하여 하나의 히든 state $h_t$ 를 얻게 된다. 각 unidirectional LSTM(한 쪽 방향 LSTM)의 히든 유닛 크기를 $u$ 라고 하자. 조금 간단하게 표현하기 위해서 모든 $n$ 개의 $h_t$ 들을 $H$ 라고 하며, $n \times 2u$ 의 크기를 가진다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H = (h_1, h_2, \cdots, h_n) \qquad\qquad (4)&lt;/script&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention0.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention4.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우리의 목적은 길이가 변화하는 문장을 어떤 &lt;strong&gt;고정된 크기&lt;/strong&gt; 의 임베딩으로 인코딩 하는 것이다. 이 목적을 달성하기 위해서 $H$ 와 attention 매커니즘이 요구되는 일종의 선형결합을 선택하게 된다. 즉, 아래와 같은 식과 $H$ 를 토대로, 어떤 벡터 $a$ 를 얻게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = softmax(w_{s2} \tanh (W_{s1}H^T)) \qquad\qquad (5)&lt;/script&gt;

&lt;p&gt;여기서 $W_{s1}$ 는 $d_a \times 2u$ 형태를 가진 매트릭스, 그리고 $w_{s2}$ 는 $d_a$ 사이즈를 가진 벡터다. $d_a$ 는 하이퍼파라미터(hyperparameter)로 우리가 정할 수 잇는 변수다. $H$ 의 크기도 $n \times 2u$ 이기 때문에, 벡터 $a$ 는 $n$ 의 크기를 가진다. 또한 $softmax()$ 함수는 모든 weight들의 합을 1로 만들어 준다.&lt;/p&gt;

&lt;p&gt;그후 우리는 LSTM 의 히든상태들의 집합인 $H$ 를 주어진 $a$ 로 곱해서 한 문장을 임베딩한 벡터 $m$ 을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이 벡터 $m$ 은 학습시 한 문장에서 어떤 단어를 중심적으로 보았는지 알 수 있다. 예를 들어 어떤 연관된 단어나 구문 등등.&lt;/p&gt;

&lt;p&gt;문장과 단어의 관계로 추가 설명하자면 아래와 같다.&lt;/p&gt;

&lt;p&gt;각 단어를 input으로 받은 hidden 상태의 노드들은 단어를 통과해서 각 단어의 숨겨진 특성을 대표하고 있다. 학습 시 Task 에 따라 다르겠지만, 분류라고 가정한다면 분류에 도움이 되는 히든 상태는 높은 값을 가지게 될 것이며, 이를 어떤 선형 변환 과정을 거쳐 softmax 취한다는 것은 한 문장에서 분류에 도움이 된 근거 단어 혹은 중요 단어의 확률을 구한다는 것이 된다. (그래서 attention 이라고 하는 것 같다.) 따라서 이는 한 문장에서 &lt;strong&gt;의미적인(semantic)&lt;/strong&gt; 부분을 나타내고 있다고 할 수 있다.&lt;/p&gt;

&lt;p&gt;이 확률 $a$ 를 기존의 hidden 상태와 곱해서 의미부분을 조금더 강조하게 되는 벡터 $m$ 을 구했다고 보면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider2&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention5.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention6.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;하지만 한 문장 내에서 중요한 부분 혹은 의미가 있는 부분은 여러군데 일 수가 있다. (여러 의미가 하나의 문장을 구성한다.) 특히 긴 문장일 수록 그렇다. 예를 들어 “아이언맨과 캡틴아메리카” 면 “과”로 이어진, “아이언맨”, “캡틴아메리카” 두 단어는 중요한 의미가 있는 단어 일 수 있다. 따라서 한 문장에서 의미가 있는 부분을 나타내려면 $m$ 이란 벡터를 여러 번 수행해서 문장의 다른 부분까지 커버해야 한다. 이는 우리가 &lt;strong&gt;attention&lt;/strong&gt; 을 &lt;strong&gt;여러번(hops)&lt;/strong&gt; 하게 되는 이유다.&lt;/p&gt;

&lt;p&gt;따라서, 문장에서 우리가 정하는 어떤 수 $r$ 번의 다른 부분을 추출 해낸다고 하면, 기존의 $w_{s2}$ 는 $r \times d_a$ 크기를 가진 $W_{s2}$ 라는 매트릭스로 확장된다. 이에따라 기존에 $a$ 벡터도 $r$ 번을 수행해 concatenate 한 $r \times n$ 크기의 매트릭스 $A$ 가 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A=softmax(W_{s2}tanh(W_{s1}H^T))  \qquad\qquad (6)&lt;/script&gt;

&lt;p&gt;여기서 $softmax()$ 는 input $W_{s2}tanh(W_{s1}H^T)$ 의 2번째 차원을 기준으로 softmax 하게 된다. (즉, 각 row 별로 softmax 해줌)&lt;/p&gt;

&lt;p&gt;사실 $(6)$ 번 수식은 bias 가 없는 2-Layers MLP 로 간주할 수도 있다.&lt;/p&gt;

&lt;p&gt;위에 식에 따라 임베딩된 벡터 $m$ 도 $r \times 2u$ 크기의 매트릭스 $M$ 로 확장된다. 가중치를 담은 매트릭스 $A(r \times n)$ 와 LSTM 의 히든 상태들인 $H(n \times 2u)$를 곱해서 새로운 임베딩 매트릭스 $M$ 을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M=AH  \qquad\qquad (7)&lt;/script&gt;

&lt;p&gt;마지막으로 $M$을 Fully Connected MLP 에 넣어서 하고 싶은 분류를 하면 된다.&lt;/p&gt;

&lt;ul id=&quot;light-slider3&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention7.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention8.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/nsmc/Self_Attention9.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;penalization-term&quot;&gt;Penalization Term&lt;/h3&gt;

&lt;p&gt;임베딩된 매트릭스 $M$ 은 $r$ hops 동안 계속해서 같은 유사도 벡터 $a$ 를 곱하게 되면 &lt;strong&gt;중복 문제(redundancy problems)&lt;/strong&gt; 가 생길 수 있다. 즉, 같은 단어 혹은 구문만 계속해서 attention 하게 되는 문제다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/penal.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;그림: 왼쪽(a)은 패널티를 안준 것, 오른쪽(b) 는 준것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, $r$ hops 동안 weight 벡터들의 합을 다양성을 높히는 일종의 패널티를 줘야한다.&lt;/p&gt;

&lt;p&gt;제일 좋은 방법은 $r$ hops 안에 있는 아무 두 벡터 간의 &lt;strong&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0&quot;&gt;쿨백-라이블러 발산 (Kullback–Leibler divergence)&lt;/a&gt;&lt;/strong&gt; 함수를 쓰는 것이다. 매트릭스 $A$ 의 각각의 행(row) 벡터들이 하나의 의미(semantic)를 가지는 단어 혹은 구문이 될 확률분포이기 때문에, 다양한 분포에서 나오는 것은 우리의 목적이 된다. (문장은 여러 단어/구문으로 구성되어 있기때문) 그러므로 KL divergence 값을 &lt;strong&gt;최대&lt;/strong&gt; 로 만들면 중복 문제는 해결된다.&lt;/p&gt;

&lt;p&gt;그러나 논문에서는 위와 같은 경우에 불안정(unstable) 한다는 것을 알아냈다. 논문 저자들은 어림짐작해 보았을 때, KL divergence 를 최대화 할때(보통의 경우 KLD를 최소화 하는 것을 한다.), 매트릭스 $A$ 구하는 단계에서 softmax 시 많은 값들이 0 이거나 아주 작은 값이라서 불안정한 학습을 야기했을 가능성이 있다는 것이다.&lt;/p&gt;

&lt;p&gt;따라서, 논문에서는 매트릭스의 &lt;strong&gt;&lt;a href=&quot;http://mathworld.wolfram.com/FrobeniusNorm.html&quot;&gt;Frobenius norm&lt;/a&gt;&lt;/strong&gt; 을 쓰게 되는데 아래와 같다. ($Norm_2$와 비슷해 보이지만 다르다)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P ={ {\|AA^T - I\|}_F}^2&lt;/script&gt;

&lt;p&gt;이 패널티 값과 기존의 Loss 와 같이 최소화 하는 방향으로 간다. 이 패널티의 뜻은 무엇일까?&lt;/p&gt;

&lt;p&gt;두 개의 다른 유사도 벡터의 합 $a^{i}$ 과 $a^{j}$ 를 생각해보자. Softmax 로 인해서 모든 $a$ 값들의 합은 1이 될 것이다. 따라서 이들을 일종의 이산 확률분포 (discrete probability distribution)에서 나오는 확률질량 함수로 간주할 수 있다.&lt;/p&gt;

&lt;p&gt;매트릭스 $AA^T$ 중, 모든 비대각 $a_{ij}\ (i \neq j)$ 원소에 대해서, 원소의 곱(elementary product)은 아래 두개의 분포를 가지고 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
0&lt; a_{ij} = \sum_{k=1}^{n} a_k^i a_k^j &lt;1 %]]&gt;&lt;/script&gt;

&lt;p&gt;여기서 $a_k^i, a_k^j$ 는 각각 $a^i, a^j$ 의 k 번째 원소다. 제일 극단적인 경우를 생각해보면, $a^i$ 와 $a^j$ 가 일치하지 않다면 (혹은 다른 분포를 나타내고 있다면) 0 이 되고, 완전이 일치해서 같은 단어 혹은 구문을 이야기 하고 있다면 (혹은 같은 분포를 나타내고 있다면) 1 에서 최대값을 가지게 될 것이다.&lt;/p&gt;

&lt;p&gt;따라서, $AA^T$ 의 대각 행렬(같은 단어 혹은 구문)을 대략 1 이 되게 강제한다. $I$ (Identity) 매트릭스를 빼줌으로써 달성하는데, 이는 자기 자신을 제외한 각기 다른 $a^i$ 간 원소들의 합인 $a_{ij}$ 들이 0 으로 최소화되게 만들어 버린다. 즉, 최대한 $a^i$ 간의 분포가 일치하지 않게 만드려고 노력하는 것이다. 이렇게 함으로써 $r$ 번의 hops 마다 각각 다른 단어에 집중하게 만드는 효과를 낼 수 있어서, 중복문제를 해결 할 수가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;네이버 영화 리뷰 테스트 결과 및 시각화&lt;/h2&gt;
&lt;p&gt;총 150000 개의 Train Set과 50000 개의 Test Set 으로 진행했고, 모델에서는 hyperparameter가 많기 때문에 몇 가지 실험을 진행 했다.&lt;/p&gt;

&lt;p&gt;간단한 실험을 위해서 사전에 단어들을 word2vec 으로 학습시키지 않고, mecab 으로 tokenizing 만해서 임베딩 시켰다. (실험을 안해봐서 사실 크게 상관있나 모르겠다. 나중에 여러가지로 실험해볼 예정)&lt;/p&gt;

&lt;p&gt;내가 주로 건드린건 LSTM 에서의 &lt;strong&gt;hidden layer의 갯수&lt;/strong&gt; 와 hops 인 &lt;strong&gt;$r$&lt;/strong&gt; 을 바꾸어 보았다.&lt;/p&gt;

&lt;h3 id=&quot;model-1-1--hidden-layer--5-hops&quot;&gt;model 1: 1 개의 Hidden Layer 와 5번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_1.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-2-1--hidden-layer--20-hops&quot;&gt;model 2: 1 개의 Hidden Layer 와 20번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hops 가 많아지면 긍정/부정을 판단하게 되는 근거도 많아지고, 모델의 정확도도 향상되는 것을 2번에서 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;model-3-3--hidden-layer--5-hops&quot;&gt;model 3: 3 개의 Hidden Layer 와 5번의 hops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nsmc/model_3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3번째 모델은 조금 이상하다고 느껴진 것이 있다. 그림을 보면 기계가 문장의 앞뒤만 보고 리뷰가 긍정인지 부정인지 판단했다는 것이다. 그림만 보면 과최적화된 느낌? 정확히 각 층의 layer 값을 보지는 못했지만, 층이 깊어 질 수록 기계가 이전 단계의 layer 에서 추출한 특징들로 학습해서 긍부정을 판단 했을 가능성이 있다. 점수는 높게 나왔으나 사람이 판단하기에는 부적절한 모델&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;향후 해볼 수 있는 과제들&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;전처리 단계에서 임베딩시 다양한 임베딩을 해볼 수 있을 것 같다. 예를 들어 word2vec으로 미리 선학습 후에 만든다던지, 아니면 N-hot 인코딩 (단어 원형 + 품사 + 어미) 등등 시도해볼 수 있는 것은 많다.&lt;/li&gt;
  &lt;li&gt;LSTM Cell 로 구현&lt;/li&gt;
  &lt;li&gt;이와 연관은 좀 덜하지만, CNN으로 분류하는 것과 비교해 성능이 더 잘나올지? &lt;strong&gt;김윤&lt;/strong&gt; 님의 논문 참고 : &lt;a href=&quot;http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf&quot;&gt;링크 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;공부에 도움 주신 분들 및 공부에 도움 되었던 싸이트:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;김성동님: https://github.com/DSKSD&lt;/li&gt;
  &lt;li&gt;같은 논문을 Tensorflow로 구현하신 flrngel님: https://github.com/flrngel/Self-Attentive-tensorflow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;감사합니다.&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Apr 2018 01:01:13 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2018/04/04/nsmcbidreclstmselfattn.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2018/04/04/nsmcbidreclstmselfattn.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>Naver AI Colloquium 2018</title>
        <description>&lt;h1 id=&quot;naver-ai-colloquium-2018--&quot;&gt;Naver AI Colloquium 2018 참석 후기&lt;/h1&gt;
&lt;p&gt;&lt;del&gt;+ 클로바스피커 후기&lt;/del&gt;&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;개요&lt;/h2&gt;

&lt;ul id=&quot;light-slider1&quot;&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/logo.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment1.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment2.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/assets/ML/naveraicol/ment3.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;출처: &lt;a href=&quot;http://naveraiconf.naver.com/&quot;&gt;네이버 AI 콜로키움&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;운 좋게 기회가 되서 &lt;a href=&quot;http://naveraiconf.naver.com/&quot;&gt;&lt;strong&gt;Naver AI Colloquium 2018&lt;/strong&gt;&lt;/a&gt; 에 다녀왔다.
싸이트에 접속해서 프로그램 표를 자세히 보면 알겠지만, 이번 콜로키움에서는 크게 4가지 주제를 다뤘다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;언어 분야(Search &amp;amp; Natural Language)&lt;/li&gt;
  &lt;li&gt;비전 분야(Computer Vision, Mobility&amp;amp;Location Intelligence)&lt;/li&gt;
  &lt;li&gt;추천 분야(Recommendation)&lt;/li&gt;
  &lt;li&gt;데이터 엔지니어 분야(AI Algorithm, System&amp;amp;Platform)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;나는 언어 분야, 즉 자연어을 다루는 쪽에 관심이 많아서 Track A 만 거의 듣고, 추천 분야 하나 정도 들었다. 각 강의에서 다루는 내용은 차후 하나씩 정리해서 올릴 예정이다.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/naveraicol/professorsungkim.JPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;“모두의 딥러닝”으로 유명하신 Sung Kim 교수님!! (문제가 되면 사진 내리겠습니다~ 댓글 달아주세요)&lt;/p&gt;

&lt;p&gt;인상 깊었던 세션과 그 이유를 몇개 꼽자면, &lt;del&gt;어쩌다보니 다 네이버 직원분들이 발표하신거네ㅎㅎ&lt;/del&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Semantic Matching Model - 김선훈 (NAVER)
시멘틱 매칭의 필요성:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;키워드(단어기반)&lt;/strong&gt; 매칭보다는 &lt;strong&gt;시멘틱(의미기반)&lt;/strong&gt; 매칭이 다양한 표현과 오타를 커버할 수 있는 가능성이 높다.&lt;/li&gt;
      &lt;li&gt;Sementic Gap: 인간이해와 기계이해의 차이, 이것을 줄이는게 큰 과제&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Speed Reading - 서민준 (NAVER)
    &lt;ul&gt;
      &lt;li&gt;Skim-RNN: “속독”에서 나온 아이디어, 중요하지 않은 단어는 적게 업데이트!&lt;/li&gt;
      &lt;li&gt;Big RNN 과 Small RNN 의 결정 짓는 Decision Function&lt;/li&gt;
      &lt;li&gt;Layer를 쌓으면 중요한 정보를 캐치 (마치 글을 두번째 읽을 때는 주요 단어만 보게 되는 것과 같음)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hybrid Natural Language Understanding 모델 - 김경덕 (NAVER)
    &lt;ul&gt;
      &lt;li&gt;문제 정의의 중요성: 잘해야 문제를 해결하기 쉽고 명확하다.&lt;/li&gt;
      &lt;li&gt;팬턴 기반 검색 NLU + 데이터 통계 기반 NLU, 뭐든지 하나만 고르는 것은 아니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;자기학습 기반 챗봇(발표세션은 아님)
    &lt;ul&gt;
      &lt;li&gt;챗봇의 전체 과정:
  Query $\rightarrow$ 언어적 특징 추출 $\rightarrow$ 쿼리 분류기(대화여부 및 도메인 인지) $\rightarrow$ 여러 모델로 부터 답변 생성 $\rightarrow$ Answer&lt;/li&gt;
      &lt;li&gt;N-hot representation: 토큰원형 + 품사태깅 + 어미&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 정도인 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;클로바 스피커(프렌즈) 후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/naveraicol/speaker.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;어쩌다 운좋게 경품에 당첨 되서 받았다. ㅎㅎㅎ 감사합니다.
이놈…생각보다 귀엽다. 아직까지 일본의 프렌즈보다 기능이 덜 있는 것 같다. 라인으로 메세지 보내는 기능 시도해보았는데, 안되드라…&lt;/p&gt;

&lt;p&gt;영상링크: &lt;a href=&quot;https://youtu.be/lK-9yDoHsZ8&quot;&gt;【公式】Clova Friendsができること &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;아무튼 아직 개선할 사항이 많다. 예를 들어, “레미제라블 Do you hear the people sing? 노래틀어줘”라고 말하면, 말씀하신 사항을 찾지 못했다고 대답한다.
어떤 세션에서 들었던것 같은데, 내 생각에는 레미제라블, 노래틀어줘는 노래틀어주는 분야로 의도로 분류되고, 나머지 영어는 번역하는 의도로 분류된 것 같다. (스피커에 말한 내용을 볼 수 있는데, 음성인식을 진짜 제대로 잘 된다. 괜히 1위라고 말한게 아닌듯 ㅋㅋ)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;내년에 또 하게되면 참가하고싶다~&lt;/p&gt;

&lt;p&gt;관심 있었던 세션들을 정리하면 바로 올리겠다.&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Mar 2018 20:14:21 +0900</pubDate>
        <link>http://simonjisu.github.io/naverai2018/2018/03/30/naveraicolloquium2018.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/naverai2018/2018/03/30/naveraicolloquium2018.html</guid>
        
        
        <category>NaverAI2018</category>
        
      </item>
    
  </channel>
</rss>
