<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soopace</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>https://simonjisu.github.io/</link>
    <atom:link href="https://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 23 Mar 2020 11:19:28 +0900</pubDate>
    <lastBuildDate>Mon, 23 Mar 2020 11:19:28 +0900</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Probability Density and Transformation</title>
        <description>&lt;h1 id=&quot;probability-density-function&quot;&gt;Probability Density Function&lt;/h1&gt;

&lt;p&gt;실수(real-valued) 확률변수 $X$가 $(x, x+ \delta x)$구간의 값을 가지고, 해당 구간의 확률이 $f_X(x)\delta x$($\delta x \rightarrow 0$일 경우)로 정의 된다면, $f_X(x)$를 $X$의 &lt;strong&gt;확률 밀도함수(probability density function)&lt;/strong&gt;라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_X(X \in (x, x+\delta x)) = \int_{x}^{x+\delta x} f_X(x) dx&lt;/script&gt;

&lt;p&gt;확률 밀도함수는 다음 두 조건을 만족해야한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$f(x) \geq 0$&lt;/li&gt;
  &lt;li&gt;$\int_{-\infty}^{\infty} f(x) dx = 1$&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;transformation-of-random-variable&quot;&gt;Transformation of Random Variable&lt;/h1&gt;

&lt;p&gt;확률변수의 &lt;strong&gt;변환(Transformation)&lt;/strong&gt;이란 기존의 확률변수$X$를 새로운 확률변수$Y$로 변환 하는 것이다. 비선형 변환시 단순 함수와 다르게 변환되는데 어떻게 변화하는지 살펴보기로 한다. 여기서 이야기하는 변환은 다음 조건을 만족해야한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;변환 함수 $g: Y \rightarrow X$는 전단사(bijective) 혹은 일대일 대응(one-to-one)이어야 한다. 일대일 대응이란, 모든 정의역$Y$에 존재하는 원소 $y$는 치역$X$에 대응하는 값이 유일하다(unique). 이를 다른 말로 하면, “$g$ 함수는 역을 가질 수 있다(invertible)”라고 한다 $g^{-1}: X \rightarrow Y$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;예를 들어, 확률변수 $X$에 해당하는 확률 밀도함수는 $f_X(x)$, 확률변수 $Y$에 해당하는 확률 밀도함수는 $f_Y(y)$인 경우에서 $x=g(y)$인 비선형 변환이 있다고 가정해본다. 그렇다면 두 확률 밀도함수는 정말 다른 것일까? 확률 밀도함수의 최댓값도 변수의 선택에 종속되어 변화했을까($\hat{x}=g(\hat{y})$의 관계를 유지하는지 아니면 변화했는지)? 이를 알아보기 위해 변화된 확률 분포를 분해해본다.&lt;/p&gt;

&lt;p&gt;확률변수 $X$의 가측 부분집합(measurable subset)을 $\mathcal{X}_0 \subset \mathcal{X}$, 확률변수 $Y$의 정의역에 해당하는 가측 부분집합을 $\mathcal{Y}_0 \subset \mathcal{Y}$라고 정의한다. 변환식 $x = g(y)$을 $y$에 관해 미분하면, $dx = g’(y)dy$를 얻을 수 있으며, $X$의 확률 분포$p_X(x)$는 다음과 같이 변환할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;여기서 가측 부분집합은 쉽게 얘기해서 정의된 범위라고 생각할 수 있다&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
p_X(x) = \int_{\mathcal{X}_0} f_X(x) dx &amp;= \int_{\mathcal{Y}_0} f_X(x) \vert \dfrac{dx}{dy} \vert dy \\
&amp;= \int_{\mathcal{Y}_0} f_X(g(y)) \vert g'(y) \vert dy \\
&amp;= \int_{\mathcal{Y}_0} f_Y(y) dy \\
&amp;= p_Y(y)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위 수식에서 확률변수$Y$에 대한 확률 밀도함수는 $f_Y(y) = f_X(g(y)) \vert g’(y) \vert$로 변화했는데, 이는 $X$에 대한 확률 밀도함수에 Jacobian Factor $\dfrac{dx}{dy}= g’(y)$를 곱한 값이 된다. 즉, Jacobian Factor로 인해서 확률 밀도함수$f_Y(y)$의 값이 $f_X(g(y))$로부터 약간 변화한다는 것을 의미한다.&lt;/p&gt;

&lt;h2 id=&quot;example-of-transformation&quot;&gt;Example of Transformation&lt;/h2&gt;

&lt;p&gt;과연 다른지 $x = g(y) = \ln(y) - \ln(1-y) + 5$ 라는 변환으로 $\hat{x}=g(\hat{y})$ 관계($y$의 최댓값 위치가 변환된 최댓값을 결정)를 유지하고 있는지 아닌지 살펴본다. $g$의 역함수는 $g^{-1}(x) = \dfrac{1}{1 + \exp(-x + 5)}$인 sigmoid 함수가 된다. 즉, $y$의 정의역은 0과 1 사이의 실수, $x$는 $-\infty$와 $\infty$의 실수 값을 취할 수 있다. 또한 함수 $g$의 미분값은 $\dfrac{dx}{dy}=\dfrac{1}{y - y^2}$ 다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;x = g(y)&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;g_inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;y = g^{-1}(x)&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dxdy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;확률변수 $X$가 평균이 6, 표준편차가 1인 가우시안 분포를 따른다고 가정하고($X \sim \mathcal{N}(6, 1)$), 5만개의 샘플을 추출하고, 역함수를 이용해 샘플링된 확률변수 $Y$의 값을 구한다. 샘플링된 분포 이외에 실제 분포를 그리기 위한 작업도 진행한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;88&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sampled_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sampled_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampled_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 5만개의 균일된 간격인 x 값
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;px&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;py_real&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dxdy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;관련 분포를 그리면 다음 그림과 같다(관련 코드는 &lt;a href=&quot;https://gist.github.com/simonjisu/57c6e2b89b4c9457541809ec5b5f51c9&quot;&gt;링크&lt;/a&gt;에서 확인 할 수 있다). 각 선의 의미는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#d40000&quot;&gt;빨강&lt;/span&gt;: 확률변수 $X$의 실제 분포(0과 0.5 사이로 rescale함)&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:#002ed4&quot;&gt;파랑&lt;/span&gt;: 확률변수 $Y$의 실제 분포(0과 1 사이로 rescale함)&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e3a205&quot;&gt;노랑&lt;/span&gt;: $y=g^{-1}(x)$로 변환된 확률변수 $X$의 분포(0과 1 사이로 rescale함)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;또한, 오른쪽 밑의 파란 막대 그래프가 샘플링된 확률변수 $X$의 분포, 왼쪽 파란 막대 그래프 부분이 $y=g^{-1}(x)$로 변환된 확률변수 $Y$의 분포다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1c83fpP9BQb7DjtK0EmTUKSPcdt6gLCA5&quot; alt=&quot;&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;이 그래프에서 명백한 것은 $X$분포(&lt;span style=&quot;color:#d40000&quot;&gt;빨강&lt;/span&gt;)의 최대값 $\hat{x}$과 실제 $Y$분포(&lt;span style=&quot;color:#002ed4&quot;&gt;파랑&lt;/span&gt;)의 최대값 $\hat{y}$은 단순 $x=g(y)$(혹은 $y=g^{-1}(x)$)의 관계를 가지지 않는다. 즉, $X$분포와 $Y$분포는 서로 다른 특성을 가지며, 확률 밀도가 변수의 변환으로 인해서 바뀌었다고 할수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;determinent-of-jacobian&quot;&gt;Determinent of Jacobian&lt;/h1&gt;

&lt;p&gt;위에서 이야기한 $\vert \dfrac{dx}{dy} \vert$인 Jacobian Factor 란 무엇일까? 야코비 행렬식(Jacobian Determinant)을 기하학적으로 풀면 좌표계가 변환할 때  변환된 면적의 너비로 풀이할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
det \Big( \begin{bmatrix} 3 &amp; 1 \\ 0 &amp; 2 \end{bmatrix} \Big) = 6 %]]&gt;&lt;/script&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1o-CffunWblVIBmwU0xJSROesmlrEMTBf&quot; alt=&quot;&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;위 행렬식값인 6의 의미는 단위 벡터 기저(basis)에서 새로운 기저로 변환했을 때 면적이 1(노란색 부분)에서 6(초록색 부분)만큼 바뀐 다는 뜻이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;단위 벡터 기저: $ \Big( \begin{bmatrix} 1 \ 0 \end{bmatrix}, \begin{bmatrix} 0 \ 1 \end{bmatrix} \Big)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 확률 변수의 변환 예제에서 작은 구간의 확률값 $f_X(x) dx$에 해당하는 면적에  $\dfrac{dx}{dy}=g’(y)=\dfrac{1}{y-y^2}$값을 곱한 만큼 바뀐다는 뜻이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-web-sol-2009-09-08.pdf&quot;&gt;prml-solution 1.4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://yousuketakada.github.io/prml_errata/prml_errata.pdf&quot;&gt;prml-errata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 22 Mar 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/math/2020/03/22/probdensity.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/math/2020/03/22/probdensity.html</guid>
        
        
        <category>math</category>
        
      </item>
    
      <item>
        <title>[VISION] Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1312.6034&quot;&gt;Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;0-abstract&quot;&gt;0. Abstract&lt;/h1&gt;

&lt;p&gt;이 논문에서는 입력 이미지에 대한 경사(gradient)를 구함으로써 두 가지 이미지 분류 모델의 시각화 기술을 중점적으로 서술했다. 첫째는 class score(최종 분류층 점수)를 극대화하여, ConvNet에서 포착된 클래스의 개념을 시각화하는 이미지를 생성한다. 둘째는 이미지와 이에 해당하는 클래스의 saliency maps(특징 지도)를 생성해내는 것이다. Saliency maps로 weakly supervised image segmentation에 적용했고, deconvolutional network와 비교도 해보았다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;이 논문의 기여는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;입력 이미지의 수치적 최적화를 통해 CNN 모델에서 이해가능한 수준의 시각화된 이미지를 얻을 수 있다.&lt;/li&gt;
  &lt;li&gt;ConvNet을 통한 분류에서 단일 역전파(back-propagation) 경로를 사용하여 주어진 이미지(이미지별 class saliency map)에서 주어진 클래스의 공간적 지지점(spatial support)을 계산하는 방법을 제안한다.&lt;/li&gt;
  &lt;li&gt;gradient 기반의 시각화 방법으로 deconvolutional network의 재구성 과정을 일반화했다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2-class-model-visualisation&quot;&gt;2. Class Model Visualisation&lt;/h1&gt;

&lt;p&gt;$S_c(I)$가 주어진 이미지($I$)의 클래스($c$) 점수(score)라고 정의한다. 그러면 다음 수식과 같이 점수$S_c$를 최대화 하는 L2 정규화된 이미지를 찾을 수 있을 것이다($\lambda$는 정규화 하이퍼파라미터).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg \underset{I}{\max} S_c(I) - \lambda \Vert I\Vert^2_2&lt;/script&gt;

&lt;p&gt;지역적으로 최적화된 이미지($I$)는 역전파(back-propagation)방법으로 찾을 수 있다. 이는 ConvNet의 훈련 과정중 역전파에서 각 층의 가중치를 최적화 할 때와 연관이 있다. 여기서 다른 점이라면 입력 이미지($I$)에 대한 최적화를 수행하는 것이고, 모델 가중치(weights)는 고정시킨다. 전체 과정은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;먼저 zero image $I$를 만든다.&lt;/li&gt;
  &lt;li&gt;$I$를 네트워크에 입력으로 해당 타겟에 해당하는 출력 스코어$S_c(I)$를 구한다.&lt;/li&gt;
  &lt;li&gt;출력 스코어$S_c(I)$에 정규화 계수 $\lambda$와 입력 이미지 $I$의 L2 Norm을 곱한 값을 빼주면 최종 손실값$L$이 된다.&lt;/li&gt;
  &lt;li&gt;손실값을 입력 이미지 $I$에 대해서 미분하여 업데이트 한다&lt;/li&gt;
  &lt;li&gt;1~4 과정을 반복한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-image-specific-class-saliency-visualization&quot;&gt;3. Image-Specific class Saliency Visualization&lt;/h1&gt;

&lt;p&gt;이번 파트에서는 ConvNet가 주어진 이미지와 클래스에 대한 공간적 지지점(spatial support)을 찾는 과정을 설명한다. 주어진 이미지를 $I_0$, 타겟 클래스를 $c$ 그리고 ConvNet에 이미지를 입력하여 얻은 점수 벡터$S_c(I)$ 라고 해보자. 이제 점수 벡터 $S_c(I_0)$에 근거하여 입력 이미지 $I_0$에 픽셀들의 순위를 정할 것이다.&lt;/p&gt;

&lt;p&gt;먼저 제일 간단한 예제인 선형모델로 시작해보면 다음과 같다(이미지 $I$는 벡터화 시켰다).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_c(I) = w_c^TI+b_c&lt;/script&gt;

&lt;p&gt;이 경우, 가중치벡터 $w$내에 있는 각 원소의 크기가 입력 이미지 $I$에 대한 중요도라고 정의할 수 있다. 그러나 심층 신경망에서 점수$S_c(I)$는 깊게 꼬인 비선형함수다. 따라서 위와 같이 적용이 불가능하다. 그러나 이미지 $I_0$가 주어졌을 때, 테일러 1차 급수로 $S_c(I)$에 대한 선형함수를 근사할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_c(I) \approx w^TI+b \quad \text{where } w= \dfrac{\partial S_c}{\partial I}\Bigg\vert_{I_0} \cdot&lt;/script&gt;

&lt;p&gt;Image-Specific class Saliency의 다른 해석으로 클래스 점수에 대한 미분값($w$)의 크기는 어떤 픽셀들이 가장 적은 변화량으로 클래스 점수에 가장 큰 영향일 미치는지를 가르다고 할 수 있다. 이를 통해 이미지의 위치를 알아내기를 기대할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;class-saliency-extraction&quot;&gt;Class Saliency Extraction&lt;/h2&gt;

&lt;p&gt;흑백이미지의 경우 절대값을 취해주면 그대로 추출할 수 있다. 컬러 이미지같은 경우 절대값에서 각 채널을 기준으로 최대 값을 뽑아내서 Saliency Map을 만든다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M_{ij} = \max_c \vert w_{h(i, j, c)} \vert&lt;/script&gt;

&lt;p&gt;이 논문에서는 ILSVRC-2013에서 높은 점수를 가진 클래스를 가지고 10장의 이미지를 서브 이미지를 crop 한 후, saliency map들을 산출하여 평균내서 한 장으로 합쳐서 그렸다.&lt;/p&gt;

&lt;h2 id=&quot;weakly-supervised-object-localisation&quot;&gt;Weakly Supervised Object Localisation&lt;/h2&gt;

&lt;p&gt;이러한 saliency map을 물체 위치 탐지 문제에 적용했다. 과정을 요약하면 다음과 같다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1V237wxA35x4oebtlzbOqc3h0-nH44cL6&quot; alt=&quot;[그림 1] Geodesic Star Convexity for Interactive Image Segmentation&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 1] Geodesic Star Convexity for Interactive Image Segmentation&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;GraphCut 이라는 것을 사용한다. 관심 가지는 클래스를 foreground, 그외에 배경을 background라고하는데, &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 1&lt;/code&gt;의 Step 2 처럼, foreground와 background 구분짓기 위해서 특정 색상으로 tagging을 해야한다.&lt;/li&gt;
  &lt;li&gt;saliency map은 특정 색상을 지정할 수 없기 때문에, 가우시안 믹스쳐(Gaussian Mixture) 모델을 활용하여 saliency map의 특정 경계값을 기준으로 foreground와 background의 경계 지도을 만든다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;에서 만들어진 태깅된 경계 지도로 GraphCut으로 Segmentation을 진행한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;자세한 설명은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.csd.uwo.ca/~yuri/Papers/iccv01.pdf&quot;&gt;GraphCut&lt;/a&gt;을 사용하게된 계기는 saliency map은 물체를 판별하는 영역만 탐지하지 물체 전체를 잡아내지 않기 때문이다. GraphCut을 사용하기 위해서 물체의 경계 지도를 전달하는게 중요하다. Foreground(관심 가지는 물체 클래스)와 background(물체 이외에 배경) 모델은 가우시안 믹스처(Gaussian Mixture)를 적용했다. Saliency 분포값의 95%를 경계로 이보다 높은 값을 가지는 픽셀들로 foreground를 추정했고, 30%를 경계로 이보다 이하의 값을 가지는 픽셀들은 background로 추정했다. 실제로 적용하면 &lt;code class=&quot;highlighter-rouge&quot;&gt;[그림 2]&lt;/code&gt;의 3번째 그림처럼 나온다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1Tqqu_QRGqMvOyrvoOVLJdaLOjuxGkGGS&quot; alt=&quot;[그림 2] 1: 원본 / 2: saliency map / 3: 경계 지도 / 4: segmentated image&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 2] 1: 원본 / 2: saliency map / 3: 경계 지도 / 4: segmentated image&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Weakly supervised 임에도 불구하고, ILSVRC-2013 테스트 데이터에서 46.4%의 Top-5 error 성적을 거두었다(당시 우승자는 29.9%를 기록). GraphCut 프로그램은 &lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/software/iseg/&quot;&gt;여기&lt;/a&gt;서 사용할 수 있다(matlab code).&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;4-relation-to-deconvolutional-networks&quot;&gt;4. Relation to Deconvolutional Networks&lt;/h1&gt;

&lt;p&gt;저자는 Deconvolution Network(Zeiler &amp;amp; Fergus, 2013) 구조를 사용해 원래 이미지를 재구성하는 것은 사실상 미분하는 것과 거의 동일하다고 이야기한다.&lt;/p&gt;

&lt;p&gt;Deconvolution과 미분의 관계는 전에 작성한 포스트를 참고하길 바란다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://simonjisu.github.io/datascience/2019/10/27/convtranspose2d.html&quot;&gt;[PyTorch] ConvTranspose2d 와 Conv2d 의 관계&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;appendix-직접-코딩하여-살펴보기&quot;&gt;Appendix: 직접 코딩하여 살펴보기&lt;/h1&gt;

&lt;p&gt;ILSVRC 2015의 1위 모델인 &lt;code class=&quot;highlighter-rouge&quot;&gt;ResNet152&lt;/code&gt;을 가져와서 &lt;a href=&quot;https://pixabay.com/ko/&quot;&gt;Pixabay&lt;/a&gt;에 있는 플라밍고(class: 130) 이미지를 사용해서 Class Model Visualization과 Saliency Map을 생성해보았다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1qyoRulVHIqlqESl0roNMSoLB8s9OB9zN&quot; alt=&quot;[그림 3] 플라밍고 Class Model Visualization과 Saliency Map&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 3] 플라밍고 Class Model Visualization과 Saliency Map&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;이미지는 256x256 크기로 재조정하고 224x224 크기로 center crop을 진행했다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Class model visualization&lt;/strong&gt;의 경우, 151스텝동안 backpropagation 진행, L2 정규화에 $\lambda$를 1.0 으로 설정한 결과다. 자세히 보면 플라밍고의 머리와 목 부분이 곳곳에서 보인다(사실 이게 어떤 의미인지는 아직 연구가 필요하다).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Saliency Map&lt;/strong&gt;의 경우, 딱 1회만 역전파를 한 결과다. 논문에서도 서술했지만, 물체를 직접 탐지하지는 않으며, 물체를 판별하는데 도움이되는 영역이 주로 표시된다.&lt;/p&gt;

&lt;p&gt;자세한 코드는 다음 항목들에서 이용할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/simonjisu/pytorch_tutorials/blob/master/02_VISION/03_deep_inside_cnn.ipynb&quot;&gt;GitHub&lt;/a&gt; 에서 보기&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/02_VISION/03_deep_inside_cnn.ipynb&quot;&gt;Jupyter Notebook&lt;/a&gt; 에서 보기&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 12 Mar 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/03/12/deepinsidecnn.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/03/12/deepinsidecnn.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>[NLP] Attention Is All You Need - 3</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이전 글: &lt;a href=&quot;https://simonjisu.github.io/paper/2020/02/02/attentionisallyouneed2.html&quot;&gt;Attention Is All You Need - 2&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;5-models&quot;&gt;5. Models&lt;/h1&gt;

&lt;h2 id=&quot;masking&quot;&gt;Masking&lt;/h2&gt;

&lt;p&gt;지금까지 미뤄온 Attention 의 마스킹(Masking)을 이야기 해보려 한다. 마스킹이 필요한 이유는 두 가지다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Decoder 의 Self Attention&lt;/p&gt;

    &lt;p&gt;Decoder 에서는 이전의 타임 스텝(t-1)의 정보를 활용하여 다음 타임 스텝(t)의 정보를 예측하게 되는데 이를 &lt;strong&gt;자기회귀(auto-regressive)&lt;/strong&gt;특성이라고 한다. 이러한 특성을 보존하기 위해서 이전 타임 스텝(t-1)을 입력으로 현재 타임 스텝(t)를 예측하려고 할 때, 다음 타임 스텝(t+1)의 정보를 참조하면 안된다. 따라서 이를 Scaled Dot-Product Attention 에서 마스킹을 통해, 음의 무한대(&lt;code class=&quot;highlighter-rouge&quot;&gt;-np.inf&lt;/code&gt;) 값을 주어서 Softmax 값을 0으로 만들어 준다.&lt;/p&gt;

    &lt;p&gt;예를 들어 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 1&lt;/code&gt;처럼 (검은색이 마스킹 위치) Decoder 의 입력 데이터 최대 길이가 4인 경우, &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 에서 0 번째 토큰은 1 번째 토큰을 예측해야 함으로 Self-Attention 시 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 의 1, 2, 3 번째의 토큰의 관계를 무시해야한다. &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 의 1 번째 토큰을 입력시 2 번째 토큰을 예측하게 되는데, 자기 자신을 포함한 그 이전의 정보를 참조 할 수는 있지만 미래의 2, 3 번째의 정보를 미리 참고하면 안된다.&lt;/p&gt;

    &lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1VnSx8Ct5_NNNoa13zGfA5p-RSgbzBIMn&quot; alt=&quot;[그림 1] Decoder Sub-sequence Attention Masking&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 1] Decoder Sub-sequence Attention Masking&lt;/figcaption&gt;&lt;/figure&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;실제 토큰의 길이&lt;/p&gt;

    &lt;p&gt;앞서 말했듯이 RNN 처럼 recurrance 하지 않기 때문에 최대 입력/출력 길이를 정해야한다. 따라서 실제 문장은 길이가 4인데도 설정한 최대 길이 때문에 그 길이만큼 &lt;code class=&quot;highlighter-rouge&quot;&gt;Padding&lt;/code&gt;을 하게 되는데, Attention 계산시 &lt;code class=&quot;highlighter-rouge&quot;&gt;Padding&lt;/code&gt; 은 인위적으로 넣은 토큰이기 때문에 이를 무시해야 한다.&lt;/p&gt;

    &lt;p&gt;예를 들어 Decoder 에 들어가는 타겟 데이터의 최대 길이는 4이지만 실제 토큰의 길이가 3이라면 Attention Matrix 에 해당하는 마스킹은 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 2&lt;/code&gt;와 같다. 여기서는 마지막 토큰이 &lt;code class=&quot;highlighter-rouge&quot;&gt;Padding&lt;/code&gt; 토큰이기 때문에 Self Attention 시 마지막 토큰은 참조하지 않는다. Attention 코드(&lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/master/transformer/modules.py&quot;&gt;GitHub&lt;/a&gt; 참고) 구현하게 되면 3 번째 행은 Softmax 를 통과시 &lt;code class=&quot;highlighter-rouge&quot;&gt;nan&lt;/code&gt; 값이 된다. 따라서 해당하는 값을 0으로 다시 마스킹하는 과정이 필요하다.&lt;/p&gt;

    &lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1KOJA8DNlTQjnKb19zn2vRtzEIRbB8Ut2&quot; alt=&quot;[그림 2] 실제 토큰 길이에 대한 Masking&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 2] 실제 토큰 길이에 대한 Masking&lt;/figcaption&gt;&lt;/figure&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;해당 모듈(Module) 코드는 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/layers.py#L11&quot;&gt;Encoder Layer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/layers.py#L42&quot;&gt;Decoder Layer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/models.py#L10&quot;&gt;Encoder&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/models.py#L54&quot;&gt;Decoder&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformer-model&quot;&gt;Transformer Model&lt;/h2&gt;

&lt;p&gt;특이한 점이라면 마지막 예측 토큰을 출력하는 선형 변환 층(&lt;code class=&quot;highlighter-rouge&quot;&gt;projection&lt;/code&gt;)을 임베딩 층으로 치환하는 방법이 있는데 이를 논문에서 Linear Weight Sharing이라고 했다. 또한, 문제에 따라서 Encoder층의 임베딩과 Decoder층의 임베딩을 공유 할 수도 있는데 Language Modeling 같은 문제가 그 예시라고 할 수 있다. 이를 논문에서 Embed Weight Sharing이라고 했다.&lt;/p&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/models.py#L112&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;6-loss-function&quot;&gt;6. Loss Function&lt;/h1&gt;

&lt;h2 id=&quot;label-smoothing&quot;&gt;Label Smoothing&lt;/h2&gt;

&lt;p&gt;Discrete한 분포를 예측하는 방법은 주로 Cross Entropy를 많이 사용하지만 논문에서는 &lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;Rethinking the inception architecture for computer vision&lt;/a&gt; 논문에서 언급한 Label Smoothing 기법을 활용했다.&lt;/p&gt;

&lt;p&gt;예측 확률 분포를 &lt;strong&gt;P&lt;/strong&gt;, 정답/타겟 확률 분포(ground-truth distribution)를 &lt;strong&gt;Q&lt;/strong&gt;라고 하겠다. $x$ 를 입력으로 예측 확률 질량함수 &lt;code class=&quot;highlighter-rouge&quot;&gt;p(y=k|x)&lt;/code&gt; 에서 구한 확률(Softmax)을 타겟 확률 질량함수 &lt;code class=&quot;highlighter-rouge&quot;&gt;q(y=k|x)=1&lt;/code&gt; 처럼 만드는 것이 원래의 최종목표다. 이제부터 $x$를 생략해서 쓰겠다. Cross Entropy의 수식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Loss = -\sum_{k=1}^{K} \log\big(p(k) \big) q(k)&lt;/script&gt;

&lt;p&gt;Cross Entropy 를 최소화 하는 것은 $k$ 라벨에 해당하는 log-likelihood 의 기댓값을 $q(k)$로 최대화 하는 것과 같다. 그리고 Cross Entropy 는 Softmax 에 사용되는 예측값의 로짓(logit, $z_k$)에 대해 미분을 구할 수 있는데, 그 미분값은 다음과 같으며 -1 과 1 사이의 치역을 갖는다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned} \dfrac{\partial Loss}{\partial z_k} = q(k)\big(p(k)-1\big) \end{aligned}&lt;/script&gt;

&lt;p&gt;정답 라벨(ground-truth label)이 $y$인 예시를 들어봅자. 논문에서는 라벨 $y$에 해당하는 로짓($z_y$) 값이 다른 라벨 $k$의 로짓($z_k$) 값에 비해 월등히 클 수록 2가지 문제가 생긴다고 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;오버피팅(over-fitting)이 될 가능성이 있다. 만약에 모델이 전체 확률 분포(모든 라벨)를 학습 시, 일반화(generalize)를 보장할 수 없다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이러한 구조는 제일 큰 로짓 값과 상대적으로 작은 로짓 값의 차이를 점점 더 크게 만들도록 학습한다. 이는 미분 값을 항상 0에 가깝게 만들어 가중치 업데이트가 안된다. 다음 미분의 수식에서 확인 할 수 있듯이, 정답 라벨 $y$ 의 로짓 값이 높을 수록, 그 확률은 1 에 가까워 경사(gradient)가 0에 가깝다.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \dfrac{\partial Loss}{\partial z_y} &amp;= -q(z_y)\cdot\frac{1}{p(z_y)} \times p(z_y)\big(1-p(z_y) \big) \\ &amp;= -q(z_y)+q(z_y)p(z_y) \\&amp;= p(z_y) -1 \end{aligned} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;결론적으로 말하면, 정답 라벨에 대해서 너무 확실한 예측을 내놓는 다는 것이다. 따라서 이러한 효과를 줄이기 위해서 해당 논문에서는 색다른 정답 확률 분포를 이야기 하는데, 기존의 &lt;code class=&quot;highlighter-rouge&quot;&gt;q(k)&lt;/code&gt;의 분포는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(k)=\delta_{k,y} \begin{cases} 1 \quad \text{if } k=y \\ 0 \quad \text{else} \end{cases}&lt;/script&gt;

&lt;p&gt;이를 새로운 &lt;code class=&quot;highlighter-rouge&quot;&gt;q'(k)&lt;/code&gt;로 치환하게 된다. $\epsilon$은 Smoothing을 위한 변수다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q'(k) = (1-\epsilon)\delta_{k,y} + \epsilon \cdot u(k)&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;u(k)&lt;/code&gt; 의 분포는 훈련데이터와 무관한 분포이며, 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;u(k) = 1/K&lt;/code&gt;인 Uniform Distribution 을 사용한다. &lt;code class=&quot;highlighter-rouge&quot;&gt;q'(k)&lt;/code&gt;와 같은 정규화의 한 방법으로 논문에서는 이를 &lt;strong&gt;Label-Smoothing Regularization (LSR)&lt;/strong&gt; 이라고 제시했다.&lt;/p&gt;

&lt;p&gt;LSR 의 목적은 원래 목표인 타겟 라벨을 맞추는 목적과 정답 라벨의 로짓(logit) 값이 학습과정에서 과도하게 다른 로짓 값보다 커지는 현상을 방지하는 것이다. 수식을 약간 변형하여 LSR 를 다른 관점에서 볼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} LSR = H(p, q') &amp;= -\sum_{k=1}^K \log \big( p(k)\big) q'(k) \\ &amp;= -\sum_{k=1}^K \log \big( p(k)\big)\big( (1-\epsilon)\delta_{k,y} + \epsilon u(k) \big) \\ &amp;= (1-\epsilon)\big(-\sum_{k=1}^K \log \big( p(k)\big)\delta_{k,y} \big) \epsilon \big( -\sum_{k=1}^K \log \big( p(k)\big)u(k) \big) \\ &amp;= (1-\epsilon) H(q, p) + \epsilon H(u, p) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;LSR 은 기존에 Cross Entropy &lt;code class=&quot;highlighter-rouge&quot;&gt;H(q, p)&lt;/code&gt;를 한 쌍의 &lt;code class=&quot;highlighter-rouge&quot;&gt;H(q, p)&lt;/code&gt; 와 &lt;code class=&quot;highlighter-rouge&quot;&gt;H(u, p)&lt;/code&gt;로 대체한 것이다. Smoothing Factor 인 $\epsilon$의 크기의 여부에 따라 정규화의 정도가 달라진다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 에서도 해당 방법을 정규화 목적으로 $K$ 는 타겟 단어장(vocab)의 크기, $\epsilon$=0.1 로 사용했다.&lt;/p&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/labelsmooth.py#L5&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;Rethinking the Inception Architecture for Computer Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html#label-smoothing&quot;&gt;The Annotated Transformer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;7-optimizer&quot;&gt;7. Optimizer&lt;/h1&gt;

&lt;h2 id=&quot;learning-rate-variation&quot;&gt;Learning Rate Variation&lt;/h2&gt;

&lt;p&gt;논문에서는 Adam Optimizer 를 기반으로 다양한 학습률을 적용하여 사용했다. 학습률 변화의 수식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;lrate = d_{model}^{-0.5} \cdot \min(\text{step_num}^{-0.5}, \text{step_num} \cdot \text{warmup_steps}^{-1.5} )&lt;/script&gt;

&lt;p&gt;해당 수식에 따르면 처음 warmup_steps 동안 학습률은 가파르게 상승하다가 차후에 천천히 하강하게 된다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1d0xw7_xjr1rv7-SjuxQKRmML4oio561j&quot; alt=&quot;[그림 3] hidden 크기 및 warmup steps 에 따른 학습률의 변화&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 3] hidden 크기 및 warmup steps 에 따른 학습률의 변화&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/warmupoptim.py#L1&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;8-training-multi30k-with-transformer&quot;&gt;8. Training Multi30k with Transformer&lt;/h1&gt;

&lt;p&gt;PyTorch의 &lt;code class=&quot;highlighter-rouge&quot;&gt;torchtext&lt;/code&gt;에 있는 Multi30k 데이터 세트(영어-독일어 번역)로 테스트 해보았다. 큰 데이터는 아니기 때문에, NVIDIA GTX 1080 ti 로 약 36분 훈련시켰다. 기존의 RNN 으로 훈련시키는 것 보다 월등히 빨랐다. 모델에서 Attention에 대한 그림도 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr&quot;&gt;github&lt;/a&gt;에 올려두었으니 확인해보길 바란다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1HsVRsp3mMjo8UBSTU81ZE4i_MUZ4Z1Xa&quot; alt=&quot;[그림 4] Multi30k 성능 테스트&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 4] Multi30k 성능 테스트&lt;/figcaption&gt;&lt;/figure&gt;
</description>
        <pubDate>Sun, 23 Feb 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/02/23/attentionisallyouneed3.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/02/23/attentionisallyouneed3.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>PRML Project</title>
        <description>&lt;p&gt;크리스토퍼 비숍의 책 “패턴 인식과 기계 학습(Pattern Recognition and Machine Learning)” 의 원서를 공부하고, 그에 필요한 코드와 수식을 정리하고 있습니다. 아래 사이트에서 확인하실 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;원서 다운로드: &lt;a href=&quot;https://aka.ms/prml&quot;&gt;https://aka.ms/prml&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;GitBook: &lt;a href=&quot;https://soo.gitbook.io/prml&quot;&gt;https://soo.gitbook.io/prml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 21 Feb 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/prml/2020/02/21/prml.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/prml/2020/02/21/prml.html</guid>
        
        
        <category>prml</category>
        
      </item>
    
      <item>
        <title>글또 4기 다짐</title>
        <description>&lt;h1 id=&quot;글또-4기를-시작하며&quot;&gt;글또 4기를 시작하며&lt;/h1&gt;

&lt;p&gt;“글또”가 궁금하다면? &lt;a href=&quot;https://www.notion.so/ac5b18a482fb4df497d4e8257ad4d516&quot;&gt;&lt;strong&gt;글또 Notion 페이지 바로가기&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;작년 여름의 어느날 글또를 시작하면서 세웠던 계획을 다시 돌아보게 됐다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“너는 다 계획이 있구나”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1Y534QRewyuENFNHTtf2WXgZe_5VH91MS&quot; alt=&quot;출처: 영화 기생충 스틸컷&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;출처: 영화 기생충 스틸컷&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;PRML(“Pattern Recognition and Machine Learning” - Christopher Bishop) 책 공부하기&lt;/li&gt;
  &lt;li&gt;NLP 논문 읽기 (BERT 등)&lt;/li&gt;
  &lt;li&gt;새로운 것 배우기: Flask, Spark, Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;대학원 준비&lt;/li&gt;
  &lt;li&gt;장롱면허 탈출&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 5가지 목표 중에서 3개는 달성하고 새로운 것을 배우는 것은 달성하지 못했고 장롱면허도 아직 탈출하지 못했다. 첫번째 목표도 100% 완성하지 못했기 때문에 사실상 50%를 완성했다고 할 수 있다. 다시 돌아보니 글또와는 크게 관련이 없는 목표들이 많았다. 그래서 이번 글또 4기에는 글또에 알맞는 목표를 세워보려고 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;최소 12편 이상의 글 작성하기&lt;/li&gt;
  &lt;li&gt;어떤 주제든 시리즈 글 연재하기(3편 이상)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;글또 4기를 마치고 꼭 두 가지 목표를 완수하기를…&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;글또-4기에서-작성하고-싶은-글&quot;&gt;글또 4기에서 작성하고 싶은 글&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;인지(행동)과학 관련 책 리뷰&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;인지과학 분야의 책을 통해 사람이 어떤 방식으로 생각을 하는지 이해하고, 딥러닝 모델의 설명과 연관지어 떠오르는 아이디어를 정리해보고자 한다. 컴퓨터 비전 분야에서 인간의 시각 시스템을 모방한 Neocognitron(1979, Kunihiko Fukushima)부터 현재의 다양한 Convolution Neural Network 까지, 인간에 대한 연구는 알고리즘을 해석하고 발전시키는데 많은 영향을 줬었다. 사람이 생각하는 과정(인지 프로세스)를 이용해 더 좋은 성능을 내는 모델을 연구하거나, 더 나아가 “설명가능한 딥러닝 모델”의 실마리가 나올 수 있지 않을까 생각하고 있다. 관련 전공이 아니기 때문에, 먼저 사람들이 주로 읽는 도서부터 시작해서 천천히 깊게 탐구해볼 생각이다. 현재 도서 목록으로 다음과 같이 정했는데, 혹시 추천할 만한 책이 있으면 댓글 부탁드린다.&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;생각에 관한 생각&lt;/li&gt;
        &lt;li&gt;넛지&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;데이터 사이언스 대학원 입학과정 및 생활&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;나는 대학원에 운좋은 타이밍에 입학했다고 생각한다. 그래도 차후에 다른 사람에게 도움이 될만한 정보와 신설된 데이터 사이언스 대학원에 입학하게된 계기 및 과정을 쓰려고 한다. 또한, 후회없는 2년(혹은 그 이상..?) 대학원 과정을 만들기 위해서 어떤 것을 계획하고 어떻게 공부를 하는지도 기록할 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;논문 리뷰(NLP, XAI 분야)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;마지막으로 최근에 보고 있는 논문들을 간략하게 정리할 예정이다. 최근 1월~2월 사이에 읽은 Vision 분야에서 사용되고 있는 XAI 관련 논문을 읽었고 이를 정리하고자 한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 19 Feb 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/others/2020/02/19/geultto4.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/others/2020/02/19/geultto4.html</guid>
        
        
        <category>others</category>
        
      </item>
    
      <item>
        <title>[NLP] Attention Is All You Need - 2</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이전 글: &lt;a href=&quot;https://simonjisu.github.io/paper/2020/01/14/attentionisallyouneed.html&quot;&gt;Attention Is All You Need - 1&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-sub-layers&quot;&gt;3. Sub Layers&lt;/h1&gt;

&lt;h2 id=&quot;multi-head-attention&quot;&gt;Multi-Head Attention&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1jpQdv3lFrYNRZ5FbCvcXF4RDtpho0og_&quot; alt=&quot;[그림 1] Multi-Head Attention&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 1] Multi-Head Attention&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;첫번째 서브층(SubLayer) Multi-Head Attention 의 구조는 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 1&lt;/code&gt; 과 같다. 연구자들은 $d_{model}$ 크기의 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 를 한 번 수행하는 것보다 $h$ 개의 각기 다른 &lt;strong&gt;선형 투영(linear projection)&lt;/strong&gt;을 시켜, 크기가 $d_k$(&lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;), $d_v$(&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt;) 인 텐서를 사용해서 Attention 을 병렬로 수행하는 것이 더 유리한 것을 찾아냈다. 각기 다른 Attention 을 수행한 $h$ 개의 출력값은 하나로 concatenate 후에 최종 선형결합을 통해 다시 $d_{model}$ 크기로 돌아오는데 이를 수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{MultiHead}(Q, K, V) &amp;= \text{Concat}(\text{head}_1, \cdots \text{head}_h)W^O  \\ \text{where head}_i &amp;= \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;W 는 선형결합을 위한 매겨변수이며 각각의 크기는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned} W^Q_i \in \Bbb{R}^{d_{model}\times d_k}, W^K_i\in \Bbb{R}^{d_{model}\times d_k}, W^V_i\in \Bbb{R}^{d_{model}\times d_v}, W^O \in \Bbb{R}^{h*d_v\times d_{model}}\end{aligned}&lt;/script&gt;

&lt;p&gt;그렇다면 이렇게 큰 차원의 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 를 선형 변환 후에 $h$ 개의 Attention 을 나눠서 학습하게 했던 이유는 무엇 일까? 논문에서 이해한 것을 정리하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;일단 선형 변환된 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 를 $h$ 개로 나눠버리는데, 이는 각 토큰을 표현하고 있던 큰 차원의 뉴런들을 $h$ 개의 블록으로 나눴다고 할 수 있다. 이렇게 위치가 상이한 각기 다른 표현 부분공간(representation subspaces) 블록들이 교차하면서(jointly) 정보를 얻게 된다. 이 말은 곧 $h$ 개의 Attention Matrix 가 생기면서 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 간의 토큰들이 더 다양한 관점으로 볼 수 있다는 말이다. 만약에 나누지 않았다면 단 하나의 Attention Matrix 를 생성하면서 이러한 효과를 뭉게버림으로, 선형 변환 층(linear projection)은 학습 과정을 반복하면서 최적의 $h$ 개의 Attention Matrix 를 생성하는 역할을 학습하게 된다.&lt;/p&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/sublayers.py#L11&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;position-wise-feed-forward-networks&quot;&gt;Position-wise Feed-Forward Networks&lt;/h2&gt;

&lt;p&gt;또 다른 서브층으로써 완전 연결층(Fully Connect Layer)인 네트워크를 Encoder, Decoder 뒤에 하나씩 추가했다. 이 완전 연결층은 두 개의 선형변환과 ReLU 활성화 함수를 사용했으며 그 수식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{FFN}(x) = \max(0, xW_1+b_1)W_2+b_2&lt;/script&gt;

&lt;p&gt;아마도 입력 텐서가 각 토큰의 위치별로 차원이 커졌다가 다시 원래 모양으로 줄어들어서 이름이 Position-wise 라고 붙여진 것으로 추정되는데, 차원의 크기가 다음과 같이 변하기 때문이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(B, T, d_{model}) \rightarrow(B, T, d_{ff}) \rightarrow (B, T, d_{model})&lt;/script&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/sublayers.py#L91&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;4-embeddings&quot;&gt;4. Embeddings&lt;/h1&gt;

&lt;h2 id=&quot;input-과-output&quot;&gt;Input 과 Output&lt;/h2&gt;

&lt;p&gt;Embedding 층과 Position Encoding 을 설명하기 전에 입출력이 어떻게 구성되어 있는지를 살펴봐야한다. 기계 번역 문제를 다시 예시로 들어보면, 다음과 같이 수치화된 문장들이 있다. 0 은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Padding&lt;/code&gt; 토큰으로써 데이터 처리를 위해 설정한 문장의 최대 길이에 맞춰서 넣은 인위적인 토큰이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{src} &amp;= \begin{bmatrix}3&amp;6&amp;4&amp;9 \\ 1&amp;3&amp;5&amp;0 \\ 3&amp;2&amp;0&amp;0 \end{bmatrix} \\ \text{trg} &amp;= \begin{bmatrix}2&amp;5&amp;4&amp;0 \\ 2&amp;5&amp;6&amp;0 \\ 2&amp;7&amp;4&amp;9 \end{bmatrix}  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;즉, 위 행렬을 해석하면 현재 Input 데이터는 미니배치가 3, 문장의 최대 길이가 4인 데이터, Target 데이터는 미니배치가 3, 문장의 최대 길이가 4인 데이터다. 각 문장의 토큰들에 순서 인덱스를 부여하여 포지션(Position) 데이터를 얻고자하면 다음과 같다. &lt;code class=&quot;highlighter-rouge&quot;&gt;Padding&lt;/code&gt; 은 인위적으로 넣은 데이터기 때문에 순서가 없어야 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{src_pos} &amp;= \begin{bmatrix}1&amp;2&amp;3&amp;4 \\ 1&amp;2&amp;3&amp;0 \\ 1&amp;2&amp;0&amp;0 \end{bmatrix} \\ \text{trg_pos} &amp;= \begin{bmatrix}1&amp;2&amp;3&amp;0 \\ 1&amp;2&amp;3&amp;0 \\ 1&amp;2&amp;3&amp;4 \end{bmatrix}  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Decoder 의 경우 이전 타임 스텝(t-1)의 토큰들로 다음 타임 스텝(t)의 토큰을 예측하기 때문에 실질적으로 모델에 입력되는 데이터(&lt;code class=&quot;highlighter-rouge&quot;&gt;trg_input&lt;/code&gt;)와 실제 예측해야하는 타겟 데이터(&lt;code class=&quot;highlighter-rouge&quot;&gt;gold&lt;/code&gt;)는 다음과 같다. 즉, 예를 들어 1, 2, 3 포지션에 해당하는 타겟 값을 입력으로 주었을때 2, 3, 4 번 포지션에 해당하는 값을 예측하는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{trg_input} &amp;= \begin{bmatrix} 2&amp;5&amp;4 \\ 2&amp;5&amp;6 \\ 2&amp;7&amp;4 \end{bmatrix} \\ \text{gold} &amp;= \begin{bmatrix}5&amp;4&amp;0 \\ 5&amp;6&amp;0 \\ 7&amp;4&amp;9 \end{bmatrix}  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;토큰에 순서 정보인 포지션을 구하는 이유는 무엇일까? 그 해답은 RNN 의 구동원리에 있는데, RNN 을 Cell 단위로 만들면 다음 코드와 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# create inputs tensor
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_dimension&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# setting rnn cell
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;hidden_dimension&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rnn_cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RNNCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# RNN Layer
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnn_cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# print: torch.Size([10, 2, 6])
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;RNN 의 특징 중 하나는 시퀀스 길이에 상관없이 한 스텝씩 처리하기 때문에 아주 긴 시퀀스도 처리를 할 수 있다. 그러나 이러한 특징은 이전 타입스텝의 정보를 다음 타임스텝에게 전달할 수 있지만 병렬 처리가 불가능 하다. 하지만 Transformer 의 목표중 하나는 시퀀스 데이터의 병렬 처리인데, 즉, 한 번에 지정된 길이의 시퀀스를 모두 모델에게 전달하고 Forward 하게 된다. 그렇다면 시퀀스의 각 토큰간 순서 관계 정보를 모델은 어떻게 알아낼 수 있을까? 바로 &lt;strong&gt;Position Encoding&lt;/strong&gt; 을 통해서 각 토큰의 순서 정보를 &lt;strong&gt;Embedding&lt;/strong&gt; 된 벡터와 결합하여 모델로 전달하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;embedding&quot;&gt;Embedding&lt;/h2&gt;

&lt;p&gt;임베딩은 분절된 토큰들을 고정된 $d_{model}$ 차원의 공간으로 표현해주는 방법이다. Decoder 의 출력층에는 선형변환 층과 Softmax 를 섞어서 예측 토큰의 확률을 구하는 기법을 사용했으며, 임베딩된 벡터에 $\sqrt{d_{model}}$ 를 곱했다.&lt;/p&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/layers.py#L107&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h2&gt;

&lt;p&gt;Positional Encoding 은 상대적이거나 절대적인 위치정보를 부여하는 방법이다. 각 Position Encoding 의 차원의 크기는 더할 수 있게 임베딩된 텐서의 차원 크기인 $d_{model}$과 같고 수식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} PE_{pos, 2i} &amp;= \sin(\frac{pos}{10000^{2i/d_{model}}}) \\ PE_{pos, 2i+1} &amp;= \cos(\frac{pos}{10000^{2i/d_{model}}})\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;결론을 말하자면 각 시퀀스의 순서 인덱서는 PE(Positonal Encoding) 테이블에서 각자의 위치를 조회후에 임베딩된 텐서와 결합하게 된다. pos 는 시퀀스의 위치정보, 예를 들어 텐서의 크기가 $d_{model}$ = 1024 의 경우, 각 1024의 짝수(2i)에 위치한 값들은 sin 함수를 적용하고, 홀수(2i+1) 에 위치한 값들은 cos 함수를 적용한다. PE 테이블을 그리면 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 2&lt;/code&gt; 과 같은데, 자세히 보시면 각 포지션에 해당하는 1 줄(1024 크기의 벡터)값은 모두 차별화 되어있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1IznpVENdNpwyKqJCD0mWnZRcQ2XaIh22&quot; alt=&quot;[그림 2] 최대 길이가 51인 Positional Encoding Table&quot; width=&quot;100%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 2] 최대 길이가 51인 Positional Encoding Table&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/layers.py#L82&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음편: &lt;a href=&quot;https://simonjisu.github.io/paper/2020/02/23/attentionisallyouneed3.html&quot;&gt;Attention Is All You Need - 3&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 02 Feb 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/02/02/attentionisallyouneed2.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/02/02/attentionisallyouneed2.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>[NLP] Attention Is All You Need - 1</title>
        <description>&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;그 동안 LSTM(&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1246450&quot;&gt;Long Short-term Memory&lt;/a&gt;, 1997) 과 GRU(&lt;a href=&quot;https://arxiv.org/abs/1412.3555&quot;&gt;Gated Recurrent Unit&lt;/a&gt;, 2014) 등의 RNN 계열은 언어 모델링, 기계번역 등의 문제와 같이 시퀀스 모델링(sequence modeling)을 하기에 최고의 알고리즘이었다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1si3KMBjwZJ3inzTuoeUUsl7mutbDLNbz&quot; alt=&quot;[그림 1] RNN의 forward propagation&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 1] RNN의 forward propagation&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;그림 1&lt;/code&gt; 처럼 이전 스텝의 은닉층 유닛인 $h_{t-1}$ 를 현재 스텝의 은닉층 유닛 $h_t$ 로 전달하면서 자연스럽게 시퀀스 데이터의 특징을 유지하지만, 아쉽게도 병렬 처리를 원천적으로 배제한다는 단점이 존재한다. 따라서 만약에 문장이 길어질 수록 훈련 속도가 현저하게 느려진다.&lt;/p&gt;

&lt;p&gt;Input 과 Output 문장의 길이와 관계없이 의존성(dependencies)을 해결해주는 &lt;strong&gt;Attention&lt;/strong&gt; 매커니즘은 시퀀스 모델링 혹은 변환 모델링&lt;span style=&quot;color:gray&quot;&gt;(transduction modeling: 각기 다른 특성을 가진 입력-출력 데이터를 변환하는 문제들, 예를 들어 기계번역)&lt;/span&gt;에서 필수적인 요소가 됐다. 예시로 다음 논문들을 참고하면 좋다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau 2014&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.00887&quot;&gt;Structured Attention Networks, Yoon Kim, 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.01933&quot;&gt;A Decomposable Attention Model for Natural Language Inference, Ankur P. Parikh, 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 두 가지를 결합하여 저자들은 Attention 매커니즘만 활용하여 Input 과 Output 의 의존성을 글로벌하게 처리하고, 병렬화까지 가능한 &lt;code class=&quot;highlighter-rouge&quot;&gt;Transformer&lt;/code&gt;라는 새로운 모델구조를 제안했다.&lt;/p&gt;

&lt;h2 id=&quot;전체-모델구조&quot;&gt;전체 모델구조&lt;/h2&gt;

&lt;p&gt;대부분의 신경망 시퀀스 변환 모델(transduction models)들은 대체로 Encoder 와 Decoder 로 구성된다. Encoder는 심볼로 표현된 입력 시퀀스(비연속적인 토큰들) $x$ 를 연속 공간(Continuous Space) $z$ 로 맵핑 후, $z$ 를 바탕으로 출력 시퀀스 심볼인 $y$ 를 얻는다. 출력 시퀀스는 이전 타임 스텝($t-1$) 시퀀스를 입력으로 다음 타임 스텝($t$)을 출력하는 자기회귀(auto-regressive) 성격을 가진다. 수식으로 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \mathbf{x}&amp;=(x_1, x_2, \cdots, x_n) \rightarrow \mathbf{z}=(z_1, z_2, \cdots, z_n)\\ \mathbf{y}&amp;=(y_1, y_2, \cdots, y_m)\ \text{for}\  y_{t}=f(y_{t-1}, \mathbf{z}) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=15FPAUru5Rm1x3LUu6pcSjaZiuRrBkj97&quot; alt=&quot;[그림 2] 모델구조: Encoder(좌), Decoder(우)&quot; width=&quot;75%&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 2] 모델구조: Encoder(좌), Decoder(우)&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;하지만 &lt;strong&gt;Transformer&lt;/strong&gt; 에서는 한 타임 스텝마다 $y$ 를 출력하지 않고 한번에 처리한다. 저자들이 제안한 전체적인 모델구조는 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 2&lt;/code&gt; 와 같다(전체적인 느낌만 보고 다음으로 넘어가도록 한다).&lt;/p&gt;

&lt;h2 id=&quot;encoder&quot;&gt;Encoder&lt;/h2&gt;

&lt;p&gt;Encoder는 각기 다른 N 개의 “Encoder Layer”라는 층으로 구성되며, 각 층에는 두 개의 서브층(SubLayer)이 존재한다. 첫번째는 Self Attention을 수행하는 “Multi-Head Attention”, 두번째는 일반적인 “Position-wise Feed Forward”로 구성되며, 각 서브층은 Residual Network(&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;Kaiming He, 2015&lt;/a&gt;)처럼 서브층의 입력과 출력을 결합하고, 그 결괏값을 다시 LayerNorm(&lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Jimmy Lei Ba, 2016&lt;/a&gt;) 을 통과시켜 출력을 얻는다. 수식으로 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{LayerNorm}(x + \text{SubLayer}(x))&lt;/script&gt;

&lt;h2 id=&quot;decoder&quot;&gt;Decoder&lt;/h2&gt;

&lt;p&gt;Decoder도 Encoder와 마찬가지로 각기 다른 N 개의 “Decoder Layer” 라는 층으로 구성된다. 다만, Encoder의 출력을 받아서 “Multi-Head Attention”을 수행하는 3번째 서브층이 추가된다. Self Attention을 수행하는 첫번째 “Multi-Head Attention”에서는 뒤에 있는 시퀀스정보로 부터 예측을 하지 않게 이를 가리게 됩니다. 따라서 $i$ 번째 토큰은 $i+1$ 번째 이후의 토큰을 참조하지 않게 됩니다. 나머지는 Encoder와 마찬가지로 잔차 연결(residual connection)을 수행하고 LayerNorm을 통과하게 된다.&lt;/p&gt;

&lt;p&gt;이제부터 모델의 세부 사항을 살펴보면서 저자가 왜 이렇게 사용했는지, 의도가 무엇인지를 알아보려고 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2-scaled-dot-product-attention&quot;&gt;2. Scaled Dot-Product Attention&lt;/h1&gt;

&lt;h2 id=&quot;attention&quot;&gt;Attention&lt;/h2&gt;

&lt;p&gt;Transformer 에서 Attention은 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;query(Q)&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;key(K)&lt;/strong&gt;&lt;/span&gt;-&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;value(V)&lt;/strong&gt;&lt;/span&gt; 세트를 입력으로 집중된 어떤 벡터를 출력하는 함수로 표현할 수 있다. 출력은 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 간의 관계(Attention), 즉 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 의 정보를 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 에 대조 했을 때, 어느 부분을 집중해서 볼 것인지를 계산하고 해당 관계를 &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 와 결합하여 출력을 만든다. 수식으로 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O = \text{Attention}(Q, K, V)&lt;/script&gt;

&lt;p&gt;직관적으로 잘 안떠오르는데, 이게 어떤 느낌인지 알아보기위해 예를 들어보면 다음과 같다.&lt;/p&gt;

&lt;h2 id=&quot;기계번역-문제&quot;&gt;기계번역 문제:&lt;/h2&gt;

&lt;p&gt;영어를 한국어로 번역하는 문제를 예로 들자면, 영어는 소스 문장, 한국어는 타겟 문장이 된다. &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;query(Q)&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;key(K)&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;value(V)&lt;/strong&gt;&lt;/span&gt; 관계는 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 3&lt;/code&gt; 과같이 표현할 수 있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=14tFq4-RDEDFbc9vEABWqiFxG0pI4qq3G&quot; alt=&quot;[그림 3] 기계번역 문제로 Q, K-V 의 관계 알아보기&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 3] 기계번역 문제로 Q, K-V 의 관계 알아보기&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;query(Q)&lt;/strong&gt;&lt;/span&gt;: 한국어 문장 정보&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;key(K)&lt;/strong&gt;&lt;/span&gt;-&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;value(V)&lt;/strong&gt;&lt;/span&gt; 세트: 인코딩된 영어 문장 정보, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;key(K)&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;value(V)&lt;/strong&gt;&lt;/span&gt; 는 같은 벡터&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 는 우리가 알고 싶어하는 문제라고 생각할 수 있다. 명칭도 “query=질의” 그대로 &lt;strong&gt;“한국어로 변역하기 위해 영어 문장에서 집중적으로 봐야하는 단어는 어느 것인가?”&lt;/strong&gt; 라는 질문을 인코딩된 영어 문장 정보인 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;  한테 물어보게 된다. 그 방법은 이 다음에 소개하도록 하고, 그렇게 얻은 결과인 &lt;strong&gt;A&lt;/strong&gt; 를 &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 와 곱하여 그 단어를 집중적으로 보게한다. 그렇게 Attention의 결과물인 &lt;span style=&quot;color:#49aa71&quot;&gt;&lt;strong&gt;O&lt;/strong&gt;&lt;/span&gt; 를 얻는다.&lt;/p&gt;

&lt;h2 id=&quot;감성-분석-문제&quot;&gt;감성 분석 문제:&lt;/h2&gt;

&lt;p&gt;꼭 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;-&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 가 다른 성격을 가진 시퀀스가 아니어도 된다. 세 토큰 모두 하나의 시퀀스를 가르킬 수도 있으며, 이를 Self-Attention 이라고 한다. 예를 들어 감성 분석(Sentiment Analysis) 문제를 예로 들면, 모델은 문장을 읽고 이를 사전에 정의해 놓은 감성 카테고리로 판단하게 되는 데, 이때 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;, &lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 모두 같은 문장을 지정하여 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 4&lt;/code&gt;처럼 Attention 을 사용할 수 있다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1vFw0wuulHhzu5kwZLQ1QStl24KjnlsgX&quot; alt=&quot;[그림 4] 감성 분류 문제를 통해 Self-Attention 에 대해 알아보기&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 4] 감성 분류 문제를 통해 Self-Attention 에 대해 알아보기&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;scaled-dot-product-attention&quot;&gt;Scaled Dot-Product Attention&lt;/h2&gt;

&lt;p&gt;Attention을 구하는 방법은 사실 다양하지만 Transformer 에서는 제일 기본적인 “Dot Product” 를 사용했으며, 그 수식은 다음과 같으며, 배치크기를 제외한 Q, K, V 의 크기를 표기해서 &lt;code class=&quot;highlighter-rouge&quot;&gt;그림 5&lt;/code&gt; 와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Attention}(Q, K, V) = \text{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V&lt;/script&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1CtBsDHkyU8hmFj2MB0IDhEQO7wCKUEkM&quot; alt=&quot;[그림 5] Q, K, V크기를 표기한 Scaled-Dot Product Attention&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;[그림 5] Q, K, V크기를 표기한 Scaled-Dot Product Attention&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;여기서 주의할 점은 $T_k$ 과 $T_v$가 같다는 점이다. 기계 번역을 예로 들면 소스 문장이 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt;-&lt;span style=&quot;color:#cfb648&quot;&gt;&lt;strong&gt;V&lt;/strong&gt;&lt;/span&gt; 세트이기 때문에 같은 길이의 내용을 담고 있지만 각 토큰이 표현하고 있는 차원만 다를 뿐이다. &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 의 길이는 다를 수 있지만 차원 $d_k$ 로 같다. 두 행렬은 행렬의 곱(matrix multiplication)을 통해서 크기가 $(T_q, T_v)$ 인 점수 행렬 &lt;strong&gt;A&lt;/strong&gt; 를 만들어 낸다.&lt;/p&gt;

&lt;p&gt;행렬 &lt;strong&gt;A&lt;/strong&gt; 는 스케일링(Scaling), 마스킹(Masking) 후 Softmax 를 통해 확률값을 도출한다. 이 행렬의 뜻은 “문제를 해결하기 위해서 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 의 토큰이 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 의 어떤 토큰을 가장 많이 참고해야하는가?” 를 뜻한다. 따라서 확률이 높게 부여된 토큰은 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 의 해당하는 토큰과 연관성이 높다고 할 수 있다. 물론 이 모든 연산은 학습이 가능하도록 DAG(Directed acyclic graph)로 연결되어 있기 때문에 학습 스텝이 진행됨에 따라 풀고자하는 문제에 최적화된 확률을 계속 도출해낸다&lt;span style=&quot;color:gray&quot;&gt;((Masking 은 차후에 다룬다)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;스케일링 작업은 행렬 곱을 구한 &lt;strong&gt;A&lt;/strong&gt; 를 $\sqrt{d_k}$ 로 나누는데, 그 이유는 다음과 같다. 차원의 크기인 $d_k$ 가 커질 수록 행렬의 곱의 수치는 점점 커지고 Softmax 수식에 의해서 그 확률 값 또한 커진다. 따라서 Softmax 의 경사(gradient) 값도 굉장히 작아지는데, 이를 막기위해서 $\frac{1}{\sqrt{d_k}}$ 값을 곱해줘야한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;왜 $\sqrt{d_k}$ 를 나눌까?&lt;/strong&gt; 평균이 0, 표준편차가 1인 랜덤한 값으로 &lt;span style=&quot;color:#e25252&quot;&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/span&gt; 와 &lt;span style=&quot;color:#5470cc&quot;&gt;&lt;strong&gt;K&lt;/strong&gt;&lt;/span&gt; 로 초기화시키고 확률로 표현된 행렬값 &lt;strong&gt;A&lt;/strong&gt; 의 경사를 구해보면 $d_k$ 가 커짐에 따라서 평균이 0, 분산이 $d_k$ 를 따르는 분포가 된다. 이러한 시뮬레이션을 다음 코드를 통해 알아 볼 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;check_dotproduct_dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampling_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    to check &quot;https://arxiv.org/abs/1706.03762&quot; Paper page 4, annotation 4
    -------------------------------
    To illustrate why the dot products get large, 
    assume that the components of q and k are independent random variables 
    with mean 0 and variance 1.
    Then their dot product has mean 0 and variance d_k
    
    print(&quot;*** notice that the gradient of softmax is y(1-y) ***&quot;)
    for d_k in [10, 100, 1000]:
        check_dotproduct_dist(d_k, sampling_size=100000, seq_len=5, threshold=1e-10)
    
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cal_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampling_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampling_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;size of vector d_k is {d_k}, sampling result, dot product distribution has&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; - mean: {attn.mean().item():.4f}, &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; - var: {attn.var().item():.4f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cal_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;le&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_percent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;count of gradients that smaller than threshod({threshold}) is {g_sum}, {g_percent:.2f}&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;attn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cal_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_sum2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;le&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_percent2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_sum2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;after divide by sqrt(d_k), count of gradients that smaller than threshod({threshold}) is {g_sum2}, {g_percent2:.2f}&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;% &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*** notice that the gradient of softmax is y(1-y) ***&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;check_dotproduct_dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampling_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;시뮬레이션 결과:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;*** notice that the gradient of softmax is y(1-y) ***
size of vector d_k is 10, sampling result, dot product distribution has

 - mean: -0.0004, 
 - var: 9.9979
count of gradients that smaller than threshod(1e-10) is 193, 0.01%
after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.00% 

size of vector d_k is 100, sampling result, dot product distribution has

 - mean: -0.0028, 
 - var: 99.9868
count of gradients that smaller than threshod(1e-10) is 402283, 16.09%
after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.00% 

size of vector d_k is 1000, sampling result, dot product distribution has

 - mean: 0.0029, 
 - var: 999.6312
count of gradients that smaller than threshod(1e-10) is 1737479, 69.50%
after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.00%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;해당 모듈(Module) 코드는 &lt;a href=&quot;https://github.com/simonjisu/annotated-transformer-kr/blob/9c1e4988e5aba3d2b971074590ce49e50c3aa823/transformer/modules.py#L8&quot;&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음편: &lt;a href=&quot;https://simonjisu.github.io/paper/2020/02/02/attentionisallyouneed2.html&quot;&gt;Attention Is All You Need - 2&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Jan 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/paper/2020/01/14/attentionisallyouneed.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/paper/2020/01/14/attentionisallyouneed.html</guid>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>What is Data Science?</title>
        <description>&lt;h1 id=&quot;데이터-사이언스란&quot;&gt;데이터 사이언스란?&lt;/h1&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Joma Tech&lt;/code&gt; 라는 분의 유튜브에서 간결하고 명료하게 데이터 사이언스의 유래 및 역할에 대해 설명하여, 이를 정리하고 현재 내 상황, 그리고 나아가야할 방향에 대해 분석해보려고 한다.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/xC-c7E5PK0Y&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;2020.02.02 추가: 데이터관련 직군에 대한 &lt;a href=&quot;https://d2wahc834rj2un.cloudfront.net/Workera%20Report.pdf?fbclid=IwAR0IEBfU-7w231SNnaJFM_DYPEqJQDgOdf5_eCVs0aGsazO9XBWaVxzrbF0&quot;&gt;Report&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;데이터-사이언스와-데이터-사이언티스트의-역할&quot;&gt;데이터 사이언스와 데이터 사이언티스트의 역할&lt;/h2&gt;

&lt;p&gt;동영상에서 소개한 데이터 사이언티스트의 근본은 &lt;strong&gt;“Problem Solver”&lt;/strong&gt;다. 현실에 존재하는 풀기 어려운 문제를 데이터를 사용해서 좋은 방향으로 이끌어가는 것이 데이터 사이언티스트의 역할이다.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1Wm5n8IMK5ruCcgu-7ccUxPIH8XOuGx2N&quot; alt=&quot;출처: https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;출처: https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;위 사진은 영상에서 소개한 데이터 사이언스의 계층적 요구에 대해서 사진이다. 피라미드 꼭대기부터 정리해보면 다음과 같다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;번호&lt;/th&gt;
      &lt;th&gt;카테고리&lt;/th&gt;
      &lt;th&gt;업무&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;LEARN/OPTIMIZE&lt;/td&gt;
      &lt;td&gt;AI, Deep Learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;LEARN/OPTIMIZE&lt;/td&gt;
      &lt;td&gt;A/B Testing, Experimentation, Simple ML Algorithm&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;AGGREGATE/LABEL&lt;/td&gt;
      &lt;td&gt;Analytics, Metrics, Segments, Aggregates, Features, Training Data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;EXPLORE/TRANSFORM&lt;/td&gt;
      &lt;td&gt;Cleaning, Anomaly Detection, Prep&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;MOVE/STORE&lt;/td&gt;
      &lt;td&gt;Reliable Data Flow, Infrastructure, Pipelines, ETL(extract, transform, load), Structured and unstructured data storage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;COLLECT&lt;/td&gt;
      &lt;td&gt;Instrumentation, Logging, Sensors, External Data, User generated content&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;동영상에 따르면 스타트업 같은 경우, 리소스가 부족하기 때문에 데이터 사이언티스트는 1~6의 업무를 다 맡게 된다. 중견 기업의 경우 약간의 리소스를 더 사용하여, 데이터의 수집(6)은 소프트웨어 엔지니어가, 데이터의 보관 및 정제 준비(4, 5)는 데이터 엔지니어가, 나머지는 데이터 사이언티스트가 하게 된다. 조금더 큰 기업이라면 탑 3개의 분야를 한번 더 나눠서 데이터 분석, 평가 방법 설정 실험 등(2, 3)은 데이터 사이언스 애널리틱스, AI, Deep Learning 부분(1)의 업무는 리서치 사이언티스트 혹은 코어 데이터 사이언스가 맡게 된다.&lt;/p&gt;

&lt;p&gt;정리하면 이 분야의 직군 분류는 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;소프트웨어 엔지니어(풀스택) - Software Engineer(Full Stack)&lt;/li&gt;
  &lt;li&gt;데이터 엔지니어 - Data Engineer&lt;/li&gt;
  &lt;li&gt;데이터 사이언스 애널릭틱스 - Data Science Analytics&lt;/li&gt;
  &lt;li&gt;리서치 사이언티스트 / 코어 데이터 사이언스 - Research Data Scientist / Core Data Science&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이쯤에서 &lt;del&gt;차후에 가고 싶은&lt;/del&gt; 페이스북(Facebook)의 채용 공고를 몇개 살펴보면 그 역할이 다 다르다는 것을 확인 할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Research Scientist, Artificial Intelligence (PhD)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1D83TD6ZtJrSNwsSYNZVl86IynC0SWnuI&quot; alt=&quot;출처: https://www.facebook.com/careers/jobs/985225105171427&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;출처: https://www.facebook.com/careers/jobs/985225105171427&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Data Scientist, Analytics (PhD)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1c_DpRBhg9yAwQYE59swch4X2OFhSt4qv&quot; alt=&quot;출처: https://www.facebook.com/careers/jobs/387405225294114&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;출처: https://www.facebook.com/careers/jobs/387405225294114&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Data Engineer, Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1mabjOctjAExkzV2SLpbv7UPo6zMpu_Uu&quot; alt=&quot;출처: https://www.facebook.com/careers/jobs/997860117229177&quot; width=&quot;auto&quot; height=&quot;auto&quot; /&gt;&lt;figcaption&gt;출처: https://www.facebook.com/careers/jobs/997860117229177&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;–&lt;/p&gt;

&lt;h2 id=&quot;상황분석-및-나아가야할-방향&quot;&gt;상황분석 및 나아가야할 방향&lt;/h2&gt;

&lt;p&gt;최근 언론에서 이야기하는 부분은 극 소수인 1번 분야이고 지금까지 내가 공부한 방향은 대부분 딥러닝 쪽이었다. 그러나 더 현실적인 문제는 4, 5 번인 데이터 엔지니어 분야, 3번인 데이터 분석 쪽에 있다고 생각한다. 특정 문제를 해결하기 위해 데이터를 수집 및 정제하고, 어플리케이션으로 과정을 경험해보면 좋은 포트폴리오가 될거라고 생각했다.&lt;/p&gt;

&lt;p&gt;그렇다면 앞으로 대학원 2년간 어떤 전략을 짤 것인가?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;기존의 딥러닝 분야의 공부는 계속하되, XAI, NLP 두 분야만 집중적으로 공부한다(+ 수학 및 통계 공부는 지속).&lt;/li&gt;
  &lt;li&gt;데이터 기반 Product를 분석 및 연구
    &lt;ul&gt;
      &lt;li&gt;각 산업별로 관련 회사 리스트업&lt;/li&gt;
      &lt;li&gt;어떤 문제들이 있었고, 어떻게 해결했는지 스터디하기&lt;/li&gt;
      &lt;li&gt;가능하면 업계사람들 많이 만나보기(묻고 싶은 질문지 준비하기)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;간단한 서비스, 웹 어플리케이션을 만들고 개선함으로써 데이터 수집 및 가공하는 연습하기(팀단위로 진행 목표)
    &lt;ul&gt;
      &lt;li&gt;실존하는 해결하지 못한 문제를 찾아보고 괜찮은 서비스 기획해보기&lt;/li&gt;
      &lt;li&gt;SQL 기반의 데이터 파이프라인 설계(ETL 프로세스 설계 및 구현)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 03 Jan 2020 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/datascience/2020/01/03/whatisdatascience.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/datascience/2020/01/03/whatisdatascience.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>2019년도 회고</title>
        <description>&lt;h1 id=&quot;올해의-공부-농사&quot;&gt;올해의 공부 농사&lt;/h1&gt;

&lt;p&gt;올 한해는 작년보다 한 층 더 성장했다고 생각한다. 작년과 비교해서 어떤 목표를 달성했는지 어떤 공부를 더 했는지 GitHub에 정리를 해두었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://github.com/simonjisu/LookBack-Me&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;작년에는 NLP 공부에 매진했다면, 올해는 확률 통계의 기초를 조금더 공고히 다지고, XAI 분야과 조우했다. 물론 쉽지 않지만 앞으로 더 수요가 있는 분야일거라 생각한다. 딥러닝의 성능은 좋지만, 기계가 어떤 결정을 내리는지 아직 명백하게 밝히기 어려운 부분들이 많이 있다. 사람(Human)을 배제한 End-to-End 학습이 현업에서 딥러닝을 도입하지 못하는 이유라고 생각한다.&lt;/p&gt;

&lt;p&gt;XAI 분야는 “기계의 설명력”을 3가지 경우로 응용 할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;기계의 성능이 사람보다 못할 경우, 연구자가 개입해서 성능 부족의 부분을 찾아내는 역할&lt;/li&gt;
  &lt;li&gt;기계의 성능이 사람과 비슷한 경우, 사용자에게 신뢰와 믿음의 근거가 되는 역할&lt;/li&gt;
  &lt;li&gt;기계의 성능이 사람보다 우월한 경우, 기계가 직접 선생님이 되어 다른 기계를 가르치는 역할&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2020년도에는 이 분야를 계속해서 공부하려고 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;올해의-top-4-사건&quot;&gt;올해의 Top 4 사건&lt;/h1&gt;

&lt;p&gt;올해의 개인 Top4 사건을 추리자면 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#1-커넥트재단-퇴사-서울대-데이터-사이언스-대학원-진학&quot;&gt;커넥트재단 퇴사, 서울대 데이터 사이언스 대학원 진학&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-책-출간-및-강의&quot;&gt;책 출간 및 강의&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-글또-3기-ai-collage&quot;&gt;글또 3기, AI Collage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-home-comming-github-블로그-복귀&quot;&gt;Home Comming, GitHub 블로그 복귀&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-커넥트재단-퇴사-서울대-데이터-사이언스-대학원-진학&quot;&gt;1. 커넥트재단 퇴사, 서울대 데이터 사이언스 대학원 진학&lt;/h2&gt;

&lt;p&gt;2018년 2월부터 다니던 네이버 커넥트재단을 2019년 8월에 퇴사했다. 주로 맡은 업무는 강의 퍼블리싱 및 기획 업무였다. 개인적으로 첫 직장에서 좋은 사람들을 만나서 행운이라 생각했다. 그동안 했던 일을 살펴보면 다음과 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;퍼블리싱 업무&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.edwith.org/deepnlp&quot;&gt;딥러닝을 이용한 자연어 처리&lt;/a&gt; - 조경현 교수님&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.edwith.org/deeplearningai1&quot;&gt;DeepLearniNg AI 시리즈&lt;/a&gt; - Andrew Ng&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;프로젝트 기반 학습 콘텐츠 기획 및 매니징&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://deeplearningzerotoall.github.io/season2/&quot;&gt;모두를 위한 딥러닝 시즌 2&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.edwith.org/boostcourse-dl-tensorflow&quot;&gt;텐서플로우로 시작하는 딥러닝 기초&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.edwith.org/boostcourse-dl-pytorch&quot;&gt;파이토치로 시작하는 딥러닝 기초&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;수강신청 데이터를 잘 보기 위한 대시보드 만들기&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.notion.so/simonjisu/87f45a6c9a264f43aa53c843157026ef&quot;&gt;Notion 포스트 링크&lt;/a&gt;, 조만간 다시 이 블로그에 새로 올릴 예정&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;커넥트재단을 다니면서 좋았던 점은 해당 분야의 유명한 사람들을 만날 수 있었다는 것이다. GRU과 기계번역 Encoder-Decoder구조를 제안하신 조경현 교수님의 오프라인 강의를 현장 맨 앞줄에서 들을 수 있었고, 캐나다 Albert 대학의 &lt;a href=&quot;https://www.ualberta.ca/science/about-us/contact-us/faculty-directory/randy-goebel&quot;&gt;Randy Geobel&lt;/a&gt; 교수님을 보고 대가의 생각을 들으면서 다시 한 번 공부뽕(?!)이 차오르는 경험을 했다.&lt;/p&gt;

&lt;p&gt;막상 퇴사하려니 조금 아쉽기도 했지만, 내가 하고싶은 공부를 하기위해 떠났다. 그리하여 올해 하반기에는 대학원에 진학하려고 부단히 노력했다. 최근에 이 분야로 진출하려는 사람들이 많아져 컨택이 쉽지는 않았다(누가 나이 30 대학원생을 받아주려나…). 다행이 이번에 신설된 서울대 데이터 사이언스 대학원에 합격하여 내년부터 하고싶은 공부를 하게 됐다. 앞으로 2년 동안 큰 목표는 다음과 같이 세웠다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;데이터 수집단계 공부를 집중적으로 한다.&lt;/li&gt;
  &lt;li&gt;NLP, 금융 두 개의 도메인에서 XAI를 적용한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;전에 다니던 회사는 비교적 작은 규모의 회사였고, 데이터팀 같은건 없었다. 수강신청 대시보드도 사실 내가 보고싶어서 혼자 만든 작은 프로젝트다. 사람들이 언제 학습을 주로하고 어떤 콘텐츠를 주로 공부하는지 살펴볼 수 있는지 궁금했기 때문이다.&lt;/p&gt;

&lt;p&gt;대부분의 기업에서는 데이터를 수집하는 pipeline이 제대로 갖춰저 있지 않기 때문에, 데이터 수집단계에서 잘 설계해야 원하는 분석을 진행하고, 데이터에서 얻은 인사이트로 의사결정을 할 수 있다. 따라서 첫번째 목표는 데이터 수집을 잘 하기 위한 공부를 할 것이다.&lt;/p&gt;

&lt;p&gt;두번째 목표는 지금까지 공부한 분야에서 최근에 공부한 XAI 분야를 접목하려고 한다. 기계의 의사결정을 잘 설명할 수 있는 모델을 만들어 사람이 보고 기계를 신뢰할 수 있게 만들어 보려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;2-책-출간-및-강의&quot;&gt;2. 책 출간 및 강의&lt;/h2&gt;

&lt;p&gt;2019년도 7월말에 내가 쓴 책 &amp;lt;딥러닝에 목마른 사람들을 위한 PyTorch&amp;gt;가 발간됐다. 책 내용은 집에서 GPU서버를 만드는 법 부터 시작해서, PyTorch로 딥러닝에 입문하는 것이다. 책을 쓰는 작업이 정말 어렵다는 것을 이번 경험으로 부터 깨달았다. 부족한 부분도 많아서 다음에는 조금더 알찬 내용으로 구성해서 다시 gitbook 형태로 써볼까 생각중이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://bit.ly/딥목파&quot;&gt;&amp;lt;딥러닝에 목마른 사람들을 위한 PyTorch&amp;gt; 홈페이지 바로가기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2019년도 말에는 멀티캠퍼스에서 책 내용을 기반으로 강의도 진행했다. 처음해보는 강의라 목소리도 많이 쉬었었다. 사람이 말을 이렇게나 많이 할 수도 있구나 생각했다… 그래도 최선을 다해서 개념 하나하나 설명해드리고 자료도 충분히 많이 준비했다고 생각한다. 3일짜리 강의라 처음 배우시는 분들이 개념을 받아들이기 쉽지는 않았을 거라 생각한다. 다음에 기회가 된다면 책처럼 만들어서 배포해보는 것도 나쁘지 않을 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;3-글또-3기-ai-collage&quot;&gt;3. 글또 3기, AI Collage&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;글또&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2019년도 여름부터 겨울까지 글또 3기에 참가했다. 역시 강제로 due를 정하더니 글을 쓰게 되더라. 이런 규제는 자신에게 긍정적인 효과가 있다고 생각한다. 그래도 초중반까지 꾸준히 글을 써왔는데 후반부에는 바쁘다는 핑계로 계속 안쓰게 됐다. 반성하자. 다음 기수에도 참가할 생각이다. 그때는 정말 한번도 놓치지 않고 예치금을 다 돌려 받으리라.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.facebook.com/groups/375431516259701/&quot;&gt;글또 페이스북 바로가기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;AI Collage&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;또 하나 새로 참가한 것은 모두의 연구소에서 진행하는 AI Collage다. AI Collage는 현실에 있는 6개 분야의 문제를 풀어보고, 최종단계에서는 논문을 써보는 장기 프로젝트다. XAI 팀에 합류하여 여러가지 논문을 읽고 구현을 진행하고 있다. 내년 5월까지 진행되는데 아직 여러 논문을 구현 중에 있다. 내년에는 꼭 논문을 써보려고 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/simonjisu/XAI&quot;&gt;XAI GitHub&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://aic.yangjaehub.com/&quot;&gt;AIC 바로가기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-home-comming-github-블로그-복귀&quot;&gt;4. Home Comming, GitHub 블로그 복귀&lt;/h2&gt;

&lt;p&gt;지난 1년간 Notion을 활용해서 블로그를 만드려고 했다. 깔끔하고 페이지를 정리하는데 좋은 기능들이 많기 때문이다. 그러나 블로그로 활용하기에 부족한 부분이 2가지 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Inline Math 의 부재&lt;/li&gt;
  &lt;li&gt;Google Analytics 의 부재&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Inline Math를 사용할 수 없는 것은 큰 타격이었다. 보통 논문을 보면 수식이 정말 많은데 이를 사용할 수 없다 점은 너무 불편했다. 한줄 짜리 간단한 수식조차 Math Block 으로 만들어야 했기 때문에, 글의 구조를 짜는데 방해가 됐다. Notion 공식 트위터에 많은 사람들이 요청중이긴 하나 언제 개발될지는 아직 미지수다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/notionhq/status/1093334827770699778&quot;&gt;Notion 공식 트위터 Inline Math 요청 바로가기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 GA를 달수 없어서 내 블로그의 유입을 파악할 수가 없었다. 작은 이미지 패킷으로 페이지 조회수를 확인 할 수 있지만, 그럴려면 모든 페이지에 패킷을 달아야하는데 여간 귀찮은 작업이 아니다.&lt;/p&gt;

&lt;p&gt;그래서 다시 복귀했다. 이번달에 블로그 스킨을 다시 깔끔하게 바꾸고 우측의 custom 메뉴바를 으로 2일동안 만들었다. 그렇다고 Notion 이 안좋은 것은 아니다. 빠르게 메모나 기록할 때, 여행 계획을 만들때 이만큼 유용한 도구는 없다. 다만 Notion 기술 블로그를 만드려고 하면 굉장히 비추천한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2020년도-목표&quot;&gt;2020년도 목표&lt;/h1&gt;

&lt;p&gt;2020년이 코앞으로 다가왔다. 내년에는 다음과 같은 목표를 세웠다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PRML 공부 계속하기&lt;/li&gt;
  &lt;li&gt;더 많은 논문을 읽고 블로그에 정리하기&lt;/li&gt;
  &lt;li&gt;글또 4기에 참가하기&lt;/li&gt;
  &lt;li&gt;AI Collage 에서 논문 한편 쓰기&lt;/li&gt;
  &lt;li&gt;데이터 프로세싱에 대한 공부하기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이중 얼만큼 달성할지 모르겠지만, 노력하여 올해보다 더 성장할 수 있는 한해가 되기를 …&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1pCe5D4-wrxlP4v1CqRx_u-_CeGZToDDJ&quot; width=&quot;240px&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;P.S. 닌텐도 스위치도 사자&lt;/del&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Dec 2019 14:19:38 +0900</pubDate>
        <link>https://simonjisu.github.io/others/2019/12/30/lookback2019.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/others/2019/12/30/lookback2019.html</guid>
        
        
        <category>others</category>
        
      </item>
    
      <item>
        <title>[PyTorch] ConvTranspose2d 와 Conv2d 의 관계</title>
        <description>&lt;p&gt;최근 XAI 에 관련된 공부를 하면서 비전쪽의 많은 논문을 살펴보고 있다. “Visualizing and Understanding Convolutional Networks (2013)” 논문에서는 이미지 처리에서는 CNN 알고리즘이 제일 좋지만, 그 이유에 대해서 탐구를 시도한 논문이다. 도대체 Convolution의 필터가 어떤 역할을 하는지, 이들이 어떤 부분을 살펴보는 지를 확인한다. 오늘은 이 논문에서 제안하는 Deconvolutional layers(정확히는 Fractionally-strided convolution 이지만 차후에 언급한다)의 실체를 낱낱이 살펴보도록 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;link: &lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;Visualizing and Understanding Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;convolution-layer&quot;&gt;Convolution layer&lt;/h1&gt;

&lt;p&gt;Deconvolutional layer을 알아보기 전에 합성곱 연산(Convolutional Operation)에 대해 알아볼 필요가 있다. 합성곱 연산은 필터가 조금씩 이동하면서 이미지의 일부와 필터간 연산을 통해 진행된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=17x4ZQ_r0FTa_mlDFiIWvMJcg22vRrBd6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반적으로 CNN 알고리즘은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Convolution&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Activation&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Maxpooling&lt;/code&gt; 과정을 거친다. 예를 들어, 위 그림과 같이 4x4 이미지는 3x3 필터를 통해 2x2 의 선형변환 값을 갖는다(padding=0, stride=1 인 경우). 그리고 활성화 함수를 통과한 뒤에 Maxpool 과정을 거친다. 이때 &lt;code class=&quot;highlighter-rouge&quot;&gt;Convolution&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Activation&lt;/code&gt;을 거쳤을 때 나오는 텐서를 Activation Map이라고 하고, &lt;code class=&quot;highlighter-rouge&quot;&gt;Pooling&lt;/code&gt; 과정을 거쳤을 때 나오는 텐서를 Pooled Map 이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1Y4kIqXn7vUYQgoZWDdprrO-SP9a-Qogs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 논문에서는 그 과정을 역으로 한번 해보는 것을 제안했다. 위 그림처럼 마지막 Pooled Maps 에서 풀링된 위치를 기억했다가(Max Locations “Switches” 부분), 이 위치를 기반으로 역으로 Unpooled Maps 를 재구축한다(이 부분에 관심있는 분들은 이 논문을 한번 살펴보는 것을 추천드린다). 이번 글에서는 그 다음 스텝인 Convolution layer 에서 역으로 돌아가는 방법에 대해서 설명하려고 한다.&lt;/p&gt;

&lt;p&gt;먼저 이미지의 크기를 $N$, 필터(커널)의 크기를 $K$, 패딩의 크기를 $P$, 스트라이드를 $S$ 라고 정의하고, 여러 변수를 정의 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{input image size} &amp;= N \times N =4 \times 4 \\ \text{filter size} &amp;= K\times K = 3 \times 3 \\ \text{padding} &amp;= P = 0 \\ \text{stride} &amp;= S = 1 \\ \text{output image size} &amp;= (\dfrac{N+2P-K}{S}+1, \dfrac{N+2P-K}{S}+1) \\&amp;= 2 \times 2 \\ \text{input image} &amp;: X^{(l)} = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp;x_{14}\\  x_{21} &amp; x_{22} &amp; x_{23} &amp;x_{24}\\ x_{31} &amp; x_{32} &amp; x_{33} &amp;x_{34}\\ x_{41} &amp; x_{42} &amp; x_{43} &amp;x_{44}\end{bmatrix}^{(l)} \\ \text{output image} &amp;: x^{(l+1)} =\begin{bmatrix} x_{11} &amp; x_{12}\\ x_{21} &amp; x_{22} \end{bmatrix}^{(l+1)} \\ \text{filter} &amp;: W = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13}\\  w_{21} &amp; w_{22} &amp; w_{23}\\ w_{31} &amp; w_{32} &amp; w_{33}\end{bmatrix} \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;이제 수식으로 합성곱 연산을 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} x_{pq}^{(l+1)} &amp;= \sum_{p=i}^{K+i-1} \sum_{q=j}^{K+j-1} w_{pq} x_{pq}^{(l)} \quad \text{for }i, j \in (1, 2, \cdots,  N-K+1)\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;위 수식으로는 어려워 보이지만 아래와 같은 연산을 &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; 라고 하면 결과는 2x2 행렬이 출력되며 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} X^{(l+1)} &amp;= X^{(l)}*W\\&amp;=\begin{bmatrix}w_{11} x^{(l)}_{11} + w_{12} x^{(l)}_{12} + w_{13} x^{(l)}_{13} + w_{21} x^{(l)}_{21} + w_{22} x^{(l)}_{22} + w_{23} x^{(l)}_{23} + w_{31} x^{(l)}_{31} + w_{32} x^{(l)}_{32} + w_{33} x^{(l)}_{33} &amp; w_{11} x^{(l)}_{12} + w_{12} x^{(l)}_{13} + w_{13} x^{(l)}_{14} + w_{21} x^{(l)}_{22} + w_{22} x^{(l)}_{23} + w_{23} x^{(l)}_{24} + w_{31} x^{(l)}_{32} + w_{32} x^{(l)}_{33} + w_{33} x^{(l)}_{34}\\ w_{11} x^{(l)}_{21} + w_{12} x^{(l)}_{22} + w_{13} x^{(l)}_{23} + w_{21} x^{(l)}_{31} + w_{22} x^{(l)}_{32} + w_{23} x^{(l)}_{33} + w_{31} x^{(l)}_{41} + w_{32} x^{(l)}_{42} + w_{33} x^{(l)}_{43} &amp; w_{11} x^{(l)}_{22} + w_{12} x^{(l)}_{23} + w_{13} x^{(l)}_{24} + w_{21} x^{(l)}_{32} + w_{22} x^{(l)}_{33} + w_{23} x^{(l)}_{34} + w_{31} x^{(l)}_{42} + w_{32} x^{(l)}_{43} + w_{33} x^{(l)}_{44}\end{bmatrix} \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;파이토치에서 Convolution Layer 는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv2d&lt;/code&gt; 로 구현되어 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;link: &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#conv2d&quot;&gt;torch.nn.Conv2d - PyTorch master documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;deconvolution-layer-transposed-convolution-layer&quot;&gt;Deconvolution Layer? Transposed Convolution Layer!&lt;/h1&gt;

&lt;p&gt;저자는 이미 2011 년도에 Deconvolution Layer 를 제안했다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;link: &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.849.3679&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Adaptive Deconvolutional Networks for Mid and High Level Feature Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;간단하게 생각해보면 다음 그림과 같이 필터를 이동시키면서 원래 4x4 이미지(초록색)를 복원하면 될것 같다. 이 과정이 맞는지 이후에 살펴볼 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1R-C4g1zSpculTzC8w00IrM9CNM0vifN_&quot; /&gt;&lt;/p&gt;

&lt;p&gt;흥미로운 것은 파이토치에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;ConvTranspose2d&lt;/code&gt; 라고 구현이 되어 있다. 그리고 다음과 같은 설명이 덧붙여져 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This module can be seen as the gradient of &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv2d&lt;/code&gt; with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;link: &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#convtranspose2d&quot;&gt;torch.nn.ConvTranspose2d - PyTorch master documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;왜 이름이 Deconvolution 이 아닐까? 읽어보면 이 연산은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv2d&lt;/code&gt;의 출력을 입력에 대해 미분을 연산하는 것과 같다고 한다. 미분을 한번 구해보고 이를 C 라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} C = \dfrac{\partial X^{(l+1)}}{\partial X^{(l)}}  &amp;= \dfrac{\partial Vec(X^{(l+1)})}{\partial Vec(X^{(l)})} \\ &amp;= \begin{bmatrix}  \dfrac{\partial x_{11}^{(l+1)}}{\partial x_{11}^{(l)}} &amp; \dfrac{\partial x_{12}^{(l+1)}}{\partial x_{11}^{(l)}} &amp; \dfrac{\partial x_{13}^{(l+1)}}{\partial x_{11}^{(l)}} &amp; \dfrac{\partial x_{14}^{(l+1)}}{\partial x_{11}^{(l)}} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ \dfrac{\partial x_{11}^{(l+1)}}{\partial x_{44}^{(l)}} &amp; \dfrac{\partial x_{12}^{(l+1)}}{\partial x_{44}^{(l)}} &amp; \dfrac{\partial x_{13}^{(l+1)}}{\partial x_{44}^{(l)}} &amp; \dfrac{\partial x_{14}^{(l+1)}}{\partial x_{44}^{(l)}} \end{bmatrix} \\ &amp; = \begin{bmatrix}w_{11} &amp; 0 &amp; 0 &amp; 0\\w_{12} &amp; w_{11} &amp; 0 &amp; 0\\w_{13} &amp; w_{12} &amp; 0 &amp; 0\\0 &amp; w_{13} &amp; 0 &amp; 0\\w_{21} &amp; 0 &amp; w_{11} &amp; 0\\w_{22} &amp; w_{21} &amp; w_{12} &amp; w_{11}\\w_{23} &amp; w_{22} &amp; w_{13} &amp; w_{12}\\0 &amp; w_{23} &amp; 0 &amp; w_{13}\\w_{31} &amp; 0 &amp; w_{21} &amp; 0\\w_{32} &amp; w_{31} &amp; w_{22} &amp; w_{21}\\w_{33} &amp; w_{32} &amp; w_{23} &amp; w_{22}\\0 &amp; w_{33} &amp; 0 &amp; w_{23}\\0 &amp; 0 &amp; w_{31} &amp; 0\\0 &amp; 0 &amp; w_{32} &amp; w_{31}\\0 &amp; 0 &amp; w_{33} &amp; w_{32}\\0 &amp; 0 &amp; 0 &amp; w_{33}\end{bmatrix} \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;이 미분 과정을 파이썬의 &lt;code class=&quot;highlighter-rouge&quot;&gt;sympy&lt;/code&gt; 패키지로 쉽게 만들 수 있다(Jupyter Notebook 에서 사용하길 권장). 다음 코드에서 C 매트릭스를 살펴보면 위와 같은 결과를 얻을 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sympy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatrixSymbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 노트북에서 수학식의 LaTeX 표현 사용
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sympy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_printing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use_latex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mathjax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;convolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply_elementwise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatrixSymbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x^{(l)}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatrixSymbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x^{(l+1)}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatrixSymbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;w&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Convolution Output
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate derivatives &amp;amp; get matrix C
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이 C 행렬은 재밌는 특징을 가진다. 매트릭스 형태의 입력 데이터를 한줄로 핀 후에 매트릭스 연산을 하고, 다시 형태를 변환 시켜주면 Convolution 의 출력값이 나온다. 정말 맞는지 살펴보기 위해서 다음 코드를 실행해보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# forward (1, 16) x (16, 4) = (1, 4) = (2, 2)
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;반대로 출력값을 한줄로 피고 C의 전치행렬과 곱한 후 다시 형태를 변환 시켜주면 입력과 다른 행렬이 나온다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# backward
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;실행하면 다음과 같은 행렬이 나오는데 이 연산 과정을 Deconvolution 연산, 정확히는 &lt;strong&gt;Fractionally-strided convolution&lt;/strong&gt; 혹은 &lt;strong&gt;Transpose convolution&lt;/strong&gt; 이라고 한다. 어떻게 계산된 것이며 어떤 뜻일까?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}w_{11} x^{(l+1)}_{11} &amp; w_{11} x^{(l+1)}_{12} + w_{12} x^{(l+1)}_{11} &amp; w_{12} x^{(l+1)}_{12} + w_{13} x^{(l+1)}_{11} &amp; w_{13} x^{(l+1)}_{12}\\w_{11} x^{(l+1)}_{21} + w_{21} x^{(l+1)}_{11} &amp; w_{11} x^{(l+1)}_{22} + w_{12} x^{(l+1)}_{21} + w_{21} x^{(l+1)}_{12} + w_{22} x^{(l+1)}_{11} &amp; w_{12} x^{(l+1)}_{22} + w_{13} x^{(l+1)}_{21} + w_{22} x^{(l+1)}_{12} + w_{23} x^{(l+1)}_{11} &amp; w_{13} x^{(l+1)}_{22} + w_{23} x^{(l+1)}_{12}\\w_{21} x^{(l+1)}_{21} + w_{31} x^{(l+1)}_{11} &amp; w_{21} x^{(l+1)}_{22} + w_{22} x^{(l+1)}_{21} + w_{31} x^{(l+1)}_{12} + w_{32} x^{(l+1)}_{11} &amp; w_{22} x^{(l+1)}_{22} + w_{23} x^{(l+1)}_{21} + w_{32} x^{(l+1)}_{12} + w_{33} x^{(l+1)}_{11} &amp; w_{23} x^{(l+1)}_{22} + w_{33} x^{(l+1)}_{12}\\w_{31} x^{(l+1)}_{21} &amp; w_{31} x^{(l+1)}_{22} + w_{32} x^{(l+1)}_{21} &amp; w_{32} x^{(l+1)}_{22} + w_{33} x^{(l+1)}_{21} &amp; w_{33} x^{(l+1)}_{22}\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;출력을 계산하는 Convolution 연산 과정에서 &lt;strong&gt;“입력 픽셀”&lt;/strong&gt; 에서 &lt;strong&gt;“출력 픽셀”&lt;/strong&gt; 과 연결된 가중치를 생각하면 편하다. 다음 그림을 살펴보면, 필터(노란색)가 지나가면서, &lt;strong&gt;“입력 픽셀”&lt;/strong&gt; ($x_{12}^{(l)}$)과 &lt;strong&gt;“출력 픽셀”&lt;/strong&gt; ($x_{11}^{(l+1)}$, $x_{12}^{(l+1)}$)사이에 연결된 두 개의 가중치 ($w_{11}$, $w_{12}$)를 통해 연산이 된다. 위 행렬에서 1행 2열에 있는 원소 값과 연관이 있는 것을 확인 할 수 있는데, fractionally-strided convolution 연산은 바로 &lt;strong&gt;“출력 픽셀”&lt;/strong&gt; 에서 &lt;strong&gt;“입력 픽셀”&lt;/strong&gt; 로 방향을 바꿔 연산하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1acZ6YvrW6xooXJd6nYpFYhm-eDHSe2f1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fractionally-strided convolution 연산은 다음 그림과 같다. “fractionally” 의 단어 뜻 처럼 필터가 출력 이미지의 일부분을 걸치면서 이동(stride)하면서 연산된다. 또한 가중치도 기존의 형태와 달리 약간의 변형(transpose)이 된다(정확한 전치행렬은 아니다). 그렇다면 “출력 픽셀”과 연관이 없는 부분은? 0으로 곱해져서 더해진다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?id=1WumIP2aCDNJ4cCWQW_2_Q0e1LCx_WkdQ&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서 다시 정리하면 Convolution Layer의 출력을 입력에 대한 미분을 구해서(C 행렬), 이를 한줄로 편 출력과 곱한 후에 형태를 입력 이미지로 변환해주는 것이 Convolution 의 반대 연산인 Fractionally-strided convolution 이다. 수식으로 다음과 같이 정리 할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X^{(l)} = [Vec\big(X^{(l+1)}\big)C^T]^{(N)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$^{(N)}$은 Vector Transpose이며, 이는 다음 노트북을 살펴보자. &lt;a href=&quot;https://nbviewer.jupyter.org/github/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/04_Backpropagation_Matrix_diff.ipynb&quot;&gt;Vector Transpose&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;additional-reference&quot;&gt;Additional Reference&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.07285&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 21:56:38 +0900</pubDate>
        <link>https://simonjisu.github.io/datascience/2019/10/27/convtranspose2d.html</link>
        <guid isPermaLink="true">https://simonjisu.github.io/datascience/2019/10/27/convtranspose2d.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
  </channel>
</rss>
