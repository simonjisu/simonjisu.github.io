<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soo</title>
    <description>My Blog
</description>
    <link>http://simonjisu.github.io/</link>
    <atom:link href="http://simonjisu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 08 Dec 2017 16:27:31 +0900</pubDate>
    <lastBuildDate>Fri, 08 Dec 2017 16:27:31 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>NUMPY with NN - 2</title>
        <description>&lt;h1 id=&quot;numpy--neural-network-basic---2&quot;&gt;Numpy로 짜보는 Neural Network Basic - 2&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;저번 시간에는 Neural Network의 기본이 되었던 XOR 문제를 풀어보았다. 그 과정에서 Feedforward 퍼셉트론이라는 개념이 등장했고 오늘은 Feedforward 과정이 어떻게 진행되는지 알아보자.&lt;/p&gt;

&lt;h2 id=&quot;activation-function&quot;&gt;활성화 함수(Activation Function)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/NN_1.jpg&quot; alt=&quot;Drawing&quot; style=&quot;width: 350px;&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = b + w_1x_1 + w_2x_2\\
y = h(a)&lt;/script&gt;

&lt;p&gt;단일 퍼셉트론의 과정을 담은 그림과 수식이다. 각 뉴런(동그라미들)에서 다음 층의 뉴런(오른쪽 큰 동그라미)로 진행하는데 우선 각 $x$ 를 가중합 $a$ 를 구하고, 그 합을 다시 어떤 함수 $h$ 를 거쳐 Output인 $y$ 가 나오게 된다.&lt;/p&gt;

&lt;p&gt;저번 시간에 이야기 했던 AND를 적용해보면 $h$ 는 0과 1을 반환하는 아래와 같은 함수일 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = h(a) =
  \begin{cases}
  0\ \ (a = b + w_1x_1 + w_2x_2 \leq 0) \\
  1\ \ (a = b + w_1x_1 + w_2x_2 &gt; 0) \\
  \end{cases}&lt;/script&gt;

&lt;p&gt;이런 $h$ 함수를 &lt;strong&gt;활성화 함수&lt;/strong&gt; 라고 부르며, 보통 비선형 함수를 쓴다.&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;왜 비선형을 쓰는가?&lt;/h3&gt;
&lt;p&gt;우리는 활성화 함수를 예측 불가능한 함수로 만들어야 하기 때문인데 비선형함수가 적합하기 때문이다. 선형함수의 특징은 더 해도 선형이라는 것인데, 예를 들어&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=3x&lt;/script&gt;

&lt;p&gt;라는 선형 함수와 간단한 산수인 $1+5=6$ 이란 것을 생각해보자. 좌변에 $1$과 $5$를 선형함수에 넣어서 더한 값인 $3 + 15 = 18$ 이란 값을 우리는 우변의 $6$ 을 선형함수에 넣었을 때 값이랑 같다는 것을 충분히 알 수 있다. 이를 “&lt;strong&gt;예측&lt;/strong&gt; 할 수 있다.” 라고 이야기 한다. 이처럼 두 개의 선형함수를 더하면 선형이 된다는 것이다.&lt;/p&gt;

&lt;p&gt;하지만 어떤 비선형 함수&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y=3x^2&lt;/script&gt;
로 아까의 과정을 똑같이 해보자, 좌변의 값을 넣어서 더하면 $3 + 75 = 78$ 인데, 우변의 값을 넣으면 $108$ 이 된다. 따라서 비선형 함수는 더해도 같은 비선형이 아니며 다른 값이 나오기 때문에 예측 불가능하다 라고 말 할 수 있다.&lt;/p&gt;

&lt;p&gt;비선형 함수를 통과함으로서 뉴런이 &lt;strong&gt;활성화&lt;/strong&gt; 된다라고 이야기 한다.&lt;/p&gt;

&lt;p&gt;또 한 가지 이유를 들자면, 선형일 경우에 여러 층을 쌓는 이유가 없어진다. 만약에 $h=c\cdot a$ 가 선형인 함수 였다면 여러 층을 거치게 되면 $h(h(h(a))) = c\cdot c\cdot c\cdot a$ 인데 이는 결국 $b\cdot a=c^3\cdot a$ 라는 선형함수로 바꿀 수 있어서 여전히 예측 가능하기 때문이다.&lt;/p&gt;

&lt;p&gt;더 자세한 비선형함수와 선형함수의 차이는 링크의 블로그를 참조해보자 [&lt;a href=&quot;http://sdolnote.tistory.com/entry/LinearityNonlinearityFunction&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크&lt;/span&gt;&lt;/a&gt;]&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;자주 쓰는 활성화 함수들&lt;/h3&gt;

&lt;h4 id=&quot;step-function&quot;&gt;계단 함수(Step Function)&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x) =
  \begin{cases}
  1\ \ (x &gt; 0) \\
  0\ \ (x  \leq 0) \\
  \end{cases}&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def step_function(x):
    y = x &amp;gt; 0
    return y.astype(np.int)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/step.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 350px;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;sigmoid-function&quot;&gt;시그모이드 함수(Sigmoid Function)&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x) = \frac{1}{1+exp(-x)}&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def sigmoid(x):
    return 1 / (1 + np.exp(-x))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/sigmoid.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 350px;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;relu-relu-function&quot;&gt;ReLu 함수(ReLu Function)&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x) =
  \begin{cases}
  x\ \ (x &gt; 0) \\
  0\ \ (x \leq 0) \\
  \end{cases}&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def ReLu(x):
    return np.maximum(0, x)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/relu.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 350px;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;feedforward-&quot;&gt;Feedforward 과정&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/nn/NN_2.jpg&quot; alt=&quot;Drawing&quot; style=&quot;width: 500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(사진출처: 밑바닥부터 시작하는 딥러닝)&lt;/p&gt;

&lt;p&gt;위와 같은 Neural Network의 과정을 한번 살펴 보자, 여러개의 Perceptron을 쌓으면 이런 모양이 나오는 것을 알 수 있다. 여기서 제일 왼쪽에 있는 $x_1, x_2$ 2개의 뉴런을 한 층으로 보며, 이를 입력층(Input Layer)라고 한다. (1 이란 뉴런은 매번 뉴런을 거칠 때마다 더 해주는 숫자기 때문에 앞으로도 한번만 쓰도록 한다.) 마찬가지로 중간에 두 개의 층을 은닉층(Hidden Layer)이라고 하며 마지막을 출력층(Output Layer)이라고 한다. 보통 입력층은 갯수로 안세며 위 그림은 총 3층인 Neural Network 라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;단계별로 살펴보자.&lt;/p&gt;

&lt;h3 id=&quot;input-rightarrow-hidden-1&quot;&gt;Input $\rightarrow$ Hidden 1&lt;/h3&gt;

&lt;p&gt;Input에서 Hidden1 층으로 가는 과정을 행렬로 표시해볼 것이다.&lt;/p&gt;

&lt;p&gt;가중치 $w$ 의 표기법은 $w_{오른쪽\ 뉴런위치,\ 왼쪽\ 뉴런위치}^{몇번째\ 층}$ 로써, 첫번째 층에서 입력층 $x_1$ 뉴런에서 히든층1 $a_2^{(1)}$ 방향인 가중치는 $w_{21}^{(1)}$ 이라고 표기합니다.&lt;/p&gt;

&lt;p&gt;따라서 각각의 가중치 합을 구하면,&lt;/p&gt;

&lt;p&gt;$A = \begin{bmatrix}
    a_1^{(1)} \newline
    a_2^{(1)} \newline
    a_3^{(1)}
    \end{bmatrix} =
    \begin{bmatrix}
    w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)} \newline
    w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)} \newline
    w_{31}^{(1)}x_1 + w_{32}^{(1)}x_2 + b_3^{(1)}
    \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;가 되는데, 이를 다시 간단하게 쓰면&lt;/p&gt;

&lt;p&gt;$X =
  \begin{bmatrix}
  x_1 \newline
  x_2
  \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;$W^{(1)} =
      \begin{bmatrix}
      w_{11}^{(1)} &amp;amp; w_{12}^{(1)} \newline
      w_{21}^{(1)} &amp;amp; w_{22}^{(1)} \newline
      w_{31}^{(1)} &amp;amp; w_{32}^{(1)}
      \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;$B^{(1)} =
  \begin{bmatrix}
  b_1^{(1)} \newline
  b_2^{(1)} \newline
  b_3^{(1)}
  \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;$A = W^{(1)} \cdot X + B^{(1)}$ 가 되며, 형태는 $(3, 1) = (3, 2) \times (2, 1) + (3, 1)$ 로 된다. 이는 간단한 내적 연산으로 구할 수 있게 된다.&lt;/p&gt;

&lt;p&gt;가중치의 합을 구하면 이제 비활성함수에 대입해서 뉴런을 활성화 시킨다.&lt;/p&gt;

&lt;p&gt;$Z^{(1)} =
    \begin{bmatrix}
    z_1^{(1)} \newline
    z_2^{(1)} \newline
    z_3^{(1)}
    \end{bmatrix} =
    \begin{bmatrix}
    h(a_1^{(1)}) \newline
    h(a_2^{(1)}) \newline
    h(a_3^{(1)})
    \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;이렇게 나온 $Z^{(1)}$ 값들은 다음 층에서 입력으로 쓰이게 된다.&lt;/p&gt;

&lt;p&gt;아래 코드의 Shape도 같이 잘 살펴보자.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.2],
               [0.3, 0.4],
               [0.5, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print('X:', X.shape)
print('W1:', W1.shape)
print('B1:', B1.shape)
# Input -&amp;gt; Hidden 1
print('=================')
print('Input -&amp;gt; Hidden1')
print('=================')
# linear sum
A1 = np.dot(W1, X) + B1
print('A1:', A1.shape)
print(A1)
# activation
Z1 = sigmoid(A1)
print('Z1:', Z1.shape)
print(Z1)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;W1: (3, 2)&lt;/p&gt;

  &lt;p&gt;B1: (3,)&lt;/p&gt;

  &lt;p&gt;=================&lt;/p&gt;

  &lt;p&gt;Input -&amp;gt; Hidden1&lt;/p&gt;

  &lt;p&gt;=================&lt;/p&gt;

  &lt;p&gt;A1: (3,)&lt;/p&gt;

  &lt;p&gt;[ 0.3  0.7  1.1]&lt;/p&gt;

  &lt;p&gt;Z1: (3,)&lt;/p&gt;

  &lt;p&gt;[ 0.57444252  0.66818777  0.75026011]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;hidden-1-rightarrow-hidden-2&quot;&gt;Hidden 1 $\rightarrow$ Hidden 2&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;W2 = np.array([[0.1, 0.2, 0.3],
               [0.4, 0.5, 0.6]])
B2 = np.array([0.1, 0.2])

print('Z1:', Z1.shape)
print('W2:', W2.shape)
print('B2:', B2.shape)
# Hidden 1 -&amp;gt; Hidden 2
print('=================')
print('Hidden 1 -&amp;gt; Hidden 2')
print('=================')
# linear sum
A2 = np.dot(W2, Z1) + B2
print('A2:', A2.shape)
print(A2)
# activation
Z2 = sigmoid(A2)
print('Z2:', Z2.shape)
print(Z2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Z1: (3,)&lt;/p&gt;

  &lt;p&gt;W2: (2, 3)&lt;/p&gt;

  &lt;p&gt;B2: (2,)&lt;/p&gt;

  &lt;p&gt;=================&lt;/p&gt;

  &lt;p&gt;Hidden 1 -&amp;gt; Hidden 2&lt;/p&gt;

  &lt;p&gt;=================
A2: (2,)&lt;/p&gt;

  &lt;p&gt;[ 0.51615984  1.21402696]&lt;/p&gt;

  &lt;p&gt;Z2: (2,)&lt;/p&gt;

  &lt;p&gt;[ 0.62624937  0.7710107 ]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;hidden-2-rightarrow-output&quot;&gt;Hidden 2 $\rightarrow$ Output&lt;/h3&gt;

&lt;p&gt;마지막 출력 층에서는 이전 층에 출력된 $Z$ 값들을 그대로 가져올 수 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def identity_function(x):
    return x

W3 = np.array([[0.1, 0.2],
               [0.3, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(W3, Z2) + B3
Y = identity_function(A3)
print(Y)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;[ 0.31682708  0.69627909]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;혹은 Softmax라는 함수를 써서 각 Output의 확률로서 나타낼 수 있다. 보통을 이걸 쓴다.&lt;/p&gt;

&lt;h4 id=&quot;softmax&quot;&gt;Softmax&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_k = \frac{exp(a_k)}{\sum_{i=1}^{n}{exp(a_i)}}&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def softmax(x):
    return np.exp(x) / np.sum(np.exp(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;얼핏 잘 만든 것 같지만 컴퓨터에서 아주 큰 수를 계산시 Overflow문제가 발생한다. 오버플로우란, 사용 가능한 하드웨어(즉, 32bit 단위 워드의 하드웨어, 레지스터 등)로 연산 결과를 표현할 수 없을 때 오버플로우가 발생한다고 한다. (오버플로우 개념 출처: [&lt;a href=&quot;https://m.blog.naver.com/PostView.nhn?blogId=osw5144&amp;amp;logNo=120206206420&amp;amp;proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크&lt;/span&gt;&lt;/a&gt;])&lt;/p&gt;

&lt;p&gt;간단히 예를 들어보면 아래의 코드를 실행해보면 금방 알 수 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a = np.array([1010, 1000, 990])
softmax(a)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;/Users/user/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp
from ipykernel import kernelapp as app&lt;/p&gt;

  &lt;p&gt;/Users/user/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: RuntimeWarning: invalid value encountered in true_divide
  from ipykernel import kernelapp as app&lt;/p&gt;

  &lt;p&gt;array([ nan,  nan,  nan])&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;경고가 뜨면서 NaN 값들만 나온다. 이를 방지하기 위해서 입력 신호 중 최대값을 이용하는게 일반적이다. 아래는 분모, 분자 변수에 어떤 상수 C’를 더해도 결국엔 Softmax가 되는 것을 증명 한 식이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_k = \frac{exp(a_k)}{\sum_{i=1}^{n}{exp(a_i)}}
\\ = \frac{Cexp(a_k)}{C\sum_{i=1}^{n}{exp(a_i)}}
\\ = \frac{exp(a_k+\log{C})}{\sum_{i=1}^{n}{exp(a_i+\log{C})}}
\\ = \frac{exp(a_k+C^{'})}{\sum_{i=1}^{n}{exp(a_i+C^{'})}}&lt;/script&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;c = np.max(a)
print(a - c)
print(softmax(a-c))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;[  0 -10 -20]&lt;/p&gt;

  &lt;p&gt;[  9.99954600e-01   4.53978686e-05   2.06106005e-09]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이번에는 경고 없이 실행이 잘 된다. 이제 최종 Softmax는 아래와 같다. 이를 출력층에 적용하면 y값에 대한 확률을 볼 수 있다. 이를 0과 1사이의 값으로 만드는 이유가 있는데 향후 학습시에 필요하기 때문이다. (네트워크 학습에서 설명)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def softmax(a):
    c = np.max(a)
    return np.exp(a - c) / np.sum(np.exp(a - c))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;feedforward--1&quot;&gt;Feedforward 실습&lt;/h2&gt;

&lt;p&gt;여태 보았던 3층 Neural Network를 만들어 보자, 어려운 것은 없고 아까 만들었던 것을 나열해보면 쉽다.&lt;/p&gt;

&lt;p&gt;실습할 데이터는 mnist 데이터 이며, 아래 링크로 받을 수 있다.&lt;/p&gt;

&lt;밑바닥 부터=&quot;&quot; 시작하는=&quot;&quot; 딥러닝=&quot;&quot;&gt; 책의 Github: [[&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크&lt;/span&gt;](
https://github.com/WegraLee/deep-learning-from-scratch)]

입력층에는 784 개의 뉴런, 은닉층1에는 50개, 은닉층2에는 100개, 마지막 층에는 10개의 뉴런으로 구성되어 있는 네트워크다. 활성화 함수는 sigmoid를 쓰고, 마지막에 Softmax로 확률을 구했다. 실행단계에서 batch라는 것이 있는데, 한번에 많은 양의 데이터를 계산하면 느리니, 조금씩 데이터를 사용해서 계산하는 방법이라고 생각하면 된다.

    # 네트워크 만들기
    from dataset.mnist import load_mnist
    import numpy as np

    class NN(object):
        def __init__(self):
            # W1(50, 784) X(784, batch_size)
            # W2(100, 50) Z1(50, batch_size)
            # W3(10, 100) Z2(100, batch_size)
            # B1(50, batch_size)
            # B2(100, batch_size)
            # B3(10, batch_size)
            self.W = {'W1': np.random.normal(size=(50, 784)),  
                      'W2': np.random.normal(size=(100, 50)),  
                      'W3': np.random.normal(size=(10, 100)),}  
            self.B = {'B1': np.random.normal(size=(50, batch_size)),  
                      'B2': np.random.normal(size=(100, batch_size)),  
                      'B3': np.random.normal(size=(10, batch_size)),}  

        def get_data(self):
            (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True,
                                                              normalize=True,
                                                              one_hot_label=False)
            return x_train, t_train, x_test, t_test

        def predict(self, X):
            W1, W2, W3 = self.W['W1'], self.W['W2'], self.W['W3']
            B1, B2, B3 = self.B['B1'], self.B['B2'], self.B['B3']
            # Input -&amp;gt; Hidden 1
            A1 = np.dot(W1, X) + B1
            Z1 = sigmoid(A1)
            # Hidden 1 -&amp;gt; Hidden 2
            A2 = np.dot(W2, Z1) + B2
            Z2 = sigmoid(A2)
            # Hidden 2 -&amp;gt; Output
            A3 = np.dot(W3, Z2) + B3
            Y = softmax(A3)

            return Y

    # 실행 단계
    model_mnist = NN()
    x_train, t_train, x_test, t_test = model_mnist.get_data()
    acc_count = 0
    batch_size = 100
    for i in range(0, len(x_train), batch_size):
        x_batch = x_train[i:i+batch_size].T  # (784, 100)
        y_batch = model_mnist.predict(x_batch) # (10, 100)
        p = np.argmax(y_batch, axis=0)
        acc_count += np.sum(p == t_train[i:i+batch_size])
    print(&quot;accuracy:&quot;, acc_count / len(x_train))

&amp;gt; accuracy: 0.0857833333333

정확도란 데이터 중에서 얼만큼 라벨 맞췃는지 측정하는 것인데, 당연하지만 결과가 아주 형편이 없다. 이제 네트워크를 학습시키면서 이를 향상 시킬 것이니까 너무 걱정하지 말자.
&lt;/밑바닥&gt;
</description>
        <pubDate>Fri, 08 Dec 2017 11:52:21 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2017/12/08/numpywithnn2.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2017/12/08/numpywithnn2.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>NUMPY with NN - 1</title>
        <description>&lt;h1 id=&quot;numpy--neural-network-basic---1&quot;&gt;Numpy로 짜보는 Neural Network Basic - 1&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Neural Network의 역사: &lt;a href=&quot;http://solarisailab.com/archives/1206&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크&lt;/span&gt;&lt;/a&gt; 참고&lt;/p&gt;

&lt;p&gt;Neural Network를 알려먼 퍼셉트론이란 개념을 우선 이야기 해보자&lt;/p&gt;
&lt;h2 id=&quot;perceptron&quot;&gt;퍼셉트론(Perceptron)&lt;/h2&gt;
&lt;p&gt;퍼셉트론(perceptron)은 인공신경망의 한 종류로서, 1957년에 코넬 항공 연구소(Cornell Aeronautical Lab)의 프랑크 로젠블라트 (Frank Rosenblatt)에 의해 고안되었다. 이것은 가장 간단한 형태의 피드포워드(Feedforward) 네트워크 - 선형분류기- 으로도 볼 수 있다.&lt;/p&gt;

&lt;p&gt;퍼셉트론이 동작하는 방식은 다음과 같다. 각 노드의 가중치와 입력치를 곱한 것을 모두 합한 값이 활성함수에 의해 판단되는데, 그 값이 임계치(보통 0)보다 크면 뉴런이 활성화되고 결과값으로 1을 출력한다. 뉴런이 활성화되지 않으면 결과값으로 -1을 출력한다.&lt;/p&gt;

&lt;p&gt;(출처: 위키백과 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크&lt;/span&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;퍼셉트론을 이야기 하면 XOR문제를 빠트릴 수가 없는데, 그 이유는 XOR문제를 푸는데 다층 퍼셉트론이 사용되며, 이게 Neural Network의 모태이기 되기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;and-or-nand&quot;&gt;AND, OR, NAND&lt;/h3&gt;
&lt;p&gt;XOR문제를 이야기 하기 전에 AND, OR, NAND에 대해 이야기 해야한다. 어떤 이산 변수 $x_1$과 $x_2$가 있다고 생각해보자. 어떤 선형식을 통해서 아래와 같은 표를 분류하고 싶다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_1$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_2$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$y$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;어떤 선형식을 세워야 $x_1$과 $x_2$를 넣었을 때 y값이 나올까? 아래 식을 한번 보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y =
\begin{cases}
  0\ \ (b + w_1x_1 + w_2x_2 \leq 0) \\
  1\ \ (b + w_1x_1 + w_2x_2 &gt; 0) \\
\end{cases}&lt;/script&gt;

&lt;p&gt;만약에 $b$ 가 임의의 음수이고, $w_1$, $w_2$ 값이 $b$ 보다 작거나 같은 임의의 양수면 이 식은 항상 성립한다. 각종 변수가 이를 만족 할 때 &lt;strong&gt;AND 게이트&lt;/strong&gt; 라고 부르며 코드로 이렇게 짜볼 수 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def AND(x1, x2):
  x = np.array([x1, x2])
  w = np.array([0.5, 0.5])
  b = -0.7
  tmp = np.sum(x*w) + b
  if tmp &amp;lt;= 0:
      return 0
  else:
      return 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;확인해보면&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xx = [[0, 0], [0, 1], [1, 0], [1, 1]]
for x in xx:
  print('AND({0},{1}) : {2}'.format(x[0], x[1], AND(x[0], x[1])))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;AND(0,0) : 0&lt;/p&gt;

  &lt;p&gt;AND(0,1) : 0&lt;/p&gt;

  &lt;p&gt;AND(1,0) : 0&lt;/p&gt;

  &lt;p&gt;AND(1,1) : 1&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위에 있는 식을를 그림으로 표현하면 아래와 같은 그림이다. ($b$는 빠졌지만 위에다가 같이 더해준다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/perceptron/perceptron_1.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 OR 게이트와, NAND게이트도 한 번 생각해보자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NAND 게이트&lt;/strong&gt; 는 아래와 같은 표로 나타낼 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_1$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_2$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$y$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp = np.sum(x*w) + b
    if tmp &amp;lt;= 0:
        return 0
    else:
        return 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;결과 값을 측정해보면&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xx = [[0, 0], [0, 1], [1, 0], [1, 1]]
for x in xx:
    print('NAND({0},{1}) : {2}'.format(x[0], x[1], NAND(x[0], x[1])))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;NAND(0,0) : 1&lt;/p&gt;

  &lt;p&gt;NAND(0,1) : 1&lt;/p&gt;

  &lt;p&gt;NAND(1,0) : 1&lt;/p&gt;

  &lt;p&gt;NAND(1,1) : 0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;마찬가지로 &lt;strong&gt;OR 게이트&lt;/strong&gt; 는 아래와 같은 표로 나타낼 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_1$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_2$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$y$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.2
    tmp = np.sum(x*w) + b
    if tmp &amp;lt;= 0:
        return 0
    else:
        return 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;결과 값을 확인해보면&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xx = [[0, 0], [0, 1], [1, 0], [1, 1]]
for x in xx:
    print('OR({0},{1}) : {2}'.format(x[0], x[1], OR(x[0], x[1])))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;OR(0,0) : 0&lt;/p&gt;

  &lt;p&gt;OR(0,1) : 1&lt;/p&gt;

  &lt;p&gt;OR(1,0) : 1&lt;/p&gt;

  &lt;p&gt;OR(1,1) : 1&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이제 정리해서 각각의 게이트가 제대로 선형으로 분리 되었는지 확인해보자. x축과 y축은 각각 $x_1$ 과 $x_2$를 나타내며, 빨간 동그라미는 1, X로 표시된 곳은 0이라는 뜻이다. 우리의 예시 $(b=-0.7\ w_1, w_2=0.5)$ AND 게이트에서 $x_1$이 0일때 절편인 $x_2$가 1.2가 되니까 맞게 분류하는 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/perceptron/perceptron_2.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;xor-&quot;&gt;XOR 문제&lt;/h3&gt;
&lt;p&gt;XOR 문제란 어떤 선형식으로 이산 변수 $x_1$과 $x_2$에 대해서 둘중에 하나라도 1이 되면 결과 값으로 1를 반환하, 둘다 0이거나 1이면 0을 반환하는 식을 찾는 것이다.&lt;/p&gt;

&lt;p&gt;표로 그려보면 아래와 같은 문제다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_1$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_2$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$y$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;그림으로 보면 아래와 같은 문제를 푸는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/perceptron/perceptron_3.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아까와 같은 방법으로 단일 선으로 이 문제를 풀수 있을까?&lt;/p&gt;

&lt;p&gt;절대 풀 수 없다. 그래서 나타난 것이 게이트를 겹쳐서 올리는 것이다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/perceptron/perceptron_4.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이런 구조로 게이트를 짜면 어떻게 될까? 예를 들어 $x_1, x_2 = 0$ 이라고 가정해보자. $x_1$과 $x_2$가 NAND 게이트를 거치면 1, OR 게이트를 거치면 0이 나온다. 1과 0이 AND 게이트를 거치면 0이 된다! 마찬가지로 해보면 XOR 문제를 풀 수가 있다! 이처럼 게이트 층을 두개 만들어 XOR문제를 풀었으며, 이것이 &lt;strong&gt;다층 퍼셉트론&lt;/strong&gt; 의 기원이라고 말 할 수 있다.&lt;/p&gt;

&lt;p&gt;표로 그려보면 아래와 같다. $s_1$ 과 $s_2$는 각각 NAND 게이트와 OR 게이트를 뜻하면 최종단에 AND 게이트를 거쳐 y 값을 구할 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_1$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x_2$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$s_1$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$s_2$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$y$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;코드로 구현하느 것은 아까 만든 코드를 나열하면 된다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;확인해보면&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xx = [[0, 0], [0, 1], [1, 0], [1, 1]]
for x in xx:
    print('XOR({0},{1}) : {2}'.format(x[0], x[1], XOR(x[0], x[1])))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;XOR(0,0) : 0&lt;/p&gt;

  &lt;p&gt;XOR(0,1) : 1&lt;/p&gt;

  &lt;p&gt;XOR(1,0) : 1&lt;/p&gt;

  &lt;p&gt;XOR(1,1) : 0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Rosenblatt이 제시한 정확한 단일 퍼셉트론은 아래의 그림과 같다. Input과 Weight를 곱해서 더한 다음에 Activation function을 적용해서 그 값이 0보다 크면 1 작으면 -1를 반환하는 Feedforward 선형 분류기의 구조다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/perceptron/perceptron_5.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이는 XOR 문제를 해결 할 수 없었고, Multi-Layer Perceptrons이 1986년에 등장 했는데 중간에 Hidden 층을 더 쌓으면서 XOR 문제를 해결하였다.&lt;/p&gt;

&lt;p&gt;다음 시간에는 Feedforward과정을 자세히 알아본다.&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Dec 2017 17:54:18 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2017/12/07/numpywithnn_1.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2017/12/07/numpywithnn_1.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>ML-DecisionTree</title>
        <description>&lt;h1 id=&quot;ml-decisiontree&quot;&gt;[ML] DecisionTree&lt;/h1&gt;
&lt;p&gt;의사결정나무의 소개는 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95&quot;&gt;링크&lt;/a&gt;로 대체하고 어떻게 진행되는지만 알아보자&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;의사결정나무의 장점: 어떤 변수가 분류에 영향을 끼쳤는지 사람이 보기 쉽게 되어있다. 사람 입장에서 해석에 용이하다.&lt;/li&gt;
  &lt;li&gt;의사결정나무의 단점: 오버피팅이 심하다. 따라서 보통 랜덤 포레스트라는 앙상블 방법론을 쓴다. 초기 단계에서 랜덤 요소를 넣어서 오버피팅을 방지한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;의사 결정나무모델은 분류를 할때 책상정리에 비유한다. 더러운 책상에 있는 물건을 어떤 기준에 따라서 하나씩 정리하는 거다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ML/DecisionTree_Desk.jpeg&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(사진출처: 네이버 블로그)&lt;/p&gt;

&lt;p&gt;그렇다면 데이터를 분류하는 기준은 도대체 무엇인가?&lt;/p&gt;

&lt;h1 id=&quot;entropy-information-gain&quot;&gt;엔트로피(Entropy)와 정보획득(Information Gain)&lt;/h1&gt;
&lt;p&gt;데이터를 분류하는 기준은 아래와 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;어떤 기준으로 분류 후에 histogram으로부터 조건부 엔트로피를 계산 함&lt;/li&gt;
  &lt;li&gt;이전 entropy와 새로구한 조건부 엔트로피의 차이(:=Infomation Gain)이 최대 인 것을 best feature로 선택한다&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC&quot;&gt;엔트로피&lt;/a&gt;를 이해하자면 정리안된 책상의 상태를 생각하면 편할 것이다. 엔트로피가 높으면 책상이 굉장히 정리가 안된 상태(혼돈의 상태)고, 엔트로피가 적을 수록 점점 정리되어가는 책상을 생각하면 된다.&lt;/p&gt;

&lt;p&gt;엔트로피의 계산 방식은 아래와 같다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;엔트로피: $H[Y] = -\sum_{k=1}^K p(y_k) \log_2 p(y_k)$&lt;/p&gt;

  &lt;p&gt;조건부 엔트로피: $H[Y \mid X] = - \sum_i \sum_j \,p(x_i, y_j) \log_2 p(y_j \mid x_i)$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;$y_k$: $k$ 카테고리에 속하는 $y$ 의 갯수&lt;/li&gt;
  &lt;li&gt;$p(y_k)$: 변수 $y$ 가 카테고리 $k$ 에 속할 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;본격적인 계산을 위해 엔트로피를 아래와 같은 데이터가 있다고 가정하고 단계별로 진행 해보자.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f3&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section&quot;&gt;계산과정&lt;/h2&gt;
&lt;h3 id=&quot;base-entropy----entropy&quot;&gt;[1 단계] base entropy: 분류가 안되었을 때의 entropy&lt;/h3&gt;
&lt;p&gt;기초 엔트로피(base entropy), 즉 분류가 되기전의 상태를 계산해야 분류후에 엔트로피의 차이를 구할 수 있다.&lt;/p&gt;

&lt;p&gt;기초 엔트로피를 $E_{base}$ 라고 하면,&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y=yes&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y=no&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;class의 히스토그램을 그리고 갯수를 세어본다. 지금의 class는 $(yes, no)$ 2개로 $K=2$ 가 되고, 히스토그램에 따라서 계산하면 기초 엔트로피를 구할 수 있다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;$E_{base} = -[\ P(y_{=yes})\log{P(y_{=yes})} + P(y_{=no})\log{P(y_{=no})}\ ]&lt;br /&gt;
= -(\frac{2}{6}\log{\frac{2}{6}}+\frac{4}{6}\log{\frac{4}{6}}) = 0.9182$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;feature----infomation-gain&quot;&gt;[2 단계] feature별로 조건부 엔트로피를 구하고 Infomation Gain구함&lt;/h3&gt;
&lt;p&gt;Information Gain 은 이전 단계 엔트로피에서 각 feature의 엔트로피를 빼면 구할 수 있다. 즉, 각 feature가 기준이 되어서 엔트로피를 제일 작게 만드는, 혹은 Information Gain을 제일 크게 만드는 쪽으로 분류를 진행하는 것이다.&lt;/p&gt;
&lt;h4 id=&quot;feature-1&quot;&gt;feature 1&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y=yes&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y=no&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x=1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x=0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;total&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;$E_1 = -[\ P(y_{=yes},x_{=1})\log{P(y_{=yes}|x_{=1})} + P(y_{=no},x_{=1})\log{P(y_{=no}|x_{=1})} +P(y_{=yes},x_{=0})\log{P(y_{=yes}|x_{=0})} + P(y_{=no},x_{=0})\log{P(y_{=no}|x_{=0})}\ ]$
$= -[\ \frac{1}{6}\log{\frac{1}{2}}+\frac{1}{6}\log{\frac{1}{2}}+\frac{1}{6}\log{\frac{1}{4}}+\frac{3}{6}\log{\frac{3}{4}}\ ] = 0.8741$&lt;/p&gt;

  &lt;p&gt;$IG_1 = E_{base} - E_1 = 0.0441$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;마찬가지로 feature2 와 feature3도 똑같이 구할 수 있다.&lt;/p&gt;
&lt;h4 id=&quot;feature-2&quot;&gt;feature 2&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y=yes&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y=no&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x=1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x=0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;total&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;$E_2 = -[\ \frac{1}{6}\log{\frac{1}{4}} + \frac{3}{6}\log{\frac{3}{4}} + \frac{1}{6}\log{\frac{1}{2}} + \frac{1}{6}\log{\frac{1}{2}}] = 0.8741$&lt;/p&gt;

  &lt;p&gt;$IG_2 = E_{base} - E_2 = 0.0441$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;feature-3&quot;&gt;feature 3&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f3&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y=yes&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y=no&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x=1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x=0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;total&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;$E_3 = -[\ \frac{2}{6}\log{\frac{2}{2}} + \frac{0}{6}\log{\frac{0}{2}} + \frac{2}{6}\log{\frac{2}{4}} + \frac{2}{6}\log{\frac{2}{4}}] = 0.6666$&lt;/p&gt;

  &lt;p&gt;$IG_3 = E_{base} - E_3=0.2516$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;section-1&quot;&gt;[3단계] 결과 및 선택:&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Entropy&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;IG&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;base&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.9182&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;f1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.8742&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0441&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;f2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.8742&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0441&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;f3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.6667&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.2516&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;결과에 따라 첫번째 기준으로 엔트로피가 가장 많이 줄고, IG가 가장 높은 feature3를 선택하게 된다.&lt;/p&gt;

&lt;p&gt;따라서 feature3 기준으로 feature값이 1인경우 y=yes, 0인 경우 y=no로 나눠지게 된다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f3&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;1&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;yes&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;0&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;no&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;0&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;no&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;1&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;yes&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;남은 데이터는 아래와 같다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;f2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;이제 다시 위에 과정을 반복하게 된다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;$E_{base2} = -(0 + 1 \cdot \log{1}) =0$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 예제에서는 더이상 나눌 엔트로피가 없기 때문에 사실상 어떤 기준으로 선택해도 no가 나오지만 컴퓨터는 계산시 둘중 아무거나 기준으로 결과를 낼 것이다.&lt;/p&gt;

&lt;p&gt;데이터를 없에지 않는 방법도 존재한다. feature3를 선택하고 남은 feature들 중에서 다시 선택하는 방법이다.&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;코드&lt;/h2&gt;
&lt;p&gt;모든 코드는 Github&lt;a href=&quot;https://github.com/simonjisu/ML/tree/master/DecisionTreeModel&quot;&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;링크&lt;/span&gt;&lt;/a&gt;, DecisionTree.py에 공개되어 있다.&lt;/p&gt;
</description>
        <pubDate>Tue, 28 Nov 2017 14:48:12 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2017/11/28/mlstudydecisiontree.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2017/11/28/mlstudydecisiontree.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>Text Mining Study [0]</title>
        <description>&lt;h1 id=&quot;text-mining-study-0&quot;&gt;&lt;strong&gt;Text Mining Study [0]&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 개요&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;텍스트 마이닝(Text Mining)&lt;/strong&gt; 혹은 &lt;strong&gt;덱스트 애널리틱스(text analytics)&lt;/strong&gt;란, 반정형 또는 비정형 텍스트 데이터의 분석과 처리를 위한 다양한 기술을 일컫는 포괄적 용어.&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;텍스트 마이닝의 핵심&lt;/h3&gt;
&lt;p&gt;텍스트 마이닝의 핵심은 비정형 혹은 반정형의 텍스트 또는 텍스트 집합으로부터 정보 가치가 있는 항목들을 추출하여 정형 텍스트, 즉 표로 구성하는 것이라고 할 수 있다. 이 과정은 대체로 정보 가치가 있는 것으로 판단되는 항목, 혹은 단위의 인식, 그리고 특정 범위 내에서의 이 단위의 출현 빈도 계수로 구성된다.&lt;/p&gt;

&lt;h3 id=&quot;text-data--&quot;&gt;Text Data 분석의 어려움&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;텍스트 데이터는 항목(단위) 구분이 명시적이지 않음&lt;/li&gt;
  &lt;li&gt;데이터 항목 구분이 중요한 이유는 Reductionism:
    &lt;ul&gt;
      &lt;li&gt;높은 단계의 사상이나 개념을 하위 단계의 요소로 세분화 하여 명확하게 정의 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기호의 성질: 기호의 자의성
    &lt;ul&gt;
      &lt;li&gt;심볼(Symbol)과 의미가 자꾸 변하거나 모든 사람들에게 항상 일치하지 않다. 또한 컴퓨터와 사람이 이해하는 언어 기호가 달라서 사람의 언어기호를 computer native하게 만들어 줘야함&lt;/li&gt;
      &lt;li&gt;해결방안:
        &lt;ol&gt;
          &lt;li&gt;규칙을 만들고 그것에 따르게 한다. 그러나 만들기 힘들고, 모두가 만족할 수가 없다는 단점이 있다.&lt;/li&gt;
          &lt;li&gt;경험, 데이터에 의해 컴퓨터에게 학습하게 한다. 최근 추세.&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2. 텍스트 분석의 과정&lt;/h2&gt;
&lt;p&gt;과제의 목표 설정 $\rightarrow$ 데이터 입수 $\rightarrow$ 데이터 준비 $\rightarrow$ 데이터 탐색 $\rightarrow$ 데이터 모델링&lt;/p&gt;
</description>
        <pubDate>Sun, 22 Oct 2017 12:59:21 +0900</pubDate>
        <link>http://simonjisu.github.io/%5Bstudy%5Dtext_mining/2017/10/22/text-mining-study.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/%5Bstudy%5Dtext_mining/2017/10/22/text-mining-study.html</guid>
        
        
        <category>[Study]Text_Mining</category>
        
      </item>
    
      <item>
        <title>algorithm study</title>
        <description>&lt;h1 id=&quot;section&quot;&gt;&lt;strong&gt;알고리즘 공부&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&amp;lt;Hello Coding 그림으로 개념을 이해하는 알고리즘&amp;gt;&lt;/strong&gt; 책을 요약 정리한 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;chapter-1--&quot;&gt;Chapter 1. 알고리즘의 소개&lt;/h2&gt;
&lt;hr /&gt;
&lt;h4 id=&quot;simple-search&quot;&gt;단순탐색 simple search&lt;/h4&gt;
&lt;p&gt;단순히 순서대로 추측을 하는 것&lt;/p&gt;

&lt;h4 id=&quot;binary-search&quot;&gt;이진탐색 binary search&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;정렬된&lt;/span&gt; 원소 리스트를 입력으로 받고, 리스트에 원하는 원소가 있으면 그 원소의 위치를 반환, 아니면 null 반환&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;running-time---big-o-notation&quot;&gt;running time 그리고 빅오 표기법(Big O notation)&lt;/h4&gt;
&lt;p&gt;알고리즘의 시간은 어떻게 증가하는가로 측정함&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;단순 탐색: &lt;span style=&quot;color: #7d7ee8&quot;&gt;선형 시간(linear time)&lt;/span&gt; 만큼 걸림, $O(n)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이진 탐색: &lt;span style=&quot;color: #7d7ee8&quot;&gt;로그 시간(logarithmic time)&lt;/span&gt; 만큼 걸림, $O(\log_{2}{n})$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;빅오 표기법(Big O notation)&lt;/strong&gt;: 연산 횟수를 나타냄&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;많이 사용하는 빅오(빠른순)&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;설명 / 예시&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$O(1)$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;고정 시간&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$O(\log{n})$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;이진 탐색&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$O(n)$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;단순 탐색&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$O(n*\log{n})$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;퀵 정렬&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$O(n^2)$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;선택 정렬&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$O(n!)$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;외판원 문제&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;chapter-2--&quot;&gt;Chapter 2. 선택 정렬&lt;/h2&gt;
&lt;hr /&gt;
&lt;h4 id=&quot;section-1&quot;&gt;배열과 연결 리스트&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;리스트&lt;/strong&gt;: 원소를 메모리 아무 곳에다 저장해두고, 각 원소에는 목록의 다음 원소에 대한 주소가 저장되어있음, 특정 원소의 위치를 알려면 앞단의 원소의 위치를 알아야함, 그러나 메모리 공간을 예약 요청해서 저장할 필요가 없음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배열&lt;/strong&gt;: 원소들을 메모리에 차례대로 저장함, 특정 원소의 위치를 알기 쉬움 대신 필요한 만큼 미리 일정한 메모리 공간을 요청함, 즉 원소를 추가할 일이 없으면 쓸데 없이 낭비하거나, 추가할 목록이 더 많이 커져 새로 다시 메모리 공간을 요청해야하는 단점이 있음.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배열&lt;/strong&gt;과 &lt;strong&gt;리스트&lt;/strong&gt;에서 읽기와 쓰기 연산을 하는 데 걸리는 시간:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;배열&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;리스트&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;읽기&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$O(1)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$O(n)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;삽입&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$O(n)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$O(1)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;삭제&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$O(n)$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$O(1)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;자료에 접근하는 방식:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;임의 접근(random access): 임이의 원소에 접근 가능, ex) 배열&lt;/li&gt;
  &lt;li&gt;순차 접근(sequential access): 원소를 첫 번째 부터 하나씩 읽는 것, ex) 연결리스트&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-2&quot;&gt;선택 정렬&lt;/h4&gt;
&lt;p&gt;리스트에에서 모든 항목을 살펴보고 최대값을 찾아서 새로운 리스트에 정렬&lt;/p&gt;

&lt;p&gt;걸리는 시간: $O(n^2)$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;왜 $O(n^2)$ 시간인가?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;매번 실행할 때마다 점검횟수는 $n-1, n-2, \cdots , 2, 1$ 로 줄어들고, 평균적으로 $n/2$ 만큼 점검한다. 따라서 실제 시간은 평균적으로 $O(n \times 1/2 \times n)$ 인데, 빅오 표기법에서는 상수항은 무시하기 때문에 $O(n \times n)$이 되는 것이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;chapter-3--recursion&quot;&gt;Chapter 3. 재귀 Recursion&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;section-3&quot;&gt;재귀&lt;/h3&gt;
&lt;p&gt;함수가 자기 자신을 호출하는 것&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;기본 단계(base case): 함수가 자기 자신을 다시 호출하지 않는 경우, 즉 무한 반복으로 빠져들지 않게 하는 부분&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;재귀 단계(recursive case): 함수가 자기 자신을 호출하는 부분&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def countdown(i):
    print i
    if i &amp;lt;= 1:  # 기본 단계
        return
    else:  # 재귀 단계
        countdown(i-1)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;section-4&quot;&gt;스택&lt;/h3&gt;
&lt;p&gt;아래와 같은 자료구조를 &lt;strong&gt;스택&lt;/strong&gt;이라고 함&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;푸시(push): 맨 위에 새로운 항목 삽입&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;팝(pop): 맨 위에 항목을 읽기&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;호출 스택(call stack)&lt;/strong&gt;: 여러 개의 함수를 호출하면서 함수에 사용되는 변수를 저장하는 스택을 호출 스택이라함&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def fact(x):
    if x == 1:
        return 1
    else:
        return x * fact(x-1)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;호출 할때 마치 레고블럭을 쌓고 떼듯이 호출함,&lt;/p&gt;

&lt;p&gt;예를 들어, factorial 함수를 들자면&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;진행 순서&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;함수&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;설명&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;스택(제일 앞에 것이 최근 스택)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;fact(3)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;fact(3) 호출&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fact(3)안의 x = 3&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;if x == 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;False&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;else:&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;return x * fact(x-1)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;fact(2) 호출&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fact(2)안의 x = 2&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fact(3)안의 x = 3&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;if x == 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;False&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;else:&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;return x * fact(x-1)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;fact(1) 호출&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fact(1)안의 x = 1&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fact(2)안의 x = 2&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fact(3)안의 x = 3&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;if x == 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;True&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;return 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;아직 함수 호출이 끝난건 아니다. 이제 스택에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;fact(1)안의 x=1&lt;/code&gt;를 pop하면 된다. 즉, 반환해야하는 첫 번째 호출인 것이다. 1을 반환&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fact(1)안의 x = 1&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fact(2)안의 x = 2&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fact(3)안의 x = 3&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;return x * fact(x-1)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2 * fact(1) 반환하기&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fact(2)안의 x = 2&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fact(3)안의 x = 3&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;return x * fact(x-1)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3 * fact(2) 반환하기&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fact(3)안의 x = 3&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;단점: 모든 정보를 저장해두어야 하기 때문에 메모리를 많이 소비한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-4--&quot;&gt;Chapter 4. 분할 정복&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;divide-and-conquer&quot;&gt;분할 정복(divide-and-conquer)&lt;/h3&gt;
&lt;p&gt;단계:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;가장 간단한 경우로 기본 단계를 찾음.&lt;/li&gt;
  &lt;li&gt;문제가 기본 단계가 될 때까지 나누거나 작게 만듬.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;예제: summation&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def sum(arr):
    total = 0
    for x in arr:
        total += x
    return total
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;분할 정복 전략으로 재귀 함수를 이요해서 합계를 구하려면?&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;기본 단계: 간단하게 원소 갯수가 0 혹은 1이면 기본 단계가 됨&lt;/li&gt;
  &lt;li&gt;재귀 함수 호출: sum([2,3,4]) 가 아닌 2 + sum([3,4]) 로 문제를 줄임, 즉 이 함수는 리스트를 받으면, 리스트가 비어 있을 시 0을 반환하고, 아니면 총합 = 리스트의 첫번째 숫자와 나머지 리스트의 총합을 더한 값으로 출력한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def newsum(lst):
    summ = 0
    if not lst:
        return 0
    elif len(lst) == 1:  
        return lst[0]
    else:
        summ = lst[0] + newsum(lst[1:])
        return summ

print(newsum([2,3,4]))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;quick-sort&quot;&gt;퀵 정렬(quick sort)&lt;/h3&gt;
&lt;p&gt;기준을 정해서 그것보다 작은 것과 큰 것을 나눠서 재귀함수를 호출하는 방법&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# ascending
def quicksort(lst):
    if len(lst) &amp;lt; 2:
        return lst
    else:
        pivot = lst[0]
        less = [i for i in lst[1:] if i &amp;lt;= pivot]
        greater = [i for i in lst[1:] if i &amp;gt; pivot]
        return quicksort(less) + [pivot] + quicksort(greater)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 27 Aug 2017 15:43:39 +0900</pubDate>
        <link>http://simonjisu.github.io/algorithm/2017/08/27/algorithm-study.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/algorithm/2017/08/27/algorithm-study.html</guid>
        
        
        <category>Algorithm</category>
        
      </item>
    
      <item>
        <title>박스앤위스커-박장시님 강연 후기</title>
        <description>&lt;h1 id=&quot;section&quot;&gt;&lt;strong&gt;박스앤위스커 - 박장시님 강연 후기&lt;/strong&gt;&lt;/h1&gt;
&lt;h2 id=&quot;section-1&quot;&gt;주제: 데이터 사이언스에 대한 몇 가지 실제 사례 소개&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;section-2&quot;&gt;사례들&lt;/h3&gt;

&lt;h4 id=&quot;section-3&quot;&gt;1. 문제: 데이터로 무엇을 분석해야하는지 모르는 경우&lt;/h4&gt;
&lt;p&gt;데이터는 많은데, 분석을 어떻게 시작하는 모르는 경우, 우선 평소에 궁금한 것이 무엇인지 알아보기&lt;/p&gt;

&lt;p&gt;streaming data의 검정: A/B test(시간 변수를 통제하기 위함)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;온라인에서의 t-test, large-scale hypothesis testing시 effect size도 고려할 것(p-value가 얼만큼 변했는지)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-4&quot;&gt;2. 문제: 인터넷 쇼핑몰에서 어떤 상품 배치가 최적?&lt;/h4&gt;
&lt;p&gt;MD vs 기계의 상품 배치 대결&lt;/p&gt;

&lt;p&gt;웹에서의 ‘땅값’ 개념 도입&lt;/p&gt;

&lt;p&gt;A/B test&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;소규모 실험을 우선시 해서, 성과를 보여준 다음에 계속 확대하는 방향으로 가라&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mab-&quot;&gt;3. 문제: MAB 테스트엔진&lt;/h4&gt;
&lt;p&gt;A/B test의 단점:&lt;/p&gt;

&lt;p&gt;1) 테스트가 끝나기 전까지(결과를 얻기전까지) 몇 초간 손해를 볼 수가 있음(기회비용)&lt;/p&gt;

&lt;p&gt;2) exploration vs exploitation&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Discovering new possibilities, conducting research, varying product lines, risk taking, innovation all fall under the realm of exploration. On the other hand, exploitation involves the refinement of current procedures: efficiency, production, execution, and so forth.&lt;/p&gt;

  &lt;p&gt;출처: http://www.indigosim.com/tutorials/exploration/t1s1.htm&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;3) 항상 변하는 세상: 언제나 옳은 진리는 없음, 상황에 맞춰서 정답도 변하는 세상&lt;/p&gt;

&lt;p&gt;MAB: Multi-armed bandit - &lt;a href=&quot;https://en.wikipedia.org/wiki/Multi-armed_bandit&quot;&gt;개념링크&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;장점: 다양한 테스트가 가능하다&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-5&quot;&gt;4. 문제: 전시장 데이터 시각화&lt;/h4&gt;
&lt;p&gt;비콘 data(거리만 나타남)의 처리: 비콘 로그를 이용한 이동범위 추정&lt;/p&gt;

&lt;p&gt;데이터의 편향 가능성: 관심있는 사람만 참가하기 때문에 보편적이지 않을 수도?&lt;/p&gt;

&lt;p&gt;데이터 이상치(outlier)의 처리: 분석후, 현장 전문가 모셔서 이상치를 검증하고 제거&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Force-Directed graph&lt;/li&gt;
  &lt;li&gt;color scheme: 히트맵 그릴시 https://colorbrewer2.org 참고할것&lt;/li&gt;
  &lt;li&gt;numpy for grid: 큰 지도 데이터 경우, 지도를 하나의 큰 matrix로 볼 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-6&quot;&gt;5. 문제: 탱시 운행 정보 시각화&lt;/h4&gt;
&lt;p&gt;택시 미터기 데이터 처리: 미터기를 안누르는 경우도 있고해서 빈 데이터가 가끔 씩 존재했음(택시를 탔는 기록이었는데 다음 데이터에도 다시 타는 기록이 남은 경우), 이때 Finite-state machine 설계를 해서 부족한 데이터를 깔끔하게 보충함(택시를 탔으면 다음에는 무조건 내려야하는 것).&lt;/p&gt;

&lt;h4 id=&quot;eda-----&quot;&gt;6. 문제: 스킬 트리 분석, EDA - 탐색적 자료분석의 중요성&lt;/h4&gt;
&lt;p&gt;특별한 과정은 없음, 우선 데이터의 분포를 그려보는 것이 중요, 그래서 이 데이터를 어떻게 처리할 지 고민할 것&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ggplot&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-7&quot;&gt;7. 문제: 던전 이탈률 분석&lt;/h4&gt;
&lt;p&gt;회귀분석, 의사결정나무를 쓰는 이유는 사람들이 이해하기 쉽기 때문이다. 퍼포먼스는 약간 떨어지지만, 사람들을 설득하는데 도움이 됨. 우선 적용하는게 좋음&lt;/p&gt;

&lt;p&gt;Validation을 어떻게 할지 처음부터 같이 고민할 것.&lt;/p&gt;

&lt;h4 id=&quot;section-8&quot;&gt;8. 다차원큐브탐색&lt;/h4&gt;
&lt;p&gt;파이콘 2017 세션 강의 참고하기!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;section-9&quot;&gt;느낀점&lt;/h3&gt;
&lt;p&gt;강연이 좋았던게 다양한 사례를 통해서 어떻게 데이터를 접근할지 알려주고, 실제 고객들에게(혹은 다른 사람들에게) 설득하는 방법을 터득할 수가 있었음.&lt;/p&gt;

&lt;p&gt;그리고 항상 프로젝트를 작게 시작하는 법도 배움, 조금씩 해서 성공하면 확장하는 방식으로 사고해야겠음.&lt;/p&gt;

&lt;p&gt;“부산으로 가는데, 모든 신호등이 한번에 초록불로 변할 수는 없는 법” 이말도 인상 깊었다.&lt;/p&gt;
</description>
        <pubDate>Fri, 18 Aug 2017 13:34:35 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2017/08/18/EB-B0-95-EC-8A-A4-EC-95-A4-EC-9C-84-EC-8A-A4-EC-BB-A4-EB-B0-95-EC-9E-A5-EC-8B-9C-EB-8B-98-EA-B0-95-EC-97-B0-ED-9B-84-EA-B8-B0.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2017/08/18/EB-B0-95-EC-8A-A4-EC-95-A4-EC-9C-84-EC-8A-A4-EC-BB-A4-EB-B0-95-EC-9E-A5-EC-8B-9C-EB-8B-98-EA-B0-95-EC-97-B0-ED-9B-84-EA-B8-B0.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>pycon2017</title>
        <description>&lt;h1 id=&quot;pycon-2017-&quot;&gt;&lt;strong&gt;PYCON 2017 후기&lt;/strong&gt;&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;section&quot;&gt;8월 12일&lt;/h2&gt;

&lt;h3 id=&quot;nlp&quot;&gt;노가다 없는 텍스트 분석을 위한 한국어 NLP&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;발표자: 김현중님&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;느낀점:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;제일 처음 들었던 발표기도 했지만, 이번 발표에서 제일 감명깊게 들은 발표, “어떻게 이런 방식으로 생각했지?”라는 생각을 계속함.&lt;/p&gt;

      &lt;p&gt;soynlp라는 패키지로 소스를 공개해주셨으니 앞으로 내가하는 프로젝트에도 아이디어를 적용할 생각이다.&lt;/p&gt;

      &lt;p&gt;이런 컨퍼런스를 많이 참가해서 다른 사람의 아이디어에서 영감을 많이 받을 수도 있다는 생각을 했음.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;python-nltk-tensorflow---&quot;&gt;전지양의 꿈을 꾸는 안드로이드: Python과 NLTK, TensorFlow를 이용한 챗봇 감정모형구현&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;발표자: 신정규님&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;느낀점:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;챗봇 팀프로젝트 처음 시작했을 때, 이분의 작년 파이콘강연을 보고 챗봇의 기본적인 구상을 참고했었다. “역시 챗봇은 쉬운게 아니구나”라면서 말이다…&lt;/p&gt;

      &lt;p&gt;다른 곳에서도 이런 생각이 들었지만, 준비하신 내용에 비해 발표시간이 너무 짧았던 것 안타까웠음…코드도 공개하신다고 했으니까 나중에 참고해서 공부하기로 함.&lt;/p&gt;

      &lt;p&gt;감정, 생각 혹은 문장의 이해를 해야하는 부분에서는 Word2vec을 많이 써야된다는 것도 생각하게 됨.&lt;/p&gt;

      &lt;p&gt;또한, 챗봇의 제일 문제점은 사람같지 않다는 점(물론 챗봇이 원래 사람은 아니다만…), 이용자 입장에서 답답하게 느끼지 않게 대화를 이어나갈수 있게하는게 최대의 과제같음.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-1&quot;&gt;8월 13일&lt;/h2&gt;

&lt;h3 id=&quot;python-tensorflow--&quot;&gt;구름이 하늘의 일이라면: Python과 TensorFlow를 이용한 기상예측&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;발표자: 윤상웅님&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;느낀점:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;데이터 분석할때의 과정과 방향성을 어떻게 잡아야할지 잘 이야기해주셨음.&lt;/p&gt;

      &lt;p&gt;“문제만들기 $\rightarrow$ 모형만들기 $\rightarrow$ 시스템만들기” 라는 순서로 정의하며, 제일 중요한 부분이 “문제를 어떻게 정의 할 것인가”다.&lt;/p&gt;

      &lt;p&gt;물론 도메인 지식이 많이 필요하지만, 공통적인 부분도 있다고 생각했다. 주로, 데이터의 입력/출력, 문제의 유형(supervised/unsupervised), 데이터의 표현 등이었다.&lt;/p&gt;

      &lt;p&gt;결국 우리가 머신러닝을 쓰는 것도 어떤 문제를 해결하려고 하는 것이기 때문에, 문제에 대한 질문을 잘 하고, 문제의 본질을 해석하는 것이 우선순위 같다.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;기업 인사담당자들과의 토크&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;느낀점:
    &lt;blockquote&gt;
      &lt;p&gt;중간에 시간이 떠가지고 발표 듣는 대신, wanted에서 마련한 몇개 기업 인사담당자들과의 토크시간에 참석했는데, 생각보다 현업에 계신 분들의 이야기를 많이 들어서 좋았다.&lt;/p&gt;

      &lt;p&gt;업계는 이제 데이터 사이언스 분야의 직업경계선이 차츰 나눠진 듯 하다. 데이터 분석가, 데이터 사이언티스트, 데이터 엔지니어, 세분류로 나눠졌고, 더 이상 머신러닝 패키지를 잘 사용한다고 해서 데이터 사이언티스트가 아니라(요즘은 누구나 시간을 들이면 사용해서 간단한 분석정돈 할 수 있다고 한다.), 어떻게 머신러닝관련 이론을 더 효율적으로 구현을 하는지가 데이터 사이언티스트의 임무라고 생각한다.&lt;/p&gt;

      &lt;p&gt;그리고 기업을 지원할 때, 자신의 무기를 잘 가져야하고(물론 이건 공통된 의견이긴 하지만), 자신이 지원한 분야가 무엇인지 정확히 파악해야한다. 아무튼 알찼던 시간이었다.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-3&quot;&gt;공통&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;느낀점:
    &lt;blockquote&gt;
      &lt;p&gt;이번 파이콘에서는 Django에 관련된 발표가 유난히 많았었던것 같다. 아무래도 장고공부도 해야할 것 같다.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Aug 2017 19:22:36 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2017/08/14/PYCON2017.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2017/08/14/PYCON2017.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>End-to-End Memory Network 논문 요약 및 정리</title>
        <description>&lt;h1 id=&quot;end-to-end-memory-network----&quot;&gt;&lt;strong&gt;End-to-End Memory Network 논문 요약 및 정리&lt;/strong&gt;&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;a-single-layer&quot;&gt;A. Single layer&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/E2EMN.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;input&quot;&gt;Input&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;n 개의 단어가 포함된 한 &lt;strong&gt;문장 sentence i&lt;/strong&gt; 는 $x_i = [x_{i1}, x_{i2}, \cdots, x_{in}]$ 로 표현 할 수 있으며, 하나의 &lt;strong&gt;단어&lt;/strong&gt;는 BoW(Bag-of-Words)방식으로 인코딩 하여 vector로 바꿔준다. 이렇게 구성된 여러 문장들의 집합을 Context ${x_i}$라고 한다.
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{ij} = [0, 0, \cdots, 1 , \cdots, 0, 0]\quad for\ j^{th}\ words\ in\ sentence\ i&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;t 개의 단어가 포함된 질문 question q도 마찬가지로 BoW방식으로 인코딩해준다.&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;sentence $x_1$ = Mary journeyed to the den. 을 예제로 들면,&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{i1} = mary = [1, 0, \cdots, 0, 0]&lt;/script&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{i2} = journeyed = [0, 1, \cdots, 0, 0]&lt;/script&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{i3} = to = [0, \cdots, 1, \cdots, 0, 0]&lt;/script&gt;

  &lt;p&gt;이런식으로 인코딩이 된다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;주의: Mary혹은 mary와 같이 같은 단어를 두 번 세는 것을 피하기 위해서, 모든 단어는 소문자로 바꿔준다.&lt;/p&gt;

&lt;h3 id=&quot;input-memory&quot;&gt;Input Memory&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;하나의 문장 $x_i$ 각각의 단어에 Embedding matrix $A$를 곱하여 각각의 단어를 Embedding Vectors로 변환하고 이를 모두 더하여 메모리 벡터(Memory Vector) $m_i$를 구한다. 이렇게 구성된 여러개의 메모리 벡터 $m_i$들 중 일부를 사용하게 된다.
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;m_i = \sum_{j}^{n} Ax_{ij} = Ax_{i1} + Ax_{i2} + \cdots + Ax_{in}&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Question도 마찬가지로 Embedding matrix $B$를 곱하여 각각의 단어를 Embedding Vectors로 변환하고 이를 모두 더하여 Internal state $u$를 구한다.
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;u = \sum_{j} Bx_{ij} = Bx_{i1} + Bx_{i2} + \cdots + Bx_{in}&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;이후 Context와 Question의 유사성(match)를 구하기 위해 inner product를 시행한 후, Softmax Function으로 출력해준다. 이러한 결과로 input에 대한 확률을 도출 해낼 수 있다.
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;p_i = Softmax(u^Tm_i)&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;즉 $p_i$가 높을 수록 높은 유사성을 띈다.&lt;/p&gt;

&lt;p&gt;이러한 과정을 통해서 &lt;strong&gt;Input Memory&lt;/strong&gt; 에는 Context 문장들(${x_i}$)과 질문($q$)의 축약된 정보가 들어가게 된다.&lt;/p&gt;

&lt;h3 id=&quot;output-memory&quot;&gt;Output Memory&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;모든 Context 문장들 ${x_i}$ 의 각각의 단어에  다시 Embedding matrix C를 곱하고 더하여 $c_i$로 변환한다.
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;c_i = \sum_{j} Cx_{ij} = Cx_{i1} + Cx_{i2} + \cdots + Cx_{in}&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;이는 출력으로 나오는 Response vector 인$o$ 를 구하기 위해서 인데, $o$는 아래와 같이 Input Memory에서 나오는 유사성(match, $p_i$)와 가중평균합을 진행한다.
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;o = \sum_{i} p_ic_i&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;final-prediction&quot;&gt;Final Prediction&lt;/h3&gt;
&lt;p&gt;output $o$와 질문으로부터 추출한 Internal state $u$에 가중치값 $W$를 곱하여 더한뒤에 Softmax Function을 적용하여 답 $\hat{a}$을 추론한다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{a} = Softmax(W(o+u))&lt;/script&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;weight-updating&quot;&gt;Weight Updating&lt;/h3&gt;
&lt;p&gt;Loss Function은 standard cross-entropy loss를 사용하여 예측치 $\hat{a}$ 와 정답인 true 값 $a$ 간의 오차를 최소화해서 학습 시킨다.&lt;/p&gt;

&lt;p&gt;Input에서 Output까지 함수들은 무한정미분가능(function is smooth)하기 때문에, 손쉽게 Gradient와 back-propagate을 진행할 수 있다.&lt;/p&gt;

&lt;p&gt;업데이트 되는 weight Matrix는 $A$, $B$, $C$ 그리고 $W$다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;b-multiple-layers&quot;&gt;B. Multiple layers&lt;/h2&gt;

&lt;p&gt;위와 같은 Final Prediction 전 단계까지를 1 hop라고 규정하며, Multiple layers $K$ hops까지 확장 시킨다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;첫번째, k번째 layer에서 나온 output으로 나온 $o^k$과 input $u^k$ 는 합쳐져서 새로운 input $u^{k+1}$ 가 되어서 k + 1 layer로 들어가게 된다.
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;u^{k+1} = u^k + o^k&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;각 layer마다 input에 embed로 사용된 embedding matrices $A^k$ 와 $C^k$ 가 존재한다. 그러나 이들은 쉽게 트레이닝하고, parameter 갯수를 줄이기 위해서 제약이 존재한다.&lt;/li&gt;
  &lt;li&gt;Network의 마지막 부분에서만 W를 곱해서 Softmax 로 출력한다.
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{a} = Softmax(Wu^{K+1}) = Softmax(W(o^K + u^K))&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section&quot;&gt;두 가지 가중치 버젼&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Adjacent:
    &lt;blockquote&gt;
      &lt;p&gt;$k_{th}$ output layer embedding matrix가 다음 input layer의 embedding matrix가 된다. 예를 들면, $A^{k+1} = C^k$. 또한, 두 가지 제약 조건을 추가했는데, (a) answer prediction matrix가 최종 output embedding과 같고 ($W^T = C^K)$, (b) question embedding 과 첫번째 layer의 input embedding과 같게 했다($B = A^1$).&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Layer-wise (RNN - like):
    &lt;blockquote&gt;
      &lt;p&gt;Input 과 Output embedding들이 layer마다 다 같다. 예를 들면, $A^1 = A^2 = \cdots = A^K$ 과 $C^1 = C^2 = \cdots = C^K$ 같은 것들. 또한, hops간 u를 업데이트하기위한 linear mapping $H$ 를 추가하는 것이 도움이 된다는 것을 알아냈다. $u^{k+1} = Hu^k + o^k$.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;층별로 가중치를 묶는 두 번째 방법은, tranditional 한 RNN 방식으로 생각할 수가 있다. Internal output($u$)을 내보내는 것은 memory에 해당하고, external outputs($\hat{a}$)는 라벨을 예측하는 것과 같다. RNN 관점에서 보면, $u$, $u^{k+1}$ 은 hidden state고, 모델은 $A$ 를 사용하여internal output $p$ 를 생성한다. 모델은 $C$ 를 사용해서 $p$ 의 정보를 흡수하고, hidden state를 업데이트면서 이런 식으로 계속 진행한다. 여기서 표준 RNN과 다르게 output들을 $K$ hops 동안 계속 메모리에 저장하고, sampling하는 대신에 soft하게 둔다. 그렇게 하여 답변이 “진짜 세상”에 나오기 전에 여러번 계산을 거치게 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;c-synthetic-question-and-answering-experiments&quot;&gt;C. Synthetic Question and Answering Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;예시1:
    &lt;blockquote&gt;
      &lt;p&gt;Sam walks into the kitchen.&lt;/p&gt;

      &lt;p&gt;Sam picks up an apple.&lt;/p&gt;

      &lt;p&gt;Sam walks into the bedroom.&lt;/p&gt;

      &lt;p&gt;Sam drops the apple.&lt;/p&gt;

      &lt;p&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Q: Where is the apple?&lt;/span&gt;&lt;/p&gt;

      &lt;p&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;A. Bedroom&lt;/span&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;예시2:
    &lt;blockquote&gt;
      &lt;p&gt;Brian is a lion.&lt;/p&gt;

      &lt;p&gt;Julius is white.&lt;/p&gt;

      &lt;p&gt;Julius is a lion.&lt;/p&gt;

      &lt;p&gt;Bernhard is green.&lt;/p&gt;

      &lt;p&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Q: What color is Brian?&lt;/span&gt;&lt;/p&gt;

      &lt;p&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;A. White&lt;/span&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;예시3:
    &lt;blockquote&gt;
      &lt;p&gt;Mary journeyed to the den.&lt;/p&gt;

      &lt;p&gt;Mary went back to the kitchen.&lt;/p&gt;

      &lt;p&gt;John journeyed to the bedroom.&lt;/p&gt;

      &lt;p&gt;Mary discarded the milk.&lt;/p&gt;

      &lt;p&gt;&lt;span style=&quot;color: #7d7ee8&quot;&gt;Q: Where was the milk before the den?&lt;/span&gt;&lt;/p&gt;

      &lt;p&gt;&lt;span style=&quot;color: #e87d7d&quot;&gt;A. Hallway&lt;/span&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;예시에도 보듯이 문장의 일부만 답변의 정답정보를 가지고 있다. 이를 support subset이라고 하며, training 할때 support subset을 명시한다. 그러나 실제 테스트할 때는 이 support subset이 표시되지 않는다.&lt;/p&gt;

&lt;h3 id=&quot;model-details&quot;&gt;Model details&lt;/h3&gt;
&lt;p&gt;$K=3$ hops이고 weight sharing(Layer-wise) 모델을 쓸 것이다.  모든 output lists(답변에 여러 단어가 있는 경우)에 대하여 단어별로 분리하여 가능성을 나타낸다.&lt;/p&gt;

&lt;h4 id=&quot;sentence-representaion&quot;&gt;Sentence Representaion:&lt;/h4&gt;
&lt;p&gt;문장들을 표현할 때 두 가지 방법을 쓰기로 한다. 첫번째로는 BoW가 하나의 문장을 표현하는 것인데, 이 방법은 문장에서 단어의 순서(the order of the words in sentence)라는 특징을 잡을 수가 없다. 따라서 두 번째 방법으로, 문장에서 단어의 순서(the position of words)를 인코딩 한다. $m_i = \sum_{j} l_j \cdot Ax_{ij}$ 여기서 $l_j$ 연산은 element-wise multiplication이다. 또한, $l_j$ 는 $l_{kj} = (1-j/J) - (k/d)(1-2j/J)$, J는 문장에 있는 단어 갯수인 column vector 구조를 가지고 있다. 이것을 $PE$ (position encoding)이라고 하며, 이는 단어의 순서가 얼만큼 문장$m_i$에 영향을 주는지 알려준다. 나머지 question, memory inputs 그리고 memory outputs에서도 두 번째 방법으로 문장을 표현할 것이다.&lt;/p&gt;

&lt;h4 id=&quot;temporal-encoding&quot;&gt;Temporal Encoding:&lt;/h4&gt;
&lt;p&gt;많은 QA tasks에서는 temporal context라는 개념이 필요한데, 예를 들어 첫 번째 예시에서 Sam이 kitchen에 간 다음에 bedroom에 들어간 것을 알 수 있다. 이것을 모델에 적용하려면, memory vector를 약간 변형시킨다. $m_i = \sum_{j} Ax_{ij} + T_A(i)$, 여기서 $T_A(i)$ 는 일시적인 정보를 저장할 특별한 행렬 $T_A$ 의 i 번째 행이다. Ouput embedding할 때도 마찬가지로 해준다. $c_i = \sum_{j} Cx_{ij} + T_C(i)$. $T_A$ 와 $T_C$ 둘다 training 할 때 갱신한다. 그리고 A 와 C 랑 마찬가지로 제약 또한 같이 공유한다. 여기서 주의할 점은 문장들이 역순으로 인덱싱되어있다.  문장이 질문으로부터 상대적인 거리를 반영한다, 즉 $x_1$ 은 이야기의 마지막 문장이 된다.&lt;/p&gt;

&lt;h4 id=&quot;learning-time-invariance-by-injecting-random-noise&quot;&gt;Learning time invariance by injecting random noise:&lt;/h4&gt;
&lt;p&gt;$T_A$ 를 정규화 시킬때 더미 변수를 넣는 것이 도움이 된다. 즉, 트레이닝할 때는 랜덤으로 10%의 빈 메모리를 스토리에 넣는 것이다. 여기서 이를 Random Noise (RN)라고 한다&lt;/p&gt;

&lt;h3 id=&quot;training-details&quot;&gt;Training Details&lt;/h3&gt;
&lt;p&gt;bAbI training set중 10%는 Validation용으로 쓴다. 이는 optimal model architecture 과 hyperparameters를 선택하기 위해서다. Learning rate $\eta$는 0.01로 설정하고, 100번째 epoch가 될때 까지, 매 25번째 epochs 마다, $\eta$ 를 2로 나눠준다. Momentum 이나 weight decay는 사용되지 않았다. 가중치들은 $\mu = 0$, $\sigma = 0.1$ 인 가우시안 정규분포로 초기값을 설정했다. 모든 training에 사용된 batch size는 32 이며, gradients는 L2로 정규화해서 40이 넘으면 어떤 스칼라를 나눠서 norm을 40으로 만들어준다.&lt;/p&gt;

&lt;p&gt;어떤 모델에서는 처음시작에 softmax를 안쓰다가 (linear하게 만드는 것) 나중에 최종 예측시에 softmax를 썼다. 그러다 validation loss가 더 이상 떨어지지 않을 때, 다시 softmax 층이 다시 입력이 되서 트레이닝을 한다. 이를 Linear Start (LS) training이라고 하며, 이때 초기 learning rate 를 $\eta = 0.005$ 로 설정한다.&lt;/p&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;MemNN: strongly supervised, softmax대신 max operation사용&lt;/li&gt;
  &lt;li&gt;MemNN-WSH: weakly supervised, 트레이닝시 supporting sentence labels를 안씀&lt;/li&gt;
  &lt;li&gt;LSTM: weakly supervised&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;p&gt;모델 선택을 다양하게 했다.&lt;/p&gt;

&lt;p&gt;1) BoW vs Position Encoding&lt;/p&gt;

&lt;p&gt;2) 20 tasks를 독립적으로 트레이닝공유($d = 20$) vs joint 트레이닝 ($d = 50$)&lt;/p&gt;

&lt;p&gt;3) Linear Start Training(Softmax처음에 없엔 것) vs Softmax가 처음부터 있는 것&lt;/p&gt;

&lt;p&gt;4) hops를 1 ~ 3까지 설정&lt;/p&gt;

&lt;p&gt;결과는 논문 참조. 퍼포먼스는 supervised models이 제일 좋게 나왔으나, MemN2N with position encoding + linear start + random noise, jointly trained 도 근접하게 나옴&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;d&quot;&gt;D.참고문헌&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1503.08895&quot;&gt;End-To-End Memory Networks: Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 04 Aug 2017 22:09:05 +0900</pubDate>
        <link>http://simonjisu.github.io/datascience/2017/08/04/E2EMN.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/datascience/2017/08/04/E2EMN.html</guid>
        
        
        <category>DataScience</category>
        
      </item>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;h2 id=&quot;adding-new-posts&quot;&gt;Adding New Posts&lt;/h2&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;h3 id=&quot;tags-and-categories&quot;&gt;Tags and Categories&lt;/h3&gt;

&lt;p&gt;If you list one or more categories or tags in the front matter of your post, they will be included with the post on the page as links. Clicking the link will bring you to an auto-generated archive page for the category or tag, created using the &lt;a href=&quot;https://github.com/jekyll/jekyll-archives&quot;&gt;jekyll-archive&lt;/a&gt; gem.&lt;/p&gt;

&lt;h3 id=&quot;cover-images&quot;&gt;Cover Images&lt;/h3&gt;

&lt;p&gt;To add a cover image to your post, set the “cover” property in the front matter with the relative URL of the image (i.e. &lt;code&gt;cover: &quot;/assets/cover_image.jpg&quot;&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&quot;code-snippets&quot;&gt;Code Snippets&lt;/h3&gt;

&lt;p&gt;You can use &lt;a href=&quot;https://highlightjs.org/&quot;&gt;highlight.js&lt;/a&gt; to add syntax highlight code snippets:&lt;/p&gt;

&lt;p&gt;Use the &lt;a href=&quot;https://github.com/Shopify/liquid/wiki/Liquid-for-Designers&quot;&gt;Liquid&lt;/a&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;highlight&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&amp;lt;language&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt; tag to add syntax highlighting to code snippets.&lt;/p&gt;

&lt;p&gt;For instance, this template…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% highlight javascript %}    
function demo(string, times) {    
  for (var i = 0; i &lt;span class=&quot;nt&quot;&gt;&amp;lt; times&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;++)&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;    
    &lt;span class=&quot;na&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;);&lt;/span&gt;    
  &lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;    
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;    
&lt;span class=&quot;na&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;(&quot;&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;hello&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;world&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;!&quot;,&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;{%&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;endhighlight&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;%}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;…will come out looking like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hello, world!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Syntax highlighting is done using &lt;a href=&quot;https://highlightjs.org/&quot;&gt;highlight.js&lt;/a&gt;. You can change the active theme in &lt;a href=&quot;https://github.com/bencentra/centrarium/blob/2dcd73d09e104c3798202b0e14c1db9fa6e77bc7/_includes/head.html#L15&quot;&gt;head.html&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;images&quot;&gt;Images&lt;/h3&gt;

&lt;p&gt;Lightbox has been enabled for images. To create the link that’ll launch the lightbox, add &lt;code&gt;data-lightbox&lt;/code&gt; and &lt;code&gt;data-title&lt;/code&gt; attributes to an &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tag around your &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag. The result is:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;//bencentra.com/assets/images/falcon9_large.jpg&quot; data-lightbox=&quot;falcon9-large&quot; data-title=&quot;Check out the Falcon 9 from SpaceX&quot;&gt;
  &lt;img src=&quot;//bencentra.com/assets/images/falcon9_small.jpg&quot; title=&quot;Check out the Falcon 9 from SpaceX&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information, check out the &lt;a href=&quot;http://lokeshdhakar.com/projects/lightbox2/&quot;&gt;Lightbox&lt;/a&gt; website.&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://github.com/jekyll/jekyll-help&quot;&gt;Jekyll’s dedicated Help repository&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sat, 18 Apr 2015 17:43:59 +0900</pubDate>
        <link>http://simonjisu.github.io/jekyll/2015/04/18/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">http://simonjisu.github.io/jekyll/2015/04/18/welcome-to-jekyll.html</guid>
        
        <category>jekyll</category>
        
        <category>welcome</category>
        
        
        <category>Jekyll</category>
        
      </item>
    
  </channel>
</rss>
