<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>RNN & LSTM - 1: RNN</title>
  <meta name="description" content="자세하게 설명한 RNN 과 LSTM 시리즈 - 1">
  
  <meta name="author" content="Soo">
  <meta name="copyright" content="&copy; Soo 2018">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="자세하게 설명한 RNN 과 LSTM 시리즈 - 1" />
  <meta property="og:url" content="http://simonjisu.github.io" />
  <meta property="og:site_name" content="Soo" />
  <meta property="og:title" content="RNN & LSTM - 1: RNN" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://simonjisu.github.io/assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RNN & LSTM - 1: RNN">
  <meta name="twitter:description" content="자세하게 설명한 RNN 과 LSTM 시리즈 - 1">
  <meta name="twitter:image" content="http://simonjisu.github.io/assets/logo.png">
  <meta name="twitter:url" content="http://simonjisu.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://simonjisu.github.io/deeplearning/2018/03/07/rnnlstm.html">
  <link rel="alternate" type="application/rss+xml" title="Soo" href="http://simonjisu.github.io/feed.xml" />


  <!-- <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" ></script> -->

  <!-- Latex -->
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    MathJax.Hub.Config({
        TeX: {
            extensions: ["mhchem.js", "cancel.js"]
        },
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true,
            ignoreClass: "no-mathjax",
        },
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            // linebreaks: { automatic: true }
        },
        menuSettings: {
            zoom: "Click",
            zscale: "200%"
        }
    });
  </script>

  <!--lightSlider-->
  <link rel="stylesheet" href="/css/lightslider.css" />


</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="Soo">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        
          
          <li class="nav-link"><a href="/about/">About</a>
          
        
          
        
          
        
          
        
          
          <li class="nav-link"><a href="/posts/">Posts</a>
          
        
          
        
          
        
          
        
          <li class="nav-link"><a href="https://github.com/simonjisu"> GitHub </a>
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">
<!-- <h1></h1>
 -->
<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">RNN & LSTM - 1: RNN</h1>
      <p class="info">by <strong>Soo</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">March 7, 2018</div>
  <div class="post-categories">
   in 
    <!--  -->
    <a href="/posts/#DeepLearning">Deeplearning</a>
    <!--  -->
  
  </div>
</section>

<article class="post-content">
  <h1 id="rnn--lstm----1">자세하게 설명한 RNN 과 LSTM 시리즈 - 1</h1>

<h2 id="rnnrecurrent-neural-network">RNN(Recurrent Neural Network)</h2>
<p>우리가 사는 세상에 연속된 일들, 혹은 시간과 연관된 일은 매우매우 많을 것이다. 예를 들자면, 지금 이 글을 읽은 당신도 앞에 있는 내용을 기억하면서 글을 읽고 있을 것이다. 일반적인 신경망 구조에서는 이 ‘기억’ 이라는 시스템이 존재 하지 않는다. 하지만 RNN은 다르다. 이놈은 ‘기억’을 할 수가 있다. 그렇다면 RNN과 기존 신경망과 어떻게 다른지를 한번 살펴보자.</p>

<h2 id="rnn-">RNN 구조</h2>
<p><img src="/assets/ML/rnn/rnn.png" alt="Drawing" style="width=500px" /></p>

<p>RNN은 중간의 Hidden 층이 순환한다고해서 순환 신경망이라고 한다. 왼쪽의 구조를 펼쳐서 보면, 중간의 Hidden 노드가 어떤 방향으로 계속 이어진 다는 것을 알 수 있다. 이러한 쇠사슬 같은 성격은 RNN으로 하여금 연속된 이벤트와 리스트에 적합한 구조로 만들어 준다.</p>

<p>이렇게 보면 엄청 어렵게 느껴질 수 있다. 그렇다면 예시를 들어서 RNN이 어떻게 돌아가는지 수학적으로 살펴보자.</p>

<h3 id="section">기본 신경망 구조</h3>

<p>기존의 신경 구조를 한번 다시 되새겨보자.</p>

<p><img src="/assets/ML/rnn/stick.png" alt="Drawing" height="200" width="200" /></p>

<p>여러개의 노드로 구성된 작은 블럭을 하나의 층이라고 가정하자. 기존의 신경망 구조는 아래와 같다.</p>

<p><img src="/assets/ML/rnn/basic_nn_mnist.png" alt="Drawing" /></p>

<p>Input $x$ 가 선형 결합 후, Hidden 에 Activation function을 거쳐 다시 선형결합을 통해 Output $y$를 구해 예측하는 알고리즘이다. 여기서 첫번째 데이터($x_1$)와 그 다음 데이터($x_2$ 등)간의 구조는 독립적이라고 할 수 있다.</p>

<h3 id="forward">Forward</h3>
<p>예시로 time step($T$)이 3인 RNN을 살펴보자. (좌우 클릭으로 프로세스 과정 볼 수 있다)</p>

<ul id="light-slider1">
    <li><img src="/assets/ML/rnn/rnn_0.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_1.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_2.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_3.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_4.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_5.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_6.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_7.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_8.png" /></li>
  </ul>

<p>Time step = 0 일때, 각각 Layer들의 Weight를 초기화하게 된다. $h_0$ 층은 0으로, 나머지는 Xavier 가중치 초기값으로 초기화한다. 또한 각 가중치는 각각 layer에서 공유하게 된다.
(가중치 초기화를 잊어 버렸다면 <a href="https://simonjisu.github.io/datascience/2018/01/24/numpywithnn_6.html"><span style="color: #7d7ee8">여기</span></a>로)</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
h_t &= \tanh(W_{hh} h_{t-1}+W_{xh}x_t+b_h) \\
y_t &= W_{hy} h_t + b_y
\end{aligned}
\quad for\ t\ in\ T %]]></script>

<p>그리고, 시간이 지날때마 위의 식 처럼 Forward가 진행된다.</p>

<p>최종 Cost는 모든 Cost Function의 평균으로 구해진다.</p>

<h3 id="backward">Backward</h3>
<p>RNN에서는 일반적인 신경망과 다른 Backward 알고리즘을 쓴다. 시간 경과에 따른 BackPropagation을 BPTT(BackPropagation Through Time)이라고 부른다.</p>

<ul id="light-slider1">
    <li><img src="/assets/ML/rnn/rnn_back0.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back1.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back2.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back3.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back4.png" /></li>
    <li><img src="/assets/ML/rnn/rnn_back5.png" /></li>
  </ul>

<p>최종적으로 학습 될 값은 Loss Function에서 각 미분한 ${\frac{\partial L}{\partial W}}^{(1)}$, ${\frac{\partial L}{\partial W}}^{(2)}$, ${\frac{\partial L}{\partial W}}^{(3)}$ 의 합으로 구해진다.</p>

<h3 id="long-term-dependency-">장기 의존성(Long-Term Dependency) 문제</h3>
<p>RNN이 이론상으로는 sequence의 첫번째 항부터 끝까지(즉, $x_1 \cdots x_T$ 까지) 학습 할 수 있을 것으로 보이나, 실제로는 장기기억, 즉 Time Step이 길어 질 수록 예전에 있던 정보를 기억 못한다. 이를 <strong>장기 의존성(Long-Term Dependency)</strong> 문제라고 한다.</p>

<p><img src="/assets/ML/rnn/rnn_bad.png" alt="Drawing" /></p>

<p>그 이유는 우리가 업데이트 하려는 미분 식을 살펴보면 알 수 있다. 예를 들어 $W_{hh}$ 를 업데이트 한다고 하자.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\dfrac{\partial L}{\partial W_{hh}}  
&= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial W_{hh}} + \cdots +
\dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial W_{hh}} \\
&= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial y_T} \dfrac{\partial y_T}{\partial h_T} \dfrac{\partial h_T}{\partial h_{T-1}}  \cdots \dfrac{\partial h_2}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}} +
\cdots + \dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}} \\
&= \dfrac{\partial L}{\partial Cost_T} \dfrac{\partial Cost_T}{\partial y_T} \dfrac{\partial y_T}{\partial h_T} \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}} \dfrac{\partial h_1}{\partial W_{hh}} + \cdots + \dfrac{\partial L}{\partial Cost_1} \dfrac{\partial Cost_1}{\partial y_1} \dfrac{\partial y_1}{\partial h_1} \dfrac{\partial h_1}{\partial W_{hh}}
\end{aligned} %]]></script>

<p>위의 식중에 $\prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}}$ 부분을 자세히 펼쳐보면 아래와 같다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial h_{T-i}}
&= \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial a_{T-i+1}} \dfrac{\partial a_{T-i+1}}{\partial h_{T-i}} \\
&= \prod_{i=1}^{T-1} \dfrac{\partial h_{T-i+1}}{\partial a_{T-i+1}} W_{hh}
\end{aligned} %]]></script>

<p>여기서 $a_t=W_{hh}h_{t-1} + W_{xh}x_t + b_h$ 이다.</p>

<p>앞부분 $\frac{\partial h_{T-i+1}}{\partial a_{T-i+1}}$은 <strong>tanh</strong> 의 미분 값이다. 아래 그림과 같이 tanh의 미분 값은 0과 1사이의 값이다.</p>

<p><img src="/assets/ML/rnn/tanh.png" style="width=500px" />
(그림출처: http://nn.readthedocs.io/en/latest/transfer/)</p>

<p>뒷부분인 $W_{hh}$의 값들은 세가지 경우가 있다. 1과 같게 되면 Gradient가 수렴될 가능성이 높다. 그러나 1보다 클 경우 gradient가 무한대로 발산하는 <strong>Exploding Gradient</strong> 문제가 발생한다. 그러나 보통의 경우 $W_{hh}$ 의 값들은 1보다 작다. (아래 논문 참고)</p>

<p>0과 1사이의 작은 값을 계속 곱하게 되면 0으로 수렴한다. 따라서, 두 가지를 종합 해보았을 때, 출력값과 멀리 떨어진 Time Step일 수록 역전파가 전달 되지 않는 <strong>Vanishing Gradient</strong> 문제가 생기게 된다.</p>

<ul>
  <li><a href="http://proceedings.mlr.press/v28/pascanu13.pdf"><span style="color: #7d7ee8">On the difficulty of training recurrent neural networks</span></a> 논문에서는 Vanishing &amp; Exploding Gradient 문제를 자세히 다루고 있다.</li>
</ul>

<p>장기기억을 하지 못한다는 문제가 생기면서, 이를 해결하기 위해서 몇 가지 방법이 나왔다. 첫째로, Activation Function을 <strong>tanh</strong> 을 쓰면 기울기가 0과 1사이의 값으로 고정되니 <strong>ReLU</strong> 를 쓰자는 방법이 있었다. 둘째로, <strong>LSTM</strong>, <strong>GRU</strong> 등 새로운 방법들이 등장했다. 이 방법은 다음 시간에 설명하겠다. 더불어 Backward 의 계산 그래프도 같이 첨부하겠다.</p>

</article>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
      <a href="//www.facebook.com/sharer.php?t=RNN+%26+LSTM+-+1%3A+RNN&u=http%3A%2F%2Fsimonjisu.github.io%2Fdeeplearning%2F2018%2F03%2F07%2Frnnlstm.html"
        onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
</section>



  

  
    
    <!-- <h2 id="DeepLearning">0</h2> -->
    <!-- <h2 id="DeepLearning">/deeplearning/2018/03/14/rnnlstm2.html</h2> -->
    
    <!-- <h2 id="DeepLearning">2</h2> -->
    <!-- <h2 id="DeepLearning">/deeplearning/2018/02/08/numpywithnn_8.html</h2> -->
    



  
	<section class="post-navigation">
		<span class="prev-post">
			
				<a href="/deeplearning/2018/02/08/numpywithnn_8.html">
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-left fa-stack-1x fa-inverse"></i>
					</span>
					<span class="page-number">NUMPY with NN - 8: Summary</span>
				</a>
			
		</span>
		<span class="next-post">
			
				<a href="/deeplearning/2018/03/14/rnnlstm2.html">
					<span class="page-number">RNN & LSTM - 2: Numpy with RNN</span>
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-right fa-stack-1x fa-inverse"></i>
					</span>
				</a>
			
		</span>
	</section>




  
  <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'soopace';

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </section>
  


</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Soo</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">
        
        
          <li class="nav-link"><a href="/about/">About</a>
        
        
        
        
        
        
        
        
        
          <li class="nav-link"><a href="/posts/">Posts</a>
        
        
        
        
        
        
        
        
      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:simonjisu@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">simonjisu@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://www.facebook.com/simonjisu" title="Friend me on Facebook">
              <i class="fa fa-facebook"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://github.com/simonjisu" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
        
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">My Blog
</p>
    </div>

  </div>
</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>

<!-- Mathjax -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();
  $("code").addClass("python")
  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});
$("code").addClass("python")
</script>

<!--lightSlider-->
<script src="/js/lightslider.js"></script>
<script src='/js/multi_slider.js' type="text/javascript"></script>





    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-102691608-1', 'auto');
  ga('send', 'pageview');
</script>


  </body>

</html>
