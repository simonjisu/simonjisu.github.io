<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>NUMPY with NN - 7: Batch Normalization</title>
  <meta name="description" content="Numpy로 짜보는 Neural Network Basic - 7">
  
  <meta name="author" content="Soo">
  <meta name="copyright" content="&copy; Soo 2018">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="Numpy로 짜보는 Neural Network Basic - 7" />
  <meta property="og:url" content="http://simonjisu.github.io" />
  <meta property="og:site_name" content="Soo" />
  <meta property="og:title" content="NUMPY with NN - 7: Batch Normalization" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://simonjisu.github.io/assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="NUMPY with NN - 7: Batch Normalization">
  <meta name="twitter:description" content="Numpy로 짜보는 Neural Network Basic - 7">
  <meta name="twitter:image" content="http://simonjisu.github.io/assets/logo.png">
  <meta name="twitter:url" content="http://simonjisu.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://simonjisu.github.io/deeplearning/2018/01/25/numpywithnn_7.html">
  <link rel="alternate" type="application/rss+xml" title="Soo" href="http://simonjisu.github.io/feed.xml" />


  <!-- <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" ></script> -->

  <!-- Latex -->
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    MathJax.Hub.Config({
        TeX: {
            extensions: ["mhchem.js", "cancel.js"]
        },
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true,
            ignoreClass: "no-mathjax",
        },
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            // linebreaks: { automatic: true }
        },
        menuSettings: {
            zoom: "Click",
            zscale: "200%"
        }
    });
  </script>

  <!--lightSlider-->
  <link rel="stylesheet" href="/css/lightslider.css" />

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5GSH93K');</script>
  <!-- End Google Tag Manager -->

  <!-- Google Ad -->
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-5855917513482122",
      enable_page_level_ads: true
    });
  </script>
  <!-- End Google Ad -->

</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="Soo">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        
          
        
          
        
          
        
          
          <li class="nav-link"><a href="/posts/">Posts</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          <li class="nav-link"><a href="https://simonjisu.gitbook.io/resume/"> ME </a>
          <li class="nav-link"><a href="https://github.com/simonjisu"> GitHub </a>
      </ul>
    </nav>
  </div>
</header>


    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5GSH93K"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div class="page-content">
        <div class="post">
<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">NUMPY with NN - 7: Batch Normalization</h1>
      <p class="info">by <strong>Soo</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">January 25, 2018</div>
  <div class="post-categories">
   in 
    
    <a href="/posts/#DeepLearning">Deeplearning</a>
    
  
  </div>
</section>

<article class="post-content">
  <h1 id="numpy--neural-network-basic---7">Numpy로 짜보는 Neural Network Basic - 7</h1>

<hr />
<h2 id="part-3">학습관련 기술 Part 3</h2>

<h3 id="batch-normalization">배치 정규화 (Batch Normalization)</h3>
<p>배치 정규화란 미니배치 단위로 선형합인 <strong>$a$</strong> 값을 정규화하는 것이다. 즉, 미니배치에 한해서 데이터 분포가 평균이 0 분산이 1이 되도록 한다. 이는 데이터 분포가 덜 치우치게 하는 효과가 있어서 가중치 초기화 값의 영향을 덜 받게한다. 또한, 학습속도를 증가시키고 regularizer 역할을 하여 Overfitting을 방지함으로 Dropout의 필요성을 줄인다. <del>자세한 내용은 논문을 참고하자!</del></p>

<p>Paper: <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>

<p>기본적인 아이디어는 아래와 같다. $D$ 차원의 미니배치 데이터 $x = (x^{(1)}, \cdots, x^{(k)}, \cdots, x^{(D)})$에 대해서 각각의 평균과 분산을 구한 후, 정규화를 통해 새로운 $x^{(k)}$ ($\hat{x}^{(k)}$) 를 구한 후에 Scaling($\gamma$) 과 Shifting($\beta$)을 거쳐 새로운 $y$ 를 기존의 선형합성 곱인 $a$ 를 대신해 활성화 함수에 넣는 것이다.</p>

<p><img src="/assets/ML/nn/6/batch_norm_idea.png" alt="Drawing" style="width=500px" /></p>

<p>따라서, 하나의 Hidden Layer 는 $Affine \rightarrow BatchNorm \rightarrow Activation$ 으로 구성된다.</p>

<h3 id="backpropogation-">배치 정규화의 BackPropogation 이해하기</h3>

<p><img src="/assets/ML/nn/NN_batchnorm.png" alt="Drawing" /></p>

<h4 id="forward">Forward:</h4>

<p>x 부터 out 까지 차근차근 진행해보자. 헷갈리지 말아야할 점은 위에 공식에서 $i$ 는 batch를 iteration 한 것이라는 점이다.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>## Forward Process
# step-1: mu (D,)
mu = x.mean(axis=0)
# step-2: xmu (N, D)
xmu = x - mu
# step-3: sq (N, D)
sq = xmu**2
# step-4: var (D,)
var = np.mean(sq, axis=0)
# step-5: std (D,)
std = np.sqrt(var + 1e-6)
# step-6: invstd (D,)
invstd = 1.0 / std
# step-7: xhat (N, D)
xhat = xmu * invstd
# step-8: scale (N, D)
scale = gamma * xhat
# step-9: out (N, D)
out = scale + beta
</code></pre>
</div>

<h4 id="backward">Backward:</h4>

<p>우리의 목표는 $\dfrac{\partial L}{\partial x}, \dfrac{\partial L}{\partial \gamma}, \dfrac{\partial L}{\partial \beta}$ 를 구해서, $\dfrac{\partial L}{\partial x}$ 는 Affine Layer로 역전파 시키고 $\gamma, \beta$ 는 학습 시키는 것이다.</p>

<p><strong>Step-9:</strong></p>

<p>Forward : $out(scale, \beta) = scale + \beta$</p>

<ul>
  <li>더하기 노드의 역전파는 그대로 흘러간다.</li>
</ul>

<script type="math/tex; mode=display">\begin{cases} dscale = \dfrac{\partial L}{\partial scale} = \dfrac{\partial L}{\partial out} \dfrac{\partial out}{\partial scale} = 1 * dout \\
\\
d\beta = \dfrac{\partial L}{\partial \beta} = \dfrac{\partial L}{\partial out} \dfrac{\partial out}{\partial \beta} = 1 * \sum_i^N dout \end{cases}</script>

<p><strong>Step-8:</strong></p>

<p>Forward : $scale(\gamma, \hat{x}_i) = \gamma \ * \ \hat{x}_i$</p>

<ul>
  <li>곱의 노드의 역전파는 들어왔던 신호를 역으로 곱해서 흘려 보낸다.</li>
</ul>

<script type="math/tex; mode=display">\begin{cases}
d\hat{x}_i = \dfrac{\partial L}{\partial \hat{x}_i} = \dfrac{\partial L}{\partial scale} \dfrac{\partial scale}{\partial \hat{x}_i} = 1 * \sum_i^N dout \\
\\
d\gamma = \dfrac{\partial L}{\partial \gamma} = \dfrac{\partial L}{\partial scale} \dfrac{\partial scale}{\partial \gamma} = \sum_i^N dout \ * \ \hat{x}_i
\end{cases}</script>

<p><strong>Step-7:</strong></p>

<p>Forward : $\hat{x}_i(xmu, invstd) = xmu \ * \ invstd$</p>

<ul>
  <li>$xmu$는 윗쪽(step-7 $\rightarrow$ step-2)과 아래쪽(step-3 $\rightarrow$ step-2) 으로 두 번 돌아가기 때문에 첨자를 단다.</li>
</ul>

<script type="math/tex; mode=display">\begin{cases}
dxmu_1= \dfrac{\partial L}{\partial xmu_1} = \dfrac{\partial L}{\partial \hat{x}_i} \dfrac{\partial \hat{x}_i}{\partial xmu_1} = d\hat{x}_i \ * \ invstd \\
\\
dinvstd = \dfrac{\partial L}{\partial \hat{x}_i} = \dfrac{\partial L}{\partial \hat{x}_i} \dfrac{\partial \hat{x}_i}{\partial invstd} = d\hat{x}_i \ * \ xmu
\end{cases}</script>

<p><strong>Step-6:</strong></p>

<p>Forward : $invstd(\sigma) = \dfrac{1}{\sigma}$</p>

<ul>
  <li>$f(x) = \dfrac{1}{x}$ 의 미분은 $f’(x) = -\dfrac{1}{x^2} = -f(x)^2$ 이기 때문에 아래와 같다.</li>
</ul>

<script type="math/tex; mode=display">d\sigma = \dfrac{\partial L}{\partial \sigma} = \dfrac{\partial L}{\partial invstd} \dfrac{\partial invstd}{\partial \sigma} = dinvstd \ * \ (-invstd^2)</script>

<p><strong>Step-5:</strong></p>

<p>Forward : $\sigma(var) = \sqrt{var + \epsilon}$</p>

<ul>
  <li>$f(x) = \sqrt{x + \epsilon}$ 의 미분은 $f’(x) = -\dfrac{1}{2}(x+\epsilon)^{-\frac{1}{2}}$ 이기 때문에 아래와 같다.</li>
</ul>

<script type="math/tex; mode=display">dvar = \dfrac{\partial L}{\partial var} = \dfrac{\partial L}{\partial \sigma} \dfrac{\partial \sigma}{\partial var} = d\sigma \ * \ (-\dfrac{1}{2}(var+\epsilon)^{-\frac{1}{2}})</script>

<p><strong>Step-4:</strong></p>

<p>Forward : $var(sq) = \dfrac{1}{N} \sum_i^N sq$</p>

<ul>
  <li>$f(x) = \dfrac{1}{N} \sum_i^N x_i$ 의 미분은 $f’(x) = \dfrac{1}{N} \sum_i^N 1$ 이기 때문에 아래와 같다. 단, x의 형상(shape)이 같아야한다.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
dsq = \dfrac{\partial L}{\partial sq} = \dfrac{\partial L}{\partial var} \dfrac{\partial var}{\partial sq} = \dfrac{1}{N} dvar \ * \ \begin{bmatrix} 1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1 \end{bmatrix}_{(N, D)} = \dfrac{1}{N} dvar \ * \ ones(N, D) %]]></script>

<p><strong>Step-3:</strong></p>

<p>Forward : $sq = xmu^2$</p>

<ul>
  <li>$f(x) = x^2$ 의 미분은 $f’(x) = 2x$ 이기 때문에 아래와 같다.</li>
</ul>

<script type="math/tex; mode=display">dxmu_2 = \dfrac{\partial L}{\partial xmu_2} = \dfrac{\partial L}{\partial sq} \dfrac{\partial sq}{\partial xmu_2} = dsq \ * \ 2 \ xmu</script>

<p><strong>Step-2:</strong></p>

<p>Forward : $xmu = x_i - \mu$</p>

<ul>
  <li>$dxmu = dxmu_1 + dxmu_2$ 로 정의 된다. 곱의 미분 법칙 생각해보면 된다. $h(x) = f(x) g(x)$ 를 $x$ 에 대해서 미분하면 $f’(x)g(x) + f(x)g’(x)$ 기 때문이다. <br />
또한 이것도 덧셈과 마찬가지로 그대로 흘러 보내는다 밑에 쪽은 -1 을 곱해서 흘려 보낸다.</li>
</ul>

<script type="math/tex; mode=display">\begin{cases}
dx_1= \dfrac{\partial L}{\partial x_1} = \dfrac{\partial L}{\partial xmu} \dfrac{\partial xmu}{\partial x_1} = dmu \ * \ 1 \\
\\
d\mu = \dfrac{\partial L}{\partial \mu} = \dfrac{\partial L}{\partial xmu} \dfrac{\partial xmu}{\partial \mu} = \sum_i^N dxmu \ * \ (-1)
\end{cases}</script>

<p><strong>Step-1:</strong></p>

<p>Forward : $\mu = \dfrac{1}{N} \sum_i^N x_i$</p>

<ul>
  <li>step-4에서 설명했다.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
dx_2 = \dfrac{\partial L}{\partial x_2} = \dfrac{\partial L}{\partial \mu} \dfrac{\partial \mu}{\partial x_2} = \dfrac{1}{N} d\mu \ * \ \begin{bmatrix} 1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1 \end{bmatrix}_{(N, D)} = \dfrac{1}{N} d\mu \ * \ ones(N, D) %]]></script>

<p><strong>Step-0:</strong></p>
<ul>
  <li>최종적으로 구하는 $dx = \dfrac{\partial L}{\partial x} = dx_1 + dx_2$ 로 정의 된다.</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>## Backward Process
# step-9: out = scale + beta
dbeta = dout.sum(axis=0)
dscale = dout
# step-8: scale = gamma * xhat
dgamma = np.sum(xhat * dout, axis=0)
dxhat = gamma * dscale
# step-7: xhat = xmu * invstd
dxmu1 = dxhat * invstd
dinvstd = np.sum(dxhat * xmu, axis=0)
# step-6: invstd = 1 / std
dstd = dinvstd * (-invstd**2)
# step-5: std = np.sqrt(var + 1e-6)
dvar = -0.5 * dstd * (1 / np.sqrt(var + 1e-6))
# step-4: var = sum(sq)
dsq = (1.0 / batch_size) * np.ones(input_shape) * dvar
# step-3: sq = xmu**2
dxmu2 = dsq * 2 * xmu
# step-2: xmu = x - mu
dxmu = dxmu1 + dxmu2
dmu = -1 * np.sum(dxmu, axis=0)
dx1 = dxmu * 1
# step-1: mu = mean(x)
dx2 = (1.0 / batch_size) * np.ones(input_shape) * dmu
# step-0:
dx = dx1 + dx2
</code></pre>
</div>

<h4 id="section">실제 구현</h4>
<p>그러나 실제 구현 시에는 training 과 testing을 나눠서 아래와 같이 진행된다.</p>

<p><img src="/assets/ML/nn/6/batch_norm_al.png" alt="Drawing" style="width=500px" /></p>

<hr />
<h3 id="backpropogation---">첨부: Backpropogation 전체 미분 수학식</h3>

<ul>
  <li>수식의 이해는 이분의 블로그에서 많은 참조를 했다. Blog: [<a href="http://cthorey.github.io/backpropagation/">Clement Thorey</a>]</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
Y &= \gamma \hat{X} + \beta \\
\hat{X} &= (X - \mu)(\sigma^2+\epsilon)^{-1/2}
\end{aligned} %]]></script>

<p><strong>size:</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
Y, \hat{X}, X &= (N, D) \\
\mu, \sigma, \gamma, \beta &= (D,)
\end{aligned} %]]></script>

<p><br /></p>

<p>$N$은 미니 배치 싸이즈고, $D$는 데이터의 차원 수다.</p>

<p>Matrix 로 정의한 수식을 다시 원소별로 표기를 정의 해보자. 매트릭스 $Y, X, \hat{X}$ 와 벡터 $\gamma, \beta$ 그리고 위에 수식은 아래와 같이 다시 정의 해볼 수 있다. (왜 매트릭스와 벡터인지는 Forward 과정에 나와있다. 각 차원별로 평균과 분산을 구하는걸 잊지말자)</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
y_{kl} &= \gamma_l \hat{x}_{kl} + \beta_l \\
\hat{x}_{kl} &= (x_{kl} - \mu_l)(\sigma_l^2+\epsilon)^{-1/2}
\end{aligned} %]]></script>

<script type="math/tex; mode=display">where\quad \mu_l = \dfrac{1}{N} \sum_{p=1}^{N} x_{pl} , \quad \sigma_l^2 = \dfrac{1}{N} \sum_{p=1}^{N} (x_{pl}-\mu_l)^2</script>

<script type="math/tex; mode=display">with\quad k = [1, \cdots, N] \ ,\  l = [1, \cdots, D]</script>

<p><br /></p>

<p>이제 우리고 구하려고 하는 미분 값들$(\dfrac{\partial L}{\partial x}, \dfrac{\partial L}{\partial \gamma}, \dfrac{\partial L}{\partial \beta})$을 하나씩 구해보자.</p>

<h4 id="xij---">$x_{ij}$ 에 대한 미분</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\dfrac{\partial L}{\partial x_{ij}}
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial x_{ij}} \\
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \hat{x}_{kl}} \dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} \\
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \cdot \gamma_l \cdot \dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} \end{aligned} %]]></script>

<p><br /></p>

<p><script type="math/tex">\dfrac{\partial \hat{x}_{kl}}{\partial {x}_{ij}} = \dfrac{\partial f}{\partial {x}_{ij}} g + f \dfrac{\partial g}{\partial {x}_{ij}} \quad where \quad \begin{cases} f = (x_{kl} - \mu_l) \\ g = (\sigma_l^2+\epsilon)^{-1/2} \end{cases}</script> 에 대한 미분을 구해보자.</p>

<ul>
  <li>우선 분자 $f = (x_{kl} - \mu_l)$ 에 대한 미분을 하면 아래와 같다.</li>
</ul>

<script type="math/tex; mode=display">\dfrac{\partial f}{\partial {x}_{ij}} = \delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl}</script>

<p><br /></p>

<script type="math/tex; mode=display">\delta_{m,n} = \begin{cases} 1 \quad where \quad m = n \\ 0 \quad otherwise \end{cases}</script>

<p><br /></p>

<p>$\delta_{m,n}$ 은 앞첨자 $m$ 이 뒷첨자 $n$과 같다면 1이 된다는 뜻이다.</p>

<p>즉, 여기서 $i$ 가 $[1 \cdots k \cdots D]$ 까지, $j$ 가 $[1 \cdots l \cdots D]$ 까지 iteration 할 것인데, 오직 $i=k, j=l$ 일때만 앞 항인 $\delta_{il} \delta_{jl} = 1$ 이 될 것이고, $j=l$ 일때만 뒷항인 $\frac{1}{N} \delta_{jl} = \frac{1}{N}$ 이 될 것이다.</p>

<ul>
  <li>분모 $g = (\sigma_l^2+\epsilon)^{-1/2}$ 에 대한 미분은 아래와 같다.</li>
</ul>

<script type="math/tex; mode=display">\dfrac{\partial g}{\partial {x}_{ij}} = -\dfrac{1}{2}(\sigma_l^2 + \epsilon)^{-3/2} \dfrac{\partial \sigma_l^2}{\partial x_{ij}}</script>

<p><br /></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} where \quad \sigma_l^2
&= \dfrac{1}{N} \sum_{p=1}^{N} (x_{pl}-\mu_l)^2 \\
\dfrac{\partial \sigma_l^2}{\partial x_{ij}}
&= \dfrac{1}{N} \sum_{p=1}^{N} 2(x_{pl}-\mu_l)(\delta_{ip} \delta_{jl} - \frac{1}{N} \delta_{jl}) \\
&= \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl} - \dfrac{2}{N^2} \sum_{p=1}^N (x_{pl}-\mu_l) \delta_{jl} \\
& = \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl} - \dfrac{2}{N} \delta_{jl} (\dfrac{1}{N}  \sum_{p=1}^N  (x_{pl}-\mu_l)) \cdots (1) \\
& = \dfrac{2}{N} (x_{il}-\mu_l) \delta_{jl}
\end{aligned} %]]></script>

<p><br /></p>

<p>(1) 번 식을 잠깐 이야기 하면 $\dfrac{1}{N} \sum_{p=1}^N  (x_{pl}-\mu_l) = 0$ 인것은 어떤 값들을 평균을 빼고 다시 평균 시키면 0이 된다.</p>

<p>$e.g)\quad \frac{(1-2)+(2-2)+(3-2)}{3}=0$</p>

<p>이제 드디어 <script type="math/tex">\dfrac{\hat{x}_{kl}}{\partial {x}_{ij}}</script> 에 대해 구할수 있다. 곱의 미분 법칙을 사용하면 아래와 같이 전개 된다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \dfrac{\hat{x}_{kl}}{\partial {x}_{ij}}
&= (\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2}  -\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl} \\
\end{aligned} %]]></script>

<p>최종적으로 우리의 목적 <script type="math/tex">\dfrac{\partial L}{\partial x_{ij}}</script> 를 구해보자.</p>

<p><br /></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\dfrac{\partial L}{\partial x_{ij}}
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \cdot \gamma_l \cdot [(\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2} - \dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [(\delta_{ik} \delta_{jl} - \frac{1}{N} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2}] - \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l (\delta_{ik} \delta_{jl})(\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l \delta_{jl}(\sigma_l^2+\epsilon)^{-1/2} - \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \gamma_l [\dfrac{1}{N} (x_{kl} - \mu_l)(\sigma_l^2 + \epsilon)^{-3/2} (x_{il}-\mu_l) \delta_{jl}] \\
&= \dfrac{\partial L}{\partial y_{ij}} \gamma_l \delta_{ii} \delta_{jj} (\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_k \dfrac{\partial L}{\partial y_{kj}} \gamma_l \delta_{jj}(\sigma_j^2+\epsilon)^{-1/2} - \dfrac{1}{N} \sum_{k} \dfrac{\partial L}{\partial y_{kj}} \gamma_l [ (x_{kj} - \mu_j)(\sigma_j^2 + \epsilon)^{-3/2} (x_{ij}-\mu_j) \delta_{jj}] \cdots (2) \\
&= \dfrac{\partial L}{\partial y_{ij}} \gamma_l (\sigma_l^2+\epsilon)^{-1/2} - \frac{1}{N} \sum_k \dfrac{\partial L}{\partial y_{kj}} \gamma_l (\sigma_j^2+\epsilon)^{-1/2} - \dfrac{1}{N} \sum_{k} \dfrac{\partial L}{\partial y_{kj}} \gamma_l (x_{kj} - \mu_j)(\sigma_j^2 + \epsilon)^{-3/2} (x_{ij}-\mu_j) \\
&= \dfrac{1}{N} \gamma_l (\sigma_l^2+\epsilon)^{-1/2} [N \dfrac{\partial L}{\partial y_{ij}} - \sum_k \dfrac{\partial L}{\partial y_{kj}} - (x_{ij}-\mu_j)(\sigma_j^2 + \epsilon)^{-1} \sum_{k} \dfrac{\partial L}{\partial y_{kj}}(x_{kj} - \mu_j)]
\end{aligned} %]]></script>

<p><br /></p>

<p>(2) 번 식으로 도출 되는 과정을 잘 살펴보면, 각 항마다 곱으로 구성되어 있다. 첫번째 항은 $\sum_{k, l}$ 에서 오직 $k=i, l=j$ 일때 남아 있고 나머지는 전부다 0 이고, 두번째 항은 오직 $l=j$ 일때 남아있고 나머지는 전부다 0 이다. 그리고 마지막도 마친가지로 $l=j$ 일때만 남아있는다.</p>

<h4 id="gammaj---">$\gamma_j$ 에 대한 미분</h4>

<p>위에 까지 이해했으면 $\gamma_l$ 에 대한 미분은 간단하다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\dfrac{\partial L}{\partial \gamma_j}
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \gamma_j} \\
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \hat{x}_{kl} \delta_{jl} \\
&= \sum_k \dfrac{\partial L}{\partial y_{kj}} \hat{x}_{kj} \\
&= \sum_k \dfrac{\partial L}{\partial y_{kj}} (x_{kj} - \mu_j)(\sigma_j^2+\epsilon)^{-1/2}
\end{aligned} %]]></script>

<h4 id="betaj---">$\beta_j$ 에 대한 미분</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\dfrac{\partial L}{\partial \beta_j}
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \dfrac{\partial y_{kl}}{\partial \gamma_j} \\
&= \sum_{k,l} \dfrac{\partial L}{\partial y_{kl}} \delta_{jl} \\
&= \sum_k \dfrac{\partial L}{\partial y_{kj}}
\end{aligned} %]]></script>

<p><br /></p>

<p>여기서 우리는 왜 위에 step-9, 8 코드 구현에서 dgamma와 dbeta를 summation 하는지 알 수 있다.</p>

<p>다음 마지막 시간에는 모든걸 종합해서 학습하는 과정을 코드로 살펴보자.</p>

  <div class="advertisement" style="height: 100px;">
    <h4 class='title'> 도움이 되셨다면, 아래 Share 와 광고 클릭을 부탁드려요~ </h4>
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- google_ad -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-5855917513482122"
     data-ad-slot="9197714545"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

  </div>
</article>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
      <a href="//www.facebook.com/sharer.php?t=NUMPY+with+NN+-+7%3A+Batch+Normalization&u=http%3A%2F%2Fsimonjisu.github.io%2Fdeeplearning%2F2018%2F01%2F25%2Fnumpywithnn_7.html"
        onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
</section>



  

  

  

  
    
    <!-- <h2 id="DeepLearning">2</h2> -->
    <!-- <h2 id="DeepLearning">/deeplearning/2018/02/08/numpywithnn_8.html</h2> -->
    
    <!-- <h2 id="DeepLearning">4</h2> -->
    <!-- <h2 id="DeepLearning">/deeplearning/2018/01/24/numpywithnn_6.html</h2> -->
    



  
	<section class="post-navigation">
		<span class="prev-post">
			
				<a href="/deeplearning/2018/01/24/numpywithnn_6.html">
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-left fa-stack-1x fa-inverse"></i>
					</span>
					<span class="page-number">NUMPY with NN - 6: Weight Initialization</span>
				</a>
			
		</span>
		<span class="next-post">
			
				<a href="/deeplearning/2018/02/08/numpywithnn_8.html">
					<span class="page-number">NUMPY with NN - 8: Summary</span>
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-right fa-stack-1x fa-inverse"></i>
					</span>
				</a>
			
		</span>
	</section>




  
  <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'soopace';

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </section>
  


</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Soo</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">
        
        
        
        
        
        
        
        
          <li class="nav-link"><a href="/posts/">Posts</a>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:simonjisu@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">simonjisu@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://www.facebook.com/simonjisu" title="Friend me on Facebook">
              <i class="fa fa-facebook"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://github.com/simonjisu" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
        
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">My Blog
</p>
    </div>

  </div>
</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>

<!-- Mathjax -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();
  $("code").addClass("python")
  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});
$("code").addClass("python")
</script>

<!--lightSlider-->
<script src="/js/lightslider.js"></script>
<script src='/js/multi_slider.js' type="text/javascript"></script>





    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-102691608-1', 'auto');
  ga('send', 'pageview');
</script>


  </body>

</html>
