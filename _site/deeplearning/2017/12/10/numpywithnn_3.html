<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>NUMPY with NN - 3: Loss Function</title>
  <meta name="description" content="Numpy로 짜보는 Neural Network Basic - 3">
  
  <meta name="author" content="Soo">
  <meta name="copyright" content="&copy; Soo 2018">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="Numpy로 짜보는 Neural Network Basic - 3" />
  <meta property="og:url" content="http://simonjisu.github.io" />
  <meta property="og:site_name" content="Soo" />
  <meta property="og:title" content="NUMPY with NN - 3: Loss Function" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://simonjisu.github.io/assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="NUMPY with NN - 3: Loss Function">
  <meta name="twitter:description" content="Numpy로 짜보는 Neural Network Basic - 3">
  <meta name="twitter:image" content="http://simonjisu.github.io/assets/logo.png">
  <meta name="twitter:url" content="http://simonjisu.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://simonjisu.github.io/deeplearning/2017/12/10/numpywithnn_3.html">
  <link rel="alternate" type="application/rss+xml" title="Soo" href="http://simonjisu.github.io/feed.xml" />


  <!-- <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" ></script> -->

  <!-- Latex -->
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    MathJax.Hub.Config({
        TeX: {
            extensions: ["mhchem.js", "cancel.js"]
        },
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true,
            ignoreClass: "no-mathjax",
        },
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            // linebreaks: { automatic: true }
        },
        menuSettings: {
            zoom: "Click",
            zscale: "200%"
        }
    });
  </script>

  <!--lightSlider-->
  <link rel="stylesheet" href="/css/lightslider.css" />


</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="Soo">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        
          
          <li class="nav-link"><a href="/about/">About</a>
          
        
          
        
          
        
          
        
          
          <li class="nav-link"><a href="/posts/">Posts</a>
          
        
          
        
          
        
          
        
          <li class="nav-link"><a href="https://github.com/simonjisu"> GitHub </a>
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">
<!-- <h1>{"layout"=>"archive", "enabled"=>["categories", "tags"], "permalinks"=>{"year"=>"/:year/", "month"=>"/:year/:month/", "day"=>"/:year/:month/:day/", "tag"=>"/tag/:name/", "category"=>"/category/:name/"}}</h1>
 -->
<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">NUMPY with NN - 3: Loss Function</h1>
      <p class="info">by <strong>Soo</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">December 10, 2017</div>
  <div class="post-categories">
   in 
    
    <a href="/category/DeepLearning">Deeplearning</a>
    
  
  </div>
</section>

<article class="post-content">
  <h1 id="numpy--neural-network-basic---3">Numpy로 짜보는 Neural Network Basic - 3</h1>
<hr />

<p>저번 시간에는 Feedforward 과정을 보았는데, 정확도가 8.578% 밖에 안됐다. 이제 Neural Network가 데이터로부터 어떻게 학습하여 정확도를 올리는지 보자.</p>

<h2 id="loss-function">손실 함수(Loss Function)</h2>
<p>왜 우리의 목표인 정확도를 안쓰고 손실 함수라는 매개변수를 설정하는 걸까?</p>

<p>그 이유는 먼저 밝히면 신경망 학습에 미분이 사용되기 때문이다. 최적의 가중치(그리고 편향)을 탐색할 때 손실 함수의 값을 가능한 작게하는 가중치 값을 찾는데, 이때 가중치의 미분을 계산하고, 그 미분 값을 단서로 가중치를 서서히 갱신하는 과정을 거친다. 그러나 손실함수에 정확도를 쓰면 가중치의 미분이 대부분의 장소에서 0이 되기 때문에 가중치 값을 갱신할 수가 없다.</p>

<p>mnist 데이터의 경우 최종 출력층에 나온 $y$ 값은 Softmax에 의해 $(10 \times 1)$ 행렬의 확률로 출력되고, 그에 응답하는 정답 $t$ 는 one-hot encoded된 행렬이다.</p>
<div class="highlighter-rouge"><pre class="highlight"><code>y = np.array([0.05, 0.01, 0.7, 0.14, 0.05, 0.0, 0.05, 0.0, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
</code></pre>
</div>

<h3 id="mse">평균 제곱 오차(MSE)</h3>

<p><script type="math/tex">E=\frac{1}{2}\sum_{k}{(y_k - t_k)^2}</script></p>
<div class="highlighter-rouge"><pre class="highlight"><code>def mean_squared_error(y, t):
    return (1/2) * np.sum((y - t) ** 2)

mean_squared_error(y, t)
</code></pre>
</div>
<blockquote>
  <p>0.05860000000000002</p>
</blockquote>

<h3 id="cross-entropy">교차 엔트로피 오차(Cross Entropy)</h3>

<p><script type="math/tex">E=-\sum_{k}{t_k\log{y_k}}</script></p>
<div class="highlighter-rouge"><pre class="highlight"><code>def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

cross_entropy_error(y, t)
</code></pre>
</div>
<blockquote>
  <p>0.51082545709933802</p>
</blockquote>

<p>여기서 delta라는 작은 값을 더해준 이유는 y값이 0이면 $\log 0= -\inf$가 되서 미분 계산이 불가능하기 때문이다.</p>

<h3 id="section">미니 배치 학습</h3>

<script type="math/tex; mode=display">E=-\frac{1}{N}\sum_{n}{\sum_{k}{t_k\log{y_k}}}</script>

<p>엄청나게 많은 양의 데이터를 사용하는데 오차를 한번에 계산하려면 오랜 시간이 든다. 따라서 작은 양의 데이터를 사용해 조금씩 오차의 합을 구한다음에 그것의 평균을 내면 전체의 근사치로 사용할 수 있다.</p>
<div class="highlighter-rouge"><pre class="highlight"><code>def cross_entropy_error(y, t):
    delta = 1e-7
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(t * np.log(y[np.arange(batch_size), t] + delta)) / batch_size
</code></pre>
</div>
<h2 id="section-1">미분</h2>

<p>목적을 정했으니 이제 학습에 들어가면된다. 손실함수를 가중치에 대한 미분을 구해야 한다.</p>

<h3 id="section-2">수치 미분과 중심 차분법</h3>

<p>수치 미분이란 변화율이라고 볼 수 있다. $x$에서 $h$만큼 변했을 때 $f(x)$의 변화량을 나타낸 것이다.</p>

<script type="math/tex; mode=display">\frac{df(x)}{dx} = \lim_{h\rightarrow0}{\frac{f(x+h) - f(x)}{h}}</script>

<p>그러나 $f(x+h) - f(x)$ 는 굉장히 작은 수라 컴퓨터로 구현시 Underflow문제에 봉착하게 된다.</p>
<div class="highlighter-rouge"><pre class="highlight"><code>np.float32(1e-50)
</code></pre>
</div>
<blockquote>
  <p>0.0</p>
</blockquote>

<p>따라서 수치 미분에서 $h$는 되도록 너무 작은 값은 못쓴다.</p>

<p><strong>중심 차분법</strong> 을 이용하면 미분은 아래와 같다.</p>

<p><script type="math/tex">\frac{df(x)}{dx} = \lim_{h\rightarrow0}{\frac{f(x+h) - f(x-h)}{2h}}</script></p>
<div class="highlighter-rouge"><pre class="highlight"><code>def numerical_diff(f, x):
    h = 1e-4
    return (f(x + h) - f(x - h)) / (2*h)
</code></pre>
</div>
<p>예시 함수 $y = 0.01 x^2 + 0.1 x$ 의 수치 미분을 보자</p>
<div class="highlighter-rouge"><pre class="highlight"><code>def f1(x):
    return 0.01 * x**2 + 0.1 * x

print(numerical_diff(f1, 5))
print(numerical_diff(f1, 10))
</code></pre>
</div>
<blockquote>
  <p>0.1999999999990898</p>

  <p>0.2999999999986347</p>
</blockquote>

<p>정확하게 0.2와 0.3이 나오지 않는 이유는 이진수 부동소수점 방식[<a href="https://ko.wikipedia.org/wiki/%EB%B6%80%EB%8F%99%EC%86%8C%EC%88%98%EC%A0%90"><span style="color: #7d7ee8">링크</span></a>]의 정확도 문제니까 round 함수를 사용해 반올림하여 사용해야한다.</p>

<p><img src="/assets/ML/nn/numerical_diff.png" alt="Drawing" style="width: 500px;" /></p>

<p>2차원 이상의 데이터는 어떻게 짜야할까? 아래의 코드를 참조하자</p>
<div class="highlighter-rouge"><pre class="highlight"><code>def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)

    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)  # f(x+h)

        x[idx] = tmp_val - h
        fxh2 = f(x)  # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2 * h)

        x[idx] = tmp_val  # 값 복원
        it.iternext()

    return grad
</code></pre>
</div>
<ul>
  <li>np.nditer: iterator 객체를 만들어 준다. 행마다 원소가 iterate 된다.</li>
</ul>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>경사 하강법이란 현 위치에서 기울어진 방향으로 일정 거리를 이동하고, 또 그 위치에서 기울기를 구해서 그 방향으로 계속 나아가는 방법이다. 이렇게 해서 손실함수를 점점 작게 만들어 손실함수의 최저점으로 이끌고 간다(가능하다면).</p>

<script type="math/tex; mode=display">w_{new} = w_{old} - \eta \cdot \frac{\partial f}{\partial w_{old}}</script>

<p>$\eta$ 는 학습률(learning rate)라고 하며 갱신하는 양을 나타낸다.</p>

<p>아래 그림은 주변이 높고 중앙이 낮은 모양(그릇을 생각하자)을 3차원에서 2차원으로 그렸을 때, $(4, 5)$ 점에서 시작해서 경사 하강법으로 최저점을 찾는 과정이다. 함수는 $f(x) = x^2\ , x\in \mathbb{R}^3$ 다.</p>

<p><img src="/assets/ML/nn/GDanimation.gif" alt="Drawing" style="width: 500px;" /></p>

<h2 id="section-3">학습 알고리즘</h2>

<h3 id="nn----">간단한 NN 으로 가중치의 미분 구해보기</h3>
<div class="highlighter-rouge"><pre class="highlight"><code>class simpleNet(object):
    def __init__(self):
        # Input size = 2
        # Output size = 3
        self.W = np.random.normal(size=(2,3))

    def predict(self, x):
        a = np.dot(x, self.W)
        y = softmax(a)

        return y

    def loss(self, x, t):
        y = self.predict(x)
        loss = cross_entropy_error(y, t)

        return loss

x = np.array([0.6, 0.9])
t = np.array([0, 0, 1])
nn = simpleNet()

f = lambda w: nn.loss(x, t)
dW = numerical_gradient(f, nn.W)
print(dW)
</code></pre>
</div>
<blockquote>
  <p>[[ 0.05244267  0.24743359 -0.29987626]</p>

  <p>[ 0.07866401  0.37115039 -0.44981439]]</p>
</blockquote>

<h3 id="sgd">확률적 경사 하강법(SGD)</h3>
<p>아래 방법은 데이터를 무작위로 가져와서 학습하는 것이기 때문에 확률적 경사 하강법(Stochastic Gradient Descent)이라고도 한다.</p>

<ul>
  <li>
    <p>1단계: 미니배치</p>

    <p>훈련 데이터 중 일부를 무작위로 가져온 데이터를 미니 배치라고 하며, 미니 배치의 손실 함수 값을 줄이는 것이 목표다.</p>
  </li>
  <li>
    <p>2단계: 기울기 산출</p>

    <p>미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 만든다.</p>
  </li>
  <li>
    <p>3단계: 매개변수(가중치) 갱신</p>

    <p>가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.</p>
  </li>
  <li>
    <p>4단계: 반복</p>

    <p>1~3 단계를 반복한다.</p>
  </li>
</ul>

<h3 id="neural-network-">2층 Neural Network 실습</h3>

<p><img src="/assets/ML/nn/NN_3.png" alt="Drawing" style="width: 350px;" /></p>

<ul>
  <li>Input Size: m = 3</li>
  <li>Hidden Size: h = 4</li>
  <li>Output Size: o = 3
<script type="math/tex">X_{(batch,\ m)} \cdot W1_{(m,\ h)} + B1_{(batch,\ h)} \rightarrow A1_{(batch,\ h)}</script>
<script type="math/tex">sigmoid(A1_{(batch,\ h)}) \rightarrow Z1_{(batch,\ h)}</script>
<script type="math/tex">Z1_{(batch,\ h)} \cdot W1_{(h,\ o)} + B1_{(batch,\ o)} \rightarrow A2_{(batch,\ o)}</script>
<script type="math/tex">\sigma(A2_{(batch,\ o)}) \rightarrow Y_{(batch,\ o)}</script></li>
</ul>

<p>이것을 구현해보자. 수치로 구현한 2층 Neural Network 코드는 [<a href="https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch04/two_layer_net.py"><span style="color: #7d7ee8">여기</span></a>]서 가져왔다.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []
train_acc_list = []
test_acc_list = []

#highper parameter
epoch_num = 1
train_size = x_train.shape[0]
batch_size = 100
alpha = 0.1  # learning rate
epsilon = 1e-6

# 1에폭당 반복 수
iter_per_epoch = max(train_size / batch_size, 1)
nn = TwoLayer(input_size=784, hidden_size=100, output_size=10)

start = time.time()
for epoch in range(epoch_num):
    # get mini batch:
    batch_mask = np.random.choice(train_size, batch_size) # shuffle 효과
    x_batch = x_train[batch_mask]
    y_batch = y_train[batch_mask]

    # gradient 계산
    grad = nn.num_gradient(x_batch, y_batch)

    # update
    for key in ['W1', 'b1', 'W2', 'b2']:
        nn.params[key] = nn.params[key] - alpha * grad[key]

    # record
    loss = nn.loss(x_batch, y_batch)
    train_loss_list.append(loss)

    # 1에폭당 정확도 계산
    if epoch % iter_per_epoch == 0:
        train_acc = nn.accuracy(x_train, y_train)
        test_acc = nn.accuracy(x_test, y_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print('trian acc: {0:.5f} | test acc: {1:.5f}'.format(train_acc, test_acc))

    # stop point
    if epoch &gt; 10:
        stop_point = np.sum(np.diff(np.array(train_loss_list[i-11:])) &lt; epsilon)
        if stop_point == 10:
            print(epoch)
            break

end = time.time()
print('total time:', (end - start))
</code></pre>
</div>
<blockquote>
  <table>
    <tbody>
      <tr>
        <td>trian acc: 0.10442</td>
        <td>test acc: 0.10280</td>
      </tr>
    </tbody>
  </table>

  <p>total time: 175.5657160282135</p>
</blockquote>

<p>2단계에서 수치미분을 구현하기는 쉬우나 업데이트 하는데 시간이 너무 오래 걸린다. 1 Epoch만 돌렸는데 175초 걸렸다.</p>

<p>따라서 가중치 매개변수의 기울기를 효율적으로 계산하는 <strong>오차역전파</strong> 방법으로 업데이트 해야한다. 이건 다음 글에서 계속 진행하겠다.</p>

</article>





<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
      <a href="//www.facebook.com/sharer.php?t=NUMPY+with+NN+-+3%3A+Loss+Function&u=http%3A%2F%2Fsimonjisu.github.io%2Fdeeplearning%2F2017%2F12%2F10%2Fnumpywithnn_3.html"
        onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
</section>



  

  

  

  

  

  

  

  
    
    <!-- <h2 id="DeepLearning">6</h2> -->
    <!-- <h2 id="DeepLearning">/deeplearning/2017/12/15/numpywithnn_4.html</h2> -->
    
    <!-- <h2 id="DeepLearning">8</h2> -->
    <!-- <h2 id="DeepLearning">/deeplearning/2017/12/08/numpywithnn_2.html</h2> -->
    



  
	<section class="post-navigation">
		<span class="prev-post">
			
				<a href="/deeplearning/2017/12/08/numpywithnn_2.html">
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-left fa-stack-1x fa-inverse"></i>
					</span>
					<span class="page-number">NUMPY with NN - 2: Activation Function</span>
				</a>
			
		</span>
		<span class="next-post">
			
				<a href="/deeplearning/2017/12/15/numpywithnn_4.html">
					<span class="page-number">NUMPY with NN - 4: Backpropagation</span>
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-right fa-stack-1x fa-inverse"></i>
					</span>
				</a>
			
		</span>
	</section>




  
  <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'soopace';

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </section>
  


</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Soo</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">
        
        
          <li class="nav-link"><a href="/about/">About</a>
        
        
        
        
        
        
        
        
        
          <li class="nav-link"><a href="/posts/">Posts</a>
        
        
        
        
        
        
        
        
      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:simonjisu@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">simonjisu@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://www.facebook.com/simonjisu" title="Friend me on Facebook">
              <i class="fa fa-facebook"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://github.com/simonjisu" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">simonjisu</span>
            </a>
          </li>
          
        
          
        
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">My Blog
</p>
    </div>

  </div>
</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>

<!-- Mathjax -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();
  $("code").addClass("python")
  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});
$("code").addClass("python")
</script>

<!--lightSlider-->
<script src="/js/lightslider.js"></script>
<script src='/js/multi_slider.js' type="text/javascript"></script>





    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-102691608-1', 'auto');
  ga('send', 'pageview');
</script>


  </body>

</html>
