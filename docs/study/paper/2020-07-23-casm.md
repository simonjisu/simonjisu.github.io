---
title: "Classifier-agnostic saliency map extraction"
hide:
  - tags
tags:
  - Saliency Map
  - XAI
  - Explainable AI
---

Paper Link: [Classifier-agnostic saliency map extraction](https://arxiv.org/abs/1805.08249)

---

# 1. Introduction

- 기존의 몇몇 논문에서 특정 클래스 점수에 대한 gradient가 네트워크의 내부 작동을 밝히는 수단으로 사용할 수 있다는 것을 증명했다.
    - [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034)(Simonyan et al., 2013): vanilla gradient를 사용한 saliency map 생성, 관련 논문 리뷰 [링크](https://simonjisu.github.io/paper/2020/03/12/deepinsidecnn.html)
    - [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)(Springenberg et al., 2014): guided backpropagation을 사용하여 정교한 saliency map 생성

- 이러한 추세에 따라서 saliency map을 정교하게 만들기 위한 몇몇 테크닉이 적용되었다.
    - [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)(Selvaraju et al., 2017): GradCAM 논문, 여러개의 saliency map을 평균내서 조금더 smooth 한 맵을 형성

- 논문의 저자들은 이러한 트릭들은 유용한 증거를 가지고 있는 saliency map을 찾는데 원칙적인 방법이 아니라고 주장한다.
- 이 논문에서는 저자들의 목표는 분류에 도움이 되는 픽셀을 알려주는 saliency map을 찾는 것이다. 문제는 기존의 방법들이 분류기(훈련된 모델)에 너무 의존한다는 것이다. 이러한 문제점을 저자들은 해결하려고 했고, **"class-agnostic saliency map extraction"**이라는 것을 제시한다.
- 이 방법은 모델에 의존하지 않고 오직 입력데이터에만 더 집중할 수 있도록 했다. 결과는 Figure2 처럼 질적으로 더 좋은 saliency map을 생성함. 각 행이 어떤 그림을 그린건지는 파트5에서 설명한다.

    {% include image.html id="1SJcqwn25JiuD4LHO-yPILhwvaxOi7Qp6" desc="Paper Figure 2" width="100%" height="auto" %}
- ImageNet 데이터로 weakly-supervised 방법중에서 SOTA를 달성, strongly supervised 모델과 비슷한 성과를 냈다고 주장한다. 심지어, 훈련하지 않은 class에 대해서 잘 작동하는 모습도 보여줬다.

---

# 2. Related work

- 생략

---

# 3. Classifier-Agnostic Saliency Map Extraction

- 이 논문에서 다루는 문제는 다음과 같이, 주어진 이미지에 해당하는 salient region을 추출하는 매핑(mapping)을 찾는 것이다. 이 매핑은 분류기(모델)에 도움이 되는 픽셀은 1을 유지하고 그렇지 않은 픽셀은 0으로 masking되어야 한다.

    $$m: \Bbb{R}^{W\times H\times3} \rightarrow [0, 1]^{W\times H} \text{ over } x \in \Bbb{R}^{W\times H\times3}$$

## 3.1 Classifier-Dependent Saliency Map Extraction

- 기존의 연구([Fong & Vedaldi, 2017](https://arxiv.org/abs/1704.03296)과 [Dabkowski & Gal, 2017](https://arxiv.org/abs/1705.07857))들은 주로 분류기 $f$ 가 주어진 상태에서 최적의 masking을 찾는 형태가 많았다.

    $$m = \arg \underset{m'}{\max} S(m', f) \qquad \cdots (1)$$ 

    이러한 방법을 **Classifier-Dependent Saliency Map Extraction** 이라고 하며 자세한 방법은 다음과 같다.

- $S$는 일종의 score function인데, 분류 오차와 연관이 있다.

    $$S(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} \bigg[ l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big) + R\big(m(x_n)\big) \bigg] \qquad \cdots (2)$$ 

- 수식을 하나씩 보자. $m(x_n)$는 마스크로 매핑된 값들이고, $1-m(x_n)$은 자연스럽게 마스크로 지워지지 않은 지역이다. 여기어 $\odot x_n$은 element-wise product로 입력 이미지에 덧씌움으로써 마스크 되지 않은 픽셀들을 가르킨다. 따라서 분류기 $f$ 의 입력으로 지워지지 않은 픽셀을 넣고, 그 예측값과 타겟을 비교한 Loss가 수식의 앞부분 $l(f( * ), y_n)$이다(분류의 경우 보통 Cross-Entropy Loss 다). 뒤에 $R( * )$ 항목은 정규화 항목이다. 
- 같은 입력에서 특정 분류기 $f$로부터 생성된 매핑 $m$은, 다른 분류기 $f'$ 로부터 생성된 $m'$과 다를 수도 있다(심지어 같은 성능을 지녀도).
- 예를 들어, 다음 수식 $L$을 성능으로 측정한다면, 마스크를 씌우지 않았을 경우 $L(0, f) = L(0, f')$와 마스크가 씌워진 경우 $L(m, f) = L(m', f')$의 성능이 같다고 해도, $m$과 $m'$은 다른 형태를 보여줄 수가 있다는 말이다.

    $$L(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big) \qquad \cdots (3)$$ 

- 이런 현상의 원인은 두 개의 성능은 동일하나 가중치가 완전히 다른 분류기들이, 같은 입력에 대해서 각자 이미지의 다른 부분집합(픽셀들)을 사용하여 분류할 가능성이 있기 때문이다.
- 극단적인 예시로 이미지의 너비를 반으로 줄이고 옆으로 복사해서 각기 다른 구조의 분류기에 넣는다면, 하나는 이미지의 왼쪽에 saliency map을 생성하고, 다른 분류기는 이미지의 오른쪽에 saliency map을 생성할 수도 있다는 것이다.

    - **코멘트:** 이 부분은 실험결과가 없어서 실제로 해봐야 할것 같다. 두 개다 탐지할 수도 있는것 아닌가?

## 3.2 Classifier-Agnostic Saliency Map Extraction

- `2.1`에서 제기한 문제를 해결하려고 모든 분류기에 대한 사후확률의 평균을 최적화는 방식으로 전환하여, 수식(1)을 다음과 같이 변형했다.

    $$m = \arg \underset{m'}{\max} \Bbb{E}_f \big[ S(m', f) \big] \qquad \cdots (4)$$

- **코멘트:** 아래는 공부한 것을 토대로 풀어써봤는데, 틀릴 수도 있으니 주의..

    사후확률 $p(f \vert D, m')$은 masking된 이미지가 주어졌을 때, 해당하는 분류기의 확률이라고 생각할 수 있겠다.

    - $D, m'$ 부분은 $(1-m'(x_n))\odot x_n \text{ where } x_n \in D$ 부분이라고 생각할 수 있다.

    - 따라서 $p(f \vert D, m') \propto p(f) p(D, m' \vert f) = p(f) \exp(-L(m', f))$ 처럼 쓸 수 있다(아마 수식(3) 형태로 가져가려고 $L$에서 입력 이미지 $x_n\in D$는 생략한듯 하다).

    - $p(D, m' \vert f)$의 뜻은 특정 분류기 $f$가 주어졌을 때, masking된 이미지를 생성할 확률이다. 이는 Classifier-Dependent Saliency Map Extraction의 목적함수를 확률로 표현한 것이다.
    - $p(D, m' \vert f) = \exp(-L(m', f))$로 쓸수 있는 이유는 해당 항이 Likelihood 인데,  $\log$를 취하고 마이너스를 곱해줌으로써 Negative Log Likelihood로 바뀐다. $-\log p(D, m' \vert f)$는 곧 수식(3)인 $L(m',f)$와 일치한다.
- 수식(4)는 모든 가능한 분류기의 공간(the space of all possible classifiers)을 탐색하고, 그 중에서 잘 작동하는 매핑 $m$을 찾는 과정이다. 모든 가능한 분류기의 공간은 모든 분류기의 파라미터($\theta_f$)의 공간과 동일하다.
- 이러한 과정을 저자들은 **Classifier-Agnostic Saliency Map Extraction**라고 부르기로 했다.

## 3.3 Algorithm

- 수식(4)의 최적화 문제는 불행하게도 풀수가 없다(intractable). 특히 기댓값안에 있는 매핑 $m$을 loop를 통해 최적화 해야한다는 것이 이 문제를 더 난해하게 만든다. 따라서 논문에서는 이 문제를 매핑 $m$과 기댓값 목적함수를 동시에 추정함으로써 해결하려고 한다.
- 구체적인 알고리즘은 다음과 같다.
    
    {% include image.html id="1xCajsDh2yozXxhMwYy82TzJ702BMv6BA" desc="Paper Algorithm 1" width="100%" height="auto" %}

    - 먼저  $\theta_f$ 대해 classification loss $L$의 미분을 구함으로써, 사후확률 $p(f \vert D, m^{(k-1)})$를 가지는 $f^{(k)}$를 샘플링한다.

        $$\theta_{f^{(k)}} \leftarrow \theta_{f^{(k-1)}} - \eta_f \triangledown_{\theta_f} L(m^{(k-1)}, f^{(k-1)})$$ 

        이러한 방법은 [Welling & Teh, 2011](https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf)와 [Mandt et al., 2017](https://arxiv.org/abs/1704.04289) 연구에서 SGD에 noise를 일부 주면  Bayesian Posterior Inference를 수행할 수 있다는 점에서 착안했다. **공부가 더 필요한 부분..**

        EM 알고리즘이랑 비슷한데, SGD로 한다는 점이 다른듯하다. 참고자료: [4. The EM Algorithm in General](http://norman3.github.io/prml/docs/chapter09/4.html)

    - 업데이트된 파라미터 공간을 $F^{(k-1)}$와 합친뒤에 $F^{(k)}$에서 새로운 파라미터 $f'$를 샘플링한다.
    - 새로운 모델을 score function $S$에 넣어서 다시 마스크 네트워크 $\theta_{m^{(k-1)}}$값을 $\theta_{m^{(k)}}$로 업데이트 한다.

        $$\theta_{m^{(k)}} \leftarrow \theta_{m^{(k-1)}} + \eta_m \triangledown_{\theta_m} S(m^{(k-1)}, f')$$ 

### Score Function

- Score Function은 saliency map의 퀄리티를 측정하는 도구다.
- 또한, Precision 과 Recall의 조건을 동시에 만족하게 디자인 되어야한다.

    $$\text{precision} = \dfrac{TP}{TP + FP} \quad \text{recall}=\dfrac{TP}{TP+FN}$$

    {% include image.html id="1OEpXqK1p1lwdTLJHzHbliw_EScvBBjO2" desc="Confusion Matrix" width="100%" height="auto" %}

    - Precision: 마킹된 픽셀들 중에서 연관된 픽셀이 얼마나 있는지
    - Recall: 실제 연관된 픽셀들중에서 얼마나 정확하게 마킹되었는지
- 기존의 score 함수를 살펴보면, $A$파트는 연관된 픽셀이 더 많이 마킹되게 만들어주는 항이다(high recall). 마스킹된 이미지를 넣어서 classification loss가 높아지면 분류에 도움되는 픽셀들을 잡아주고 있다는 뜻이고, 낮아지면 마스킹이 잘 안되고 있다는 뜻으로 해석할 수 있다.

    $$S(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} \bigg[ \underbrace{ l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big)}_{A} + \underbrace{ R\big(m(x_n)\big) }_{B}\bigg] \qquad \cdots (2)$$

- 하지만 단순히 $A$파트를 쓰기에는 문제가 있는데, sampling할때 마스킹된 입력을 넣어서 파라미터 $\theta_f$를 업데이트하기 때문에, 모델의 성능을 저하시킬 가능성이 있다. 변형된 파라미터에 같은 classification loss를 그대로 사용하는 것은 이치에 안맞을 수도 있다. 
- 추가로 연관된 픽셀이 마스킹된 이미지를 분류기에게 넣었을 때, 정답 클래스가 아니라는 것만 판단해야지, 다른 클래스로 예측하면 안되기 때문에, classification loss를 그대로 사용하는 것은 문제가 있어 보인다. 
- 따라서 저자들은 $A$파트를 $\mathcal{H}\big( f( (1-m(x)) \odot x_n) \big)$ Entropy로 바꿨다. 즉, 마스킹을 찾아내는 작업은 정확도를 최소화하는 것이 아닌 불확실성(uncertainty)을 최대화하는 방향으로 진행해야한다.
- 또한, Entropy로 바꾸면서 ground-truth label의 필요성을 제거했다.
- $B$파트는 정규화 항목인데, trivial solution을 배제하고 있다. 만약에 마스크 $m$이 전부 1인 경우, 최대 recall 및 아주 낮은 precision을 달성할 수가 있다. 그래서 total variation([Rudin et al., 1992](https://www.sciencedirect.com/science/article/abs/pii/016727899290242F))과 L1 Norm을 사용하기로 한다.
    - 임성빈님의 자료 참고: [링크](https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i)
- 따라서 수식(2)는 다음과 같이 변한다

    $$S(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} \bigg[ \mathcal{H}\Big( f\big( (1-m(x_n))\odot x_n \big) \Big) + \lambda_R \Vert m(x_n) \Vert_1 \bigg] \qquad \cdots (7)$$

### Thining

- 알고리즘이 사후확률 분포에서 분류기 집합 $f^{(k)}$을 저장해서 많은 양의 데이터를 수집하기 때문에, 그중 작은 부분집합만 보존하는 전략을 취하기로 했다.
- 고정된 크기 $F^{(k)}$를 취하는 방식으로 다음과 같이 정한다.
    - **F**: 첫번째 분류기만 저장 $F^{(k)} = \{ f^{(0)}\}$
    - **L**: 마지막 분류기만 저장 $F^{(k)} = \{ f^{(k)}\}$
    - **FL**: 첫번째와 마지막 분류기만 저장 $F^{(k)} = \{ f^{(0)}, f^{(k)}\}$
    - **L1000**: 1000번째 iteration 마다 저장하고, $\vert F^{(k)} \vert =30$ 을 넘어갈때, 랜덤하게 하나를 제거한다.
    - **L100**: 100번째 iteration 마다 저장하고, $\vert F^{(k)} \vert =30$ 을 넘어갈때, 랜덤하게 하나를 제거한다.

### Classification loss

- 수식(3)처럼 classification loss를 정의해도 되지만 꼭 그럴 필요도 없다.

    $$L(m, f) = \dfrac{1}{N} \sum_{n=1}^{N} l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big) \qquad \cdots (3)$$ 

- 대신 [Szegedy et al., 2013](https://arxiv.org/abs/1312.6199) 방식이 더 잘 됐다.

    $$L(m, f) = \dfrac{1}{2N} \sum_{n=1}^{N} \bigg[ l\Big( f\big( (1-m(x_n))\odot x_n \big), y_n \Big) + l\big( f(x_n), y_n \big) \bigg] \qquad \cdots (8)$$

---

# 4. Training and evaluation details

### Dataset

- Official ImageNet Training Set

### Classifier $f$ and mapping $m$

{% include image.html id="1FHVgUhAdmJqAlJ0UE1l55lbnna070zJq" desc="Paper Figure 1" width="100%" height="auto" %}

- 모델 $f$는 ResNet-50을 사용했다. 그리고 Encoder-Decoder 구조를 취해서 마스크 $m$를 생성한다.
- Encoder는 ResNet-50 구조를 사용하고, 가중치는 $f$와 공유할 수도 있고 아닐수도 있다(실험결과 공유하는게 더 유리하다).
- Decoder는 Deconvolutional Network를 사용하여 마스크를 생성한다.

### Regularization coeffieient $\lambda_R$

- [Fan et al. 2017](https://lld-workshop.github.io/2017/papers/LLD_2017_paper_64.pdf) 에서 최적의 $\lambda_R$는 쉽게 찾을 수 없다고(not trivial)하다고 이야기하면서, adaptive 전략을 취해서 인위적인 $\lambda_R$을 고르는 것을 배제했다.
- 저자들을 같은 방법을 사용하면 saliency map의 평균 크기를 제어하기가 어려워 $f(x)$와 $f((1-m(x)) \odot x)$간에 차이가 있을 때만 $\lambda_R$를 적용했다.
- 각 실험에서 대략 50%의 픽셀이 연관되게 하도록 마스크 $m$를 생성했다.

### Baseline and CASM

- Baseline 모델은 CASM과 같은 모델 구조를 가지지만 CDSM(classifier-dependent saliency mapping) 방법으로 훈련킨 모델이(Thinning은 **F**를 사용).

### Mask discretization

- 마스크는 다음과 같이 생성한다. $\bar{m}(x)$은 마스크의 평균값이고, $\alpha$는 하이퍼파라미터다.

    $$b_{ij}(x) = \begin{cases} 1, \quad \text{if } m_{ij}(x) \geq \alpha \bar{m}(x) \\ 0, \quad \text{otherwise} \end{cases}$$

- $\alpha$를 1로 설성하면 마스크 평균값이 그대로 binary mask를 생성한다.

---

# 5. Qualitative comparisons

- 논문의 그림에 대해서 설명한다. 각 행에 대하여 다음과 같은 visualization을 했다.
    1. **original image**: 원본 이미지
    2. **masked-in image**( $b(x) \odot x$ ): 마스크안의 이미지
    3. **masked-out image**( $(1-b(x)) \odot x$ ): 마스킹된 이미지
    4. **inpainted masked-out image**: inpainting 알고리즘 [Telea, 2004](https://www.researchgate.net/publication/238183352_An_Image_Inpainting_Technique_Based_on_the_Fast_Marching_Method)을 사용해서 마스크 내부 이미지를 채웠다.
- Random 하게 7개의 연속된 그림을 골라서 visualization 했다.

---

# 6. Evaluation

## 6.1 Basic statistics

- Validation Set로 saliency map을 만들었다.
- CASM 모델로 마스크를 추출시 total variation이 더 낮았다( $2.5 \times 10^3$ vs $7.0 \times 10 ^3$ ). 즉, total variation 정규화를 적게 줘도 CASM이 더 많은 mask를 생성한다. > 무슨말이냐
- Entropy는 Baseline 보다 많이 작았는데( $0.05$ vs $0.21$ ), 이는 mask intensities가 거의 0과 1 사이의 값을 평균적으로 가진다는 것을 뜻한다.
- masked out volume의 표준편차가 더 컸는데( $0.19$ vs $0.14$ ), 이를 통해 CASM이 입력 이미지에 따라서 더 다양한 크기의 saliency map을 생성할 수 있다는 것을 알 수 있다.

## 6.2 Classification by multiple classifiers

{% include image.html id="11zPpcGsV3JDEPhvOBVGsIm9uO-mtIcGz" desc="Paper Figure 3" width="100%" height="auto" %}

- CASM이 정말로 classifier-agnostic인지 `torchvision.models`에 있는 모델들로 주장을 확인해봤다. 자신들이 기대한 것은 CASM을 통해 만든 inpainted masked-out 이미지로 해당 모델들의 정확도를 깎아 내리고, masked-in 이미지들은 좋은 성능을 내는거다. 그리고 그 기대는 맞아 떨어졌다.
- Masked-out 이미지에서 Baseline 모델이 성능이 굉장히 낮게 나왔는데, 저자들이 어림짐작으로 보았을때 Baseline 모델이 생성한 saliency map의 적대적인 성질인것 겉다. 그 이유는 masked-out 이미지를 채운 inpainted masked-out 이미지와 비교했을 때, Baseline 모델은 성능이 드라마틱하게 향상하는데, CASM 모델은 그 혜택을 많이 못보기 때문이다.
    - **코멘트**: 기존에 완전한 이미지로 모델을 학습 할 때, 주변 사물등 다른 정보를 사용하여 물체를 분류했을 가능성이 있는데, Baseline 모델은 일부 연관된 물체도 지워버리니까 오히려 더 CASM 보다 성능이 하락하는 것 같음. 예를 들어, 아래 그림 처럼 나무를 아예 지워버리니까, 판별을 더 못하는 듯. 어떻게 생각하면 훈련 데이터가 다양하지 못했다는 것이, `f(나무 + 새) = 새` 라는 것이 되니까, 주변 사물이 분류기에 많은 영향을 끼치는 것을 알 수 있음.

    {% include image.html id="1UAR_h0c0RE4uLM5LK3SBu21qj3rcuQLL" desc="Comparing CASM vs Baseline" width="100%" height="auto" %}


- **코멘트**: 자신들이 만든 baseline과 비교한것도 좋지만, 다른 방법(CAM방식) 등 하고 비교해보았어도 괜찮을 것 같음

## 6.3 Object localization

{% include image.html id="1WZ6W1xGCCfgnpA-8irC-_l-3Fljbe3aY" desc="Paper Table 1" width="100%" height="auto" %}

- saliency map으로 weakly supervised localization도 같이 수행했다.
- 3가지 metrics으로 localization을 계량했다.
    - **OM**: ImageNet Localization 챌린지에서 사용하는 official metric, 예측 bounding box와 정답 bounding box의 IOU가 0.5 이상이여야하고, 클래스를 맞춰야한다. 맞췄을 경우 0, 그렇지 않을 경우 1이 되서, OM이 낮을 수록 좋다.
    - **LE**: OM방식은 분류기에 따라 다르다. 저자들은 분류기와 상관없이 훈련했기에 bounding box만 예측하는 "Localization Error"라는 다른 방법을 사용했다. (Cao et al. 2015, Fong & Vedaldi 2017) 이 방법도 낮을 수록 좋다.
    - 마지막으로 원본 saliency map을 conintious F1 score로 평가했다.
        - Precision과 Recall은 다음과 같이 정해진다.

            $$P=\dfrac{\sum_{(i,j) \in B^*(x)} m_{ij}(x)}{\sum_{ij} m_{ij}(x)} \quad \text{and} \quad R=\dfrac{\sum_{(i,j) \in B^*(x)} m_{ij}(x)}{\vert B^*(x) \vert}$$

- supervised 보다 성능이 대부분 뛰어났다고 주장, 다만 전제 자체가 조금 다르기에 적절한 비교가 힘들다고 함

### Thinning strategies & Score function & Sharing the encoder and classifier

{% include image.html id="1YPJ_KmdfPrHgIeZOSAB1m0ZDi97zsamg" desc="Paper Table 2" width="100%" height="auto" %}

- $S$: score function 의 선택 (E: entropy loss, C: classification loss)
- $SHR$: Encoder와 Classifier를 공유 했는지 여부
- $THIN$: Thinning 전략
- Entropy 쓰고, parameter sharing하고, L100 thinning 전략인 E가 제일 좋음

## 6.4 Unseen classes

{% include image.html id="1pyIiM1Np5OWJB85TAczggvXwdFl69pdX" desc="Paper Table 3" width="100%" height="auto" %}

- 제안한 방법이 클래스의 정답을 필요로 하지 않기 때문에, 학습하지 않았던 데이터의 localization을 수행할 수 있다.
- 이를 테스트하기 위해서 1000개의 클래스를 5개(6개아닌가..?)의 서로소 부분집합(disjoint subset)으로 쪼갠다. 즉, 각각의 집합에 서로 다른 클래스가 들어가 있다. 그리고 각 집합에는 다음과 같은 크기의 데이터를 담고, 각각 해당하는 % 만큼 CASM 모델을 훈련시킨다(Thinning은 **L** 전략 사용).
    - A: 50, B: 50, C: 100, D: 300, E: 300, F: 200
    - 95%: B, C, D, E, F
    - 90%: C, D, E, F
    - 80%: D, E, F
    - 50%: E, F
    - 20%: F
- 모든 모델들의 일반화가 좋은 편이었고, 정확도는 무시할정도 수준으로 작았다(20% 훈련한 모델을 제외).
- seen과 unseen의 **LE** 차이는 훈련 데이터가 적어질 수록 높아졌다. 그러나 적당한 크기의 traning set 이라면 차이는 크게 나지 않는다.