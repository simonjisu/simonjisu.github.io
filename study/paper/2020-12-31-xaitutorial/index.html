
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="https://simonjisu.github.io/study/paper/2020-12-31-xaitutorial/" rel="canonical"/>
<link href="../2020-07-23-casm/" rel="prev"/>
<link href="../2021-04-12-spider/" rel="next"/>
<link href="../../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.5.3, mkdocs-material-9.4.2+insiders-4.42.0" name="generator"/>
<title>Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI - Soopace</title>
<link href="../../../assets/stylesheets/main.f2778614.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.46987102.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-2D0S4P2SJ9"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-2D0S4P2SJ9",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-2D0S4P2SJ9",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src="../../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="indigo" data-md-color-primary="black" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Soopace" class="md-header__button md-logo" data-md-component="logo" href="../../.." title="Soopace">
<img alt="logo" src="../../../img/logo/logo.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Soopace
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI
            
          </span>
</div>
</div>
</div>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/simonjisu.github.io" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</nav>
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../..">
        
  
    
  
  About

      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../blog/">
          
  
    
  
  Blog

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../../">
          
  
    
  
  Study

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../project/">
          
  
    
  
  Project

        </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Soopace" class="md-nav__button md-logo" data-md-component="logo" href="../../.." title="Soopace">
<img alt="logo" src="../../../img/logo/logo.png"/>
</a>
    Soopace
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/simonjisu.github.io" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../..">
<span class="md-ellipsis">
    
  
    About
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../../blog/">
<span class="md-ellipsis">
    
  
    Blog
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
            
  
    Blog
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../blog/archive/2020/">
<span class="md-ellipsis">
    
  
    Archive
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../blog/category/lifelog/">
<span class="md-ellipsis">
    
  
    Categories
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../">
<span class="md-ellipsis">
    
  
    Study
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            
  
    Study
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../">
<span class="md-ellipsis">
    
  
    Paper
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_3_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_2">
<span class="md-nav__icon md-icon"></span>
            
  
    Paper
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../2017-08-04-E2EMN/">
<span class="md-ellipsis">
    
  
    End-to-End Memory Network
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2018-04-04-nsmcbidreclstmselfattn/">
<span class="md-ellipsis">
    
  
    A Structured Self-Attentive Sentence Embedding
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2019-08-22-neuralnetworklm/">
<span class="md-ellipsis">
    
  
    A Neural Probabilistic Language Model
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2019-09-18-introxai/">
<span class="md-ellipsis">
    
  
    Explaining Explanations: An Overview of Interpretability of Machine Learning
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2020-01-14-attentionisallyouneed/">
<span class="md-ellipsis">
    
  
    Attention Is All You Need
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2020-03-12-deepinsidecnn/">
<span class="md-ellipsis">
    
  
    Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2020-07-19-maskpredict/">
<span class="md-ellipsis">
    
  
    Mask-Predict: Parallel Decoding of Conditional Masked Language Models
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2020-07-23-casm/">
<span class="md-ellipsis">
    
  
    Classifier-agnostic saliency map extraction
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    
  
    Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-04-12-spider/">
<span class="md-ellipsis">
    
  
    Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-04-20-featurevisualization/">
<span class="md-ellipsis">
    
  
    Feature Visualization
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-05-14-bridge/">
<span class="md-ellipsis">
    
  
    Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-08-13-hybridranking/">
<span class="md-ellipsis">
    
  
    Hybrid Ranking Network for Text-to-SQL
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-11-21-nbdt/">
<span class="md-ellipsis">
    
  
    NBDT: Neural-Backed Decision Trees
  

    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../tutorial/">
<span class="md-ellipsis">
    
  
    Tutorial
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../../project/">
<span class="md-ellipsis">
    
  
    Project
  

    
  </span>
</a>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
            
  
    Project
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<nav aria-label="Navigation" class="md-path">
<ol class="md-path__list">
<li class="md-path__item">
<a class="md-path__link" href="../../..">
<span class="md-ellipsis">
    About
  </span>
</a>
</li>
<li class="md-path__item">
<a class="md-path__link" href="../../">
<span class="md-ellipsis">
    Study
  </span>
</a>
</li>
<li class="md-path__item">
<a class="md-path__link" href="../">
<span class="md-ellipsis">
    Paper
  </span>
</a>
</li>
</ol>
</nav>
<article class="md-content__inner md-typeset">
<nav class="md-tags" hidden="">
<span class="md-tag">Survey Paper</span>
<span class="md-tag">XAI</span>
<span class="md-tag">Explainable AI</span>
</nav>
<h1 id="explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI<a class="headerlink" href="#explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai" title="Permanent link">Â¶</a></h1>
<p>Paper Link: <a href="https://arxiv.org/abs/1910.10045">https://arxiv.org/abs/1910.10045</a></p>
<p>XAIì— ëŒ€í•œ ì „ë°˜ì ì¸ ì†Œê°œë¥¼ ì •ë¦¬í•œ ë…¼ë¬¸ì´ ë‚˜ì™€ì„œ ì°¨ê·¼ ì°¨ê·¼ ìš”ì•½ ì •ë¦¬í•´ë³´ë ¤ê³  í•œë‹¤(ë¬´ë ¤ 115í˜ì´ì§€, referenceë§Œ 6í˜ì´ì§€). ì•½ê°„ì˜ ë²ˆì—­ ì–´íˆ¬ì™€ ìƒëµëœ ê²ƒë„ ìˆìœ¼ë‹ˆ ì˜ì–´ ì›ë¬¸ì„ ì°¸ê³ í•˜ê¸¸ ë°”ë€ë‹¤.</p>
<ol>
<li><a href="https://simonjisu.github.io/paper/2020/12/31/xaitutorial1.html"><span style="color:#e25252">Introduction(ì´ë²ˆí¸)</span></a></li>
<li><a href="https://simonjisu.github.io/paper/2021/01/14/xaitutorial2.html">Explainability: What, why, what for and how?</a></li>
<li><a href="https://simonjisu.github.io/paper/2021/01/23/xaitutorial3.html">Transparent machine learning models</a></li>
<li>Post-hoc explainability techniques for machile learning models: Taxonomy, shallow models and deep learning</li>
<li>XAI: Opportunities, challenges and future research needs</li>
<li>Toward responsible AI: Principles of artificial intelligence, fairness, privacy and data fusion</li>
<li>Conclusions and outlook</li>
</ol>
<h1 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">Â¶</a></h1>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Artificial Intelligence (AI) lies at the core of many activity sectors that have embraced new information technologies [1]. While the roots of AI trace back to several decades ago, there is a clear consensus on the paramount importance featured nowadays by intelligent machines endowed with learning, reasoning and adaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedented levels of performance when learning to solve increasingly complex computational tasks, making them pivotal for the future development of the human society [2]. The sophistication of AI-powered systems has lately increased to such an extent that almost no human intervention is required for their design and deployment. When decisions derived from such systems ultimately affect humansâ€™ lives (as in e.g. medicine, law or defense), there is an emerging need for understanding how such decisions are furnished by AI methods [3].</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ì¸ê³µì§€ëŠ¥ì´ ì •êµí•´ì§€ë©´ì„œ ê³„ì‚°ì´ ì ì  ë³µì¡í•´ì§€ëŠ” ë°˜ë©´, ê¶ê·¹ì ìœ¼ë¡œ ì¸ê°„ì˜ ì‚¶ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”(ì˜í•™, ë²•ë¥ , êµ­ë°©) ì‹œìŠ¤í…œ(ê¸°ê³„)ì˜ ê²°ì •ì´ ì–´ë–»ê²Œ ë‚´ë ¤ì¡ŒëŠ”ì§€, ìš°ë¦¬ëŠ” ì´í•´í•  í•„ìš”ê°€ ìˆë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>While the very first AI systems were easily interpretable, the last years have witnessed the rise of opaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning (DL) models such as DNNs stems from a combination of efficient learning algorithms and their huge parametric space. The latter space comprises hundreds of layers and millions of parameters, which makes DNNs be considered as complex black-box models [4]. The opposite of black-box-ness is transparency, i.e., the search for a direct understanding of the mechanism by which a model works [5].</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ íš¨ìœ¨ì ì¸ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ê³¼ ê±°ëŒ€í•œ íŒŒë¼ë¯¸í„° ê³µê°„ì˜ ê²°í•©ì—ì„œ ë¹„ë¡¯ëœë‹¤. ê·¸ë¦¬ê³  black-box ëª¨ë¸ë¡œ ê°„ì£¼ ëœë‹¤. ì´ì˜ ë°˜ëŒ€ëŠ” <strong>íˆ¬ëª…ì„±(transparency)</strong>ì´ë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>As black-box Machine Learning (ML) models are increasingly being employed to make important predictions in critical contexts, the demand for transparency is increasing from the various stakeholders in AI [6]. The danger is on creating and using decisions that are not justifiable, legitimate, or that simply do not allow obtaining detailed explanations of their behaviour [7]. Explanations supporting the output of a model are crucial, e.g., in precision medicine, where experts require far more information from the model than a simple binary prediction for supporting their diagnosis [8]. Other examples include autonomous vehicles in transportation, security, and finance, among others.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> Machine Learning ëª¨ë¸ì´ ì ì  ë§ì´ í™œìš©ë˜ë©´ì„œ, ì´í•´ê´€ê³„ìë“¤ë¡œë¶€í„° íˆ¬ëª…ì„±ì˜ ìš”êµ¬ê°€ ë†’ì•„ì§€ê³  ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜ë£Œ(ì§„ë‹¨), êµí†µ(ììœ¨ì£¼í–‰), ë³´ì•ˆ, ê¸ˆìœµë“± ì´ ìˆë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>In general, humans are reticent to adopt techniques that are not directly interpretable, tractable and trustworthy [9], given the increasing demand for ethical AI [3]. It is customary to think that by focusing solely on performance, the systems will be increasingly opaque. This is true in the sense that there is a trade-off between the performance of a model and its transparency [10]. However, an improvement in the understanding of a system can lead to the correction of its deficiencies. When developing a ML model, the consideration of interpretability as an additional design driver can improve its implementability for 3 reasons:</p>
<ul>
<li>Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correct from bias in the training dataset.</li>
<li>
<p>Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbations that could change the prediction.</p>
</li>
<li>
<p>Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeing that an underlying truthful causality exists in the model reasoning.</p>
</li>
</ul>
<p>All these means that the interpretation of the system should, in order to be considered practical, provide either an understanding of the model mechanisms and predictions, a visualization of the modelâ€™s discrimination rules, or hints on what could perturb the model [11].</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> í†µìƒì ìœ¼ë¡œ ì„±ê³¼ì—ë§Œ ì¹˜ì¤‘í•  ìˆ˜ë¡ ì‹œìŠ¤í…œì€ ì ì  ë¶ˆíˆ¬ëª…í•´ì§ˆ ê²ƒì´ë¼ ìƒê°í•œë‹¤. ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ íˆ¬ëª…ì„± ì‚¬ì´ì— trade-offê°€ ìˆë‹¤ëŠ” ì ì€ ì‚¬ì‹¤ì´ë‚˜, ëª¨ë¸ì— ëŒ€í•œ ì´í•´ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ ë‚¼ ìˆ˜ë„ ìˆë‹¤. ì¶”ê°€ë¡œ MLëª¨ë¸ì„ ê°œë°œí•  ë•Œ, í•´ì„ ê°€ëŠ¥ì„±ì„ ëª¨ë“ˆë¡œ ë„£ìœ¼ë©´ ì„¸ ê°€ì§€ ì´ìœ ë¡œ êµ¬í˜„ ê°€ëŠ¥ì„±ì„ í–¥ìƒ ì‹œí‚¬ ìˆ˜ ìˆë‹¤.</p>
<ul>
<li>í•´ì„ê°€ëŠ¥ì„±ì€ ì˜ì‚¬ê²°ì •ì—ì„œ ê³µì •ì„±ì„ ë³´ì¥í•˜ëŠ”ë° ë„ì›€ì´ ëœë‹¤. ì¦‰, êµìœ¡ ë°ì´í„° ì§‘í•©ì˜ í¸í–¥ì„±ì„ íƒì§€í•˜ê³  ê²°ê³¼ì ìœ¼ë¡œ ìˆ˜ì •í•œë‹¤.</li>
<li>í•´ì„ê°€ëŠ¥ì„±ì€ ì˜ˆì¸¡ì„ ë°”ê¿€ ìˆ˜ ìˆëŠ” ì ì¬ì  ì ëŒ€ì  ì„­ë™ì„ ê°•ì¡°í•¨ìœ¼ë¡œì¨ ê±´ì „ì„±ì˜ ì œê³µì„ ì´‰ì§„í•œë‹¤.</li>
<li>í•´ì„ê°€ëŠ¥ì„±ì€ ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ë§Œìœ¼ë¡œ ì‚°ì¶œë¬¼ì„ ìœ ì¶”í•˜ëŠ” ë³´í—˜ìœ¼ë¡œì„œ, ì¦‰ ëª¨í˜• ì¶”ë¡ ì—ì„œ ê·¼ë³¸ì ì¸ ì§„ì‹¤ì  ì¸ê³¼ê´€ê³„ê°€ ì¡´ì¬í•¨ì„ ë³´ì¦í•˜ëŠ” ë³´í—˜ìœ¼ë¡œ ì‘ìš©í•  ìˆ˜ ìˆë‹¤.</li>
</ul>
<p>ì¦‰, í•´ì„ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì€ ëª¨ë¸ ë§¤ì»¤ë‹ˆì¦˜ê³¼ ì˜ˆì¸¡ì— ëŒ€í•œ ì´í•´, ëª¨ë¸ì˜ íŒê²° ê·œì¹™ ì‹œê°í™”, ë˜ëŠ” ëª¨ë¸ì„ ë°©í•´í•˜ëŠ” ê²ƒì— ëŒ€í•œ íŒíŠ¸ ë“±ì„ ì œê³µí•´ì•¼í•œë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI (XAI) [7] proposes creating a suite of ML techniques that 1) produce more explainable models while maintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners. XAI draws as well insights from the Social Sciences [12] and considers the psychology of explanation.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> í˜„ì¬ì˜ íš¨ê³¼ì ì¸ AI ì‹œìŠ¤í…œì„ ì œí•œì‹œí‚¤ì§€ ì•ŠëŠ” ì„ ì—ì„œ, eXplainable AI(XAI)ì€ 1) í•™ìŠµ í¼í¬ë¨¼ìŠ¤ëŠ” ìµœëŒ€í•œìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ ì„¤ëª…ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë§Œë“¤ê²ƒì„ ì œì•ˆ 2) ì‚¬ëŒì´ ì´í•´í•˜ê³ , ì ì ˆí•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Fig. 1 displays the rising trend of contributions on XAI and related concepts. This literature outbreak shares its rationale with the research agendas of national governments and agencies. Although some recent surveys [8], [10], [13], [14], [15], [16], [17] summarize the upsurge of activity in XAI across sectors and disciplines, this overview aims to cover the creation of a complete unified framework of categories and concepts that allow for scrutiny and understanding of the field of XAI methods. Furthermore, we pose intriguing thoughts around the explainability of AI models in data fusion contexts with regards to data privacy and model confidentiality. This, along with other research opportunities and challenges identified throughout our study, serve as the pull factor toward Responsible Artificial Intelligence, term by which we refer to a series of AI principles to be necessarily met when deploying AI in real applications. As we will later show in detail, model explainability is among the most crucial aspects to be ensured within this methodological framework. All in all, the novel contributions of this overview can be summarized as follows:</p>
<ol>
<li>Grounded on a first elaboration of concepts and terms used in XAI-related research, we propose a novel definition of explainability that places audience (Fig. 2) as a key aspect to be considered when explaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques, from trustworthiness to privacy awareness, which round up the claimed importance of purpose and targeted audience in model explainability.</li>
<li>We define and examine the different levels of transparency that a ML model can feature by itself, as well as the diverse approaches to post-hoc explainability, namely, the explanation of ML models that are not transparent by design.</li>
<li>We thoroughly analyze the literature on XAI and related concepts published to date, covering approximately 400 contributions arranged into two different taxonomies. The first taxonomy addresses the explainability of ML models using the previously made distinction between transparency and post-hoc explainability, including models that are transparent by themselves, Deep and non-Deep (i.e., shallow) learning models. The second taxonomy deals with XAI methods suited for the explanation of Deep Learning models, using classification criteria closely linked to this family of ML methods (e.g. layerwise explanations, representation vectors, attention).</li>
<li>We enumerate a series of challenges of XAI that still remain insufficiently addressed to date. Specifically, we identify research needs around the concepts and metrics to evaluate the explainability of ML models, and outline research directions toward making Deep Learning models more understandable. We further augment the scope of our prospects toward the implications of XAI techniques in regards to confidentiality, robustness in adversarial settings, data diversity, and other areas intersecting with explainability.</li>
<li>After the previous prospective discussion, we arrive at the concept of Responsible Artificial Intelligence, a manifold concept that imposes the systematic adoption of several AI principles for AI models to be of practical use. In addition to explainability, the guidelines behind Responsible AI establish that fairness, accountability and privacy should also be considered when implementing AI models in real environments.</li>
<li>Since Responsible AI blends together model explainability and privacy/security by design, we call for a profound reflection around the benefits and risks of XAI techniques in scenarios dealing with sensitive information and/or confidential ML models. As we will later show, the regulatory push toward data privacy, quality, integrity and governance demands more efforts to assess the role of XAI in this arena. In this regard, we provide an insight on the implications of XAI in terms of privacy and security under different data fusion paradigms.</li>
</ol>
<p>[/expand]</p>
<p>{% include image.html id="119QnRBvYV4gHiuKz7kpaOVo_2b2tlhz5" desc="Fig 1. í•™ê³„ì—ì„œ XAI ë° ì—°ê´€ëœ ê°œë…ì˜ ê¸°ì—¬ë„ ì¶”ì„¸" width="100%" height="auto" %}</p>
<p><span style="color:#e25252">ìš”ì•½:</span> <code>Fig 1</code>ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ êµ­ê°€ ì •ë¶€ ë° ê¸°ê´€ì˜ ì—°êµ¬ì˜ì œì˜ í‚¤ì›Œë“œ ì¶”ì„¸ë¥¼ ì‚´í´ë³´ë©´ XAIê´€ë ¨ í™œë™ì´ ìµœê·¼ ê¸‰ì¦í–ˆì§€ë§Œ, í†µì¼ëœ í”„ë ˆì„ì›Œí¬ê°€ ì—†ë‹¤. ì´ë²ˆ ë…¼ë¬¸ì—ì„œëŠ” í†µì¼ëœ í”„ë ˆì„ì›Œí¬ì˜ ì‘ì„±í•˜ê³ , ê°œì¸ì •ë³´ ë³´í˜¸ ë° ëª¨ë¸ ê¸°ë°€ì„±ì— ëŒ€í•´ì„œ ì˜ê²¬ì„ ì œì‹œí•  ê²ƒì´ë‹¤. </p>
<ol>
<li>ì§€ê¸ˆê¹Œì§€ XAI ê´€ë ¨ ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ ê°œë…ê³¼ ìš©ì–´ì˜ ê¸°ì´ˆí•˜ì—¬, ML ëª¨ë¸ì„ ì„¤ëª…í•  ë•Œ ì²­ì¤‘(audience)ì„ í•µì‹¬ìœ¼ë¡œ ê³ ë ¤í•  ê²ƒì´ë‹¤(ê·¸ë¦¼ 2). ë˜í•œ XAI ê¸°ë²•ì„ ì‚¬ìš©í•  ë•Œ ì¶”êµ¬í•˜ëŠ” ë‹¤ì–‘í•œ ëª©ì ì— ë”°ë¼ ì„¸ë¶„í™”í•  ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì„¤ëª…ê°€ëŠ¥ì„±ì—ì„œ ëª©ì ê³¼ íƒ€ê²Ÿ ì²­ì¤‘ì˜ ì¤‘ìš”í•¨ì„ ì´ì•¼ê¸° í•œë‹¤.</li>
<li>ë‹¤ì–‘í•œ ë ˆë²¨ì˜ íˆ¬ëª…ì„±ì„ ì •ì˜í•˜ê³  ê²€í† í•œë‹¤. ëŒ€ìƒì—ëŠ” ì‚¬í›„(post-hoc) ì„¤ëª…ì´ ê°€ëŠ¥í•œ, ìì²´ ì„¤ëª…ê°€ëŠ¥í•œ í˜¹ì€ ì„¤ê³„ì— ì˜í•´ ì„¤ëª…ì´ ë¶ˆê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ë“±ì´ ìˆë‹¤.</li>
<li>XAIì— ê´€í•œ ë¬¸í—Œê³¼ ì§€ê¸ˆê¹Œì§€ ì¶œíŒëœ ê´€ë ¨ ê°œë…ë“¤ì„ ì² ì €í•˜ê²Œ ë¶„ì„í•˜ì—¬, ëŒ€ëµ 400ê°œì˜ ê¸°ì—¬ë¥¼ ë‘ ê°œì˜ ë‹¤ë¥¸ ë¶„ë¥˜ë²•ìœ¼ë¡œ ë°°ì—´í•˜ì˜€ë‹¤.Â ì²« ë²ˆì§¸ ë¶„ë¥˜ë²•ì€ ì´ì „ì— ë§Œë“  íˆ¬ëª…ì„±(transparency)ê³¼ ì‚¬í›„ ì„¤ëª…ì„±(post-hoc explainability) ì‚¬ì´ì˜ êµ¬ë³„ì„ ì‚¬ìš©í•˜ì—¬ ML ëª¨ë¸ì˜ ì„¤ëª…ê°€ëŠ¥ì„±ì„ ë‹¤ë£¨ê³  ìˆìœ¼ë©°, ì—¬ê¸°ì—ëŠ” ìŠ¤ìŠ¤ë¡œ íˆ¬ëª…í•˜ê³  ê¹Šì§€ ì•Šì€(ì¦‰,Â shallow ì–‰ì€) í•™ìŠµ ëª¨ë¸ì´ í¬í•¨ëœë‹¤.Â ë‘ ë²ˆì§¸ ë¶„ë¥˜ë²•ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„¤ëª…ì— ì í•©í•œ XAI ë°©ë²•ì„ ë‹¤ë£¨ë©°, ì´ ML ë°©ë²• ê³„ì—´ê³¼ ë°€ì ‘í•˜ê²Œ ì—°ê³„ëœ ë¶„ë¥˜ ê¸°ì¤€(ì˜ˆ: ê³„ì¸µì  ì„¤ëª… layer-wise explanations, í‘œí˜„ ë²¡í„° representation vectors, ì–´í…ì…˜ attention)ì„ ì‚¬ìš©í•œë‹¤.</li>
<li>ì§€ê¸ˆê¹Œì§€ë„ ë¶ˆì¶©ë¶„í•˜ê²Œ ë‹¤ë£¨ì–´ì§€ì§€ ì•Šê³  ìˆëŠ” XAIì˜ ì¼ë ¨ì˜ ê³¼ì œë¥¼ ì—´ê±°í•œë‹¤.Â êµ¬ì²´ì ìœ¼ë¡œëŠ” ML ëª¨ë¸ì˜ ì„¤ëª… ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê°œë… ë° ë©”íŠ¸ë¦­ìŠ¤ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì—°êµ¬ ìš”êµ¬ë¥¼ íŒŒì•…í•˜ê³ , ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë³´ë‹¤ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì—°êµ¬ ë°©í–¥ì„ ì •ë¦¬í•œë‹¤.Â ê¸°ë°€ì„±, ì ëŒ€ì  ì„¤ì •ì˜ ê²¬ê³ ì„±, ë°ì´í„° ë‹¤ì–‘ì„± ë° ì„¤ëª… ê°€ëŠ¥ì„±ê³¼ êµì°¨í•˜ëŠ” ê¸°íƒ€ ì˜ì—­ì— ê´€í•œ XAI ê¸°ë²•ì˜ í•¨ì¶•ì„±ì„ í–¥í•œ ì „ë§ì˜ ë²”ìœ„ë¥¼ ë”ìš± í™•ëŒ€í•©ë‹ˆë‹¤.</li>
<li>ì•ì„œì˜ ì¥ë˜ì˜ ë…¼ì˜ë¥¼ ê±°ì³, AI ëª¨ë¸ì´ ì‹¤ìš©í™”í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°€ì§€ AI ì›ë¦¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì±„íƒí•˜ëŠ” ë§¤ë‹ˆí´ë“œ ê°œë…ì¸ ì±…ì„ê° ìˆëŠ” ì¸ê³µì§€ëŠ¥ì˜ ê°œë…ì— ë„ë‹¬í•œë‹¤.Â ì±…ì„ AIë¥¼ ë’·ë°›ì¹¨í•˜ëŠ” ê°€ì´ë“œë¼ì¸ì€ ì„¤ëª…ê°€ëŠ¥ì„± ì™¸ì—ë„ ì‹¤ì œ í™˜ê²½ì—ì„œ AI ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ ê³µì •ì„±, ì±…ì„ì„±, í”„ë¼ì´ë²„ì‹œ ë“±ë„ ê³ ë ¤í•´ì•¼ í•œë‹¤ê³  ê·œì •í•˜ê³  ìˆë‹¤.</li>
<li>ì±…ì„ ìˆëŠ” AIëŠ” ëª¨ë¸ ì„¤ëª… ê°€ëŠ¥ì„±ê³¼ ê°œì¸ ì •ë³´ ë³´í˜¸/ë³´ì•ˆì„±ì„ ì„¤ê³„ë³„ë¡œ í˜¼í•©í•˜ë¯€ë¡œ, ë¯¼ê°í•œ ì •ë³´ ë°/ë˜ëŠ” ê¸°ë°€ ML ëª¨ë¸ì„ ë‹¤ë£¨ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ XAI ê¸°ë²•ì˜ ìœ ìµì„±ê³¼ ìœ„í•´ì„±ì— ëŒ€í•´ ì‹¬ì˜¤í•œ ë°˜ì„±ì„ ìš”êµ¬í•œë‹¤.Â ë‚˜ì¤‘ì— ë³´ì—¬ë“œë¦¬ê² ì§€ë§Œ, ë°ì´í„° ê°œì¸ ì •ë³´ ë³´í˜¸, í’ˆì§ˆ, ë¬´ê²°ì„± ë° ê±°ë²„ë„ŒìŠ¤ë¥¼ í–¥í•œ ê·œì œëŠ” ì´ ë¶„ì•¼ì—ì„œ XAIì˜ ì—­í• ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë” ë§ì€ ë…¸ë ¥ì„ ìš”êµ¬í•©ë‹ˆë‹¤.Â ì´ì™€ ê´€ë ¨í•˜ì—¬, ìš°ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ìœµí•© íŒ¨ëŸ¬ë‹¤ì„ í•˜ì—ì„œì˜ í”„ë¼ì´ë²„ì‹œ ë° ë³´ì•ˆ ì¸¡ë©´ì—ì„œ XAIì˜ ì˜ë¯¸ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•œë‹¤.</li>
</ol>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>The remainder of this overview is structured as follows: first, Section 2 and subsections therein open a discussion on the terminology and concepts revolving around explainability and interpretability in AI, ending up with the aforementioned novel definition of interpretability (Section 2.1 and 2.2), and a general criterion to categorize and analyze ML models from the XAI perspective. Sections 3 and 4 proceed by reviewing recent findings on XAI for ML models (on transparent models and post-hoc techniques respectively) that comprise the main division in the aforementioned taxonomy. We also include a review on hybrid approaches among the two, to attain XAI. Benefits and caveats of the synergies among the families of methods are discussed in Section 5, where we present a prospect of general challenges and some consequences to be cautious about. Finally, Section 6 elaborates on the concept of Responsible Artificial Intelligence. Section 7 concludes the survey with an outlook aimed at engaging the community around this vibrant research area, which has the potential to impact society, in particular those sectors that have progressively embraced ML as a core technology of their activity.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆë‹¤: </p>
<ul>
<li>Section 2: ì„¤ëª…ê°€ëŠ¥ì„±(explainability)ì™€ í•´ì„ê°€ëŠ¥ì„±(interpretability)ì˜ ìƒˆë¡œìš´ ì •ì˜, XAI ê´€ì ì—ì„œ ML ëª¨ë¸ ë¶„ë¥˜ ë° ë¶„ì„ì„ ìœ„í•œ ìš©ì–´ ë° ê°œë…ì— ëŒ€í•œ ì´ì•¼ê¸°</li>
<li>Section 3, 4: ìµœê·¼ ì—°êµ¬ ê²°ê³¼ì™€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•</li>
<li>Section 5: í•´ë‹¹ ë°©ë²•ë“¤ì— ëŒ€í•œ ì¥ë‹¨ì  ë° ì£¼ì˜í•´ì•¼í•  ëª‡ ê°€ì§€ ê²°ê³¼ë“¤ ì œì‹œ</li>
<li>Section 6: "ì±…ì„ê° ìˆëŠ” ì¸ê³µì§€ëŠ¥" ê°œë…ì— ëŒ€í•œ ì„¤ëª…</li>
<li>Section 7: ì‚¬íšŒì— ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì—°êµ¬ ì˜ì—­ì¸ ë§Œí¼, ML ê¸°ìˆ ì„ ì±„íƒí•œ ì‚¬ëŒë“¤ì„ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì°¸ì—¬ì‹œí‚¤ëŠ” ëª©í‘œë¡œ ê²°ë¡ ì„ ë‚´ë¦¬ê³ ì í•œë‹¤.</li>
</ul>
<h1 id="explainable-artificial-intelligence-xai-2">Explainable Artificial Intelligence (XAI) - 2<a class="headerlink" href="#explainable-artificial-intelligence-xai-2" title="Permanent link">Â¶</a></h1>
<h1 id="explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai_1">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI<a class="headerlink" href="#explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai_1" title="Permanent link">Â¶</a></h1>
<p>Paper Link: <a href="https://arxiv.org/abs/1910.10045">https://arxiv.org/abs/1910.10045</a></p>
<p>XAIì— ëŒ€í•œ ì „ë°˜ì ì¸ ì†Œê°œë¥¼ ì •ë¦¬í•œ ë…¼ë¬¸ì´ ë‚˜ì™€ì„œ ì°¨ê·¼ ì°¨ê·¼ ìš”ì•½ ì •ë¦¬í•´ë³´ë ¤ê³  í•œë‹¤(ë¬´ë ¤ 115í˜ì´ì§€, referenceë§Œ 6í˜ì´ì§€). ì•½ê°„ì˜ ë²ˆì—­ ì–´íˆ¬ì™€ ìƒëµëœ ê²ƒë„ ìˆìœ¼ë‹ˆ ì˜ì–´ ì›ë¬¸ì„ ì°¸ê³ í•˜ê¸¸ ë°”ë€ë‹¤.</p>
<ol>
<li><a href="https://simonjisu.github.io/paper/2020/12/31/xaitutorial1.html">Introduction</a></li>
<li><a href="https://simonjisu.github.io/paper/2021/01/14/xaitutorial2.html"><span style="color:#e25252">Explainability: What, why, what for and how?(ì´ë²ˆí¸)</span></a></li>
<li><a href="https://simonjisu.github.io/paper/2021/01/23/xaitutorial3.html">Transparent machine learning models</a></li>
<li>Post-hoc explainability techniques for machile learning models: Taxonomy, shallow models and deep learning</li>
<li>XAI: Opportunities, challenges and future research needs</li>
<li>Toward responsible AI: Principles of artificial intelligence, fairness, privacy and data fusion</li>
<li>Conclusions and outlook</li>
</ol>
<h1 id="2-explainability-what-why-what-for-and-how">2. Explainability: What, Why, What For and How?<a class="headerlink" href="#2-explainability-what-why-what-for-and-how" title="Permanent link">Â¶</a></h1>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Before proceeding with our literature study, it is convenient to first establish a common point of understanding on what the term explainability stands for in the context of AI and, more specifically, ML. This is indeed the purpose of this section, namely, to pause at the numerous definitions that have been done in regards to this concept (what?), to argue why explainability is an important issue in AI and ML (why? what for?) and to introduce the general classification of XAI approaches that will drive the literature study thereafter (how?).</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ì‹œì‘í•˜ê¸° ì „ì— <strong>ì„¤ëª…ê°€ëŠ¥ì„±(explainability)</strong>ì´ë¼ëŠ” ìš©ì–´ê°€ AI í˜¹ì€ MLì˜ ë§¥ë½ì—ì„œ ë¬´ì—‡ì„ ëœ»í•˜ëŠ”ì§€, ê³µí†µì˜ ì´í•´ì ì„ í™•ë¦½í•´ì•¼í•œë‹¤.</p>
<ul>
<li>What? ì´ ê°œë…ì— ëŒ€í•œ ì •ë¦½ëœ ìˆ˜ ë§ì€ ì •ì˜ë“¤ì„ ì •ë¦¬</li>
<li>Why? What for? ì™œ ì„¤ëª…ê°€ëŠ¥ì„±ì´ AIì™€ MLì—ì„œ ì¤‘ìš”í•œ ì´ìŠˆì¸ì§€</li>
<li>How? ì´í›„ ì—°êµ¬í•  XAI ì ‘ê·¼ ë°©ì‹ì˜ ì¼ë°˜ì ì¸ ë¶„ë¥˜ë°©ì‹ë“¤ ì†Œê°œ</li>
</ul>
<hr/>
<h2 id="21-terminology-clarification">2.1 Terminology Clarification<a class="headerlink" href="#21-terminology-clarification" title="Permanent link">Â¶</a></h2>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>One of the issues that hinders the establishment of common grounds is the interchangeable misuse of interpretability and explainability in the literature. There are notable differences among these concepts. To begin with, interpretability refers to a passive characteristic of a model referring to the level at which a given model makes sense for a human observer. This feature is also expressed as transparency. By contrast, explainability can be viewed as an active characteristic of a model, denoting any action or procedure taken by a model with the intent of clarifying or detailing its internal functions.</p>
<p>To summarize the most commonly used nomenclature, in this section we clarify the distinction and similarities among terms often used in the ethical AI and XAI communities.</p>
<ul>
<li>UnderstandabilityÂ (or equivalently,Â intelligibility) denotes the characteristic of a model to make a human understand its function â€“ how the model works â€“ without any need for explaining its internal structure or the algorithmic means by which the model processes data internallyÂ [18].</li>
<li>Comprehensibility: When conceived for ML models, comprehensibility refers to the ability of a learning algorithm to represent its learned knowledge in a human understandable fashionÂ [19],Â [20],Â [21]. This notion of model comprehensibility stems from the postulates of MichalskiÂ [22], which stated thatÂ <em>â€œthe results of computer induction should be symbolic descriptions of given entities, semantically and structurally similar to those a human expert might produce observing the same entities. Components of these descriptions should be comprehensible as single â€˜chunksâ€™ of information, directly interpretable in natural language, and should relate quantitative and qualitative concepts in an integrated fashionâ€</em>. Given its difficult quantification, comprehensibility is normally tied to the evaluation of the model complexityÂ [17].</li>
<li>Interpretability: It is defined as the ability to explain or to provide the meaning in understandable terms to a human.</li>
<li>Explainability: Explainability is associated with the notion of explanation as an interface between humans and a decision maker that is, at the same time, both an accurate proxy of the decision maker and comprehensible to humansÂ [17].</li>
<li>Transparency: A model is considered to be transparent if by itself it is understandable. Since a model can feature different degrees of understandability, transparent models inÂ SectionÂ 3Â are divided into three categories: simulatable models, decomposable models and algorithmically transparent modelsÂ [5].</li>
</ul>
<p>[/expand]</p>
<p><span style="color:#aaa"> ì°¸ê³ : ìš©ì–´ê°€ í•œêµ­ì–´ë¡œ ê±°ì˜ë‹¤ ë¹„ìŠ·í•´ì„œ ìµœëŒ€í•œ ì˜ë¯¸ë¥¼ ë¶™ì—¬ì„œ ì¶”ê°€í•¨ </span> </p>
<p><span style="color:#e25252">ìš”ì•½:</span> ê³µí†µì˜ ì´í•´ì  í™•ë¦½ì„ ë°©í•´í•˜ëŠ” ìš”ì†Œì¤‘ í•˜ë‚˜ëŠ” <strong>ì„¤ëª…ê°€ëŠ¥ì„±(explainability)</strong>ê³¼ <strong>í•´ì„ê°€ëŠ¥ì„±(interpretability)</strong>ìš©ì–´ì˜ í˜¼ìš©ì´ë‹¤. ì´ ë‘˜ì˜ ê°œë…ìœ ì°¨ì´ì ì´ ìˆë‹¤. ê²°ë¡ ë¶€í„° ë§í•˜ìë©´</p>
<ul>
<li>
<p><strong>í•´ì„ê°€ëŠ¥ì„±(Interpretability):</strong> ëª¨ë¸ì´ ì¸ê°„ì—ê²Œ ë§ì¶°ì„œ ì„¤ëª…í•˜ëŠ” <u>ëª¨ë¸ì˜ ìˆ˜ë™ì  íŠ¹ì„±</u>, íˆ¬ëª…ì„±(transparency)ê³¼ ê°™ì€ ë§</p>
</li>
<li>
<p><strong>ì„¤ëª…ê°€ëŠ¥ì„±(Explainability):</strong> ëª¨ë¸ì˜ ë‚´ë¶€ ê¸°ëŠ¥ì„ ëª…í™•íˆ í•˜ê±°ë‚˜ ìì„¸íˆ ì„¤ëª…í•  ëª©ì ìœ¼ë¡œ, ìˆ˜í–‰ëœ ëª¨ë“  í–‰ë™ ë˜ëŠ” ì ˆì°¨ë¥¼ ë‚˜íƒ€ë‚´ëŠ” <u>ëª¨ë¸ì˜ ëŠ¥ë™ì  íŠ¹ì„±</u></p>
</li>
</ul>
<p>ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ëª…ëª…ë²•ì„ ì´ì•¼ê¸°í•˜ê³ ì ethical AI ë° XAI ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ìš©ì–´ ê°„ì˜ êµ¬ë³„ê³¼ ê·¸ ìœ ì‚¬ì„±ì„ ëª…í™•íˆí•œë‹¤.</p>
<ul>
<li>
<p><strong>ì´í•´ê°€ëŠ¥ì„±(Understandability)</strong> í˜¹ì€ <strong>ëª…ë£Œì„±(Intelligibility):</strong> ëª¨ë¸ êµ¬ì¡° í˜¹ì€ ë‚´ë¶€ì˜ ì•Œê³ ë¦¬ì¦˜ ê¸°ëŠ¥ì˜ ë¶€ê°€ ì„¤ëª… ì—†ì´ë„ ì¸ê°„ì´ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì˜ íŠ¹ì„± <a href="https://www.sciencedirect.com/science/article/pii/S1051200417302385">[18]</a></p>
</li>
<li>
<p><strong>í¬ê´„ì  ì´í•´ê°€ëŠ¥ì„±(Comprehensibility):</strong> ëª¨ë¸ì´ í•™ìŠµí•œ ì§€ì‹ì„ ì¸ê°„ì˜ ì´í•´ë°©ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ëŠ¥ë ¥Â <a href="https://scholar.google.com/scholar?q=Evolutionary%20fuzzy%20systems%20for%20explainable%20artificial%20intelligence:%20Why,%20when,%20what%20for,%20and%20where%20to">[19]</a>,Â <a href="https://scholar.google.com/scholar_lookup?title=A%20framework%20for%20considering%20comprehensibility%20in%20modeling&amp;publication_year=2016&amp;author=M.%20Gleicher">[20]</a>,Â <a href="https://scholar.google.com/scholar_lookup?title=Extracting%20comprehensible%20models%20from%20trained%20neural%20networks&amp;publication_year=1996&amp;author=M.W.%20Craven">[21]</a>. ì´ ê°œë…ì€ Michalski<a href="https://scholar.google.com/scholar_lookup?title=A%20theory%20and%20methodology%20of%20inductive%20learning&amp;publication_year=1983&amp;author=R.S.%20Michalski">[22]</a>ì˜ ê°€ì •ì—ì„œ ë¹„ë¡¯ëë‹¤.</p>
<p>"ì»´í“¨í„° ìœ ë„ ê²°ê³¼ëŠ” ì£¼ì–´ì§„ ì‹¤ì²´ì— ëŒ€í•œ ìƒì§•ì  ì„¤ëª…ì´ì–´ì•¼ í•˜ë©°, ì¸ê°„ ì „ë¬¸ê°€ê°€ ë™ì¼í•œ ì‹¤ì²´ë¥¼ ê´€ì°°í•˜ëŠ” ê²ƒê³¼ ì˜ë¯¸ë¡ ì ì´ê³  êµ¬ì¡°ì ìœ¼ë¡œ ìœ ì‚¬í•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ì„¤ëª…ì˜ êµ¬ì„±ìš”ì†ŒëŠ” ìì—°ì–´ë¡œ ì§ì ‘ í•´ì„í•  ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ë‹¨ì¼ 'ì²­í¬'ë¡œ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•˜ë©°, í†µí•©ëœ ë°©ì‹ìœ¼ë¡œ ì–‘ì  ë° ì§ˆì  ê°œë…ì„ ì—°ê´€ì‹œì¼œì•¼ í•œë‹¤." </p>
<p>ì •ëŸ‰í™”ê°€ ì–´ë µë‹¤ëŠ” ì ì„ ê³ ë ¤í–ˆì„ ë•Œ, í¬ê´„ì  ì´í•´ê°€ëŠ¥ì„±ì€ ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ë³µì¡ë„ í‰ê°€ì™€ ì—°ê´€ëœë‹¤<a href="https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20methods%20for%20explaining%20black%20box%20models&amp;publication_year=2018&amp;author=R.%20Guidotti&amp;author=A.%20Monreale&amp;author=S.%20Ruggieri&amp;author=F.%20Turini&amp;author=F.%20Giannotti&amp;author=D.%20Pedreschi">[17]</a>.</p>
</li>
<li>
<p><strong>í•´ì„ê°€ëŠ¥ì„±(Interpretability)</strong>:Â ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìš©ì–´ë¡œ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ê±°ë‚˜ ì œê³µí•˜ëŠ” ëŠ¥ë ¥</p>
</li>
<li>
<p><strong>ì„¤ëª…ê°€ëŠ¥ì„±(Explainability):</strong> ì‚¬ëŒê³¼ ëª¨ë¸ê°„ì˜ "ì¸í„°í˜ì´ìŠ¤(interface)" ì—­í• ë¡œì„œ ì„¤ëª…(explanation)ê³¼ ì—°ê´€ë˜ì–´ ìˆë‹¤. ì„¤ëª…ì€ ëª¨ë¸ì´ ë‚´ë¦° ì˜ì‚¬ê²°ì •ì˜ ì •í™•í•œ ëŒ€ë¦¬ì´ì ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê²ƒì´ì–´ì•¼ í•œë‹¤<a href="https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20methods%20for%20explaining%20black%20box%20models&amp;publication_year=2018&amp;author=R.%20Guidotti&amp;author=A.%20Monreale&amp;author=S.%20Ruggieri&amp;author=F.%20Turini&amp;author=F.%20Giannotti&amp;author=D.%20Pedreschi">[17]</a>.</p>
</li>
<li>
<p><strong>íˆ¬ëª…ì„±(Transparency):</strong> ëª¨ë¸ì€ ìì‹ ì´ ìŠ¤ìŠ¤ë¡œ ì´í•´ê°€ëŠ¥í•˜ë‹¤ë©´ íˆ¬ëª…í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼ëœë‹¤. ëª¨ë¸ì€ ì •ë„ì— ë”°ë¼ ì´í•´ë ¥ë¥¼ ë‹¤ë¥´ê²Œ ì œê³µí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, íˆ¬ëª…ì„± ìˆëŠ” ëª¨ë¸ì€ section 3ì—ì„œ 3ê°€ì§€ í•­ëª©(ì‹œë®¬ë ˆì´ì…˜ ê°€ëŠ¥í•œ ëª¨ë¸, ë¶„í•´ê°€ëŠ¥í•œ ëª¨ë¸ ê·¸ë¦¬ê³  ì•Œê³ ë¦¬ì¦˜ ìì²´ê°€ íˆ¬ëª…í•œ ëª¨ë¸)ìœ¼ë¡œ ë‚˜ëˆŒ ê²ƒì´ë‹¤.</p>
</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>In all the above definitions, understandability emerges as the most essential concept in XAI. Both transparency and interpretability are strongly tied to this concept: while transparency refers to the characteristic of a model to be, on its own, understandable for a human, understandability measures the degree to which a human can understand a decision made by a model. Comprehensibility is also connected to understandability in that it relies on the capability of the audience to understand the knowledge contained in the model. All in all, understandability is a two-sided matter: model understandability and human understandability. This is the reason why the definition of XAI given in Section 2.2 refers to the concept of audience, as the cognitive skills and pursued goal of the users of the model have to be taken into account jointly with the intelligibility and comprehensibility of the model in use. This prominent role taken by understandability makes the concept of audience the cornerstone of XAI, as we next elaborate in further detail.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ìœ„ì˜ ëª¨ë“  ì •ì˜ì—ì„œ ì´í•´ê°€ëŠ¥ì„±(understandability)ì€ XAIì—ì„œ ê°€ì¥ í•„ìˆ˜ì ì¸ ê°œë…ì´ë‹¤. íˆ¬ëª…ì„±(transparency)ê³¼ í•´ì„ê°€ëŠ¥ì„±(interpretability)ì€ ëª¨ë‘ ì´ ê°œë…ê³¼ ê°•í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆë‹¤. íˆ¬ëª…ì„±(transparency)ì€ ì¸ê°„ì—ê²ŒëŠ” ì´í•´ê°€ëŠ¥í•´ì•¼ í•˜ê³ , ëª¨ë¸ ìŠ¤ìŠ¤ë¡œëŠ” ì´í•´í•  ìˆ˜ ìˆëŠ” íŠ¹ì„±ì´ì§€ë§Œ, ì´í•´ê°€ëŠ¥ì„±(interpretability)ì€ ëª¨ë¸ì˜ ê²°ì •ì„ ì¸ê°„ì´ ì–¼ë§Œí¼ ì´í•´ê°€ëŠ¥í•œì§€ ì¸¡ì •í•œë‹¤. </p>
<p>ë˜í•œ í¬ê´„ì  ì´í•´ê°€ëŠ¥ì„±(comprehensibility)ì€ <strong>ì²­ì¤‘</strong>(audience, <span style="color:#aaa">[ì£¼] ì„¤ëª…ì„ ë“£ëŠ” ì‚¬ëŒ</span>)ì´ ì–¼ë§Œí¼ ëª¨ë¸ì— í¬í•¨ëœ ì§€ì‹ì„ ì´í•´í•˜ëŠ” ì§€ë¥¼ ì¸¡ì •í•œë‹¤ëŠ” ì ì—ì„œ ì´í•´ê°€ëŠ¥ì„±(understandability)ê³¼ ì—°ê²°ëœë‹¤. </p>
<p>ëŒ€ì²´ë¡œ, ì´í•´ê°€ëŠ¥ì„±(understandability)ì€ "ëª¨ë¸"ê³¼ "ì¸ê°„"ì˜ ì´í•´ê°€ëŠ¥ì„±ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ëª¨ë¸ ì´ìš©ìì˜ ì¸ì§€ëŠ¥ë ¥ ë° ì¶”êµ¬ëª©í‘œëŠ” ëª¨ë¸ì˜ ëª…ë£Œì„±ê³¼ í¬ê´„ì  ì´í•´ê°€ëŠ¥ì„±ì´ í•¨ê»˜ ê³ ë ¤ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, Section 2.2ì—ì„œëŠ” XAIê°œë…ì„ ì •ì˜í•  ë•Œ, <strong>ì²­ì¤‘</strong><span style="color:#aaa">([ì£¼] ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì¸ê°„)</span>ì˜ ê°œë…ì„ ë¨¼ì € ì´ì•¼ê¸°í•˜ë ¤ê³  í•œë‹¤. ì´í•´ê°€ëŠ¥ì„±ì€ XAIì˜ ì´ˆì„ì´ ë˜ëŠ” ì²­ì¤‘ì˜ ê°œë…ì„ ë§Œë“œëŠ” ì—­í• ì„ í•œë‹¤.</p>
<hr/>
<h2 id="22-what">2.2 What?<a class="headerlink" href="#22-what" title="Permanent link">Â¶</a></h2>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Although it might be considered to be beyond the scope of this paper, it is worth noting the discussion held around general theories of explanation in the realm of philosophy [23]. Many proposals have been done in this regard, suggesting the need for a general, unified theory that approximates the structure and intent of an explanation. However, nobody has stood the critique when presenting such a general theory. For the time being, the most agreed-upon thought blends together different approaches to explanation drawn from diverse knowledge disciplines. A similar problem is found when addressing interpretability in AI. It appears from the literature that there is not yet a common point of understanding on what interpretability or explainability are. However, many contributions claim the achievement of interpretable models and techniques that empower explainability.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ì² í•™ì˜ ì˜ì—­ì—ì„œ ì„¤ëª…(explanation)ì— ëŒ€í•´ í† ì˜ì— ì£¼ëª©í•  í•„ìš”ê°€ ìˆë‹¤<a href="https://scholar.google.com/scholar_lookup?title=General%20theories%20of%20explanation%3A%20buyer%20beware&amp;publication_year=2013&amp;author=J.%20D%C3%ADez&amp;author=K.%20Khalifa&amp;author=B.%20Leuridan">[23]</a>. ì™œëƒë©´ ì¼ë°˜ì ì´ê³  í†µì¼ëœ ì„¤ëª… ì´ë¡ ì˜ êµ¬ì¡°ì™€ ì˜ë„ë¥¼ ê·¼ì‚¬í•˜ê²Œë‚˜ë§ˆ ì œì‹œí–ˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë ‡ì§€ë§Œ íŠ¼íŠ¼í•œ ì´ë¡ ì€ ì•„ë‹ˆì˜€ë‹¤. ê·¸ë˜ì„œ ê·¸ë™ì•ˆ ë‹¤ì–‘í•œ ì§€ì‹ ë¶„ì•¼ì—ì„œ ë„ì¶œëœ ì„¤ëª…ì— ëŒ€í•œ ì ‘ê·¼ ë°•ì‹±ì„ í˜¼í•©í•œ ì •ì˜ë¥¼ ì‚¬ìš©í–ˆë‹¤. AIì—ì„œ í•´ì„ê°€ëŠ¥ì„±(interpretability)ì„ ë‹¤ë£° ë•Œë„ ë¹„ìŠ·í•œ ë¬¸ì œê°€ ë°œê²¬ëë‹¤. í•´ì„ê°€ëŠ¥ì„±(interpretability)ì´ë‚˜ ì„¤ëª…ê°€ëŠ¥ì„±(explainability)ì˜ ê³µí†µì ì„ ì°¾ì§€ ëª»í–ˆìœ¼ë‚˜, ë§ì€ ì—°êµ¬ìë“¤ì€ í•´ì„ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ìƒì„±ê³¼ ëª¨ë¸ ì„¤ëª…ë ¥ì„ ê°•í™”í–ˆë‹¤ëŠ” ì—°êµ¬ì„±ê³¼ë¥¼ ì£¼ì¥í•´ì™”ë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>To shed some light on this lack of consensus, it might be interesting to place the reference starting point at the definition of the term Explainable Artificial Intelligence (XAI) given by D. Gunning in [7]:
<br/>
'XAI will create a suite of machine learning techniques that enables human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.'
<br/>
This definition brings together two concepts (understanding and trust) that need to be addressed in advance. However, it misses to consider other purposes motivating the need for interpretable AI models, such as causality, transferability, informativeness, fairness and confidence [5], [24], [25], [26]. We will later delve into these topics, mentioning them here as a supporting example of the incompleteness of the above definition.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ë¶€ì¡±í•œ ê³µê°ëŒ€ë¥¼ ì–´ëŠ ì •ë„ í˜•ì„±í•˜ê¸° ìœ„í•´ì„œ D. Gunningì´ ì œì‹œí•œ Explainable Artificial Intelligence(XAI) ìš©ì–´ì˜ ì •ì˜ë¥¼ ê¸°ì¤€ì ìœ¼ë¡œ ì‹œì‘í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤. <a href="https://scholar.google.com/scholar?q=Explainable%20artificial%20intelligence">[7]</a></p>
<blockquote>
<p>"ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆê³ , ì ì ˆí•˜ê²Œ ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©°, íš¨ê³¼ì ìœ¼ë¡œ ì„¸ë¡œìš´ ì„¸ëŒ€ì˜ ì¸ê³µì§€ëŠ¥ íŒŒíŠ¸ë„ˆë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²• ì œí’ˆêµ°ì„ XAIëŠ” ë§Œë“¤ì–´ ë‚¼ ê²ƒì´ë‹¤" </p>
</blockquote>
<p>ì´ ì •ì˜ëŠ” ë‹¤ë£¨ì–´ì•¼ í•  ë‘ ê°€ì§€ ê°œë…(ì´í•´ì™€ ì‹ ë¢°)ì„ ë‹´ì•˜ë‹¤. ê·¸ëŸ¬ë‚˜ í•´ì„ê°€ëŠ¥í•œ AI ëª¨ë¸ì— í•„ìš”í•œ ì¸ê³¼ì„±(causality), ì „ì´ì„±(transferability), ì •ë³´ì„±(informativeness), ê³µì •ì„±(fairness)ê³¼ í™•ì‹¤ì„±(confidence) ë“±ì„ ë‹´ì§€ ì•Šì•˜ë‹¤<a href="https://scholar.google.com/scholar_lookup?title=The%20mythos%20of%20model%20interpretability&amp;publication_year=2018&amp;author=Z.C.%20Lipton">[5]</a>, <a href="https://scholar.google.com/scholar?q=D.%20Doran,%20S.%20Schulz,%20T.R.%20Besold,%20What%20does%20explainable%20AI%20really%20mean%20a%20new%20conceptualization%20of%20perspectives,%202017.">[24]</a>, <a href="https://scholar.google.com/scholar?q=F.%20Doshi-Velez,%20B.%20Kim,%20Towards%20a%20rigorous%20science%20of%20interpretable%20machine%20learning,%202017.">[25]</a>, <a href="https://scholar.google.com/scholar_lookup?title=Making%20machine%20learning%20models%20interpretable.&amp;publication_year=2012&amp;author=A.%20Vellido&amp;author=J.D.%20Mart%C3%ADn-Guerrero&amp;author=P.J.%20Lisboa">[26]</a>. ì–¸ê¸‰í•œ ì£¼ì œë“¤ì€ D. Gunningì˜ ì •ì˜ì— ëŒ€í•œ ë¶ˆì™„ì „ì„±ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì‚¬ë¡€ë¡œ ì—¬ê¸° ì–¸ê¸‰í•˜ë©´ì„œ, ë‚˜ì¤‘ì— ì´ ì£¼ì œë“¤ì„ íŒŒí—¤ì¹  ê²ƒì´ë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>As exemplified by the definition above, a thorough, complete definition of explainability in AI still slips from our fingers. A broader reformulation of this definition (e.g. 'An explainable Artificial Intelligence is one that produces explanations about its functioning') would fail to fully characterize the term in question, leaving aside important aspects such as its purpose. To build upon the completeness, a definition of explanation is first required.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ìœ„ ì˜ˆì‹œì™€ ê°™ì´, ì„¤ëª…ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì™„ë²½í•œ ì •ì˜ë¥¼ ë‚´ë¦¬ê¸° ì–´ë µë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "ì„¤ëª… ê°€ëŠ¥í•œ ì¸ê³µì§€ëŠ¥ì€ ê·¸ì— ëŒ€í•œ ê¸°ëŠ¥ì„ ì„¤ëª…í•˜ëŠ” ê²ƒì´ë‹¤" ê²½ìš°, ì„¤ëª…ê°€ëŠ¥ì„±ì˜ ëª©ì ì„± ì¸¡ë©´ë§Œ ì´ì•¼ê¸°í•œë‹¤. ë”°ë¼ì„œ, ë¨¼ì € ì„¤ëª…(explanation)ì— ëŒ€í•œ ì •ì˜ê°€ í•„ìš”í•˜ë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>As extracted from the Cambridge Dictionary of English Language, an explanation is 'the details or reasons that someone gives to make something clear or easy to understand' [27]. In the context of an ML model, this can be rephrased as: 'the details or reasons a model gives to make its functioning clear or easy to understand'. It is at this point where opinions start to diverge. Inherently stemming from the previous definitions, two ambiguities can be pointed out. First, the details or the reasons used to explain, are completely dependent of the audience to which they are presented. Second, whether the explanation has left the concept clear or easy to understand also depends completely on the audience. Therefore, the definition must be rephrased to reflect explicitly the dependence of the explainability of the model on the audience. To this end, a reworked definition could read as: 'Given a certain audience, explainability refers to the details and reasons a model gives to make its functioning clear or easy to understand.' </p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ì¼€ì„ë¸Œë¦¿ì§€ ì˜ì–´ì‚¬ì „ì„ ì¸ìš©í•˜ë©´, ì„¤ëª…(explanation)ì€ "ì–´ë–¤ ê²ƒì„ ëª…ë°±í•˜ê²Œ í˜¹ì€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•´ ëˆ„êµ°ê°€ê°€ ë°í˜€ì£¼ëŠ” ì„¸ë¶€ ì‚¬í•­ì´ë‚˜ ì´ìœ "ë‹¤<a href="https://scholar.google.com/scholar?q=Cambridge%20advanced%20learners%20dictionary">[27]</a>. <span style="color:#aaa">([ì£¼] ë„¤ì´ë²„ êµ­ì–´ì‚¬ì „ì˜ ê²½ìš°, "ì–´ë–¤ ì¼ì´ë‚˜ ëŒ€ìƒì˜ ë‚´ìš©ì„ ìƒëŒ€í¸ì´ ì˜ ì•Œ ìˆ˜ ìˆë„ë¡ ë°í˜€ ë§í•¨. ë˜ëŠ” ê·¸ëŸ° ë§.") </span></p>
<p>Machine learningì˜ ë§¥ë½ ìƒ, "ëª¨ë¸ì´ ìì‹ ì˜ ê¸°ëŠ¥ì„ ëª…ë°±íˆ í•˜ê±°ë‚˜ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¸ë¶€ì‚¬í•­ í˜¹ì€ ì´ìœ ë¥¼ ë°íˆëŠ” ê²ƒ"ìœ¼ë¡œ ë°”ê¿€ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì—ì„œ ì˜ê²¬ì´ ê°ˆë¦¬ê¸° ì‹œì‘í•œë‹¤. ë³¸ì§ˆì ìœ¼ë¡œ ì´ì „ ì •ì˜ì—ì„œ ë¹„ë¡¯ ëœê²ƒìœ¼ë¡œ ë‘ ê°€ì§€ ì• ë§¤ëª¨í˜¸í•œ ì ì´ ìˆë‹¤.</p>
<ol>
<li>ì„¤ëª…ì— ê´€í•œ ì„¸ë¶€ ì‚¬í•­ì´ë‚˜ ì´ìœ ëŠ” ì´ë¥¼ ë“£ëŠ” ì²­ì¤‘(audience)ê³¼ ê°€ì¥ ì—°ê´€ì´ ìˆë‹¤.</li>
<li>ì„¤ëª…ì´ ëª…ë£Œí•˜ê²Œ í˜¹ì€ ì•Œê¸° ì‰½ê²Œ ë˜ì—ˆëŠ”ì§€ì˜ ì—¬ë¶€ë„ ì™„ì „íˆ ì²­ì¤‘ì—ê²Œ ë‹¬ë ¤ìˆë‹¤.</li>
</ol>
<p>ë”°ë¼ì„œ, ë‘ ê°€ì§€ë¥¼ ë°˜ì˜í•´ ë‹¤ì‹œ ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<blockquote>
<p><strong>"íŠ¹ì • ì²­ì¤‘ì—ê²Œ ëª¨ë¸ì´ ìì‹ ì˜ ê¸°ëŠ¥ì„ ëª…ë°±í•˜ê²Œ í˜¹ì€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë°íˆëŠ” ì„¸ë¶€ì‚¬í•­/ì´ìœ ë¥¼ ì„¤ëª…ê°€ëŠ¥ì„±ì´ë¼ê³  ë§í•œë‹¤."</strong></p>
</blockquote>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Since explaining, as argumenting, may involve weighting, comparing or convincing an audience with logic-based formalizations of (counter) arguments [28], explainability might convey us into the realm of cognitive psychology and the psychology of explanations [7], since measuring whether something has been understood or put clearly is a hard task to be gauged objectively. However, measuring to which extent the internals of a model can be explained could be tackled objectively. Any means to reduce the complexity of the model or to simplify its outputs should be considered as an XAI approach. How big this leap is in terms of complexity or simplicity will correspond to how explainable the resulting model is. An underlying problem that remains unsolved is that the interpretability gain provided by such XAI approaches may not be straightforward to quantify: for instance, a model simplification can be evaluated based on the reduction of the number of architectural elements or number of parameters of the model itself (as often made, for instance, for DNNs). On the contrary, the use of visualization methods or natural language for the same purpose does not favor a clear quantification of the improvements gained in terms of interpretability. The derivation of general metrics to assess the quality of XAI approaches remain as an open challenge that should be under the spotlight of the field in forthcoming years. We will further discuss on this research direction in Section 5.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ì–´ë–¤ ê²ƒì´ ë¶„ëª…í•˜ê²Œ ì´í•´ë˜ì—ˆëŠ”ì§€ëŠ” ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì—, ì„¤ëª…ê°€ëŠ¥ì„±ì€ ì¸ì§€ ì‹¬ë¦¬í•™ì˜ì—­ì„ ëŒì–´ë“¤ì¼ ìˆ˜ë„ ìˆë‹¤. </p>
<p><span style="color:#aaa">([ì£¼] ì£¼ê´€ì ì¸ í•´ì„ì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤.)</span> ê·¸ëŸ¬ë‚˜ ëª¨ë¸ì˜ ë‚´ë¶€ê°€ ì–´ëŠ ì •ë„ê¹Œì§€ ì„¤ëª…ë  ìˆ˜ ìˆëŠ”ì§€ëŠ” ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •ê°€ëŠ¥í•˜ë‹¤. ëª¨ë¸ì˜ ë³µì¡ì„±/ê²°ê³¼ì˜ ë‹¨ìˆœí™” ìˆ˜ë‹¨ë“¤ì„ XAI ì ‘ê·¼ë²•ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ë‹¨ìˆœí™” ì •ë„ë¥¼ ì¸¡ì •í•˜í•˜ì—¬, ê·¸ ì„±ê³¼ë¥¼ ì„¤ëª…ê°€ëŠ¥ì„±ìœ¼ë¡œ ê³„ëŸ‰í•˜ëŠ” ê²ƒì´ë‹¤. <span style="color:#aaa">(ì™œ? ... ì ì ˆí•œ ì˜ˆì‹œê°€ ë– ì˜¤ë¥´ì§€ ì•ŠëŠ”ë‹¤...)</span> í•˜ì§€ë§Œ í•´ê²°ë˜ì§€ ì•Šì€ ê·¼ë³¸ì ì¸ ë¬¸ì œëŠ” ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ì œê³µí•˜ëŠ” í•´ì„ê°€ëŠ¥ì„±ì´ ì§ê´€ì ìœ¼ë¡œ ì •ëŸ‰í™”í•˜ê¸°ê°€ ì‰½ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.</p>
<p>ì˜ˆë¥¼ë“¤ì–´, ëª¨ë¸ì˜ ë‹¨ìˆœí™”(simplification)ëŠ” ì•„í‚¤í…ì³ë¥¼ ê°„ì†Œí™” í•˜ê±°ë‚˜ ë§¤ê°œë³€ìˆ˜(parameters) ìˆ˜ë¥¼ ì¤„ì„ìœ¼ë¡œì„œ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤. ë°˜ë©´, ì‹œê°í™” ë²™ë²•ë“¤ì´ë‚˜ ìì—°ì–´ ì„¤ëª…ì€ í•´ì„ê°€ëŠ¥ì„±ì„ ê³„ëŸ‰í•˜ê¸°ì— ì¢‹ì€ ë°©ë²•ì€ ì•„ë‹ˆë‹¤.
XAI ë°©ë²•ë“¤ì˜ í€„ë¦¬í‹°ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ ì¼ë°˜ì ì¸ ì¸¡ì •ì§€í‘œì˜ ë„ì¶œì€ í–¥í›„ ëª‡ ë…„ê°„ ê³¼ì œë¡œ ë‚¨ì•„ìˆë‹¤. ì´ë¥¼ Section 5ì—ì„œ ë…¼ì˜ í•  ê²ƒì´ë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Explainability is linked to post-hoc explainability since it covers the techniques used to convert a non-interpretable model into a explainable one. In the remaining of this manuscript, explainability will be considered as the main design objective, since it represents a broader concept. A model can be explained, but the interpretability of the model is something that comes from the design of the model itself. Bearing these observations in mind, explainable AI can be defined as follows:
<br/>
'Given an audience, anÂ explainableÂ Artificial Intelligence is one that produces details or reasons to make its functioning clear or easy to understand.'
<br/>
This definition is posed here as a first contribution of the present overview, implicitly assumes that the ease of understanding and clarity targeted by XAI techniques for the model at hand reverts on different application purposes, such as a better trustworthiness of the modelâ€™s output by the audience.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ì„¤ëª…ê°€ëŠ¥ì„±(explainability)ëŠ” í•´ì„ì´ ë¶ˆê°€ëŠ¥í•œ ëª¨ë¸ì„ ê°€ëŠ¥ì¼€í•œë‹¤ëŠ” ì ì—ì„œ ì‚¬í›„(post-hoc) ì„¤ëª…ì„±ê³¼ ì—°ê´€ì´ ìˆë‹¤. ì´ í›„ì˜ ë…¼ë¬¸ì—ì„œëŠ” ë” ë„“ì€ ì˜ë¯¸ì¸ ì„¤ëª…ê°€ëŠ¥ì„±ì„ ì£¼ìš” ëª©í‘œë¡œ ìƒê°í•  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë¸ì˜ í•´ì„ê°€ëŠ¥ì„±(interpretability)ì€ ëª¨ë¸ ìì²´ ì„¤ê³„ì—ì„œ ë¹„ë¡¯ëœë‹¤. ì´ëŸ¬í•œ ìƒê°ì„ ì—¼ë‘í•´ë‘ê³ , explainable AIëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.</p>
<blockquote>
<p><strong>ì„¤ëª… ê°€ëŠ¥í•œ ì¸ê³µì§€ëŠ¥(explainable Artificial Intelligence)ì€ ì²­ì¤‘ì—ê²Œ ìì‹ ì˜ ê¸°ëŠ¥ì„ ëª…ë°±í•˜ê²Œ í˜¹ì€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¸ë¶€ì‚¬í•­/ì´ìœ ë¥¼ ìƒì‚°í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì„ ë§í•œë‹¤.</strong></p>
</blockquote>
<hr/>
<h2 id="23-why">2.3 Why?<a class="headerlink" href="#23-why" title="Permanent link">Â¶</a></h2>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>As stated in the introduction, explainability is one of the main barriers AI is facing nowadays in regards to its practical implementation. The inability to explain or to fully understand the reasons by which state-of-the-art ML algorithms perform as well as they do, is a problem that find its roots in two different causes, which are conceptually illustrated in Fig. 2.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ë„ì…ë¶€ì— ê¸°ìˆ í•œ ë°”ì™€ ê°™ì´, ì„¤ëª…ê°€ëŠ¥ì„±ì€ AIì˜ ì‹¤ì§ˆì  í™œìš©ì— ì§ë©´í•˜ê³  ìˆëŠ” ì£¼ìš” ì¥ë²½ ì¤‘ í•˜ë‚˜ì´ë‹¤. ìµœì²¨ë‹¨ ML ì•Œê³ ë¦¬ì¦˜ì´ ì˜ ì‘ë™í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì§€ ëª»í•˜ê±°ë‚˜ ì™„ì „íˆ ì´í•´í•  ìˆ˜ ì—†ëŠ” ê²ƒì€ ë‘ ê°€ì§€ ë‹¤ë¥¸ ì›ì¸ì— ê·¸ ë¿Œë¦¬ë¥¼ ì°¾ëŠ” ë¬¸ì œì´ë©°, ì´ëŠ” ê°œë…ì ìœ¼ë¡œ <code>Fig 2</code>ì— ë‚˜íƒ€ë‚˜ ìˆë‹¤. <span style="color:#aaa">([ì£¼] ê·¸ ì›ì¸ì€ ì²­ì¤‘ì´ ëˆ„êµ¬ì¸ê°€ì— ë”°ë¼ì„œ ML ì•Œê³ ë¦¬ì¦˜ì˜ í•„ìš”í•œ ì„¤ëª…ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì´ë‹¤)</span></p>
<p>{% include image.html id="1KcJ-gbJw7a8xSghhYs5uM3eefZ9h2vo7" desc="Fig 2. ê°ê¸° ë‹¤ë¥¸ ì²­ì¤‘ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ì„¤ëª…ê°€ëŠ¥ì„±ì˜ ëª©ì " width="100%" height="auto" %}</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Without a doubt, the first cause is the gap between the research community and business sectors, impeding the full penetration of the newest ML models in sectors that have traditionally lagged behind in the digital transformation of their processes, such as banking, finances, security and health, among many others. In general this issue occurs in strictly regulated sectors with some reluctance to implement techniques that may put at risk their assets.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ì˜ì‹¬í•  ì—¬ì§€ ì—†ì´, ì²« ë²ˆì§¸ ì›ì¸ì€ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì™€ ì‚¬ì—… ë¶€ë¬¸ ì‚¬ì´ì˜ ê²©ì°¨ë¡œ ì¸í•´ ì€í–‰, ê¸ˆìœµ, ë³´ì•ˆ, ê±´ê°• ë“± ì „í†µì ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ì˜ ë””ì§€í„¸ ì „í™˜ì—ì„œ ë’¤ì²˜ì§„ ë¶„ì•¼ì—ì„œ ìµœì‹  ML ëª¨ë¸ì˜ ì™„ì „í•œ ë³´ê¸‰ì— ì¥ì• ê°€ ë˜ê³  ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì´ ë¬¸ì œëŠ” ì—„ê²©í•˜ê²Œ ê·œì œë˜ëŠ” ë¶€ë¬¸ì—ì„œ ë°œìƒí•˜ë©° ìì‚°ì˜ ìœ„í—˜ì„ ì´ˆë˜í•  ìˆ˜ ìˆëŠ” ê¸°ë²•ì˜ ì‹œí–‰ì„ ì¼ë¶€ êº¼ë¦°ë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>The second axis is that of knowledge. AI has helped research across the world with the task of inferring relations that were far beyond the human cognitive reach. Every field dealing with huge amounts of reliable data has largely benefited from the adoption of AI and ML techniques. However, we are entering an era in which results and performance metrics are the only interest shown up in research studies. Although for certain disciplines this might be the fair case, science and society are far from being concerned just by performance. The search for understanding is what opens the door for further model improvement and its practical utility.
<br/>
The following section develops these ideas further by analyzing the goals motivating the search for explainable AI models.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ë‘ë²ˆì§¸ëŠ” ì§€ì‹ì„ ì¶”êµ¬í•˜ëŠ” ì¸¡ë©´ì´ë‹¤. AIëŠ” ì¸ê°„ì˜ ì¸ì§€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ì—°êµ¬ë¥¼ ë„ì™”ë‹¤. ë§‰ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ëª¨ë“  ë¶„ì•¼ëŠ” AIì™€ ML ê¸°ìˆ ì„ ë„ì…í•¨ìœ¼ë¡œì„œ í° í˜œíƒì„ ì…ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—°êµ¬ ì„±ê³¼ ì§€í‘œì—ë§Œ ê´€ì‹¬ì„ ê°€ì§€ëŠ” ì‹œëŒ€ê°€ ì ‘ì–´ë“¤ ë©´ì„œ ì´ëŠ” ë¬¸ì œê°€ ëœë‹¤. ì„±ê³¼ ì§€í‘œë¡œë§Œ ê³¼í•™ê³¼ ì‚¬íšŒë¥¼ ì´ì•¼ê¸° í•˜ê¸°ì—ëŠ” ì˜¬ë°”ë¥´ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ì´í•´ë¥¼ ì—°êµ¬í•œë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì„ ê°œì„ ì‹œí‚¤ê³  ê·¸ ìœ ìš©ì„±ì„ ì¦ì§„ì‹œí‚¤ëŠ” ì¼ì´ë‹¤.</p>
<hr/>
<h2 id="24-what-for">2.4 What for?<a class="headerlink" href="#24-what-for" title="Permanent link">Â¶</a></h2>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>The research activity around XAI has so far exposed different goals to draw from the achievement of an explainable model. Almost none of the papers reviewed completely agrees in the goals required to describe what an explainable model should compel. However, all these different goals might help discriminate the purpose for which a given exercise of ML explainability is performed. Unfortunately, scarce contributions have attempted to define such goals from a conceptual perspective [5], [13], [24], [30]. We now synthesize and enumerate definitions for these XAI goals, so as to settle a first classification criteria for the full suit of papers covered in this review:</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> "ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì´ ë¬´ì—‡ì„ ê°•ì¡°í•´ì•¼í•˜ëŠ”ê°€?"ë¼ëŠ” ëª©ì (í˜¹ì€ ì´ì—ëŒ€í•œ í•©ì˜ì œì‹œ)ì„ ê°€ì§„ ë…¼ë¬¸ì€ ê±°ì˜ ì—†ì—ˆë‹¤. ì´ì œë¶€í„° ë¶„ë¥˜ê¸°ì¤€ì„ ì •í•˜ê³ , XAIì˜ ëª©í‘œì— ëŒ€í•œ ì •ì˜ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì—´ê±°í•˜ë ¤ê³  í•œë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Trustworthiness: Several authors agree upon the search for trustworthiness as the primary aim of an explainable AI model [31], [32]. However, declaring a model as explainable as per its capabilities of inducing trust might not be fully compliant with the requirement of model explainability. Trustworthiness might be considered as the confidence of whether a model will act as intended when facing a given problem. Although it should most certainly be a property of any explainable model, it does not imply that every trustworthy model can be considered explainable on its own, nor is trustworthiness a property easy to quantify. Trust might be far from being the only purpose of an explainable model since the relation among the two, if agreed upon, is not reciprocal. Part of the reviewed papers mention the concept of trust when stating their purpose for achieving explainability. However, as seen in Table 1, they do not amount to a large share of the recent contributions related to XAI.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì‹ ë¢°ë„(Trustworthiness):</strong> ëª‡ëª‡ ì—°êµ¬ìë“¤ì€ ì‹ ë¢°ë„ë¥¼ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ìš°ì„ ì  ëª©í‘œë¥¼ ë‘¬ì•¼í•œë‹¤ê³  ì£¼ì¥í•œë‹¤(<a href="https://scholar.google.com/scholar_lookup?title=iBCM%3A%20Interactive%20Bayesian%20case%20model%20empowering%20humans%20via%20intuitive%20interaction&amp;publication_year=2015&amp;author=B.%20Kim&amp;author=E.%20Glassman&amp;author=B.%20Johnson&amp;author=J.%20Shah">[31]</a>, <a href="https://scholar.google.com/scholar?q=Why%20should%20I%20trust%20you:%20Explaining%20the%20predictions%20of%20any%20classifier">[32]</a>). ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ì£¼ì¥ì€ ëª¨ë¸ ì„¤ëª…ì„±ì˜ ìš”êµ¬ì¡°ê±´ì„ ì™„ì „íˆ ì¶©ì¡±í•˜ì§€ ëª»í•œë‹¤. ì‹ ë¢°ë„ëŠ” ëª¨ë¸ì´ ì§ë©´í•œ ì–´ë–¤ ë¬¸ì œì—ì„œ ì„¤ê³„ ì˜ë„ëœ ë°”ë¡œ í–‰ë™í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼ í•  ìˆ˜ ìˆë‹¤. ì‹ ë¢°ë„ëŠ” ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ì†ì„±ì´ ë˜ì–´ì•¼ í•˜ì§€ë§Œ, ëª¨ë“  ì‹ ë¢°ì„±ìˆëŠ” ëª¨ë¸ì´ ì„¤ëª… ê°€ëŠ¥í•˜ì§€ëŠ” ì•Šìœ¼ë©°, ì´ íŠ¹ì„±ì„ ê³„ëŸ‰í•˜ê¸° ì‰½ì§€ë„ ì•Šë‹¤. <code>í‘œ1</code>ì—ì„œë„ í™•ì¸ í•  ìˆ˜ ìˆì§€ë§Œ ìµœê·¼ì˜ ì—°êµ¬ê¸°ì—¬ë“¤ ì¤‘ì—ì„œ í° ë¹„ì¤‘ì„ ì°¨ì§€ í•˜ì§€ ì•ŠëŠ”ë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Causality: Another common goal for explainability is that of finding causality among data variables. Several authors argue that explainable models might ease the task of finding relationships that, should they occur, could be tested further for a stronger causal link between the involved variables [159], [160]. The inference of causal relationships from observational data is a field that has been broadly studied over time [161]. As widely acknowledged by the community working on this topic, causality requires a wide frame of prior knowledge to prove that observed effects are causal. A ML model only discovers correlations among the data it learns from, and therefore might not suffice for unveiling a cause-effect relationship. However, causation involves correlation, so an explainable ML model could validate the results provided by causality inference techniques, or provide a first intuition of possible causal relationships within the available data. Again, Table 1 reveals that causality is not among the most important goals if we attend to the amount of papers that state it explicitly as their goal.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì¸ê³¼ì„±(Causality):</strong> ì„¤ëª…ê°€ëŠ¥ì„±ì˜ ë‹¤ë¥¸ ëª©í‘œë¡œëŠ” ë³€ìˆ˜ë“¤ ê°„ì˜ ì¸ê³¼ì„±ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ëª‡ëª‡ ì €ìë“¤ì€ ì´ ê³¼ì •ì„ ìš©ì´í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•œë‹¤(<a href="https://scholar.google.com/scholar?q=Smoking%20and%20the%20occurence%20of%20alzheimers%20disease:%20Cross-sectional%20and%20longitudinal%20data%20in%20a%20population-based%20study">[159]</a>, <a href="https://scholar.google.com/scholar?q=An%20empirical%20study%20of%20machine%20learning%20techniques%20for%20affect%20recognition%20in%20humanrobot%20interaction">[160]</a>). ì¸ê³¼ ê´€ê³„ì˜ ì¶”ë¡ ì€ ìƒë‹¹íˆ ì˜¤ëœì‹œê°„ ì—°êµ¬ë˜ì—ˆë‹¤(<a href="https://scholar.google.com/scholar_lookup?title=Causality&amp;publication_year=2009&amp;author=J.%20Pearl">[161]</a>). ìš°ë¦¬ê°€ ê´€ì°°í•œ ì˜í–¥(effects)ì´ ì¸ê³¼ì„±ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•˜ê¸° ìœ„í•´ì„œ, ì¸ê³¼ ê´€ê³„ ë¶„ì•¼ëŠ” ê´‘ë²”ìœ„í•œ ì‚¬ì „ ì§€ì‹ì˜ í”„ë ˆì„ì„ í•„ìš”ë¡œ í•œë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ë°ì´í„°ì˜ ìƒê´€ ê´€ê³„ë¥¼ ì°¾ê¸°ë§Œ í•˜ì§€, ì¸ê³¼ ê´€ê³„ë¥¼ ì¶©ë¶„í•˜ê²Œ ë°íˆì§€ëŠ” ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜, ì¸ê³¼ì„±ì€ ìƒê´€ì„±ì„ í¬í•¨í•˜ê¸° ë•Œë¬¸ì—, ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì´ìš©í•´ ì„¤ëª… ê°€ëŠ¥í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ê²°ê³¼ì— ëŒ€í•´ì„œ ê²€ì¦í•˜ê±°ë‚˜, ì¸ê³¼ê´€ê³„ë¥¼ ì°¾ì•„ë³¼ ìˆ˜ëŠ” ìˆë‹¤. í•˜ì§€ë§Œ <code>í‘œ1</code>ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ë…¼ë¬¸ì˜ ì–‘ì„ ê¸°ì¤€ìœ¼ë¡œ í•œë‹¤ë©´, ë©”ì¸ ëª©í‘œëŠ” ì•„ì§ ì•„ë‹ˆë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Transferability: Models are always bounded by constraints that should allow for their seamless transferability. This is the main reason why a training-testing approach is used when dealing with ML problems [162], [163]. Explainability is also an advocate for transferability, since it may ease the task of elucidating the boundaries that might affect a model, allowing for a better understanding and implementation. Similarly, the mere understanding of the inner relations taking place within a model facilitates the ability of a user to reuse this knowledge in another problem. There are cases in which the lack of a proper understanding of the model might drive the user toward incorrect assumptions and fatal consequences [44], [164]. Transferability should also fall between the resulting properties of an explainable model, but again, not every transferable model should be considered as explainable. As observed in Table 1, the amount of papers stating that the ability of rendering a model explainable is to better understand the concepts needed to reuse it or to improve its performance is the second most used reason for pursuing model explainability.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì „ì´ê°€ëŠ¥ì„±(Transferability):</strong> ëª¨ë¸ì€ ì›í™œí•œ ì „ì´ê°€ëŠ¥ì„±ì„ ê°€ì§€ê¸° ìœ„í•´ì„œ ì¼ë°˜í™”ê°€ ì˜ ë˜ì–´ì•¼ í•œë‹¤. ê·¸ë˜ì„œ training-testing ë°©ë²•ì„ ì‚¬ìš©í•´ì„œ í›ˆë ¨í•˜ëŠ” ê²ƒì´ë‹¤(<a href="https://scholar.google.com/scholar_lookup?title=Applied%20predictive%20modeling&amp;publication_year=2013&amp;author=M.%20Kuhn&amp;author=K.%20Johnson">[162]</a>, <a href="https://scholar.google.com/scholar_lookup?title=An%20introduction%20to%20statistical%20learning&amp;publication_year=2013&amp;author=G.%20James&amp;author=D.%20Witten&amp;author=T.%20Hastie&amp;author=R.%20Tibshirani">[163]</a>). ì„¤ëª…ê°€ëŠ¥ì„±ë„ ì „ì´ê°€ëŠ¥ì„±ì´ í•„ìš”í•œë°, ëª¨ë¸ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ê²½ê³„ë¥¼ í™•ì¥ í•˜ë©´ì„œ ë” ë‚˜ì€ ì´í•´ì™€ êµ¬í˜„ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ë•Œë¬¸ì´ê³ , ë‚´ë¶€ê´€ê³„ì˜ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ë¬¸ì œì—ì„œ ì´ ì§€ì‹ì„ ì¬ì‚¬ìš© í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë‹¤ë§Œ, ëª¨ë¸ì— ëŒ€í•œ ì´í•´ê°€ ë¶€ì¡±í•˜ì—¬ ì˜ëª»ëœ ê°€ì •ê³¼ ì¹˜ëª…ì ì¸ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ê²½ìš°ë„ ìˆë‹¤(<a href="https://scholar.google.com/scholar_lookup?title=Intelligible%20models%20for%20healthcare%3A%20Predicting%20pneumonia%20risk%20and%20hospital%2030-day%20readmission&amp;publication_year=2015&amp;author=R.%20Caruana&amp;author=Y.%20Lou&amp;author=J.%20Gehrke&amp;author=P.%20Koch&amp;author=M.%20Sturm&amp;author=N.%20Elhadad">[44]</a>, <a href="https://scholar.google.com/scholar?q=C.%20Szegedy,%20W.%20Zaremba,%20I.%20Sutskever,%20J.%20Bruna,%20D.%20Erhan,%20I.%20Goodfellow,%20R.%20Fergus,%20Intriguing%20properties%20of%20neural%20networks,%202013.">[164]</a>). ë˜í•œ, ì „ì´ê°€ëŠ¥ì„±ì€ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì˜ íŠ¹ì„±ì´ì§€ë§Œ, ëª¨ë“  ì „ì´ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì„¤ëª…ê°€ëŠ¥í•˜ì§€ëŠ” ì•Šë‹¤. <code>í‘œ1</code>ì—ì„œ ë³¼ìˆ˜ ìˆë“¯ì´, ëª¨ë¸ì˜ ì „ì´ê°€ëŠ¥ì„±ì€ (ì—°êµ¬ê¸°ì—¬ì˜ ì–‘ì ìœ¼ë¡œ ë”°ì¡Œì„ ë•Œ) ëª¨ë¸ì˜ ì„¤ëª…ì„±ì„ ì¶”êµ¬í•˜ëŠ” 2ë²ˆì§¸ ì´ìœ ê°€ ë˜ë©°, ì„¤ëª…ê°€ëŠ¥ì„±ì€ ëª¨ë¸ì„ ì¬ì‚¬ìš© í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê°œë…ì„ ë” ì˜ ì´í•´í•˜ê³ , ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Informativeness: ML models are used with the ultimate intention of supporting decision making [92]. However, it should not be forgotten that the problem being solved by the model is not equal to that being faced by its human counterpart. Hence, a great deal of information is needed in order to be able to relate the user's decision to the solution given by the model, and to avoid falling in misconception pitfalls. For this purpose, explainable ML models should give information about the problem being tackled. Most of the reasons found among the papers reviewed is that of extracting information about the inner relations of a model. Almost all rule extraction techniques substantiate their approach on the search for a simpler understanding of what the model internally does, stating that the knowledge (information) can be expressed in these simpler proxies that they consider explaining the antecedent. This is the most used argument found among the reviewed papers to back up what they expect from reaching explainable models.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì •ë³´ì„±(Informativeness):</strong> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ê¶ê·¹ì ì¸ ëª©ì ì€ ì˜ì‚¬ ê²°ì •ì˜ ì§€ì›ì´ë‹¤(<a href="https://scholar.google.com/scholar_lookup?title=An%20empirical%20evaluation%20of%20the%20comprehensibility%20of%20decision%20table%2C%20tree%20and%20rule%20based%20predictive%20models&amp;publication_year=2011&amp;author=J.%20Huysmans&amp;author=K.%20Dejaeger&amp;author=C.%20Mues&amp;author=J.%20Vanthienen&amp;author=B.%20Baesens">[92]</a>). ê·¸ëŸ¬ë‚˜ ëª¨ë¸ì´ í’€ê³  ìˆëŠ” ë¬¸ì œëŠ” ì¸ê°„ì´ ì§ë©´í•˜ê³  ìˆëŠ” ë¬¸ì œì™€ í•­ìƒ ê°™ì€ ê²ƒì€ ì•„ë‹ˆë‹¤. <span style="color:#aaa">([ì£¼] ì•„ì§ê¹Œì§€ ëª¨ë¸ì€ ë” ë‹¨ìˆœí•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìˆê¸° ë•Œë¬¸, ì•„ì§ ë³µí•©ì ì¸ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì§€ëŠ” ëª»í•œë‹¤.)</span> ë”°ë¼ì„œ ì‚¬ìš©ìì˜ ì˜ì‚¬ê²°ì •ì— ëª¨ë¸ì´ ë‚´ë†“ì€ ì†”ë£¨ì…˜ì„ ì˜¤í•´í•˜ì§€ ì•Šê²Œ ì˜ ì—°ê´€ ì‹œí‚¤ë ¤ë©´ ë” ë§ì€ ì–‘ì˜ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤. ì´ë¥¼ ìœ„í•´, ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì€ ë¬¸ì œì— íƒœí´ì´ ë  ë§Œí•œ ì •ë³´ë¥¼ ë” ë§ì´ ì œê³µí•´ì•¼í•œë‹¤. ëŒ€ë¶€ë¶„ ë…¼ë¬¸ì„ ì‚´í´ë³¸ ê²°ê³¼, ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì˜ ë‚´ë¶€ ê´€ê³„ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•¨ì´ì—ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê·œì¹™ê¸°ë°˜ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ë ¤ëŠ” ì‚¬ëŒë“¤ì€ ëª¨ë¸ í–‰ë™ì˜ ì´í•´ë¥¼ ë” ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ë°©ë²•ë¡ ì„ ì—°êµ¬í•˜ê³  ìˆì—ˆë‹¤. ê·¸ë“¤ì€ ëª¨ë¸ì˜ ì§€ì‹ì´ë‚˜ ì •ë³´ë¥¼ ë” ê°„ë‹¨í•˜ê²Œ ëŒ€ì²´í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ ì£¼ì¥í–ˆë‹¤. ì´ëŠ” ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì—ì„œ ê°€ì¥ ë§ì€ ê¸°ëŒ€ë¥¼ í•˜ëŠ” íŠ¹ì„±ì´ë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Confidence: As a generalization of robustness and stability, confidence should always be assessed on a model in which reliability is expected. The methods to maintain confidence under control are different depending on the model. As stated in [165], [166], [167], stability is a must-have when drawing interpretations from a certain model. Trustworthy interpretations should not be produced by models that are not stable. Hence, an explainable model should contain information about the confidence of its working regime.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>í™•ì‹ (Confidence):</strong> ê±´ì „ì„±(robustness)ì™€ ì•ˆì „ì„±(stability)ì˜ ì¼ë°˜í™”ë¡œì„œ, ì‹ ë¢°ì„±ì´ ê¸°ëŒ€ë˜ëŠ” ëª¨ë¸ì€ ê·¸ í™•ì‹ ì˜ ì •ë„ë¥¼ ê°€ëŠ í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. í™•ì‹ ì„ ìœ ì§€í•˜ëŠ” ë°©ë²•ì€ ëª¨ë¸ì— ë”°ë¼ì„œ ë‹¤ë¥´ë‹¤. <a href="https://scholar.google.com/scholar_lookup?title=Robust%20statistics%3A%20The%20approach%20based%20on%20influence%20functions&amp;publication_year=1987&amp;author=D.%20Ruppert">[165]</a>, <a href="https://scholar.google.com/scholar_lookup?title=Iterative%20random%20forests%20to%20discover%20predictive%20and%20stable%20high-order%20interactions&amp;publication_year=2018&amp;author=S.%20Basu&amp;author=K.%20Kumbier&amp;author=J.B.%20Brown&amp;author=B.%20Yu">[166]</a>, <a href="https://scholar.google.com/scholar_lookup?title=Stability&amp;publication_year=2013&amp;author=B.%20Yu">[167]</a>ì—ì„œ ê¸°ìˆ í•œ ë°”ì™€ ê°™ì´, ì•ˆì •ì„±ì€ ì–´ë–¤ ëª¨ë¸ì—ì„œ í•´ì„ì„ ë„ì¶œí•˜ê¸° ìœ„í•´ì„œ ê¼­ í•„ìš”í•œ íŠ¹ì„±ì´ë‹¤. <span style="color:#aaa">([ì£¼] ì¼ë‹¨ ëª¨ë¸ì´ ì¼ë°˜í™”ê°€ ì˜ ë˜ì–´ ìˆì–´ì•¼ ê·¸ í•´ì„ ë˜í•œ ì•ˆì •ì ìœ¼ë¡œ ë„ì¶œ í•  ìˆ˜ ìˆë‹¤.)</span></li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Fairness: From a social standpoint, explainability can be considered as the capacity to reach and guarantee fairness in ML models. In a certain literature strand, an explainable ML model suggests a clear visualization of the relations affecting a result, allowing for a fairness or ethical analysis of the model at hand [3], [100]. Likewise, a related objective of XAI is highlighting bias in the data a model was exposed to [168], [169]. The support of algorithms and models is growing fast in fields that involve human lives, hence explainability should be considered as a bridge to avoid the unfair or unethical use of algorithmâ€™s outputs.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ê³µì •ì„±(Fairness):</strong> ì‚¬íšŒì  ê´€ì ì—ì„œ ì„¤ëª…ê°€ëŠ¥ì„±ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê³µì •ì„±ì„ ë³´ì¥í•˜ëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹ì • ë¬¸í—Œì—ì„œ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì€ ëª…ë°±í•œ ê²°ê³¼ì˜ ê´€ê³„ ì‹œê°í™”ë¥¼ í†µí•´ ê³µì •ì„± í˜¹ì€ ìœ¤ë¦¬ì  ë¶„ì„ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤(<a href="https://scholar.google.com/scholar?q=European%20union%20regulations%20on%20algorithmic%20decision-making%20and%20a%20right%20to%20explanation">[3]</a>, <a href="https://scholar.google.com/scholar_lookup?title=Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments&amp;publication_year=2017&amp;author=A.%20Chouldechova">[100]</a>). ì´ì™€ ê´€ë ¨ëœ ëª©í‘œë¡œëŠ” <a href="https://scholar.google.com/scholar?q=K.%20Burns,%20L.A.%20Hendricks,%20K.%20Saenko,%20T.%20Darrell,%20A.%20Rohrbach,%20Women%20also%20Snowboard:%20Overcoming%20Bias%20in%20Captioning%20Models,%202018.">[168]</a>, <a href="https://scholar.google.com/scholar_lookup?title=Towards%20explainable%20neural-symbolic%20visual%20reasoning&amp;publication_year=2019&amp;author=A.%20Bennetot&amp;author=J.-L.%20Laurent&amp;author=R.%20Chatila&amp;author=N.%20D%C3%ADaz-Rodr%C3%ADguez">[169]</a>ì—ì„œ ë³´ì—¬ì¤€ ë°”ì™€ ê°™ì´, ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ì˜ í¸í–¥ì„ ê°•ì¡°í•˜ëŠ” ê²ƒì´ë‹¤. ì‚¬ëŒë“¤ì˜ ì‚¶ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(ì•Œê³ ë¦¬ì¦˜)ì€ ì•ìœ¼ë¡œ ê³„ì† ë¹ ë¥´ê²Œ ë…¸ì¶œë  ìˆ˜ ë°–ì— ì—†ê¸°ì—, ì„¤ëª…ê°€ëŠ¥ì„±ì€ ì´ëŸ¬í•œ ë¶ˆê³µí‰ê³¼ ë¹„ìœ¤ë¦¬ì ì¸ ì•Œê³ ë¦¬ì¦˜ì˜ ì‚°ì¶œë¬¼ì„ í”¼í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê°€êµì—­í• ì´ ë˜ì–´ì•¼ í•œë‹¤. <span style="color:#aaa">([ì£¼] ìµœê·¼ <a href="https://blog.pingpong.us/luda-issue-faq/?fbclid=IwAR15-eiWeIPSnv8lT0WXlO07HBpP0aJaoN36vThaGDmIaBeZpU6jiIy_oJw">ì´ë£¨ë‹¤</a> ì¼ì€ ì‹ ë¢°ë„ì—ë„ ì†í•˜ì§€ë§Œ, ì´ ë²”ì£¼ì—ë„ ì†í•œë‹¤ê³  í•  ìˆ˜ ìˆê² ë‹¤.)</span></li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Accessibility: A minor subset of the reviewed contributions argues for explainability as the property that allows end users to get more involved in the process of improving and developing a certain ML model [37], [86]. It seems clear that explainable models will ease the burden felt by non-technical or non-expert users when having to deal with algorithms that seem incomprehensible at first sight. This concept is expressed as the third most considered goal among the surveyed literature.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì ‘ê·¼ì„±(Accessibility):</strong> ì¼ë¶€ ì—°êµ¬ìë“¤ì€ ì„¤ëª…ê°€ëŠ¥ì„±ì€ ìµœì¢… ì‚¬ìš©ìê°€ íŠ¹ì • ëª¨ë¸ì„ ê°œë°œí•˜ê³  ê°œì„ í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì— ë” ë§ì´ ê´€ì—¬í•  ìˆ˜ ìˆëŠ” ì†ì„±ì´ë¼ê³  ì£¼ì¥í•œë‹¤(<a href="https://scholar.google.com/scholar_lookup?title=Working%20with%20beliefs%3A%20AI%20transparency%20in%20the%20enterprise.&amp;publication_year=2018&amp;author=A.%20Chander&amp;author=R.%20Srinivasan&amp;author=S.%20Chelian&amp;author=J.%20Wang&amp;author=K.%20Uchino">[38]</a>, <a href="https://scholar.google.com/scholar_lookup?title=Explainable%20AI%3A%20Beware%20of%20inmates%20running%20the%20asylum&amp;publication_year=2017&amp;author=T.%20Miller&amp;author=P.%20Howe&amp;author=L.%20Sonenberg">[86]</a>). ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì€ ë¹„ì „ë¬¸ê°€ ì‚¬ìš©ìì—ê²Œ ì²˜ìŒ ë³´ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ë¶€ë‹´ì„ ëœì–´ ì¤„ ìˆ˜ ìˆì–´ë³´ì¸ë‹¤. ì´ëŠ” ì´ë²ˆ ì¡°ì‚¬ì—ì„œ 3ë²ˆì§¸ë¡œ ë§ì´ ê³ ë ¤ëœ ëª©í‘œë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Interactivity: Some contributions [50], [59] include the ability of a model to be interactive with the user as one of the goals targeted by an explainable ML model. Once again, this goal is related to fields in which the end users are of great importance, and their ability to tweak and interact with the models is what ensures success.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ìƒí˜¸ì‘ìš©(Interactivity):</strong> íŠ¹ì • ë…¼ë¬¸([50], [59])ì—ì„œëŠ” ì‚¬ìš©ìì™€ ëª¨ë¸ì´ ìƒí˜¸ì‘ìš©í•˜ëŠ” ëŠ¥ë ¥ì„ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ëª©í‘œë¡œ ì¡ì•˜ë‹¤. ì´ëŠ” ìµœì¢… ì‚¬ìš©ìì™€ ë” ê´€ë ¨ì´ ìˆìœ¼ë©°, ì´ë“¤ì˜ ëª¨ë¸ì„ ìˆ˜ì •í•˜ê³  ìƒí˜¸ì‘ìš©í•˜ëŠ” ëŠ¥ë ¥ì´ ëª©í‘œë‹¬ì„±ì˜ ì„±ê³µì„ ë³´ì¥í•œë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Privacy awareness: Almost forgotten in the reviewed literature, one of the byproducts enabled by explainability in ML models is its ability to assess privacy. ML models may have complex representations of their learned patterns. Not being able to understand what has been captured by the model [4] and stored in its internal representation may entail a privacy breach. Contrarily, the ability to explain the inner relations of a trained model by non-authorized third parties may also compromise the differential privacy of the data origin. Due to its criticality in sectors where XAI is foreseen to play a crucial role, confidentiality and privacy issues will be covered further in Sections 5.4 and 6.3, respectively.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span><strong>í”„ë¼ì´ë²„ì‹œ ì¸ì‹(Privacy awareness):</strong> ëŒ€ë¶€ë¶„ì˜ ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ë¶€ì‚°ë¬¼ ì¤‘ì— í•˜ë‚˜ëŠ” í”„ë¼ì´ë²„ì‹œë¥¼ í‰ê°€í•˜ëŠ” ëŠ¥ë ¥ì´ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ í•™ìŠµí•œ íŒ¨í„´ì€ ë³µì¡í•œ í‘œí˜„(representations)<span style="color:#aaa">([ì£¼] ì—¬ê¸°ì„œ "í‘œí˜„"ì´ë€ ë°ì´í„° í˜¹ì€ íŒ¨í„´ì„ ì••ì¶•í•˜ì—¬ ë‚˜íƒ€ë‚´ëŠ” ì–´ë–¤ ìƒíƒœë‹¤.)</span>ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ëª¨ë¸ ë‚´ë¶€ì— ì–´ë–¤ ê²ƒì´ í¬ì°©ë˜ì–´ ìˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ë‹¤ë©´, ê°œì¸ì •ë³´ì˜ ì¹¨í•´ê°€ ì¼ì–´ë‚  ìˆ˜ ìˆë‹¤. ë°˜ëŒ€ë¡œ, ë¹„ì¸ê°€ ì œ3ìì— ì˜í•´ í›ˆë ¨ëœ ëª¨ë¸ì˜ ë‚´ë¶€ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤ë©´, ê·¸ê²ƒ ë˜í•œ ë‹¤ë¥¸ ì˜ë¯¸ë¡œì¨ ë°ì´í„° ì¶œì²˜ì— ëŒ€í•œ í”„ë¼ì´ë²„ì‹œ ì¹¨í•´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì‚¬ì•ˆì˜ ì¤‘ìš”ì„± ë•Œë¬¸ì—, ì´ ê¸°ë°€ì„±(confidentiality)ì™€ ê°œì¸ì •ë³´ ë¬¸ì œëŠ” section 5.4ì™€ 6.3ì—ì„œ ë” ë‹¤ë£° ì˜ˆì •ì´ë‹¤.</li>
</ul>
<table>
<thead>
<tr>
<th>XAI Goal</th>
<th>Main target audience (Fig.Â 2)</th>
<th>References</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trustworthiness</td>
<td>Domain experts, users of the model affected by decisions</td>
<td>[5], [10], [24], [32], [33], [34], [35], [36], [37]</td>
</tr>
<tr>
<td>Causality</td>
<td>Domain experts, managers and executive board members, regulatory entities/agencies</td>
<td>[35], [38], [39], [40], [41], [42], [43]</td>
</tr>
<tr>
<td>Transferability</td>
<td>Domain experts, data scientists</td>
<td>[5], [21], [26], [30], [32], [37], [38], [39], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85]</td>
</tr>
<tr>
<td>Informativeness</td>
<td>All</td>
<td>[5], [21], [25], [26], [30], [32], [34], [35], [37], [38], [41], [44], [45], [46], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [59], [60], [63], [64], [65], [66], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154]</td>
</tr>
<tr>
<td>Confidence</td>
<td>Domain experts, developers, managers, regulatory entities/agencies</td>
<td>[5], [35], [45], [46], [48], [54], [61], [72], [88], [89], [96], [108], [117], [119], [155]</td>
</tr>
<tr>
<td>Fairness</td>
<td>Users affected by model decisions, regulatory entities/agencies</td>
<td>[5], [24], [35], [45], [47], [99], [100], [101], [120], [121], [128], [156], [157], [158]</td>
</tr>
<tr>
<td>Accessibility</td>
<td>Product owners, managers, users affected by model decisions</td>
<td>[21], [26], [30], [32], [37], [50], [53], [55], [62], [67], [68], [69], [70], [71], [74], [75], [76], [86], [93], [94], [103], [105], [107], [108], [111], [112], [113], [114], [115], [124], [129]</td>
</tr>
<tr>
<td>Interactivity</td>
<td>Domain experts, users affected by model decisions</td>
<td>[37], [50], [59], [65], [67], [74], [86], [124]</td>
</tr>
<tr>
<td>Privacy awareness</td>
<td>Users affected by model decisions, regulatory entities/agencies</td>
<td>[89]</td>
</tr>
</tbody>
</table>
<p><code>í‘œ 1</code> ì„¤ëª…ê°€ëŠ¥ì„±ì— ë„ë‹¬í•˜ê¸° ìœ„í•´ ê²€í† ëœ ë¬¸í—Œì—ì„œ ì¶”êµ¬ëœ ëª©í‘œë“¤ê³¼ ê·¸ë“¤ì˜ ì£¼ìš” ëª©í‘œ ì²­ì¤‘ë“¤.</p>
<hr/>
<h2 id="25-how">2.5 How?<a class="headerlink" href="#25-how" title="Permanent link">Â¶</a></h2>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>The literature makes a clear distinction among models that are interpretable by design, and those that can be explained by means of external XAI techniques. This duality could also be regarded as the difference between interpretable models and model interpretability techniques; a more widely accepted classification is that of transparent models and post-hoc explainability. This same duality also appears in the paper presented in [17] in which the distinction its authors make refers to the methods to solve the transparent box design problem against the problem of explaining the black-box problem. This work, further extends the distinction made among transparent models including the different levels of transparency considered.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ë³¸ ë…¼ë¬¸ì€ í•´ì„ ê°€ëŠ¥í•œ ëª¨ë¸ì„ <strong>í•´ì„ ê°€ëŠ¥í•œ ëª¨ë¸</strong>(êµ¬ì¡°ì ìœ¼ë¡œ ì„¤ëª… ê°€ëŠ¥í•œ ë¶€ë¶„)ê³¼ <strong>í•´ì„ ê°€ëŠ¥í•œ ê¸°ë²•</strong>(ì™¸ë¶€ XAI ê¸°ë²•ì— ì˜í•´ ì„¤ëª…ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„)ìœ¼ë¡œ ë‚˜ëˆˆë‹¤. í˜„ì¬ ë³´ë‹¤ ë„ë¦¬ ë°›ì•„ë“¤ì—¬ì§€ê³  ìˆëŠ” ë¶„ë¥˜ë²• ìš©ì–´ëŠ” íˆ¬ëª…í•œ ëª¨ë¸(transparent models)ê³¼ ì‚¬í›„ ì„¤ëª…ê°€ëŠ¥ì„±(post-hoc explainability)ì´ë‹¤. [17]ì—ì„œë„ ì–¸ê¸‰í•˜ëŠ”ë°, ë¸”ë™ë°•ìŠ¤ ë¬¸ì œë¥¼ ì„¤ëª…í•˜ë ¤ë©´ íˆ¬ëª…í•œ ëª¨ë¸ì˜ ë””ìì¸ ë¬¸ì œë¥¼ ì°¸ê³ í•´ì•¼í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” íˆ¬ëª…í•œ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë‹¨ê³„ë¡œ ì„¸ë¶„í™”í•´ì„œ ê·¸ ì°¨ì´ë¥¼ ì•Œì•„ ë³¼ ê²ƒì´ë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Within transparency, three levels are contemplated: algorithmic transparency, decomposability and simulatability Among post-hoc techniques we may distinguish among text explanations, visualizations, local explanations, explanations by example, explanations by simplification and feature relevance. In this context, there is a broader distinction proposed by [24] discerning between 1) opaque systems, where the mappings from input to output are invisible to the user; 2) interpretable systems, in which users can mathematically analyze the mappings; and 3) comprehensible systems, in which the models should output symbols or rules along with their specific output to aid in the understanding process of the rationale behind the mappings being made. This last classification criterion could be considered included within the one proposed earlier, hence this paper will attempt at following the more specific one.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> íˆ¬ëª…ì„±(transparency)ì€ ì•Œê³ ë¦¬ì¦˜ íˆ¬ëª…ì„±(algorithmic transparency), ë¶„í•´ê°€ëŠ¥ì„±(decomposability) ê·¸ë¦¬ê³  ì‹œë®¬ë ˆì´ì…˜ì„±(simulatability) <span style="color:#aaa">([ì£¼] ì‹œìŠ¤í…œ í˜¹ì€ í”„ë¡œì„¸ìŠ¤ì˜ ì‹œë®¬ë ˆì´ì…˜ ëŠ¥ë ¥, the capacity of a system or process to be simulated, ì•„ì§ ì–´ë–¤ ëŠë‚Œì´ ì•ˆì˜¨ë‹¤.)</span>ìˆœìœ¼ë¡œ 3 ê°€ì§€ ë‹¨ê³„ë¥¼ ê³ ë ¤í•´ì•¼í•œë‹¤. ì‚¬í›„ ë¶„ì„(post-hoc) ê¸°ë²•ì€ í…ìŠ¤íŠ¸ ì„¤ëª…, ì‹œê°í™”, êµ­ì§€ì  ì„¤ëª…, ì˜ˆì‹œ ì„¤ëª…, ë‹¨ìˆœí™” ë° í”¼ì²˜ì¤‘ìš”ë„ë“± ë°©ë²•ê³¼ êµ¬ë³„ë˜ì–´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ë§¥ë½ì—ì„œ ë” ê´‘ë²”ìœ„í•œ êµ¬ë³„ë²•ì´ <a href="https://scholar.google.com/scholar?q=D.%20Doran,%20S.%20Schulz,%20T.R.%20Besold,%20What%20does%20explainable%20AI%20really%20mean%20a%20new%20conceptualization%20of%20perspectives,%202017.">[24]</a>ì—ì„œ ì œì‹œ ë˜ì—ˆë‹¤. 1) ì…ë ¥ì—ì„œ ì¶œë ¥ê¹Œì§€ì˜ ë§¤í•‘ì´ ì‚¬ìš©ìì—ê²Œ ë³´ì´ì§€ ì•ŠëŠ” ë¶ˆíˆ¬ëª…í•œ(opaque) ì‹œìŠ¤í…œ 2) ì‚¬ìš©ìê°€ ìˆ˜í•™ì ìœ¼ë¡œ ë§¤í•‘ì„ ë¶„ì„í•  ìˆ˜ ìˆëŠ” í•´ì„ ê°€ëŠ¥í•œ(interpretable) ì‹œìŠ¤í…œ 3) ëª¨ë¸ì´ ê²°ì •í•œ ë§¤í•‘ì˜ ì´ìœ ë¥¼ ì‚¬ëŒì´ ì´í•´ ê°€ëŠ¥í•˜ê²Œ ì¶œë ¥í•˜ëŠ” í¬ê´„ì  ì´í•´ ê°€ëŠ¥í•œ(comprehensible) ì‹œìŠ¤í…œ</p>
<h3 id="251-levels-of-transparency-in-machine-learning-models">2.5.1. Levels of transparency in machine learning models<a class="headerlink" href="#251-levels-of-transparency-in-machine-learning-models" title="Permanent link">Â¶</a></h3>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Transparent models convey some degree of interpretability by themselves. Models belonging to this category can be also approached in terms of the domain in which they are interpretable, namely, algorithmic transparency, decomposability and simulatability. As we elaborate next in connection to Fig. 3, each of these classes contains its predecessors, e.g. a simulatable model is at the same time a model that is decomposable and algorithmically transparent:</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> íˆ¬ëª…í•œ ëª¨ë¸ì€ ëª¨ë¸ ê·¸ ìì²´ë¡œ ì–´ëŠ ì •ë„ì˜ í•´ì„ê°€ëŠ¥ì„±ì„ ê°€ì§€ê³  ìˆë‹¤. ìœ„ì— ì–¸ê¸‰í•œ ëŒ€ë¡œ ì•Œê³ ë¦¬ì¦˜ íˆ¬ëª…ì„±, ë¶„í•´ê°€ëŠ¥ì„± ê·¸ë¦¬ê³  ì‹œë®¬ë ˆì´ì…˜ì„± ìˆœìœ¼ë¡œ ì ‘ê·¼ í•  ìˆ˜ ìˆë‹¤. Fig 3ì—ì„œ ì„¤ëª…í•˜ê² ì§€ë§Œ, ê° ë¶„ë¥˜ëŠ” ì´ì „ ë‹¨ê³„ì˜ êµ¬ì„±ì„ í¬í•¨í•œë‹¤.. ì˜ˆë¥¼ ë“¤ì–´, ì‹œë®¬ë ˆì´ì…˜ ê°€ëŠ¥í•œ ëª¨ë¸ì€ ë¶„í•´ ê°€ëŠ¥í•˜ë©°, íˆ¬ëª…í•œ ì•Œê³ ë¦¬ì¦˜ì„ í¬í•¨í•œë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Simulatability denotes the ability of a model of being simulated or thought about strictly by a human, hence complexity takes a dominant place in this class. This being said, simple but extensive (i.e., with too large amount of rules) rule based systems fall out of this characteristic, whereas a single perceptron neural network falls within. This aspect aligns with the claim that sparse linear models are more interpretable than dense ones [170], and that an interpretable model is one that can be easily presented to a human by means of text and visualizations [32]. Again, endowing a decomposable model with simulatability requires that the model has to be self-contained enough for a human to think and reason about it as a whole.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì‹œë®¬ë ˆì´ì…˜ì„±(Simulatability):</strong> ì‚¬ëŒì— ì˜í•´ ì—„ê²©í•˜ê²Œ ì‹œë®¬ë ˆì´ì…˜ëœ ëª¨ë¸ì˜ ëŠ¥ë ¥ì´ë‹¤. ë”°ë¼ì„œ ë³µì¡ì„±ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤. ë‹¨ìˆœí•˜ì§€ë§Œ ê´‘ë²”ìœ„í•œ ê·œì¹™ê¸°ë°˜ ì‹œìŠ¤í…œ ë³´ë‹¤ëŠ” í¼ì…‰íŠ¸ë¡  ì‹ ê²½ë§ì´ ê¸°ì¤€ì— ë” ë¶€í•©í•œë‹¤. ì´ëŸ¬í•œ ê´€ì ì—ì„œ sparseí•œ ì„ í˜•ëª¨ë¸ì´ dense í•œ ê²ƒë³´ë‹¤ ë” í•´ì„ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë©° <a href="https://scholar.google.com/scholar_lookup?title=Regression%20shrinkage%20and%20selection%20via%20the%20lasso&amp;publication_year=1996&amp;author=R.%20Tibshirani">[170]</a>, ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ì™€ ì‹œê°í™”ë¥¼ í†µí•´ ì¸ê°„ì´ ë” ì‰½ê²Œ ì„¤ëª… í•  ìˆ˜ ìˆë‹¤ë¼ëŠ” ì£¼ì¥<a href="https://scholar.google.com/scholar?q=Why%20should%20I%20trust%20you:%20Explaining%20the%20predictions%20of%20any%20classifier">[32]</a>ê³¼ ì¼ì¹˜í•œë‹¤. ë‹¤ì‹œ ë§í•˜ì§€ë§Œ ì‹œë®¬ë ˆì´ì…˜ì„±ì„ ê°€ì§€ëŠ” ë¶„í•´ ê°€ëŠ¥í•œ ëª¨ë¸ì€ ì¸ê°„ì—ê²Œ ìƒê°ê³¼ ì´ìœ ë¥¼ í˜¼ì ì„¤ëª…í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Decomposability stands for the ability to explain each of the parts of a model (input, parameter and calculation). It can be considered as intelligibility as stated in [171]. This characteristic might empower the ability to understand, interpret or explain the behavior of a model. However, as occurs with algorithmic transparency, not every model can fulfill this property. Decomposability requires every input to be readily interpretable (e.g. cumbersome features will not fit the premise). The added constraint for an algorithmically transparent model to become decomposable is that every part of the model must be understandable by a human without the need for additional tools.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ë¶„í•´ê°€ëŠ¥ì„±(Decomposability):</strong> ëª¨ë¸ì˜ ê° ë¶€ë¶„ì„ ì„¤ëª…í•˜ëŠ” ëŠ¥ë ¥ì¸ë°, <a href="https://scholar.google.com/scholar_lookup?title=Intelligible%20models%20for%20classification%20and%20regression&amp;publication_year=2012&amp;author=Y.%20Lou&amp;author=R.%20Caruana&amp;author=J.%20Gehrke">[171]</a>ì—ì„œ ì–¸ê¸‰ëœ ëª…ë£Œì„±(intelligibility)ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ì´ íŠ¹ì„±ì€ ëª¨ë¸ì˜ í–‰ë™ì„ ì´í•´í•˜ê³ , í•´ì„í•˜ê±°ë‚˜ ì„¤ëª…í•˜ëŠ” ëŠ¥ë ¥ì„ ê°•ì¡°í•œë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë“  ëª¨ë¸ì´ ì´ íŠ¹ì„±ì´ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ë¶„í•´ê°€ëŠ¥ì„±ì€ ëª¨ë“  ì…ë ¥ì˜ ì‰½ê²Œ í•´ì„í•  ìˆ˜ ìˆì–´ì•¼ í•˜ëŠ”ë°, ì•Œê³ ë¦¬ì¦˜ì  íˆ¬ëª…ì„±ê¹Œì§€ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œëŠ” ì¸ê°„ì´ ëª¨ë¸ì˜ ëª¨ë“  ë¶€ë¶„ì—ì„œ ì¶”ê°€í•´ì„ ì—†ì´ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Algorithmic transparency can be seen in different ways. It deals with the ability of the user to understand the process followed by the model to produce any given output from its input data. Put it differently, a linear model is deemed transparent because its error surface can be understood and reasoned about, allowing the user to understand how the model will act in every situation it may face [163]. Contrarily, it is not possible to understand it in deep architectures as the loss landscape might be opaque [172], [173] since it cannot be fully observed and the solution has to be approximated through heuristic optimization (e.g. through stochastic gradient descent). The main constraint for algorithmically transparent models is that the model has to be fully explorable by means of mathematical analysis and methods.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì•Œê³ ë¦¬ì¦˜ì  íˆ¬ëª…ì„±(Algorithmic transparency):</strong> ì‚¬ìš©ìê°€ ëª¨ë¸ì— ëŒ€í•œ í”„ë¡œì„¸ìŠ¤(ëª¨ë¸ì´ ë„ì¶œí•œ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ì¶œë ¥) ì´í•´ ëŠ¥ë ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì„ í˜•ëª¨ë¸ì€ ì‚¬ìš©ìê°€ ëª¨ë¸ì´ ì–´ë–»ê²Œ í–‰ë™í•  ì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆê³ , error surfaceë¥¼ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— íˆ¬ëª…í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤<a href="https://scholar.google.com/scholar_lookup?title=An%20introduction%20to%20statistical%20learning&amp;publication_year=2013&amp;author=G.%20James&amp;author=D.%20Witten&amp;author=T.%20Hastie&amp;author=R.%20Tibshirani">[163]</a>. ë°˜ëŒ€ë¡œ, ê¹Šì€ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” ëª¨ë¸ì€ ì†ì‹¤ê°’ì´ ë¶ˆíˆ¬ëª…í•˜ì—¬(<a href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20without%20poor%20local%20minima&amp;publication_year=2016&amp;author=K.%20Kawaguchi">[172]</a>, <a href="https://scholar.google.com/scholar_lookup?title=Algorithmic%20transparency%20via%20quantitative%20input%20influence%3A%20Theory%20and%20experiments%20with%20learning%20systems&amp;publication_year=2016&amp;author=A.%20Datta&amp;author=S.%20Sen&amp;author=Y.%20Zick">[173]</a>), íŠ¹ì • íœ´ë¦¬ìŠ¤í‹±í•œ ìµœì í™”(ì˜ˆ, stochastic gradient descent)ë¥¼ í†µí•´ì„œ ê·¼ì‚¬ì¹˜ë¥¼ êµ¬í•´ì•¼í•œë‹¤. <span style="color:#aaa">([ì£¼] ìˆ˜í•™ì ìœ¼ë¡œ ëª…ì¾Œí•œ solutionì´ ì•ˆë³´ì´ë©´ ë¶ˆíˆ¬ëª…í•˜ë‹¤ê³  ë³´ëŠ” ê²ƒ ê°™ë‹¤. íŠ¹íˆ ë¹„ì„ í˜•í•¨ìˆ˜ì˜ ì†ì‹¤ê°’)</span> ì•Œê³ ë¦¬ì¦˜ì  íˆ¬ëª…ì„±ì˜ ì£¼ëœ ì œì•½ ì¡°ê±´ì€ ëª¨ë¸ì´ ìˆ˜í•™ì  ë¶„ì„ê³¼ ë°©ë²•ì„ í†µí•´ ì™„ì „ì´ íƒêµ¬ ê°€ëŠ¥í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.</li>
</ul>
<p>{% include image.html id="1iZXqf9hnwcu-N9If2_R4WWF5RinPIhPX" desc="Fig 3. ë‹¤ì–‘í•œ ë‹¨ê³„ì˜ íˆ¬ëª…ì„±" width="100%" height="auto" %}</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ë‹¤ì–‘í•œ ë‹¨ê³„ì˜ íˆ¬ëª…ì„±ì„ ì„¤ëª…í•´ë‘ì—ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ <span class="arithmatex">\(M_{\varphi}\)</span>, ê·¸ì— í•´ë‹¹í•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” <span class="arithmatex">\(\varphi\)</span> ë¡œ í‘œê¸°í–ˆë‹¤. (a) ì‹œë®¬ë ˆì´ì…˜ì„± (b) ë¶„í•´ê°€ëŠ¥ì„± (c) ì•Œê³ ë¦¬ì¦˜ì  íˆ¬ëª…ì„±. ê° ì˜ˆì œëŠ” ì¼ë°˜ì ì¸ ì†ì‹¤ê°’ ê°€ì •ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ì„ ì—ì„œ, ëª¨ë¸ì´ ì„¤ëª… ëŒ€ìƒì— ë”°ë¼ì„œ ì–¼ë§ˆë‚˜ ë‹¬ë¼ì§€ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤.</p>
<p><span style="color:#aaa"> [ì£¼] ì´ íŒŒíŠ¸ê°€ ì œì¼ ì´í•´í•˜ê¸° ì–´ë ¤ì› ëŠ”ë°, (a) ê°™ì€ ê²½ìš° ì…ë ¥ ë°ì´í„°ëŠ” ì˜ ëª°ë¼ë„, ë‚´ë¶€ì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì¬í˜„ ê°€ëŠ¥í•˜ë©´ ë§Œì¡±í•˜ëŠ” ê²ƒ ê°™ê³ , (b) ì˜ ê²½ìš° ì‚¬ëŒì´ ê° ì…ë ¥ í”¼ì²˜ê¹Œì§€ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. (c)ì˜ ê²½ìš° ì‚¬ëŒì´ ì „ì²´ ë°ì´í„°ì˜ íŠ¹ì„±ê¹Œì§€ íŒŒì•…í•˜ê³  ì´ì— ëŒ€í•œ ê·œì¹™ì„ ì´í•´í•´ì•¼ í•œë‹¤ë¼ê³  ì´í•´í–ˆë‹¤. </span></p>
<h3 id="252-post-hoc-explainability-techniques-for-machine-learning-models">2.5.2. Post-hoc explainability techniques for machine learning models<a class="headerlink" href="#252-post-hoc-explainability-techniques-for-machine-learning-models" title="Permanent link">Â¶</a></h3>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Post-hoc explainability targets models that are not readily interpretable by design by resorting to diverse means to enhance their interpretability, such as text explanations, visual explanations, local explanations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves.
<br/>
Further along this river, actual techniques, or better put, actual group of techniques are specified to ease the future work of any researcher that intends to look up for an specific technique that suits its knowledge. Not ending there, the classification also includes the type of data in which the techniques has been applied. Note that many techniques might be suitable for many different types of data, although the categorization only considers the type used by the authors that proposed such technique. Overall, post-hoc explainability techniques are divided first by the intention of the author (explanation technique e.g. Explanation by simplification), then, by the method utilized (actual technique e.g. sensitivity analysis) and finally by the type of data in which it was applied (e.g. images).</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> <strong>ì‚¬í›„ ì„¤ëª…ê°€ëŠ¥ì„±(post-hoc explainability)</strong>ì€ ì‰½ê²Œ í•´ì„í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸ì„ ìœ„í•´ ê³ ì•ˆëœ ë°©ë²•ì´ë‹¤. í…ìŠ¤íŠ¸, ì‹œê°í™”, ë¶€ë¶„, ì˜ˆì‹œ, ë‹¨ìˆœí™” ê·¸ë¦¬ê³  í”¼ì²˜ ì—°ê´€ ì„¤ëª… ë“± ë°©ë²•ë“¤ì´ ìˆë‹¤. </p>
<p>ê° ë°©ë²•ë“¤ì— ëŒ€í•´ íŠ¹ì • ê¸°ìˆ  ë¿ë§Œ ì•„ë‹ˆë¼ ì ìš©ë˜ëŠ” ë°ì´í„° ìœ í˜•ê¹Œì§€ ì†Œê°œí•œë‹¤. ì—¬ê¸°ì„œëŠ” ì¸ìš©í•œ ì €ìë“¤ì´ ì ìš©í•œ ë°ì´í„°ì— ì˜ê±°í•´ ë¶„ë¥˜ë¥¼ í–ˆì§€ë§Œ, ì´ ì¤‘ì— ì–´ë–¤ ë°©ë²•ë“¤ì€ ë‹¤ë¥¸ ë¶„ì•¼(ë°ì´í„°)ì—ë„ ì¶©ë¶„íˆ ì ìš©í•  ìˆ˜ ìˆë‹¤.</p>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Text explanations deal with the problem of bringing explainability for a model by means of learning to generate text explanations that help explaining the results from the model [169]. Text explanations also include every method generating symbols that represent the functioning of the model. These symbols may portrait the rationale of the algorithm by means of a semantic mapping from model to symbols.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>í…ìŠ¤íŠ¸ ì„¤ëª…(Text explanation)</strong>ì€ ëª¨ë¸ì´ ìì‹ ì˜ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ëŠ” "í…ìŠ¤íŠ¸ ìƒì„± í•™ìŠµ" ë¬¸ì œë‹¤. ì´ ë°©ë²•ì€ ëª¨ë¸ì˜ ê¸°ëŠ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹¬ë³¼ì„ ìƒì„±<span style="color:#aaa">([ì£¼] ì¸ê°„ì˜ ì–¸ì–´ê°€ ë  ìˆ˜ë„ ìˆê³ , ìˆ˜ì‹ì¼ ìˆ˜ë„)</span>í•˜ëŠ” ë°©ì‹ì¸ë°, ì´ ì‹¬ë³¼ë“¤ì€ ì˜ë¯¸ë¡ ì (semantic)ìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ ë°©ì‹ì„ ë§¤í•‘í•œë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Visual explanation techniques for post-hoc explainability aim at visualizing the model's behavior. Many of the visualization methods existing in the literature come along with dimensionality reduction techniques that allow for a human interpretable simple visualization. Visualizations may be coupled with other techniques to improve their understanding, and are considered as the most suitable way to introduce complex interactions within the variables involved in the model to users not acquainted to ML modeling.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì‹œê°ì  ì„¤ëª…(Visual explanation)</strong>ì˜ ëª©í‘œëŠ” ëª¨ë¸ì˜ í–‰ë™ì„ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…í•œëŠ” ê²ƒì´ë‹¤. ë§ì€ ë°©ë²•ë“¤ ì¤‘ì—ì„œ ëŒ€ë¶€ë¶„ ì¸ê°„ì´ í•´ì„í•˜ê¸° ì‰½ê²Œ ì°¨ì›ê°ì†Œ(dimension reduction) ê¸°ë²•ê³¼ í•¨ê»˜ ì‚¬ìš©ëœë‹¤. ì´ ë°©ë²•ì€ ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì„œ ì´í•´ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ì— ìµìˆ™í•˜ì§€ ì•Šì€ ì‚¬ìš©ìì—ê²Œ ëª¨ë¸ê³¼ ê´€ë ¨ëœ ë³€ìˆ˜ì˜ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì„ ì•Œë¦¬ëŠ”ë° ìˆì–´ ê°€ì¥ ì í•©í•œ ë„êµ¬ë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Local explanations tackle explainability by segmenting the solution space and giving explanations to less complex solution subspaces that are relevant for the whole model. These explanations can be formed by means of techniques with the differentiating property that these only explain part of the whole systemâ€™s functioning.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ë¶€ë¶„ ì„¤ëª…(Local explanation)</strong>ì€ ì „ì²´ëª¨ë¸ì˜ ì¼ë¶€ë¶„ì„ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ”ë° ì§‘ì¤‘í•œë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Explanations by example consider the extraction of data examples that relate to the result generated by a certain model, enabling to get a better understanding of the model itself. Similarly to how humans behave when attempting to explain a given process, explanations by example are mainly centered in extracting representative examples that grasp the inner relationships and correlations found by the model being analyzed.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ì˜ˆì‹œ ì„¤ëª…(Example explanation)</strong>ì€ ë°ì´í„° ìƒ˜í”Œì—ì„œ ê²°ê³¼ì™€ ê´€ë ¨ëœ ì˜ˆì œë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì£¼ë¡œ ëª¨ë¸ ê²°ê³¼ì˜ ë‚´ì  ê´€ê³„ í˜¹ì€ ìƒê´€ê´€ê³„ì— ê´€ë ¨ëœ ì˜ˆì œë¥¼ ì¶”ì¶œí•˜ê²Œ ëœë‹¤. ì´ ë°©ë²•ì€ ì‚¬ëŒì´ ì–´ë–¤ í”„ë¡œì„¸ìŠ¤ë¥¼ ì„¤ëª…í•˜ë ¤ê³  í•  ë•Œë‘ ë¹„ìŠ·í•˜ê²Œ í–‰ë™í•œë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Explanations by simplification collectively denote those techniques in which a whole new system is rebuilt based on the trained model to be explained. This new, simplified model usually attempts at optimizing its resemblance to its antecedent functioning, while reducing its complexity, and keeping a similar performance score. An interesting byproduct of this family of post-hoc techniques is that the simplified model is, in general, easier to be implemented due to its reduced complexity with respect to the model it represents.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>ë‹¨ìˆœí™” ì„¤ëª…(Simplification explanation)</strong>ì€ í›ˆë ¨ëœ ëª¨ë¸ì— ê¸°ì´ˆí•˜ì—¬ ì„¤ëª…ì„ ìœ„í•œ ìƒˆë¡œìš´ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ì´ ìƒˆë¡œìš´ ì‹œìŠ¤í…œì€ ë³µì¡ì„±ì„ ìµœëŒ€í•œ ì¤„ì´ê³ , ìœ ì‚¬í•œ ê¸°ëŠ¥ ë° ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ì— ë³µì¡í•œ ëª¨ë¸ì— ë°˜í•´, ë” ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ì¥ì ì´ ìˆë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>Finally, feature relevance explanation methods for post-hoc explainability clarify the inner functioning of a model by computing a relevance score for its managed variables. These scores quantify the affection (sensitivity) a feature has upon the output of the model. A comparison of the scores among different variables unveils the importance granted by the model to each of such variables when producing its output. Feature relevance methods can be thought to be an indirect method to explain a model.</p>
<p>[/expand]</p>
<ul>
<li><span style="color:#e25252">ìš”ì•½:</span> <strong>í”¼ì²˜ ì—°ê´€ ì„¤ëª…(Feature relevance explanation)</strong>ì€ ëª¨ë¸ì˜ ë³€ìˆ˜ì™€ ê´€ë ¨ëœ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ëœë‹¤. ì´ ì ìˆ˜ëŠ” í”¼ì²˜ì˜ ì˜í–¥ë ¥(affection í˜¹ì€ sensitivity)ë¥¼ ê³„ëŸ‰í™”í•œë‹¤. ì¶œë ¥ì— ëŒ€í•œ ì ìˆ˜ê°€ ê°ê¸° ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ê° í”¼ì²˜ì˜ ì¤‘ìš”ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ëª¨ë¸ì„ ì„¤ëª…í•˜ëŠ” ê°„ì ‘ì ì¸ ë°©ë²•ì´ë‹¤.</li>
</ul>
<p>[expand]summary:ì˜ì–´ì›ë¬¸ ğŸ‘ˆ </p>
<p>The above classification (portrayed graphically in Fig. 4) will be used when reviewing specific/agnostic XAI techniques for ML models in the following sections (Table 2). For each ML model, a distinction of the propositions to each of these categories is presented in order to pose an overall image of the fieldâ€™s trends.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">ìš”ì•½:</span> ìœ„ ë¶„ë¥˜ë¥¼ Fig 4ë¡œ í‘œí˜„í–ˆìœ¼ë©°, í‘œ 2ì—ì„œë„ ì •ë¦¬í–ˆë‹¤. </p>
<p>{% include image.html id="1CDEwm6jcM8SKvE3YT21n9HlTTPq2Orol" desc="Fig 4. ì‚¬í›„ ì„¤ëª…ê°€ëŠ¥ì„± ë°©ë²•ì— ëŒ€í•œ ì»¨ì…‰ ë‹¤ì´ì–´ê·¸ë¨" width="100%" height="auto" %}</p>
<hr/>
<p>| | Transparent ML Models | Transparent ML Models |Transparent ML Models | Post-hoc analysis | 
| Model | Simulatability | Decomposability | Algorithmic Transparency | Post-hoc analysis | 
| --- | --- | --- | --- | --- |
| Linear/Logistic Regression | ì˜ˆì¸¡ ë³€ìˆ˜ëŠ” ì‚¬ëŒì´ íŒë…í•  ìˆ˜ ìˆìœ¼ë©° ì˜ˆì¸¡ ë³€ìˆ˜ ê°„ì˜ ìƒí˜¸ ì‘ìš©ì€ ìµœì†Œí•œìœ¼ë¡œ ìœ ì§€ë¨ | ë³€ìˆ˜ëŠ” ì—¬ì „íˆ ì½ì„ ìˆ˜ ìˆì§€ë§Œ, ë³€ìˆ˜ì™€ ê´€ë ¨ëœ ìƒí˜¸ì‘ìš©ê³¼ ì˜ˆì¸¡ ë³€ìˆ˜ì˜ ìˆ˜ëŠ” ë¶„í•´ë¥¼ ê°•ìš”í•˜ëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì¦ê°€í–ˆë‹¤. | ë³€ìˆ˜ì™€ êµí˜¸ì‘ìš©ì´ ë„ˆë¬´ ë³µì¡í•˜ì—¬ ìˆ˜í•™ì  ë„êµ¬ê°€ ì—†ìœ¼ë©´ ë¶„ì„í•  ìˆ˜ ì—†ìŒ | í•„ìš” ì—†ìŒ |
| Decision Trees | ì‚¬ëŒì€ ì–´ë–¤ ìˆ˜í•™ì  ë°°ê²½ë„ ìš”êµ¬í•˜ì§€ ì•Šê³  ìŠ¤ìŠ¤ë¡œ ì˜ì‚¬ê²°ì • ë‚˜ë¬´ì˜ ì˜ˆì¸¡ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  ì–»ì„ ìˆ˜ ìˆë‹¤. | ë°ì´í„°ê°€ ì–´ë–»ë“  ëª¨ë¸ì€ ê·œì¹™ì„ ì „í˜€ ë³€ê²½í•˜ì§€ ì•Šê³  ê°€ë…ì„±ì„ ìœ ì§€í•¨ | ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•œ ì§€ì‹ì„ ì‚¬ëŒì´ ì´í•´ í•  ë§Œí•œ ê·œì¹™ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìŒ | í•„ìš” ì—†ìŒ |
| K-Nearest Neighbors | ì¸ê°„ì˜ ë‚˜ì´ë¸Œí•œ ëŠ¥ë ¥ì— ë”°ë¼ì„œ ëª¨ë¸ ë³µì¡ë„ê°€ ê²°ì •ëœë‹¤(ë³€ìˆ˜ì˜ ê°œìˆ˜, ë³€ìˆ˜ê°„ ìœ ì‚¬ì„±ì˜ ì´í•´ë„) | ë³€ìˆ˜ì˜ ì–‘ì´ ë„ˆë¬´ ë§ê±°ë‚˜ ìœ ì‚¬ì„± ì¸¡ë„ê°€ ë„ˆë¬´ ë³µì¡í•˜ì—¬ ëª¨í˜•ì„ ì™„ì „íˆ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ì—†ì§€ë§Œ ìœ ì‚¬ì„± ì¸¡ë„ì™€ ë³€ìˆ˜ ì§‘í•©ì„ ë³„ë„ë¡œ ë¶„í•´í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ìˆë‹¤.   | ìœ ì‚¬ì„± ì¸¡ì •ì€ ë¶„í•´ë  ìˆ˜ ì—†ìœ¼ë©°/ë˜ëŠ” ë³€ìˆ˜ ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„ì„œ ì‚¬ìš©ìëŠ” ëª¨ë¸ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ìˆ˜í•™ì  ë° í†µê³„ì  ë„êµ¬ì— ì˜ì¡´í•´ì•¼í•¨ | í•„ìš” ì—†ìŒ |
| Rule Based Learners | ê·œì¹™ì— í¬í•¨ëœ ë³€ìˆ˜ëŠ” ì½ì„ ìˆ˜ ìˆìœ¼ë©° ê·œì¹™ ì§‘í•©ì˜ í¬ê¸°ëŠ” ì™¸ë¶€ ë„ì›€ ì—†ì´ ì‚¬ìš©ìê°€ ê´€ë¦¬í•  ìˆ˜ ìˆìŒ | ê·œì¹™ ì§‘í•©ì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì„œ ì‘ì€ ê·œì¹™ ì²­í¬ë¡œ ë¶„í•´í•˜ì§€ ì•Šê³  ë¶„ì„í•  ìˆ˜ ì—†ìŒ | ê·œì¹™ì´ ë„ˆë¬´ ë³µì¡í•´ì ¸ì„œ(ê·¸ë¦¬ê³  ê·œì¹™ ì§‘í•© í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì ¸ì„œ) ëª¨ë¸ ë™ì‘ì„ ê²€ì‚¬í•˜ëŠ” ë° ìˆ˜í•™ì  ë„êµ¬ê°€ í•„ìš”í•¨ | í•„ìš” ì—†ìŒ |
| General Additive Models | ëª¨ë¸ì— í¬í•¨ëœ ì›í™œí•œ ê¸°ëŠ¥ì— ë”°ë¼ ë³€ìˆ˜ ë° ë³€ìˆ˜ ê°„ì˜ ìƒí˜¸ ì‘ìš©ì€ ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë²”ìœ„ì— ì œí•œë˜ì–´ì•¼ í•¨ | êµí˜¸ì‘ìš©ì´ ë„ˆë¬´ ë³µì¡í•´ì ¸ì„œ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ëª¨ë¸ì„ ë¶„ì„í•˜ë ¤ë©´ ë¶„í•´ ê¸°ë²•ì´ í•„ìš”í•¨ | ë³µì¡ì„± ë•Œë¬¸ì—, ë³€ìˆ˜ì™€ ìƒí˜¸ì‘ìš©ì€ ìˆ˜í•™ì , í†µê³„ì  ë„êµ¬ë¥¼ ì ìš©í•˜ì§€ ì•Šê³ ëŠ” ë¶„ì„í•  ìˆ˜ ì—†ìŒ | í•„ìš” ì—†ìŒ |
| Bayesian Models | ë³€ìˆ˜ ë° ë³€ìˆ˜ ìì²´ì˜ í†µê³„ì  ê´€ê³„ëŠ” ì²­ì¤‘ë“¤ì´ ì§ì ‘ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•¨ | í†µê³„ì  ê´€ê³„ê°€ ë„ˆë¬´ ë§ì´ í¬í•¨ë˜ì–´ ìˆì–´ì„œ, ë¶„í•´ë¥¼ í•´ì•¼ ë¶„ì„ì´ ìš©ì´í•¨ | í†µê³„ì  ê´€ê³„ëŠ” ì´ë¯¸ ë¶„í•´ë˜ì–´ë„ í•´ì„í•  ìˆ˜ ì—†ëŠ” ìˆ˜ì¤€ì´ê³  ë³µì¡í•œ ìˆ˜í•™ì  ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ ëª¨ë¸ì„ ë¶„ì„í•  ìˆ˜ ìˆìŒ | í•„ìš” ì—†ìŒ |
| Tree Ensembles | âœ— | âœ— | âœ— | í•„ìš”: ëª¨ë¸ ë‹¨ìˆœí™” í˜¹ì€ í”¼ì²˜ ì—°ê´€ ì„¤ëª… |
| Support Vector Machines | âœ— | âœ— | âœ— | í•„ìš”: ëª¨ë¸ ë‹¨ìˆœí™” í˜¹ì€ ë¶€ë¶„ ì„¤ëª… |
| Multiâ€“layer Neural Network | âœ— | âœ— | âœ— | í•„ìš”: ëª¨ë¸ ë‹¨ìˆœí™”, í”¼ì²˜ ì—°ê´€ ì„¤ëª… í˜¹ì€ ì‹œê°í™” ì„¤ëª… |
| Convolutional Neural Network | âœ— | âœ— | âœ— | í•„ìš”: ëª¨ë¸ ë‹¨ìˆœí™”, í”¼ì²˜ ì—°ê´€ ì„¤ëª… í˜¹ì€ ì‹œê°í™” ì„¤ëª… |
| Recurrent Neural Network | âœ— | âœ— | âœ— | í•„ìš”: í”¼ì²˜ ì—°ê´€ ì„¤ëª… |</p>
<p><code>í‘œ 2</code> ì„¤ëª…ê°€ëŠ¥ì„± ìˆ˜ì¤€ì— ë”°ë¥¸ ML ëª¨ë¸ì˜ ë¶„ë¥˜ì— ëŒ€í•œ ì „ì²´ì ì¸ ê·¸ë¦¼.</p>
<form class="md-feedback" hidden="" name="feedback">
<fieldset>
<legend class="md-feedback__title">
        Was this page helpful?
      </legend>
<div class="md-feedback__inner">
<div class="md-feedback__list">
<button class="md-feedback__icon md-icon" data-md-value="1" title="This page was helpful" type="submit">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81Z"></path></svg>
</button>
<button class="md-feedback__icon md-icon" data-md-value="0" title="This page could be improved" type="submit">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14Z"></path></svg>
</button>
</div>
<div class="md-feedback__note">
<div data-md-value="1" hidden="">
              
              
                
              
              Thanks for your feedback!
            </div>
<div data-md-value="0" hidden="">
</div>
</div>
</div>
</fieldset>
</form>
<!-- Giscus -->
<h2 id="__comments">Comments</h2>
<script async="" crossorigin="anonymous" data-category="General" data-category-id="DIC_kwDOHRhxjc4CQSuI" data-emit-metadata="0" data-input-position="top" data-lang="ko" data-mapping="pathname" data-reactions-enabled="1" data-repo="simonjisu/comments_bot" data-repo-id="R_kgDOHRhxjQ" data-theme="light" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    /* Set palette on initial load */
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate" ? "dark" : "light"
      giscus.setAttribute("data-theme", theme) 
    }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate" ? "dark" : "light"

          /* Instruct Giscus to change theme */
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Back to top
          </button>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs Insiders
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/simonjisu" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.top", "toc.follow", "navigation.prune", "navigation.path", "content.tooltips", "content.code.annotate"], "search": "../../../assets/javascripts/workers/search.f2da59ea.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../../../assets/javascripts/bundle.65061dd4.min.js"></script>
<script src="../../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": false, "draggable": false, "openEffect": "none", "closeEffect": "none", "slideEffect": "slide"});})</script></body>
</html>