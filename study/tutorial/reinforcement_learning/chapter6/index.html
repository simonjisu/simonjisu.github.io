
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="https://simonjisu.github.io/study/tutorial/reinforcement_learning/chapter6/" rel="canonical"/>
<link href="../chapter5/" rel="prev"/>
<link href="../chapter7/" rel="next"/>
<link href="../../../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.2, mkdocs-material-9.0.12+insiders-4.30.2" name="generator"/>
<title>6. Temporal-Difference Learning - Soopace</title>
<link href="../../../../assets/stylesheets/main.b6d2c4d8.min.css" rel="stylesheet"/>
<link href="../../../../assets/stylesheets/palette.2505c338.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-2D0S4P2SJ9"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-2D0S4P2SJ9",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-2D0S4P2SJ9",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<link href="../../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src="../../../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="" data-md-color-primary="black" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#td-prediction">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--lifted" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Soopace" class="md-header__button md-logo" data-md-component="logo" href="../../../.." title="Soopace">
<img alt="logo" src="../../../../img/logo/logo.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Soopace
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              6. Temporal-Difference Learning
            
          </span>
</div>
</div>
</div>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/simonjisu.github.io" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</nav>
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../..">
        
  
    
  
  About

      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../blog/">
          
  
    
  
  Blog

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link md-tabs__link--active" href="../../../">
          
  
    
  
  Study

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../project/">
          
  
    
  
  Project

        </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Soopace" class="md-nav__button md-logo" data-md-component="logo" href="../../../.." title="Soopace">
<img alt="logo" src="../../../../img/logo/logo.png"/>
</a>
    Soopace
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/simonjisu.github.io" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../..">
<span class="md-ellipsis">
    
  
    About
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../blog/">
<span class="md-ellipsis">
    
  
    Blog
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../../">
<span class="md-ellipsis">
    
  
    Study
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            
  
    Study
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../paper/">
<span class="md-ellipsis">
    
  
    Paper
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../">
<span class="md-ellipsis">
    
  
    Tutorial
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_3_3">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_3_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_3">
<span class="md-nav__icon md-icon"></span>
            
  
    Tutorial
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../algorithm/">
<span class="md-ellipsis">
    
  
    Algorithm
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../deeplearning/">
<span class="md-ellipsis">
    
  
    Deeplearning
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../gcp/">
<span class="md-ellipsis">
    
  
    Gcp
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../math/">
<span class="md-ellipsis">
    
  
    Math
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../mongodb/">
<span class="md-ellipsis">
    
  
    Mongodb
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../pgm/">
<span class="md-ellipsis">
    
  
    Pgm
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3_3_8" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../">
<span class="md-ellipsis">
    
  
    Reinforcement learning
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_3_3_8">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_3_3_8_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_3_3_8">
<span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement learning
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter1/">
<span class="md-ellipsis">
    
  
    1. Introduction
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter2/">
<span class="md-ellipsis">
    
  
    2. Multi-armed Bandits
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter3/">
<span class="md-ellipsis">
    
  
    3. Finite Markov Decision Processes
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter4/">
<span class="md-ellipsis">
    
  
    4. Dynamic Programming
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter5/">
<span class="md-ellipsis">
    
  
    5. Monte Carlo Method
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    
  
    6. Temporal-Difference Learning
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    
  
    6. Temporal-Difference Learning
  

    
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#td-prediction">
<span class="md-ellipsis">
      
        TD Prediction
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sarsa">
<span class="md-ellipsis">
      
        SARSA
      
    </span>
</a>
<nav aria-label="SARSA" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#convergence-of-sarsa">
<span class="md-ellipsis">
      
        Convergence of SARSA
      
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#q-learning">
<span class="md-ellipsis">
      
        Q-Learning
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#expected-sarsa">
<span class="md-ellipsis">
      
        Expected SARSA
      
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter7/">
<span class="md-ellipsis">
    
  
    7. n-step Bootstrapping
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter8/">
<span class="md-ellipsis">
    
  
    8. Planning and Learning with Tabular Methods
  

    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../project/">
<span class="md-ellipsis">
    
  
    Project
  

    
  </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#td-prediction">
<span class="md-ellipsis">
      
        TD Prediction
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sarsa">
<span class="md-ellipsis">
      
        SARSA
      
    </span>
</a>
<nav aria-label="SARSA" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#convergence-of-sarsa">
<span class="md-ellipsis">
      
        Convergence of SARSA
      
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#q-learning">
<span class="md-ellipsis">
      
        Q-Learning
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#expected-sarsa">
<span class="md-ellipsis">
      
        Expected SARSA
      
    </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<nav aria-label="Navigation" class="md-path">
<ol class="md-path__list">
<li class="md-path__item">
<a class="md-path__link" href="../../../..">
<span class="md-ellipsis">
          About
        </span>
</a>
</li>
<li class="md-path__item">
<a class="md-path__link" href="../../../">
<span class="md-ellipsis">
            Study
          </span>
</a>
</li>
<li class="md-path__item">
<a class="md-path__link" href="../../">
<span class="md-ellipsis">
            Tutorial
          </span>
</a>
</li>
<li class="md-path__item">
<a class="md-path__link" href="../">
<span class="md-ellipsis">
            Reinforcement learning
          </span>
</a>
</li>
</ol>
</nav>
<article class="md-content__inner md-typeset">
<nav class="md-tags" hidden="">
<span class="md-tag">reinforcement learning</span>
<span class="md-tag">temporal difference</span>
</nav>
<h1>6. Temporal-Difference Learning</h1>
<blockquote>
<p>데이터사이언스 대학원 강화학습 수업을 듣고 정리한 내용입니다.</p>
</blockquote>
<p><strong>Temporal Difference Learning</strong> 방법은 <a href="../chapter5/">Monte Carlo(MC)</a>와 <a href="../chapter4/">Dynamic Programming(DP)</a>의 아이디어를 결합한 방식이다. MC와 비슷하게 경험으로부터 직접 배우고, DP처럼 학습된 추정치(V, Q, ...)로 다른 추정치를 업데이트 한다. </p>
<h2 id="td-prediction">TD Prediction<a class="headerlink" href="#td-prediction" title="Permanent link">¶</a></h2>
<p>Monte Carlo 업데이트 규칙(every-visit)을 생각해보면 다음과 같다.</p>
<div class="arithmatex">\[V(S_t) \leftarrow V(S_t) + \alpha \lbrack G_t - V(S_t) \rbrack\]</div>
<p>여기서 <span class="arithmatex">\(G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \gamma^{T-t-1} R_{T}\)</span>다. 그리고 확인할 수 있는 것은 MC 방법은 <span class="arithmatex">\(V(S_t)\)</span>를 결정하기 위해서 항상 에피소드의 끝까지 기다려야 한다.</p>
<p>하지만, TD는 다르다. <strong>One-step TD</strong>, TD(0)의 업데이트 규칙은 다음과 같다.</p>
<div class="arithmatex">\[V(S_t) \leftarrow V(S_t) + \alpha \lbrack R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \rbrack\]</div>
<p>따라서 one-step TD 방법은 하나의 타임 스텝만 기다리면 업데이트 할 수 있다. </p>
<div class="admonition info">
<p class="admonition-title">Tabular TD(0)</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:1"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio"><div class="tabbed-labels"><label for="__tabbed_1_1">pseudo code</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=192F6SuvRtuevZNYf8BTWaLOrTRdNW1H9" width="100%"/></p>
</div>
</div>
</input></div>
</div>
<p>TD(0)가 기존에 추정된 값들을 기반으로 업데이트 하기 때문에 <strong>bootstrapping</strong> 방법이라고도 한다. 그러기에 오차(TD Error)도 존재한다. 오차는 현재의 추정치 <span class="arithmatex">\(V(S_t)\)</span>와 TD의 목표인 <span class="arithmatex">\(R_{t+1} + \gamma V(S_{t+1})\)</span>의 차이로 계산된다.</p>
<div class="arithmatex">\[\delta_t := R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\]</div>
<p>다만, <span class="arithmatex">\(t+1\)</span> 시점에서 이 오차를 확인 할 수 있다.</p>
<div class="admonition note">
<p class="admonition-title">Example: Driving-Home</p>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=199Y4Lb6Ps237IIIlxbWjaBCZ70F7-pt9" width="100%"/></p>
<p>책에서 소개한 차를 타고 집으로가는 예제를 보자. 집으로 가는데 얼마나 걸리는 지를 예측하고 싶다. 그림과 같이 6개의 state가 있고, 실제 걸린 시간(Elapsed Time)과 예측된 남은 시간(Predicted Time to Go)이 있다. 매 State가 변할 때마다 예측된 남은 시간은 달라진다. 여기서 Reward는 실제 걸린시간이 된다.</p>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=19E5_EMYFU46_VHGmvqpt3v0eZWC2gkB6" width="100%"/></p>
<p>위 그림은 MC 방법과 TD 방법의 차이를 극명하게 보여주고 있다. y 축은 예측된 전체 소모 시간(=<span class="arithmatex">\(V(S_t)\)</span>)을 나타나는데, 빨간색 화살표가 가르치는 것은 각각 MC의 <span class="arithmatex">\(G_t\)</span>와 TD(0)의 <span class="arithmatex">\(R_{t+1}+\gamma V(S_{t+1})\)</span>를 나타낸다(<span class="arithmatex">\(\gamma=1\)</span>). 예를 들어, Reach Car 상태에서 MC 방법의 <span class="arithmatex">\(G_t\)</span>는 해당 에피소드의 최종 시간 43분이 return이 되는데, TD(0)의 경우 다음 타임스텝의 <span class="arithmatex">\(R_{t+1}=20\)</span>과 <span class="arithmatex">\(V(S_{t+1})=15\)</span>의 합이 된다. 따라서, 집에 도착 할 때까지 기다릴 필요없이 value를 업데이트 할 수 있다.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Example: Chain MDP</p>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=19HPcKQVTI1MXYd2fBeZV-FrcEV8dvBCh" width="100%"/></p>
<p>두 번째 예시로 5개의 state를 가지고, <span class="arithmatex">\(C\)</span> 에서 시작하는 Markove reward process(MRP)가 있다. <span class="arithmatex">\(E\)</span> 오른쪽 Terminal state <span class="arithmatex">\(T\)</span> 에 도달할 경우만 1의 보상을 얻고 나머지는 0을 얻는다.</p>
<p>해당 MRP의 true value는 bellman equation <span class="arithmatex">\(V_\pi(s) = \sum_{r, s'} p(r, s' \vert s) (r + V(s'))\)</span>을 이용할 수 있다. 여기서 policy <span class="arithmatex">\(\pi\)</span>는 deterministic하기 때문에 어떤 액션을 취하든 <span class="arithmatex">\(\pi(a \vert s)=1\)</span> 이다. 그리고 state transition probability <span class="arithmatex">\(p(r, s' \vert s)=0.5\)</span>이다(좌로 가든 우로가든 반반이다). 또한, <span class="arithmatex">\(V(T)=0\)</span>이다. 따라서 다음과 같이 계산된다<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<div class="arithmatex">\[\begin{aligned} 
V(A) &amp;= 0.5 \big( V(T_L) + V(B) \big) =  0.5 V(B) \\
V(B) &amp;= 0.5 \big( V(A) + V(C) \big) \\
V(C) &amp;= 0.5 \big( V(B) + V(D) \big) \\
V(D) &amp;= 0.5 \big( V(C) + V(E) \big) \\
V(E) &amp;= 0.5 \big( V(D) + V(T_R) \big) = 0.5 V(D) + 0.5\\
\end{aligned}\]</div>
<p>따라서 <span class="arithmatex">\(V(A)\)</span>를 차례대로 <span class="arithmatex">\(V(E)\)</span>까지 대입하면 <span class="arithmatex">\(V(A) = 1/6\)</span>이 된다.</p>
<p>아래 있는 그림 중 왼쪽은 TD(0) 방법을 사용 했을 때, 경험을 반복 할 때마다 emsimated 한 value function이 실제 값(검은선)에 근접하는 것을 확인 할 수 있다. 오른쪽 그림은 MC 방법과 TD 방법이 learning rate <span class="arithmatex">\(\alpha\)</span>의 차이에 따라 실제 값의 차이를 episode의 흐름에 따라 표시한 것이다.</p>
</div>
<h2 id="sarsa">SARSA<a class="headerlink" href="#sarsa" title="Permanent link">¶</a></h2>
<p>보통 우리는 진짜 모델을 모르기 때문에, <span class="arithmatex">\(v_\pi(s)\)</span> 보다 대신에 <span class="arithmatex">\(q_\pi(s, a)\)</span> 학습할 수 있게 한다. SARSA의 업데이트 방식은 다음과 같다.</p>
<div class="arithmatex">\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \lbrack R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \rbrack\]</div>
<div class="admonition info">
<p class="admonition-title">Sarsa (on-policy TD Control)</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:1"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio"><div class="tabbed-labels"><label for="__tabbed_2_1">pseduo code</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=11e37aCLYIU-KLd_AKp36bwmyrfgpGOgh" width="100%"/></p>
</div>
</div>
</input></div>
</div>
<p>여기서 Greedy in the Limit with Infinite Exploration(GLIE)라는 가정이 들어간다. </p>
<ol>
<li>
<p>모든 state-action 쌍은 무한의 횟수로 탐색이 가능하다는 것이다. </p>
<div class="arithmatex">\[\lim_{t \rightarrow \infty} N_t(s, a) = \infty\]</div>
</li>
<li>
<p>또한, policy는 greedy policy으로 수렴한다. 즉, state-action 쌍의 추정치가 개선됨에 따라 policy가 점점 결정론적으로 항상 최대의 가치를 가지는 policy가 된다는 뜻이다. </p>
<div class="arithmatex">\[\lim_{t \rightarrow \infty} \pi_t(a \vert s) = \Bbb{1} \bigg\lbrack a = \underset{a'}{\arg\max}  Q_k(s, a') \bigg\rbrack \]</div>
</li>
</ol>
<p>예를 들어, <span class="arithmatex">\(\epsilon\)</span>-greedy에서 <span class="arithmatex">\(\epsilon = \dfrac{1}{t}\)</span>으로 설정하면 이는 GLIE를 만족하는 방법이다. </p>
<h3 id="convergence-of-sarsa">Convergence of SARSA<a class="headerlink" href="#convergence-of-sarsa" title="Permanent link">¶</a></h3>
<p>아래의 조건하에 SARSA는 최적의 action-value function으로 수렴한다 <span class="arithmatex">\(Q(s, a) \rightarrow q_*(s, a)\)</span>:</p>
<ul>
<li><span class="arithmatex">\(\pi_t(a \vert s)\)</span> 가 GLIE를 만족한다.</li>
<li>학습률(learning rate = step size)가 확률적 수렴에 만족한다.</li>
</ul>
<div class="arithmatex">\[\begin{aligned} \sum_{t=1}^\infty \alpha_t &amp;= \infty \\ \sum_{t=1}^\infty \alpha_t^2 &lt; \infty \end{aligned}\]</div>
<div class="admonition note">
<p class="admonition-title">Windy grid world</p>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=11fSZyEHIOpnG80j6ym1KGOXYKck5WOE1" width="100%"/></p>
<p>한번 움직일 때마다 <span class="arithmatex">\(-1\)</span>의 보상, <span class="arithmatex">\(\gamma = 1\)</span>인 바람이 부는 grid world 예제다. 뒤에 있는 빨강색 그래프는 에피소드가 끝날 때까지 걸린 Time steps를 <span class="arithmatex">\(x\)</span>축, episode는 <span class="arithmatex">\(y\)</span> 축에 드려지고 있다. 예를 들어 그래프 처럼 첫번째 eposide는 거의 1500대에 종료가 되었고 그 다음 에피소드는 대략 2100번 초반에 위치한 것으로 보인다. Pseudo Code로 다음과 같이 쓸 수 있다.</p>
<div class="highlight"><pre><span></span><code><a href="#__codelineno-0-1" id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<a href="#__codelineno-0-2" id="__codelineno-0-2" name="__codelineno-0-2"></a><span class="n">episodes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">1500</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="mi">600</span> <span class="o">+</span> <span class="o">...</span> <span class="p">[</span><span class="n">episode번호</span><span class="p">]</span><span class="o">*</span><span class="n">종료까지_걸린_횟수</span> <span class="p">]</span>
<a href="#__codelineno-0-3" id="__codelineno-0-3" name="__codelineno-0-3"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span>
<a href="#__codelineno-0-4" id="__codelineno-0-4" name="__codelineno-0-4"></a><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'time steps'</span><span class="p">)</span>
<a href="#__codelineno-0-5" id="__codelineno-0-5" name="__codelineno-0-5"></a><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'episodes'</span><span class="p">)</span>
<a href="#__codelineno-0-6" id="__codelineno-0-6" name="__codelineno-0-6"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><span class="arithmatex">\(\epsilon = 0.1, \alpha = 0.5\)</span>를 적용했을 시, 우리는 시간이 지날 수록 목적에 더 빨리 도달함을 알 수 있다. </p>
</div>
<h2 id="q-learning">Q-Learning<a class="headerlink" href="#q-learning" title="Permanent link">¶</a></h2>
<p>Q-Learning은 SARSA와 비슷하지만, 다음 state의 action을 greedy하게 선택한다는 점이 다르다. 즉, <span class="arithmatex">\(Q(S_{t+1}, A_{t+1})\)</span> 대신에 <span class="arithmatex">\(Q(S_{t+1}, \underset{a'}{\arg\max} Q(S_{t+1}, a'))\)</span>를 사용한다. 다음 policy에 관계없이 최적의 action-value function을 추정한다.</p>
<div class="arithmatex">\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \lbrack R_{t+1} + \gamma \underset{a}{\max} Q(S_{t+1}, a) - Q(S_t, A_t) \rbrack\]</div>
<div class="admonition info">
<p class="admonition-title">Q-Learning (off-policy TD Control)</p>
<div class="tabbed-set tabbed-alternate" data-tabs="3:1"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio"><div class="tabbed-labels"><label for="__tabbed_3_1">pseduo code</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1AZylxL_ByZJaBzaG6NnV5iDxfZY7Bj0U" width="100%"/></p>
</div>
</div>
</input></div>
</div>
<p>Q-Learning은 off policy이다. 학습하고자 하는 target policy는 <span class="arithmatex">\(\pi(a \vert S_t) = \underset{a'}{\arg \max}\ Q(S_t, a')\)</span>이며, action을 선택하는 behavior policy는 <span class="arithmatex">\(\pi\)</span>과 같지 않다. 예를 들어, <span class="arithmatex">\(\epsilon\)</span>-greedy가 될 수가 있다. 즉, Q-Learning에서의 state-action쌍에 대한 return은 greedy policy을 가정하여 업데이트 하는데, 실제로는 greedy 하지 않기 때문이다. 반면에 SARSA가 on policy인 이유는 Q-values를 다음 state <span class="arithmatex">\(s'\)</span>와 현재 policy에 의해 결정되는 action <span class="arithmatex">\(a'\)</span> 를 사용하여 업데이트하기 때문이다.</p>
<div class="arithmatex">\[\begin{aligned}R_{t+1} + \gamma Q(S_{t+1}, A') &amp;= R_{t+1} + \gamma Q \big(S_{t+1}, \underset{a'}{\arg \max}\ Q(S_t, a') \big) \\ &amp;= R_{t+1} + \gamma \underset{a'}{\max}\ Q(S_{t+1}, a') \end{aligned}\]</div>
<div class="admonition note">
<p class="admonition-title">Cliff Walking</p>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1A_vlajqkBNErD91Mr4Mw831-vLBvD9Tt" width="100%"/></p>
<ul>
<li>조건: undiscounted, episodic, deterministic environment</li>
<li>Action: 4개(상, 하, 좌, 우)</li>
<li>Reward: 모든 state에서 -1 이지만, "The Cliff"에서는 -100의 보상을 받고 즉시 Start state <span class="arithmatex">\(S\)</span>로 돌아간다.</li>
<li><span class="arithmatex">\(\epsilon\)</span>-greedy policy를 사용하며 <span class="arithmatex">\(\epsilon = 0.1\)</span>로 설정한다.</li>
</ul>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1Agtb17w3Vo6Uty1okdqEVG01inYLm5VZ" width="80%"/></p>
<p>위 그림은 Sarsa와 Q-Learning을 비교한 것이다. Q-Learning이 각 에피소드 별로 더 많은 negative reward를 얻는 것을 볼 수 있다. 이는 고정적인 <span class="arithmatex">\(\epsilon\)</span>을 사용하기도 하고, Q-Learning이 항상 최대의 Q-value에 근거하여 action을 선택하기 때문에, 더 많이 cliff로 떨어질 가능성도 높다. </p>
<p>다만, <span class="arithmatex">\(\epsilon\)</span>을 줄이는 전략을 택했다면, 두 방법다 optimal policy에 접근할 수 있다. </p>
</div>
<h2 id="expected-sarsa">Expected SARSA<a class="headerlink" href="#expected-sarsa" title="Permanent link">¶</a></h2>
<p>Expected SARSA의 업데이트 규칙은 다음과 같다.</p>
<div class="arithmatex">\[\begin{aligned} Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \alpha \big\lbrack R_{t+1} + \gamma \Bbb{E}_\pi \lbrack Q(S_{t+1}, a) \vert S_{t+1} \rbrack - Q(S_t, A_t) \big\rbrack \\
&amp;\leftarrow Q(S_t, A_t) + \alpha \big\lbrack R_{t+1} + \gamma \sum_{a} \pi(a \vert S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \big\rbrack  \end{aligned}\]</div>
<p>Expected SARSA는 SARSA 보다 계산적으로 복잡하지만, action 선택에 있어서 variance 줄여주기 때문에 조금 더 안정적이다.</p>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p><a href="https://datascience.stackexchange.com/questions/40899/reinforcement-learning-how-are-these-state-values-in-mrp-calculated">Reinforcement Learning - How are these state values in MRP calculated?</a> <a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
<!-- Giscus -->
<h2 id="__comments">Comments</h2>
<script async="" crossorigin="anonymous" data-category="General" data-category-id="DIC_kwDOHRhxjc4CQSuI" data-emit-metadata="0" data-input-position="top" data-lang="ko" data-mapping="pathname" data-reactions-enabled="1" data-repo="simonjisu/comments_bot" data-repo-id="R_kgDOHRhxjQ" data-theme="light" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    /* Set palette on initial load */
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate" ? "dark" : "light"
      giscus.setAttribute("data-theme", theme) 
    }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate" ? "dark" : "light"

          /* Instruct Giscus to change theme */
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<a class="md-top md-icon" data-md-component="top" hidden="" href="#">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Back to top
          </a>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs Insiders
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/simonjisu" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.top", "toc.follow", "navigation.prune", "navigation.path", "content.tooltips", "content.code.annotate"], "search": "../../../../assets/javascripts/workers/search.6c7302c4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../../../../assets/javascripts/bundle.fbce390c.min.js"></script>
<script src="../../../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": false, "draggable": false, "openEffect": "none", "closeEffect": "none", "slideEffect": "slide"});})</script></body>
</html>