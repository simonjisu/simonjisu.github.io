
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="https://simonjisu.github.io/study/tutorial/reinforcement_learning/CS285_L19/" rel="canonical"/>
<link href="../" rel="prev"/>
<link href="../CS285_L20/" rel="next"/>
<link href="../../../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.2, mkdocs-material-9.0.12+insiders-4.30.2" name="generator"/>
<title>CS 285: Lecture 19, Control as Inference - Soopace</title>
<link href="../../../../assets/stylesheets/main.b6d2c4d8.min.css" rel="stylesheet"/>
<link href="../../../../assets/stylesheets/palette.2505c338.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-2D0S4P2SJ9"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-2D0S4P2SJ9",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-2D0S4P2SJ9",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<link href="../../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src="../../../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="" data-md-color-primary="black" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#optimal-control-as-a-model-of-human-behavior">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--lifted" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Soopace" class="md-header__button md-logo" data-md-component="logo" href="../../../.." title="Soopace">
<img alt="logo" src="../../../../img/logo/logo.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Soopace
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              CS 285: Lecture 19, Control as Inference
            
          </span>
</div>
</div>
</div>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/simonjisu.github.io" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</nav>
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../..">
        
  
    
  
  About

      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../blog/">
          
  
    
  
  Blog

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link md-tabs__link--active" href="../../../">
          
  
    
  
  Study

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../project/">
          
  
    
  
  Project

        </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Soopace" class="md-nav__button md-logo" data-md-component="logo" href="../../../.." title="Soopace">
<img alt="logo" src="../../../../img/logo/logo.png"/>
</a>
    Soopace
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/simonjisu.github.io" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../..">
<span class="md-ellipsis">
    
  
    About
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../blog/">
<span class="md-ellipsis">
    
  
    Blog
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../../">
<span class="md-ellipsis">
    
  
    Study
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            
  
    Study
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../paper/">
<span class="md-ellipsis">
    
  
    Paper
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../">
<span class="md-ellipsis">
    
  
    Tutorial
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_3_3">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_3_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_3">
<span class="md-nav__icon md-icon"></span>
            
  
    Tutorial
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../algorithm/">
<span class="md-ellipsis">
    
  
    Algorithm
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../deeplearning/">
<span class="md-ellipsis">
    
  
    Deeplearning
  

    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../gcp/">
<span class="md-ellipsis">
    
  
    Gcp
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../math/">
<span class="md-ellipsis">
    
  
    Math
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../mongodb/">
<span class="md-ellipsis">
    
  
    Mongodb
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3_3_7" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../">
<span class="md-ellipsis">
    
  
    Reinforcement learning
  

    
  </span>
</a>
<label class="md-nav__link" for="__nav_3_3_7">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_3_3_7_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_3_3_7">
<span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement learning
  

          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    
  
    CS 285: Lecture 19, Control as Inference
  

    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    
  
    CS 285: Lecture 19, Control as Inference
  

    
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#optimal-control-as-a-model-of-human-behavior">
<span class="md-ellipsis">
      
        Optimal control as a Model of Human Behavior
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#control-as-inference">
<span class="md-ellipsis">
      
        Control as Inference
      
    </span>
</a>
<nav aria-label="Control as Inference" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#backward-messages">
<span class="md-ellipsis">
      
        Backward Messages
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#policy-computation">
<span class="md-ellipsis">
      
        Policy Computation
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#forward-messages">
<span class="md-ellipsis">
      
        Forward Messages
      
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#message-intersection">
<span class="md-ellipsis">
      
        Message Intersection
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#control-as-variational-inference">
<span class="md-ellipsis">
      
        Control as Variational Inference
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#algorithms-for-rl-as-inference">
<span class="md-ellipsis">
      
        Algorithms for RL as Inference
      
    </span>
</a>
<nav aria-label="Algorithms for RL as Inference" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#q-learning-with-soft-optimality">
<span class="md-ellipsis">
      
        Q-Learning with soft optimality
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#benefits-of-soft-optimality">
<span class="md-ellipsis">
      
        Benefits of Soft Optimality
      
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reinforcement-learning-and-control-as-probabilistic-inference-tutorial-and-review2">
<span class="md-ellipsis">
      
        논문 맛보기: Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review2
      
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../CS285_L20/">
<span class="md-ellipsis">
    
  
    CS 285: Lecture 20, Inverse Reinforcement Learning
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter1/">
<span class="md-ellipsis">
    
  
    1. Introduction
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter2/">
<span class="md-ellipsis">
    
  
    2. Multi-armed Bandits
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter3/">
<span class="md-ellipsis">
    
  
    3. Finite Markov Decision Processes
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter4/">
<span class="md-ellipsis">
    
  
    4. Dynamic Programming
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter5/">
<span class="md-ellipsis">
    
  
    5. Monte Carlo Method
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter6/">
<span class="md-ellipsis">
    
  
    6. Temporal-Difference Learning
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter7/">
<span class="md-ellipsis">
    
  
    7. n-step Bootstrapping
  

    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../chapter8/">
<span class="md-ellipsis">
    
  
    8. Planning and Learning with Tabular Methods
  

    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../project/">
<span class="md-ellipsis">
    
  
    Project
  

    
  </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#optimal-control-as-a-model-of-human-behavior">
<span class="md-ellipsis">
      
        Optimal control as a Model of Human Behavior
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#control-as-inference">
<span class="md-ellipsis">
      
        Control as Inference
      
    </span>
</a>
<nav aria-label="Control as Inference" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#backward-messages">
<span class="md-ellipsis">
      
        Backward Messages
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#policy-computation">
<span class="md-ellipsis">
      
        Policy Computation
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#forward-messages">
<span class="md-ellipsis">
      
        Forward Messages
      
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#message-intersection">
<span class="md-ellipsis">
      
        Message Intersection
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#control-as-variational-inference">
<span class="md-ellipsis">
      
        Control as Variational Inference
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#algorithms-for-rl-as-inference">
<span class="md-ellipsis">
      
        Algorithms for RL as Inference
      
    </span>
</a>
<nav aria-label="Algorithms for RL as Inference" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#q-learning-with-soft-optimality">
<span class="md-ellipsis">
      
        Q-Learning with soft optimality
      
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#benefits-of-soft-optimality">
<span class="md-ellipsis">
      
        Benefits of Soft Optimality
      
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reinforcement-learning-and-control-as-probabilistic-inference-tutorial-and-review2">
<span class="md-ellipsis">
      
        논문 맛보기: Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review2
      
    </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<nav aria-label="Navigation" class="md-path">
<ol class="md-path__list">
<li class="md-path__item">
<a class="md-path__link" href="../../../..">
<span class="md-ellipsis">
          About
        </span>
</a>
</li>
<li class="md-path__item">
<a class="md-path__link" href="../../../">
<span class="md-ellipsis">
            Study
          </span>
</a>
</li>
<li class="md-path__item">
<a class="md-path__link" href="../../">
<span class="md-ellipsis">
            Tutorial
          </span>
</a>
</li>
<li class="md-path__item">
<a class="md-path__link" href="../">
<span class="md-ellipsis">
            Reinforcement learning
          </span>
</a>
</li>
</ol>
</nav>
<article class="md-content__inner md-typeset">
<nav class="md-tags" hidden="">
<span class="md-tag">reinforcement learning</span>
<span class="md-tag">control</span>
</nav>
<h1>CS 285: Lecture 19, Control as Inference</h1>
<p>영상링크: <a href="https://youtu.be/MzVlYYGtg0M">https://youtu.be/MzVlYYGtg0M</a></p>
<h2 id="optimal-control-as-a-model-of-human-behavior">Optimal control as a Model of Human Behavior<a class="headerlink" href="#optimal-control-as-a-model-of-human-behavior" title="Permanent link">¶</a></h2>
<blockquote>
<p>Optimal control is a mathematical framework for computing control policies that optimize a given objective.</p>
</blockquote>
<ul>
<li><span class="arithmatex">\(r(s_t, a_t)\)</span> 함수를 찾아서 데이터(trajactories)를 설명하려고함. 기본적 세팅은 다음과 같음.</li>
</ul>
<div class="arithmatex">\[\begin{aligned} 
a_1, \dots, a_T &amp;= \underset{a_1, \dots, a_T}{\arg \max} \sum_{t=1}^T r(s_t, a_t) \\
s_{t+1} &amp;= f(s_t, a_t) \\
\pi &amp;= \underset{\pi}{\arg \max} \mathbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t, a_t), a_t \sim \pi(a_t \vert s_t)} \lbrack r(s_t, a_t) \rbrack \\
a_t &amp;\sim \pi(a_t \vert s_t) 
\end{aligned}\]</div>
<ul>
<li>하지만 이는 적용하기 쉽지 않음. 우리는 목표에 최적화된 행동을 항상 하지 않음. 같은 목표라도 직진하는 경우도 있고, 멀리 돌아갈 때도 있음(결국에 목적에 도달함). 현 프레임워크에서 이를 설정하거나 설명하기 어려움. 어떤 행동이 최적화된 행동인지에 대한 변수자체가 없음.</li>
<li>그래서 이를 설명하기 위해 binary 변수 <span class="arithmatex">\(\mathcal{O}\)</span> 도입함. 최적화인 행동일 때 1, 그렇지 아니할 때 0의 값을 가짐. </li>
</ul>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1Gz6gRWt-SPcNthouYTtUBPCezg_ikEi-" width="100%"/></p>
<div class="arithmatex">\[\begin{aligned}
p(\mathcal{O}_{1:T}) &amp;= \exp \big( r(s_t, a_t) \big) \\
p(\tau \vert \mathcal{O}_{1:T}) &amp;= \dfrac{p(\tau, \mathcal{O}_{1:T})}{p(\mathcal{O}_{1:T})} \\
&amp;\propto p(\tau) \prod_t \exp \big( r(s_t, a_t) \big)  = p(\tau) \exp \big( \sum_t r(s_t, a_t) \big)
\end{aligned}\]</div>
<ul>
<li>이렇게 함으로써 suboptimal behavior를 모델링 할 수 있고, inference 알고리즘을 적용하여 control과 planning 문제를 해결할 수 있음. 그리고 stochastic behavior가 왜 선호되는지 설명할 수 있음. 이는 exploration과 transfer learning을 아는데 도움이 됨.</li>
<li>
<p>그러면 어떻게 inference할까?</p>
</li>
<li>
<p>Backward Messeage 계산: <span class="arithmatex">\(\beta(s_t, a_t) = p(\mathcal{O}_{1:T} \vert s_t, a_t)\)</span></p>
</li>
<li>Policy 계산: <span class="arithmatex">\(\pi(a_t \vert s_t, \mathcal{O}_{1:T})\)</span></li>
<li>Forward Message 계산: <span class="arithmatex">\(\alpha(s_t) = p(s_t \vert \mathcal{O}_{1:t-1})\)</span></li>
</ul>
<h2 id="control-as-inference">Control as Inference<a class="headerlink" href="#control-as-inference" title="Permanent link">¶</a></h2>
<h3 id="backward-messages">Backward Messages<a class="headerlink" href="#backward-messages" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[\begin{aligned} 
\beta(s_t, a_t) &amp;= p(\mathcal{O}_{t:T} \vert s_t, a_t) \\
&amp;= \int p(\mathcal{O}_{t:T}, s_{t+1} \vert s_t, a_t) ds_{t+1} \\
&amp;= \int p(\mathcal{O}_{t+1:T} \vert s_{t+1}) p(s_{t+1} \vert s_t, a_t) p(\mathcal{O}_t \vert s_t, a_t) ds_{t+1}
\end{aligned}\]</div>
<ul>
<li><span class="arithmatex">\(p(s_{t+1} \vert s_t, a_t)\)</span>는 transition dynamics이고, <span class="arithmatex">\(p(\mathcal{O}_t \vert s_t, a_t)\)</span>는 observation likelihood임.</li>
<li>그러면 제일 앞에 있는 <span class="arithmatex">\(p(\mathcal{O}_{t+1:T} \vert s_{t+1})\)</span>를 풀어서 쓰면 다음과 같음.</li>
</ul>
<div class="arithmatex">\[\begin{aligned}
p(\mathcal{O}_{t+1:T} \vert s_{t+1}) &amp;= \int p(\mathcal{O}_{1:T} \vert s_{t+1}, a_{t+1}) p(a_{t+1}\vert s_{t+1}) da_{t+1}
\end{aligned}\]</div>
<ul>
<li>여기서 <span class="arithmatex">\(p(\mathcal{O}_{1:T} \vert s_{t+1}, a_{t+1})\)</span> 를 다시 <span class="arithmatex">\(\beta(s_{t+1}, a_{t+1})\)</span>로 바꿔서 쓸수 있음.</li>
<li>그리고 <span class="arithmatex">\(p(a_{t+1}\vert s_{t+1})\)</span>는 <strong>action prior</strong>이라고 하는데 (policy는 아님) 우선은 uniform distribution으로 가정함. 왜냐면 아무도 어떤 행동을 할지 모르기 때문임. 게다가 수학적으로 해당 항을 지울 수 있음<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup>.</li>
<li>따라서 Backward message passing은 다음과 같이 진행된다. </li>
</ul>
<div class="admonition info">
<p class="admonition-title">Backward Message Passing</p>
<div class="arithmatex">\[\begin{aligned}
\text{for } \ t = T-1, &amp;\dots, 1: \\
\beta(s_t, a_t) &amp;= p(\mathcal{O}_t \vert s_t, a_t) \Bbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t, a_t)} \big\lbrack \beta_{t+1}(s_{t+1}) \big\rbrack\\
\beta(s_t) &amp;= \Bbb{E}_{a_t \sim p(a_t \vert s_t)} \big\lbrack \beta(s_t, a_t) \big\rbrack\\
\end{aligned}\]</div>
</div>
<ul>
<li><span class="arithmatex">\(V_t(s_t) = \log \beta(s_t)\)</span>, <span class="arithmatex">\(Q_t(s_t, a_t) = \log \beta(s_t, a_t)\)</span> 라고 재정의 하자. 따라서 <span class="arithmatex">\(V_t(s_t) = \log \int \exp \big( Q_t(s_t, a_t) \big) da_t\)</span>로 쓸 수 있으며, 이는 <strong>soft value function</strong>이라고 함.</li>
<li><span class="arithmatex">\(Q_t(s_t, a_t)\)</span>가 커짐에 다라서 <span class="arithmatex">\(V_t(s_t)\)</span> 도 커짐. <span class="arithmatex">\(V_t(s_t) \rightarrow \underset{a_t}{\max} Q(s_t, a_t)\)</span>.</li>
<li><span class="arithmatex">\(Q_t(s_t, a_t) = r(s_t, a_t) + \log \Bbb{E} \big\lbrack \exp( V_{t+1}(s_{t+1}) ) \big\rbrack\)</span><ul>
<li>deterministic transition: <span class="arithmatex">\(Q_t(s_t, a_t) = r(s_t, a_t) + V_{t+1}(s_{t+1})\)</span></li>
<li>sthocastic case는 차후에 다룸</li>
</ul>
</li>
<li>Log domain에서 알고리즘을 다시 쓰면 다음과 같음.</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Backward Message Passing(Log Domain)</p>
<div class="arithmatex">\[\begin{aligned}
\text{for } \ t = T-1, &amp;\dots, 1: \\
Q_t(s_t, a_t) &amp;= r(s_t, a_t) + \log \Bbb{E}\big\lbrack \exp( V_{t+1}(s_{t+1}) ) \big\rbrack \\
V_t(s_t) &amp;= \log \int \exp \big( Q_t(s_t, a_t) \big) da_t \\
\end{aligned}\]</div>
</div>
<h3 id="policy-computation">Policy Computation<a class="headerlink" href="#policy-computation" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[\begin{aligned}
p(a_t \vert s_t, \mathcal{O}_{1:T}) &amp;= \pi(a_t \vert s_t) = p(a \vert s_t, \mathcal{O}_{t:T}) \\
&amp;= \dfrac{p(a_t, s_t \vert \mathcal{O}_{t:T})}{p(s_t \vert \mathcal{O}_{t:T})} \\
&amp;= \dfrac{p(a_t, s_t \vert \mathcal{O}_{t:T})p(a_t, s_t) / p(\mathcal{O}_{t:T}) }{p(\mathcal{O}_{t:T} \vert s_t) p(s_t) / p(\mathcal{O}_{t:T}) } \\
&amp;= \dfrac{p(a_t, s_t \vert \mathcal{O}_{t:T})}{p(\mathcal{O}_{t:T} \vert s_t) } \dfrac{p(a_t, s_t)}{p(s_t)} = \dfrac{\beta_t(s_t, a_t)}{\beta_t(s_t)} p(a_t \vert s_t)
\end{aligned}\]</div>
<ul>
<li><span class="arithmatex">\(p(a_t \vert s_t)\)</span>는 action prior이라 무시하고, policy <span class="arithmatex">\(\pi(a_t \vert s_t) = \dfrac{\beta_t(s_t, a_t)}{\beta_t(s_t)}\)</span>를 얻을 수 있음.</li>
<li>Log domain에서 <span class="arithmatex">\(\pi(a_t \vert s_t) = \exp \big( Q_t(s_t, a_t) - V_t(s_t) \big) = \exp \big( A_t(s_t, a_t) \big)\)</span> 로 쓸 수 있음.</li>
<li>Temperature <span class="arithmatex">\(\alpha\)</span> 를 도입하면 <span class="arithmatex">\(\pi(a_t \vert s_t) = \exp \big( \dfrac{1}{\alpha} Q_t(s_t, a_t) - \dfrac{1}{\alpha} V_t(s_t) \big) = \exp \big( \dfrac{1}{\alpha} A_t(s_t, a_t) \big)\)</span></li>
<li><span class="arithmatex">\(\alpha\)</span> 가 0으로 갈 수록 deterministic policy가 되고 greedy policy에 가까움.</li>
</ul>
<h3 id="forward-messages">Forward Messages<a class="headerlink" href="#forward-messages" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[\begin{aligned}
\alpha(s_t) &amp;= p(s_t \vert \mathcal{O}_{1:t-1} ) \\
&amp;= \int p(s_t, s_{t-1}, a_{t-1} \vert \mathcal{O}_{1:t-1}) da_{t-1} ds_{t-1} = \int p(s_t \vert s_{t-1}, a_{t-1}, \mathcal{O}_{1:t-1}) p(a_{t-1} \vert s_{t-1}, \mathcal{O}_{1:t-1}) p(s_{t-1} \vert \mathcal{O}_{1:t-1} ) da_{t-1} ds_{t-1} \\
&amp;= \int p(s_t \vert s_{t-1}, a_{t-1}) p(a_{t-1} \vert s_{t-1}, \mathcal{O}_{1:t-1}) p(s_{t-1} \vert \mathcal{O}_{1:t-1} ) da_{t-1} ds_{t-1}
\end{aligned}\]</div>
<ul>
<li><span class="arithmatex">\(p(s_t \vert s_{t-1}, a_{t-1}, \mathcal{O}_{1:t-1})\)</span> 에서 <span class="arithmatex">\(\mathcal{O}_{1:t-1}\)</span>는 <span class="arithmatex">\(s_{t-1}\)</span>과 <span class="arithmatex">\(a_{t-1}\)</span>에 의존하지 않으므로 생략 가능.</li>
<li><span class="arithmatex">\(\alpha_1(s_1) = p(s_1)\)</span> 보통 알고 시작함.</li>
<li><span class="arithmatex">\(p(s_t \vert \mathcal{O}_{1:T})\)</span> 를 계산하고 싶으면?</li>
</ul>
<div class="arithmatex">\[p(s_t \vert \mathcal{O}_{1:T}) = \dfrac{ p(s_t, \mathcal{O}_{1:T}) }{ p(\mathcal{O}_{1:T}) } = \dfrac{p(\mathcal{O}_{t:T} \vert s_t) p(s_t, \mathcal{O}_{1:t-1})}{p(\mathcal{O}_{1:T})} \propto \beta_t(s_t) \alpha_t(s_t) \]</div>
<h2 id="message-intersection">Message Intersection<a class="headerlink" href="#message-intersection" title="Permanent link">¶</a></h2>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1H-xkwcHqXyKbBtFqVwWyLpB2Q_X03P3u" width="100%"/></p>
<ul>
<li>예를 들어 그림과 같이 목표지점에 공을 가져다 놓는 Task가 있다.</li>
<li>Backward message는 목표지점에 도달하기 위한 상태의 확률을 나타내고, Forward message는 처음 상태에서 목표지점에 도달되는 상태를 표현한다(높은 reward 와 함께).</li>
<li>그리고 두 메시지의 곱은 목표지점에 도달하기 위한 상태의 확률을 나타낸다.</li>
</ul>
<h2 id="control-as-variational-inference">Control as Variational Inference<a class="headerlink" href="#control-as-variational-inference" title="Permanent link">¶</a></h2>
<ul>
<li>Inference Problem: <span class="arithmatex">\(p(s_{1:T}, a_{1:T} \vert \mathcal{O}_{1:T})\)</span></li>
<li>Marginalizaing과 conditioning을 통해 목적 policy <span class="arithmatex">\(p(a_t \vert s_t, \mathcal{O}_{1:T})\)</span> 를 계산하고 싶음. "높은 리워드가 주어졌을 때, action probability가 어떻게 되는가?"</li>
<li><span class="arithmatex">\(q(s_{1:T}, a_{1:T})\)</span> 분포로 <span class="arithmatex">\(p(s_{1:T}, a_{1:T} \vert \mathcal{O}_{1:T})\)</span> 를 근사하면 어떻까(단 dynamics <span class="arithmatex">\(p(s_{t+1} \vert s_t, a_t)\)</span> 하에서)?</li>
<li><span class="arithmatex">\(x = \mathcal{O}_{1:T}\)</span>, <span class="arithmatex">\(z = (s_{1:T}, a_{1:T})\)</span> 로 두어서 <span class="arithmatex">\(p(z \vert x)\)</span> 를 <span class="arithmatex">\(q(z)\)</span> 로 근사해보자! </li>
</ul>
<div class="admonition info">
<p class="admonition-title">과정<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:4"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio"><input id="__tabbed_1_2" name="__tabbed_1" type="radio"><input id="__tabbed_1_3" name="__tabbed_1" type="radio"><input id="__tabbed_1_4" name="__tabbed_1" type="radio"><div class="tabbed-labels"><label for="__tabbed_1_1">1</label><label for="__tabbed_1_2">2</label><label for="__tabbed_1_3">3</label><label for="__tabbed_1_4">4</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1H0dW_tU8kTzz3jlzowFNJQlQwmZcmai2" width="100%"/></p>
</div>
<div class="tabbed-block">
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1H3afCl4d1HC5Lk3SUhWv0FupT20t79PK" width="100%"/></p>
</div>
<div class="tabbed-block">
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1H4ZVOYG5Jk1gPeI7bqxt3lhyAYHjF9bg" width="100%"/></p>
</div>
<div class="tabbed-block">
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1HId1TGidfizjX-fn4EkgQIDubMfyZZKC" width="100%"/></p>
</div>
</div>
</input></input></input></input></div>
</div>
<h2 id="algorithms-for-rl-as-inference">Algorithms for RL as Inference<a class="headerlink" href="#algorithms-for-rl-as-inference" title="Permanent link">¶</a></h2>
<h3 id="q-learning-with-soft-optimality">Q-Learning with soft optimality<a class="headerlink" href="#q-learning-with-soft-optimality" title="Permanent link">¶</a></h3>
<p><img alt="HeadImg" class="skipglightbox" src="https://drive.google.com/uc?id=1HJ8Rg5E_JTrVSc3leWtvwCWYStO-ZbBu" width="100%"/></p>
<h3 id="benefits-of-soft-optimality">Benefits of Soft Optimality<a class="headerlink" href="#benefits-of-soft-optimality" title="Permanent link">¶</a></h3>
<ul>
<li>exploration을 향상시키고 entropy collapse를 방지할 수 있음</li>
<li>policies를 더 쉽게 fine-tuning 할 수 있음</li>
<li>더 로버스트함. 더 많은 state를 커버하기 때문</li>
</ul>
<hr/>
<h1 id="reinforcement-learning-and-control-as-probabilistic-inference-tutorial-and-review2">논문 맛보기: Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup><a class="headerlink" href="#reinforcement-learning-and-control-as-probabilistic-inference-tutorial-and-review2" title="Permanent link">¶</a></h1>
<p>논문을 통해서 위에 내용을 다시 복습해본다.</p>
<ul>
<li>문제: Optimal Control Problem</li>
<li>Task: policy <span class="arithmatex">\(p(a_t \vert s_t, \theta)\)</span> 를 구하려고함</li>
<li>보통의 rl policy search 문제: 파라미터 <span class="arithmatex">\(\theta\)</span>를 찾기</li>
</ul>
<div class="arithmatex">\[\theta^* = \underset{\theta}{\arg \max}\ \sum_{t=1}^T \Bbb{E}_{(s_t, a_t \vert \theta) \sim p(s_t, a_t \vert \theta)} \lbrack r(s_t, a_t) \rbrack\]</div>
<ul>
<li>Optimization 문제: 보상의 기대합 <span class="arithmatex">\(\sum_t r(s_t, a_t)\)</span> 을 최대화 할 수 있는 파라미터 <span class="arithmatex">\(\theta\)</span> 찾기</li>
</ul>
<div class="arithmatex">\[p(\tau) = p(s_1, a_t, \dots, s_T, a_T \vert \theta) = p(s_1) \prod_{t=1}^T p(a_t \vert s_t, \theta) p(s_{t+1} \vert s_t, a_t) \quad \cdots (1)\]</div>
<ul>
<li>문제를 standard planning 문제로 대입, <span class="arithmatex">\(\theta\)</span> 뉴럴넷을 사용하는 대신, 일련의 액션으로 정의함(open-loop plan?)</li>
<li>optimal policy로 부터 생성된 trajectory가 그럴듯하게 되도록 PGM을 구현해야함</li>
<li>이를 위해 binary variable <span class="arithmatex">\(\mathcal{O}_t\)</span> 를 도입함. <span class="arithmatex">\(\mathcal{O}_t = 1\)</span> 이면 optimal action을 취한 것임. <span class="arithmatex">\(\mathcal{O}_t = 0\)</span> 이면 not optimal action을 취한 것임. 아래와 같은 분포로 정의함.</li>
</ul>
<div class="arithmatex">\[p(\mathcal{O}_t=1 \vert s_t, a_t) = \exp \big( r(s_t, a_t) \big)\]</div>
<ul>
<li>휴리스틱해보이지만 자연스럽게 actions에 대한 posterior distribution 정의 가능(강의의 graph model 확인). 모든 optimal action(<span class="arithmatex">\(\mathcal{O}_t=1\)</span>)에 대해서:</li>
</ul>
<div class="arithmatex">\[\begin{aligned} p(\tau \vert \mathbf{o}_{1:T}) \propto p(\tau, \mathbf{o}_{1:T}) 
&amp;= p(s_1) \prod_{t=1}^T p(\mathcal{O}_t=1 \vert s_t, a_t) p(s_{t+1} \vert s_t, a_t) \\
&amp;= p(s_1) \prod_{t=1}^T \exp \big( r(s_t, a_t) \big) p(s_{t+1} \vert s_t, a_t)\\
&amp;= \bigg\lbrack p(s_1) \prod_{t=1}^T p(s_{t+1} \vert s_t, a_t) \bigg\rbrack \exp \big( \sum_{t=1}^T r(s_t, a_t) \big) 
\end{aligned}\]</div>
<ul>
<li>앞부분은 dynamics의 가능성, 뒷부분은 exponential 보상총합으로 나타낼 수 있음. deterministic dynamics에서(첫 항을 알고 있다고 할 때), 높은 리워드를 가지는 trajectory는 높은 확률(<span class="arithmatex">\(p(\tau \vert \mathbf{o}_{1:T})\)</span>)을 가지고, 낮은 리워드를 가지는 trajectory는 exponential 하게 낮은 확률을 가짐.</li>
<li>이 PGM에서 optimal policy는 <span class="arithmatex">\(p(a_t \vert s_t, \mathcal{O}_{1:T})\)</span>로 씀. <span class="arithmatex">\(\mathcal{O}_{1:T}=1\)</span> 에서 1 생략. 이는 <span class="arithmatex">\(p(a_t \vert s_t, \theta)\)</span>와 굉장히 유사하지만 두가지 다른 점이 있음.<ol>
<li>파라미터 <span class="arithmatex">\(\theta\)</span>와 상관이 없음</li>
<li>omptimization objective가 다름 (뒤에서 이야기함)</li>
</ol>
</li>
<li>sum-product inference inference algorithm으로 optimal policy를 계산할 수 있음. 이는 backward message passing과 forward message passing으로 구성됨.</li>
<li>
<p>Beta: 상태 <span class="arithmatex">\(s_t\)</span> 혹은 상태 <span class="arithmatex">\(s_t\)</span>와 행동 <span class="arithmatex">\(a_t\)</span> 쌍이 주어졌을 때 앞으로 <span class="arithmatex">\(t\)</span>부터 <span class="arithmatex">\(T\)</span>까지 trajactory가 optimal일 확률</p>
<ul>
<li><span class="arithmatex">\(\beta(s_t, a_t) = p(\mathcal{O}_{t:T} \vert s_t, a_t) = \int_{\mathcal{B}} \beta_{t+1}(s_{t+1}) p(s_{t+1} \vert s_t, a_t) p(\mathcal{O}_t \vert s_t, a_t) ds_{t+1}\)</span></li>
<li><span class="arithmatex">\(\beta(s_t) = p(\mathcal{O}_{t:T} \vert s_t) = \int_{\mathcal{A}} p(\mathcal{O}_{1:T} \vert s_t, a_t) p(a_t \vert s_t) da_t = \int_{\mathcal{A}} \beta(s_t, a_t) p(a_t \vert s_t) da_t\)</span></li>
<li><span class="arithmatex">\(p(a_t \vert s_t) = \frac{1}{\vert \mathcal{A} \vert}\)</span> 는 action prior이며, 간단하게 하기위해 uniform distribution으로 가정함. 꼭, uniform은 아니라도 상관없다. 차후에 <span class="arithmatex">\(p(\mathcal{O}_{t:T} \vert s_t, a_t)\)</span> 에 포함된다.</li>
<li>따라서 optimal action distribution은 <span class="arithmatex">\(p(a_t \vert s_t, \mathcal{O}_{1:T}) \propto \frac{\beta(s_t, a_t)}{\beta(s_t)}\)</span> 로 표현할 수 있음. <span class="arithmatex">\(p(a_t \vert s_t)\)</span> 은 uniform이라 가정하여 사라짐.</li>
<li>이것의 intuition은 log를 취함으로사 나타남.</li>
<li><span class="arithmatex">\(Q(s_t, a_t) = \log \beta(s_t, a_t)\)</span>, <span class="arithmatex">\(V(s_t) = \log \beta(s_t)\)</span> 로 정의하면 <span class="arithmatex">\(V(s_t) = \log \int_{\mathcal{A}} \exp \big( Q(s_t, a_t) \big) da_t\)</span> 로 쓸 수 있음. 이는 soft value function이라고 함. </li>
<li><span class="arithmatex">\(Q(s_t, a_t)\)</span>가 큰 경우, 수식에 따라 <span class="arithmatex">\(\mathcal{A}\)</span>로 <span class="arithmatex">\(Q\)</span>를 marginalize하면 <span class="arithmatex">\(\underset{a_t}{\max}\ Q(s_t, a_t)\)</span>로 근사됨. 즉, 최대 Q-value가 결국에 제일 좋은 상태의 value가된다는 뜻임.</li>
<li><span class="arithmatex">\(Q(s_t, a_t)\)</span>가 작은 경우 maximum은 soft한 특징을 가짐. 그래서 <span class="arithmatex">\(V\)</span> 함수를 soft value function이라고 함.</li>
<li>
<p>Bellman Backup: </p>
<div class="arithmatex">\[Q(s_t, a_t) = \begin{cases}(s_t, a_t) + V(s_{t+1}) &amp; \text{deterministic dynamics}\\ r(s_t, a_t) + \log \Bbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t, a_t)} \big\lbrack \exp \big( V(s_{t+1}) \big) \big\rbrack &amp; \text{stochastic dynamics}\end{cases}\]</div>
</li>
<li>
<p>근데 stochastic dynamics 상태의 식을 보면 약간 이상하다. 다음 상태의 value 기댓값이 아니라 softmax를 취한 기대 value값이다. 만약에 가능한 출력값 중에 하나가 엄청 높은 값이 생성되면 다른 모든 값을 압도하게 된다. 다른 가치는 낮지만 가능성이 높은 상태가 있더라도 말이다. 이는 risk seeking behavior를 생성하며 대부분의 경우에 원하지 않는 케이스다.</p>
</li>
</ul>
</li>
<li>
<p>어떤 objective를 policy가 optimize 하는가?</p>
<ul>
<li>
<p><span class="arithmatex">\((1)\)</span>번식을 inference framework 하에 다시 쓰면 아래와 같이 된다.</p>
<div class="arithmatex">\[p(\tau) = \bigg\lbrack p(s_1) \prod_{t=1}^T p(s_{t+1} \vert s_t, a_t) \bigg\rbrack \exp \big( \sum_{t=1}^T r(s_t, a_t) \big)\]</div>
</li>
<li>
<p>만약 deterministic 한 세팅이라면 다음과 같다.</p>
<div class="arithmatex">\[p(\tau) \propto \Bbb{1}[p(\tau \neq 0)] \exp \big( \sum_{t=1}^T r(s_t, a_t) \big)\]</div>
</li>
<li>
<p>다시 상기해볼 것은 policy <span class="arithmatex">\(p(a_t \vert s_t, \mathcal{O}_{1:T}) \propto \frac{\beta(s_t, a_t)}{\beta(s_t)}\)</span> 인데, <span class="arithmatex">\(\beta(s_t, a_t) = p(\mathcal{O_{t:T}} \vert s_t, a_t)\)</span> 정의를 생각하면 policy와 연관 지을 수 있다.</p>
</li>
<li>
<p>이전에 말한 optimal policy <span class="arithmatex">\(p(a_t \vert s_t, \mathcal{O}_{1:T})\)</span> 를 optimization-based approach를 사용해 <span class="arithmatex">\(\pi(a_t \vert s_t)\)</span> 로 근사 해서 같은지 확인</p>
<div class="arithmatex">\[\hat{p}(\tau) \propto \Bbb{1}[p(\tau \neq 0)] \prod_{t=1}^T \pi(a_t \vert s_t)\]</div>
</li>
<li>
<p>그러면 이제 KLD로 두 분포를 최대한 같게 만들어 본다. 그러면 추정하는 분포의 보상 기댓값과 conditional entropy를 최대화해야하는 식이 도출된다. 이는 보상만 최대화하는 <span class="arithmatex">\((1)\)</span> 식과 대조된다. 그래서 이러한 control objective를 maximum entropy RL 혹은 maximum entropy control이라고 한다.</p>
<div class="arithmatex">\[\begin{aligned} -D_{KL} (\hat{p}(\tau) \Vert p(\tau)) &amp;= \Bbb{E}_{\tau \sim \hat{p}(\tau)} [\log p(\tau) - \log \hat{p}(\tau)] \\ &amp;= \sum_{t=1}^T \Bbb{E}_{(s_t, a_t) \sim \hat{p}(s_t, a_t)} \lbrack r(s_t, a_t) \rbrack + \Bbb{E}_{s_t \sim \hat{p}(s_t)} \lbrack \mathcal{H} \big( \pi(s_t \vert a_t) \big) \rbrack
\end{aligned}\]</div>
</li>
<li>
<p>stochastic 세팅에서는 약간 복잡하다. 왜냐면 초기 상태의 확률과 dynamics가 <span class="arithmatex">\(\mathcal{O}_{1:T}\)</span>에 conditional 하기 때문이다. 이는 <span class="arithmatex">\(p(s_{t+1} \vert s_t, a_t, \mathcal{O}_{1:T})\)</span> 가 true dynamics <span class="arithmatex">\(p(s_{t+1} \vert s_t, a_t)\)</span> 와 같이 않을 수도 있다는 뜻이다. 그래서 agent는 제한적으로 dynamics를 영향줄 수 있다고 생각한다.</p>
<div class="arithmatex">\[\hat{p}(\tau) = p(s_1 \vert \mathcal{O}_{1:T}) \prod_{t=1}^T p(s_{t+1} \vert s_t, a_t, \mathcal{O}_{1:T})p(a_t \vert s_t, \mathcal{O}_{1:T})\]</div>
</li>
<li>
<p>그래서 KL-Divergence Term도 다음과 같이 바뀌며 <span class="arithmatex">\(\log p(s_{t+1} \vert s_t, a_t)\)</span> 항 때문에 model-free setting에서 optimize 하기 어렵다.</p>
<div class="arithmatex">\[\begin{aligned} -D_{KL} (\hat{p}(\tau) \Vert p(\tau)) &amp;= \Bbb{E}_{\tau \sim \hat{p}(\tau)} \bigg\lbrack 
\log p(s_1) + \sum_{t=1}^T r(s_t, a_t) + \log p(s_{t+1} \vert s_t, a_t) \bigg\rbrack \\
&amp; \quad + \mathcal{H} \big( \hat{p}(\tau) \big)
\end{aligned}\]</div>
</li>
<li>
<p>그래서 vairational inference 과정을 통해 objective function의 불편함과 이전에 이야기한 risk-seeking behavior 를 줄이고자 한다.</p>
</li>
</ul>
</li>
<li>
<p>Maximum Entropy RL with Fixed Dynamics</p>
<ul>
<li>
<p>Dynamics가 <span class="arithmatex">\(\mathcal{O}_{1:T}\)</span>에 conditional 한 문제를 해결하기 위해서 posterior dynamics와 초기상태의 분포를 <span class="arithmatex">\(p(s_{t+1} \vert s_t, a_t)\)</span> 와 <span class="arithmatex">\(p(s_1)\)</span>으로 고정시킨다.</p>
<div class="arithmatex">\[\hat{p}(\tau) = p(s_1) \prod_{t=1}^T p(s_{t+1} \vert s_t, a_t)\pi(a_t \vert s_t)\]</div>
</li>
<li>
<p>그러면 KL-Divergence Term은 다음과 같이 바뀐다. </p>
<div class="arithmatex">\[\begin{aligned} -D_{KL} (\hat{p}(\tau) \Vert p(\tau)) &amp;= \sum_{t=1}^T \Bbb{E}_{(s_t, a_t) \sim \hat{p}(s_t, a_t)} \lbrack r(s_t, a_t) + \mathcal{H} \big( \pi(s_t \vert a_t) \big) \rbrack
\end{aligned}\]</div>
</li>
<li>
<p>여전히 보상과 entropy를 최대화 하는 형태지만 stochastic transition dynamics 하에서 진행된다. 이러한 형태의 objective면 DP를 이용해 backward message를 계산할 수 있다. 우선 <span class="arithmatex">\(\pi(a_T \vert s_T)\)</span>를 최적화 하면, 즉 아래의 식을 최대화 하는 것이다.</p>
<div class="arithmatex">\[\begin{aligned} \Bbb{E}_{(s_T, a_T) \sim \hat{p}(s_T, a_T)} \lbrack r(s_T, a_T) - \log \pi(a_T \vert s_T) \rbrack \\ = \Bbb{E}_{(s_T) \sim \hat{p}(s_T)} \bigg\lbrack -D_{KL} \big( \pi(a_T \vert s_T) \Vert \dfrac{\exp( r(s_T, a_T))}{exp(V(s_T))} \big) + V(S_T) \bigg\rbrack
\end{aligned}\]</div>
</li>
<li>
<p>잘보면 <span class="arithmatex">\(D_{KL}\)</span> 항에 우측은 soft maximization 형태를 취하고 있다. <span class="arithmatex">\(V(s_T) = \log \int_\mathcal{A} \exp \big( r(s_T, a_T) \big)\)</span>. 그래서 최대화 한다는 것은 <span class="arithmatex">\(-D_{KL}\)</span>항이 최소화 되기 때문에 optimal policy는 <span class="arithmatex">\(\pi(a_T \vert s_T) = \exp \big( r(s_T, a_T) - V(s_T) \big)\)</span> 로 표현할 수 있다.</p>
</li>
<li>
<p>나머지 <span class="arithmatex">\(t\)</span>에서 <span class="arithmatex">\(\pi(a_t \vert s_t)\)</span>는 항상 두 파트를 최대화 해야한다. 앞에 항은 원래의 objective function이고, 뒤에 항은 <span class="arithmatex">\(\pi(a_t \vert s_t)\)</span> 가 기여하는 부분</p>
<div class="arithmatex">\[\begin{aligned} \Bbb{E}_{(s_t, a_t) \sim \hat{p}(s_t, a_t)} \lbrack r(s_t, a_t) - \log \pi(a_t \vert s_t) \rbrack 
+ \Bbb{E}_{(s_t, a_t) \sim \hat{p}(s_t, a_t)} \big\lbrack \Bbb{E}_{s_{t+1} \sim \hat{p}(s_{t+1} \vert s_t, a_t)} \lbrack V(s_{t+1}) \rbrack \big\rbrack
\\ = \Bbb{E}_{(s_t) \sim \hat{p}(s_t)} \bigg\lbrack -D_{KL} \big( \pi(a_t \vert s_t) \Vert \dfrac{\exp( r(s_t, a_t))}{exp(V(s_t))} \big) + V(S_t) \bigg\rbrack
\end{aligned}\]</div>
</li>
<li>
<p>그래서 <span class="arithmatex">\(Q\)</span>와 <span class="arithmatex">\(V\)</span>를 그리고 <span class="arithmatex">\(\pi(s_t \vert a_t)\)</span> 재정의함</p>
<div class="arithmatex">\[\begin{aligned} Q(s_t, a_t) &amp;= r(s_t, a_t) + \Bbb{E}_{s_{t+1} \sim p(s_{t+1} \vert s_t, a_t)} \lbrack V(s_{t+1})\rbrack \\ V(s_t) &amp;= \log \int_\mathcal{A} \exp \big( Q(s_t, a_t) \big) da_t \\
\pi(a_t \vert s_t) &amp;= \exp \big( Q(s_t, a_t) - V(s_t) \big)
\end{aligned}\]</div>
</li>
</ul>
</li>
</ul>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p><a href="https://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-19.pdf">CS 285 L19 - LectureNote</a> <a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">↩</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p><a href="https://arxiv.org/abs/1805.00909">Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review</a> <a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
</ol>
</div>
<!-- Giscus -->
<h2 id="__comments">Comments</h2>
<script async="" crossorigin="anonymous" data-category="General" data-category-id="DIC_kwDOHRhxjc4CQSuI" data-emit-metadata="0" data-input-position="top" data-lang="ko" data-mapping="pathname" data-reactions-enabled="1" data-repo="simonjisu/comments_bot" data-repo-id="R_kgDOHRhxjQ" data-theme="light" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    /* Set palette on initial load */
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate" ? "dark" : "light"
      giscus.setAttribute("data-theme", theme) 
    }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate" ? "dark" : "light"

          /* Instruct Giscus to change theme */
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<a class="md-top md-icon" data-md-component="top" hidden="" href="#">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Back to top
          </a>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs Insiders
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/simonjisu" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.top", "toc.follow", "navigation.prune", "navigation.path", "content.tooltips", "content.code.annotate"], "search": "../../../../assets/javascripts/workers/search.6c7302c4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../../../../assets/javascripts/bundle.fbce390c.min.js"></script>
<script src="../../../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": false, "draggable": false, "openEffect": "none", "closeEffect": "none", "slideEffect": "slide"});})</script></body>
</html>